@relation _Users_mani_Courses_CS6604DigitalLibraries_Project_codeBase_mani_files

@attribute text string
@attribute @@class@@ {'Access control',Authorization,'Block and stream ciphers','Browser security','Cryptanalysis and other attacks','Data anonymization and sanitization','Database activity monitoring','Denial-of-service attacks','Digital rights management','Digital signatures','Distributed systems security','Domain-specific security and privacy architectures','Economics of security and privacy','Embedded systems security','File system security','Formal security models','Hash functions and message authentication codes','Information accountability and usage control','Information flow control','Key management','Logic and verification','Management and querying of encrypted data','Mathematical foundations of cryptography','Mobile and wireless security','Penetration testing','Privacy protections','Privacy-preserving protocols','Pseudonymity, anonymity and untraceability','Public key encryption','Security protocols','Security requirements','Side-channel analysis and countermeasures','Social aspects of security and privacy','Social network security and privacy','Software reverse engineering','Software security engineering','Spoofing attacks','Tamper-proof and tamper-resistant designs','Trust frameworks','Trusted computing','Usability in security and privacy','Virtualization and security','Vulnerability scanners','Web application security','Web protocol security'}

@data
'in this paper, we design, implement and evaluate 802.11ec (encoded control), an 802.11-based protocol without control messages: instead, 802.11ec employs correlatable symbol sequences, which together with the timing the codes are transmitted, encode all control information and change the fundamental design properties of the mac. the use of correlatable symbol sequences provides two key advantages: (i) efficiency, as it permits a near order of magnitude reduction of the control time; (ii) robustness, because codes are short and easily detectable even at low sinr and even while a neighbor is transmitting data. we implement 802.11ec on an fpga-based software defined radio. we perform a large number of experiments and show that, compared to 802.11 (with and without rts/cts), 802.11ec achieves a vast efficiency gain in conveying control information and resolves key throughput and fairness problems in the presence of hidden terminals, asymmetric topologies, and general multi-hop topologies.','Access control'
'we address the problem of defining access control policies that may be used in the evaluation of requests made by client actors, in the course of e-trading, to perform actions on the resources maintained by an e-collective. an e-collective is a group of agents that may act individually or in conjunction with other agents to satisfy a client\'s request to act. our principal contribution to this key problem is to define formally an access control model in terms of which policies may be specified for helping to ensure that only legitimate forms of client actions are performed in the course of engaging in e-trading. we call this model the action control model. in action control, the notion of intentional, empowered, authorized actions, that may be performed individualistically or jointly with other agents, and in a manner that is consistent with a group ethos, is the basis for specifying a set of permissives. a permissive is a generalization of the notion of a permission (as the latter term is usually interpreted in access control). in addition to the formal definition of the action control model, we give examples of action control policy specifications and we describe a candidate implementation and performance measures.','Access control'
'','Access control'
'we introduce a generalization of role-based access control that we call the action-status access control (asac) model. the asac model addresses certain shortcomings with rbac models when applied in distributed computing contexts. the asac model is based on the notion of status, and a nonmonotonic theory of access control that is founded upon the notions of events, actions and times. the approach allows automatic changes to be made to policy requirements and agent authorizations that may be based, in part, on an agent\'s intentional behaviors.','Access control'
'we present blockrate, a wireless bitrate adaptation algorithm designed for blocks, or large contiguous units of transmitted data, as opposed to small packets. in contrast to state-of-the-art algorithms that can either have the amortization benefits of blocks or high responsiveness to underlying channel conditions of packets, blockrate has both. our evaluation shows that blockrate achieves up to 2.8&#215; goodput improvement in a variety of mobility scenarios.','Access control'
'content-dependent access control, where the access decisions depend upon the value of an attribute of the object itself, is required in many applications. however problems arise in an object-based environment, because obtaining the value of an object\'s attribute requires an operation upon the object. we discuss the conceptual and performance implications of introducing content-dependent access control, and suggest how the problems can be avoided in some cases by using a domain-based approach to access control.','Access control'
'as users store and share more digital content at home, effective access control becomes increasingly important. one promising mechanism for helping non-expert users create accurate access policies is reactive policy creation, in which users can update their policy dynamically in response to access requests that cannot otherwise succeed. an earlier study suggested that reactive policy creation may be a good fit for file access control at home. to test this theory, we designed and piloted an experience sampling study in which participants used a simulated reactive access control system for a week. preliminary results suggest a neutral to positive response to using this kind of system and indicate that reactive policy creation may help meet users\' need for dynamic, contextual policy decisions.','Access control'
'much previous work has examined the wireless power control problem using tools from game theory, an economic concept which describes the behavior of interdependent but non-cooperative users. in this paper, we expand these ideas to the antecedent process of deciding which users may participate in the network, i.e. the admission control problem. in particular, we propose three distinct pricing schemes for influencing users as they make their participation decisions. we fully characterize the equilibria induced by each and then test their performance in a simulated, wireless environment. our preliminary results show that these schemes have the potential to produce high quality outcomes in an incentive-compatible way.','Access control'
'in this paper, we propose a subscription-based policy control framework that implements a subscription-centered approach for policy control for ip multimedia subsystem (ims), defined by 3rd generation partnership project (3gpp), release 7. the framework also enables flexible policy definitions based on the subscriber\'s profile at the application level. in addition, the framework provides functionalities of organizing the subscription data, identifying the policy, stipulating the policy control process, interpreting, managing and enforcing the corresponding policies. the main objective of this framework is to qualify the subscribers and thus, enhance the network customization reflected by the capabilities of defining various flexible policies based on different subscriber policy control requirements.','Access control'
'the access matrix is a useful model for understanding the behaviour and properties of access control systems. while the matrix is rarely implemented, access control in real systems is usually based on access control mechanisms, such as access control lists or capabilities, that have clear relationships with the matrix model. in recent times a great deal of interest has been shown in role based access control (rbac) models. however, the relationship between rbac models and the access matrix is not clear. in this paper we present a model of rbac based on the access matrix which makes the relationships between the two explicit. in the process of constructing this model, some fundamental similarities between certain capability models and rbac are revealed.','Access control'
'conducting enterprise-wide vulnerability assessment (va) on a regular basis plays an important role in assessing an enterprise\'s information system security status. however, an enterprise network is usually very complex, divided into different types of zones, and consisting of hundreds of hosts in the networks. the complexity of it systems makes va an extremely time-consuming task for security professionals. they are seeking for an automated tool that helps monitor and manage the overall vulnerability of an enterprise. this paper presents a novel methodology that provides a dashboard solution for managing enterprise level vulnerability. in our methodology, we develop a multi-layer tree based model to describe enterprise vulnerability topology. then we apply a client/server structure to gather vulnerability information from enterprise resources automatically. finally a set of well-defined metric formulas is applied to produce a normalized vulnerability score for the whole enterprise. as a prototype, we developed the implementation of our methodology, evmat, an enterprise vulnerability management and assessment tool, to test our method. experiments on a small e-commerce company and a small it company demonstrate the great potentials of our tool for enterprise-level security.',Authorization
'this paper presents three encoding strategies based on digital logic for steganography on voice over ip (voip), which aim to enhance the embedding transparency. differing from previous approaches, our strategies reduce the embedding distortion by improving the similarity between the cover and the covert message using digital logical transformations, instead of reducing the amount of the substitution bits. therefore, by contrast, our strategies will improve the embedding transparency without sacrificing the embedding capacity. of these three strategies, the first one adopts logical operations, the second one employs circular shifting operations, and the third one combines the operations of the first two. all of them are evaluated through comparing their prototype implementations with some existing methods in a prototypical covert communication system based on voip (called stegvoip). the experimental results show that the proposed strategies can effectively enhance the embedding transparency while maintaining the maximum embedding capacity.',Authorization
'',Authorization
'information security is very important nowadays. every it system needs protection mechanisms for stability and safety of work. to solve this task, there are proposed a variety of security-providing solutions, but most of them are very expensive and non-systematic. the paper discusses up-to-date techniques implemented for security aims and addresses to the technique of security control based on settings monitoring of variable program components of the trusted information environment. there is proposed a formal basis of security control based on finding the security settings which provide the system with stability and integrity. the specified technique allows proposing a schema of dynamic security and integrity control system which provides an automated process of security assurance and management. these security control technique and system extend security-relevant approaches making security reachable, permanent, and effective.',Authorization
'security certification includes assessing an information system to verify its compliance with diverse, pre-selected security controls. the goal of certification is to identify where controls are implemented correctly and where they are violated, creating potential vulnerability risks. certification complexity is magnified in software composed of systems of systems where there are limited formal methodologies to express management policies, given a set of security control properties, and verify them against the interaction of the participating components and their individual security policy implementations. in this paper, we extend context unity, a formal, distributed, and context aware coordination language to support policy controls. the new language features enforce security controls and provide a means to declare policy specifics in a manner similar to declaring variable types. we use these features in a specification to show how verifying system compliance with selected security controls, such as those found in the nist sp800-53 document, can be accomplished.',Authorization
'this paper presents a unique summer project for a group of undergraduate students and high school computer teachers to gain research experiences in the area of cybersecurity. the students and teachers were selected from the participants in the nsf reu and ret programs at the host institution. through the research on security testing of a real-world online banking system, the students and teachers have not only learned about the cutting-edge security testing techniques, but also made publishable contributions to the research base. the two collaborating graduate assistants served as an immediate role model for the undergraduates and an indirect role model for high school students through the teachers. with the help from the graduate assistants, the students and teachers were able to work effectively toward achieving their research objectives. the internal competition helped the participants get a better sense of achievement and satisfaction. the research experiences also prepared the teachers with the necessary knowledge for introducing cybersecurity topics (e.g., secure programming) into future classroom activity. as such, the project described in this paper provides a model summer program for undergraduate and/or k-12 teachers to gain research experiences.',Authorization
'',Authorization
'in this paper, we discuss the creation of a student blue team to assist campus organizations with security incident response. we also explore approaches for establishing a relationship with university information technology staff, informing blue team members of professional and ethical responsibilities, and aiding system administrators with incident response and system hardening. finally, we discuss the benefits to students taking part in these activities, as well as their contributions to improving an organizations security posture.',Authorization
'',Authorization
'',Authorization
'in stream ciphers, the ratio of performance to the security is the most important issue. however, the s-boxes used in a stream cipher can become a bottleneck of speed due to use of large memory, difficulty in hardware realization and more processing. this paper proposes an s-box construction that is easy to implement both in hardware and software. the proposed s-box is efficient in speed, parameterized and scalable with excellent security properties. it also provides a designer with the flexibility to trade-off among speed, area and the security properties. the security analysis has been performed on the s-box. the security properties are found to be comparable with the existing standards.','Block and stream ciphers'
'py and pypy are efficient array-based stream ciphers designed by biham and seberry. both were submitted to the estream competition. this paper shows that py and pypy are practically insecure. if one key is used with about 216ivs with special differences, with high probability two identical keystreams will appear. this can be exploited in a key recovery attack. for example, for a 16-byte key and a 16-byte iv, 223chosen ivs can reduce the effective key size to 3 bytes. for a 32-byte key and a 32-byte iv, the effective key size is reduced to 3 bytes with 224chosen ivs. py6, a variant of py, is more vulnerable to these attacks.','Block and stream ciphers'
'in this paper, we surveyed hc-128 and hc-256 as methods for protecting the distribution of digital images in an efficient and secure way. we proposed the hongjun cipher (hc) image encryption algorithm based on column-wise raster scanning of the plain image. then, we performed a series of tests and some comparisons to justify the efficiency of surveyed algorithms for image encryption. these tests included key space analysis, visual test and histogram analysis, randomness analysis, information entropy, encryption quality, correlation analysis, differential analysis, sensitivity analysis and performance analysis. based on all analysis and experimental results, it can be concluded that the two variants of hc scheme are efficient, feasible and trustworthy to be adopted for image encryption.','Block and stream ciphers'
'we study the observability of a permutation on a finite set by a complex-valued function. the analysis is done in terms of the spectral theory of the unitary operator on functions defined by the permutation. any function f can be written uniquely as a sum of eigenfunctions of this operator; we call these eigenfunctions the eigencomponents of f. it is shown that a function observes the permutation if and only if its eigencomponents separate points and if and only if the function has no nontrivial symmetry that preserves the dynamics. some more technical conditions are discussed. an application to the security of stream ciphers is discussed.','Block and stream ciphers'
'in this paper, the complexity of applying a guess and determine attack to so-called linear feedback shift register (lfsr)-based stream ciphers is analyzed. this family of stream ciphers uses a single or several lfsr and a filtering function f : gf(2)n &#8594; gf(2)m to generate the blocks of m &#8805; 1 keystream bits at the time. in difference to a classical guess and determine attack, a method based on guessing certain bits in order to determine the remaining secret key/state bits, our approach efficiently takes advantage of the reduced preimage space for relatively large m and at the same time employing the design structure of the cipher. several variations of the algorithm are derived to circumvent the sensitivity of attack to the input data, n, m and the key length. in certain cases, our attack outperforms classical algebraic attacks [10]; these being considered as one of the most efficient cryptanalyst tools for this type of ciphers. a superior performance of our attack over algebraic attacks is demonstrated in case the filtering function belongs to the extended maiorana-mcfarland class.','Block and stream ciphers'
'synchronous stream ciphers are commonly used in applications with high throughput requirements or on hardware devices with restricted resources. well known stream ciphers are a5/1, used in gsm, rc4, used in ssl, or e0 as specified in bluetooth, but also some block cipher modes of operation. a review of the development of stream ciphers is given which starts with classical designs and is directed to modern dedicated stream ciphers as in the european noe estream project. the history of stream ciphers is rich in new proposals followed by devastating breaks, e.g., by statistical or algebraic attacks. differential cryptanalysis is probably the most popular tool for chosen plaintext attacks on block ciphers. it also applies to the initialization step in stream ciphers, but here, high order differential attacks are shown to be surprisingly successful, namely on constructions based on linear and nonlinear feedback shift registers. the process of designing and cryptanalyzing stream ciphers has not only resulted in a number of building blocks for stream ciphers: similar components turn out to be useful as well in the design of lightweight block ciphers, hash functions and in algorithms for authenticated encryption.','Block and stream ciphers'
'','Block and stream ciphers'
'for security applications in wireless sensor networks (wsns), choosing best algorithms in terms of energy-efficiency and of small-storage requirements is a real challenge because the sensor networks must be autonomous. in [22], the authors have benchmarked on a dedicated platform some block-ciphers using severalmodes of operations and have deduced the best block cipher to use in the context of wsns. this article proposes to study on a dedicated platform of sensors some stream ciphers. first, we sum-up the security provided by the chosen stream ciphers (especially the ones dedicated to software uses recently proposed in the european project ecrypt, workpackage estream [27]) and presents some implementation tests performed on the platform [16].','Block and stream ciphers'
'','Block and stream ciphers'
'','Block and stream ciphers'
'','Browser security'
'','Browser security'
'','Browser security'
'','Browser security'
'this paper gives a security and specification-oriented semantics for systems. the semantic model is derived from that for the trace model of hoare\'s communicating sequential processes[ho85] and is used to define various security concepts, such as multi-level secure system, trusted users and integrity. we indicate how implementations of secure systems may be derived from their specifications.','Browser security'
'','Browser security'
'interest in the security of information systems has increased partly because of evolving systems maturity, and partly in response to dramatic intrusions into major systems. these have included intrusions by amateur \'hackers\' which, although embarrassing have caused no substantial damage. intrusions from employees are far more damaging but have not been widely publicized. the paper describes the us government\'s security policy and its implications for private organizations. a security policy is basic to the concept of security and defines the manner in which an information system can access and manipulate data. protection mechanisms which enforce security policies are discussed. mandatory and discretionary policies which form a particular security policy are outlined. the characteristics of a formal security model are also defined, and the design of a secure operating system is discussed. the present status of information systems security is outlined.','Browser security'
'like much legislation, the computer misuse act 1990 was implemented to plug a hole in the law. it may have made it easier for prosecutors to punish those abusing computer systems nearly 20 years ago, but conditions have changed dramatically since then. the police and justice act 2006 was passed at the end of last year, and nestled among its non-computer related measures is a raft of new rules designed to bring the terms and conditions relating to computer crime into the 21^s^t century. it criminology expert stefan fafinski examines the ramifications of the law, and wonders whether its scope might cause collateral damage for legitimate security practitioners. society is becoming increasingly reliant on electronic communications and information storage. e-commerce has really taken off and more individuals communicate, shop, bank, and search for information electronically. governments and other institutions gather, store and communicate vast quantities of information, much of it sensitive or private, electronically. the internet enables all of us to do so much more, so much more efficiently, than we could before, but a great deal of this e-freedom relies on secure communications. this in turn requires cryptography.','Browser security'
'from the early days of mobile phone communications, modern mobile networks offer impressive features and capabilities that were only dreamt about 15 years ago. mobile phones, once considered a luxury, are now owned by millions worldwide. through technology improvements and new, advanced protocols, modern cell phones offer international roaming, data connectivity, instant messaging, and emergency services.','Browser security'
'side channels attacks (sca) are very effective and low cost methods to extract secret information from supposedly secure cryptosystems. differential power analysis (dpa) and differential electromagnetic analysis (dema) are among the most cited attack types. the traditional synchronous design flow used to create such systems favors the leakage of information that enables attackers to draw correlations between data processes and circuit power consumption or electromagnetic radiations. by using well known analysis techniques these correlations may allow that an attacker retrieve secret cryptographic keys. in recent years, several countermeasures against sca have been proposed. globally asynchronous locally synchronous (gals) and fully asynchronous design methods appear as alternatives to design tamper resistant cryptosystems. however, according to previous works they use to achieve this with significant area, throughput, latency and power penalties. this paper proposes a new gals pipeline architecture for the data encryption standard (des) that explores the trade-off between circuit area and robustness. robustness is enhanced by replicating the des hardware structure in asynchronously communicating module instances, coupled with self-varying operating frequencies. designs prototyped on fpgas using the proposed technique and submitted to dema attacks presented promising robustness against attacks and throughput superior to previously reported results.','Cryptanalysis and other attacks'
'nvd and exploit-db are the de facto standard databases used for research on vulnerabilities, and the cvss score is the standard measure for risk. on open question is whether such databases and scores are actually representative of attacks found in the wild. to address this question we have constructed a database (ekits) based on the vulnerabilities currently used in exploit kits from the black market and extracted another database of vulnerabilities from symantec\'s threat database (sym). our final conclusion is that the nvd and edb databases are not a reliable source of information for exploits in the wild, even after controlling for the cvss and exploitability subscore. an high or medium cvss score shows only a significant sensitivity (i.e. prediction of attacks in the wild) for vulnerabilities present in exploit kits (ekits) in the black market. all datasets exhibit a low specificity.','Cryptanalysis and other attacks'
'this paper analyzes malicious activity collected from a test-bed, consisting of two target computers dedicated solely to the purpose of being attacked, over a 109 day time period. we separated port scans, icmp scans, and vulnerability scans from the malicious activity. in the remaining attack data, over 78\% (i.e., 3,677 attacks) targeted port 445, which was then statistically analyzed. the goal was to find the characteristics that most efficiently separate the attacks. first, we separated the attacks by analyzing their messages. then we separated the attacks by clustering characteristics using the k-means algorithm. the comparison between the analysis of the messages and the outcome of the k-means algorithm showed that 1) the mean of the distributions of packets, bytes and message lengths over time are poor characteristics to separate attacks and 2) the number of bytes, the mean of the distribution of bytes and message lengths as a function of the number packets are the best characteristics for separating attacks.','Cryptanalysis and other attacks'
'the public key cryptosystem based on rank error correcting codes (the gpt cryptosystem) was proposed in 1991. several attacks against this system were published, including gibson\'s attacks and recent overbeck\'s attacks. in this paper, we improve the gpt system by more careful choice of parameters to withstand these attacks.','Cryptanalysis and other attacks'
'in this article we present some weaknesses in the rc4 cipher and their cryptographic applications. especially we improve the attack described by fluhrer, mantin, shamir (in: selected areas in cryptography, 2001) in such a way, that it will work, if the weak keys described in that paper are avoided. a further attack will work even if the first 256 byte of the output remain unused. finally we show that variants of the rc4 algorithm like ngg and rc4a are also vulnerable by these techniques.','Cryptanalysis and other attacks'
'the galois/counter mode (gcm) of operation has been standardized by nist to provide single-pass authenticated encryption. the ghash authentication component of gcm belongs to a class of wegman-carter polynomial hashes that operate in the field gf(2128). we present message forgery attacks that are made possible by its extremely smooth-order multiplicative group which splits into 512 subgroups. gcm uses the same block cipher key k to both encrypt data and to derive the generator h of the authentication polynomial for ghash. in present literature, only the trivial weak key h=0 has been considered. we show that ghash has much wider classes of weak keys in its 512 multiplicative subgroups, analyze some of their properties, and give experimental results on aes-gcm weak key search. our attacks can be used not only to bypass message authentication with garbage but also to target specific plaintext bits if a polynomial mac is used in conjunction with a stream cipher. these attacks can also be applied with varying efficiency to other polynomial hashes and macs, depending on their field properties. our findings show that especially the use of short polynomial-evaluation macs should be avoided if the underlying field has a smooth multiplicative order.','Cryptanalysis and other attacks'
'new forms of internet attacks, such as sql slammer,have become increasingly sophisticated. although codedin a simple way, the sql slammer worm propagated allover the world at an extremely high speed in a short periodof time, rendering it impossible for humans to counterit using manual intervention. in this paper, we proposea security framework called japonica to detect and respondto unknown attacks at the early stage through the dynamicorchestration of prevention, detection, and responsemechanisms. we identify important requirements to supportthe proposed framework and corresponding system entities.also, we describe our model using colored petri netsto discover a uniform message exchange format among theentities. one unique characteristic of japonica is an activeresponse coordinator and we demonstrate its feasibilityin a proof-of-concept prototype, utilizing a honeypot as anactive entity. our results indicate that japonica can successfullyprevent the spread of sql slammer without humanintervention. we are currently extending the framework tocounter other forms of sophisticated internet attacks.','Cryptanalysis and other attacks'
'in this paper, we describe two attacks on ieee 802.11 based wireless lans. the first attack is an improved key recovery attack on wep, which reduces the average number of packets an attacker has to intercept to recover the secret key. the second attack is (according to our knowledge) the first practical attack on wpa secured wireless networks, besides launching a dictionary attack when a weak pre-shared key (psk) is used. the attack works if the network is using tkip to encrypt the traffic. an attacker, who has about 12-15 minutes access to the network is then able to decrypt an arp request or response and send 7 packets with custom content to network.','Cryptanalysis and other attacks'
'this paper considers and validates the applicability of leveraging pervasively-available performance counters for detecting and reasoning about security breaches. our key observation is that many security breaches, which typically cause abnormal control flow, usually incur precisely identifiable deviation in performance samples captured by processors. based on this observation, we implement a prototype system called eunomia, which is the first non-intrusive system that can detect emerging attacks based on return-oriented programming without any changes to applications (either source or binary code) or special-purpose hardware. our security evaluation shows that eunomia can detect some realistic attacks including code-injection attacks, return-to-libc attacks and return-oriented programming attacks on unmodified binaries with relatively low overhead.','Cryptanalysis and other attacks'
'by carefully measuring the amount of time required tm perform private key operalions, attackers may be able to find fixed diffie-hellman exponents, factor rsa keys, and break other cryptosystems. against, a valnerable system, the attack is computationally inexpensive and often requires only known ciphertext. actual systems are potentially at risk, including cryptographic tokens, network-based cryptosystems, and other applications where attackers can make reasonably accurate timing measurements. techniques for preventing the attack for rsa and diffie-hellman are presented. some cryptosystems will need to be revised to protect against the attack, and new protocols and algorithms may need to incorporate measures to prevenl timing attacks.','Cryptanalysis and other attacks'
'   Bud1           \n                                                           w a r e   r                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           H a r d w a r e   r e v e r s e   e n g i n e e r i n gIlocblob   ??????????????     ! H a r d w a r e - b a s e d   s e c u r i t y   p r o t o c o l sIlocblob   ??????????????       I n f o r m a t i o n - t h e o r e t i c   t e c h n i q u e sIlocblob   ??????????????      I n t r u s i o n   d e t e c t i o n   s y s t e m sIlocblob   ??????????????     \u001E M a l i c i o u s   d e s i g n   m o d i f i c a t i o n sIlocblob   ??????????????      M a l w a r e   a n d   i t s   m i t i g a t i o nIlocblob   ??????????????      M o b i l e   p l a t f o r m   s e c u r i t yIlocblob   ??????????????      M u l t i - f a c t o r   a u t h e n t i c a t i o nIlocblob   ??????????????                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              @      ?                                        @      ?                                          @      ?                                          @                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   E  \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       DSDB                                 `          ?                                         @      ?                                          @      ?                                          @                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ','Data anonymization and sanitization'
'recent research studied the problem of publishing microdata without revealing sensitive information, leading to the privacy-preserving paradigms of k-anonymity and l-diversity. k-anonymity protects against the identification of an individual\'s record. l-diversity, in addition, safeguards against the association of an individual with specific sensitive information. however, existing approaches suffer from at least one of the following drawbacks: (i) l-diversification is solved by techniques developed for the simpler k-anonymization problem, causing unnecessary information loss. (ii) the anonymization process is inefficient in terms of computational and i/o cost. (iii) previous research focused exclusively on the privacy-constrained problem and ignored the equally important accuracy-constrained (or dual) anonymization problem. in this article, we propose a framework for efficient anonymization of microdata that addresses these deficiencies. first, we focus on one-dimensional (i.e., single-attribute) quasi-identifiers, and study the properties of optimal solutions under the k-anonymity and l-diversity models for the privacy-constrained (i.e., direct) and the accuracy-constrained (i.e., dual) anonymization problems. guided by these properties, we develop efficient heuristics to solve the one-dimensional problems in linear time. finally, we generalize our solutions to multidimensional quasi-identifiers using space-mapping techniques. extensive experimental evaluation shows that our techniques clearly outperform the existing approaches in terms of execution time and information loss.','Data anonymization and sanitization'
'published data is prone to privacy attacks. sanitization methods aim to prevent these attacks while maintaining usefulness of the data for legitimate users. quantifying the trade-off between usefulness and privacy of published data has been the subject of much research in recent years. we propose a pragmatic framework for evaluating sanitization systems in real-life and use data mining utility as a universal measure of usefulness and privacy. we propose a definition for data mining utility that can be tuned to capture the needs of data users and the adversaries\' intentions in a setting that is specified by a database, a candidate sanitization method, and privacy and utility concerns of data owner. we use this framework to evaluate and compare privacy and utility offered by two well-known sanitization methods, namely k-anonymity and &#949;-differential privacy, when uci\'s \"adult\" dataset and the weka data mining package is used, and utility and privacy measures are defined for users and adversaries. in the case of k-anonymity, we compare our results with the recent work of brickell and shmatikov (kdd 2008), and show that using data mining algorithms increases their proposed adversarial gains.','Data anonymization and sanitization'
'user data is often unprotected on disk and tape drives or not erased when no longer needed, creating data security vulnerabilities that many computer users are unaware of. federal and state laws require data sanitization, which comprises a variety of data eradication methods. secure sanitization refers to methods meeting those federal and state laws. companies that fail to meet these laws can be subject to fines of $5 million, and individuals can be imprisoned for up to 10 years. physical destruction of storage devices offers the highest security. but executing the disk drive internal secure-erase command also offers a higher security level than external-block-overwrite software, according to federal guideline nist 800-88. recent disk drives with internal full disk encryption now implement an enhanced secure-erase command that takes only milliseconds to complete.','Data anonymization and sanitization'
'in this paper we observe that k-anonymizing a data set is strikingly similar to building a spatial index over the data set, so similar in fact that classical spatial indexing techniques can be used to anonymize data sets. we use this observation to leverage over 20 years of work on database indexing to provide efficient and dynamic anonymization techniques. experiments with our implementation show that the r-tree index-based approach yields a batch anonymization algorithm that is orders of magnitude more efficient than previously proposed algorithms and has the advantage of supporting incremental updates. finally, we show that the anonymizations generated by the r-tree approach do not sacrifice quality in their search for efficiency; in fact, by several previously proposed quality metrics, the compact partitioning properties of r-trees generate anonymizations superior to those generated by previously proposed anonymization algorithms.','Data anonymization and sanitization'
'the technique of k-anonymization allows the releasing of databases that contain personal information while ensuring some degree of individual privacy. anonymization is usually performed by generalizing database entries. we formally study the concept of generalization, and propose three information-theoretic measures for capturing the amount of information that is lost during the anonymization process. the proposed measures are more general and more accurate than those that were proposed by meyerson and williams [23] and aggarwal et al. [1]. we study the problem of achieving k-anonymity with minimal loss of information. we prove that it is np-hard and study polynomial approximations for the optimal solution. our first algorithm gives an approximation guarantee of o(\\ln k) for two of our measures as well as for the previously studied measures. this improves the best-known o(k)-approximation in [1]. while the previous approximation algorithms relied on the graph representation framework, our algorithm relies on a novel hypergraph representation that enables the improvement in the approximation ratio from o(k) to o(\\ln k). as the running time of the algorithm is o(n^{2k}), we also show how to adapt the algorithm in [1] in order to obtain an o(k)-approximation algorithm that is polynomial in both n and k.','Data anonymization and sanitization'
'data mining mechanisms have widely been applied in various businesses and manufacturing companies across many industry sectors. sharing data or sharing mined rules has become a trend among business partnerships, as it is perceived to be a mutually benefit way of increasing productivity for all parties involved. nevertheless, this has also increased the risk of unexpected information leaks when releasing data. to conceal restrictive itemsets (patterns) contained in the source database, a sanitization process transforms the source database into a released database that the counterpart cannot extract sensitive rules from. the transformed result also conceals non-restrictive information as an unwanted event, called a side effect or the \'\'misses cost\'\'. the problem of finding an optimal sanitization method, which conceals all restrictive itemsets but minimizes the misses cost, is np-hard. to address this challenging problem, this study proposes the maximum item conflict first (micf) algorithm. experimental results demonstrate that the proposed method is effective, has a low sanitization rate, and can generally achieve a significantly lower misses cost than those achieved by the minfia, maxfia, iga and algo2b methods in several real and artificial datasets.','Data anonymization and sanitization'
'research in data sanitization (including anonymization) emphasizes ways to prevent an adversary from desanitizing data. most work focuses on using mathematical mappings to sanitize data. a few papers examine incorporation of privacy requirements, either in the guise of templates or prioritization. essentially these approaches reduce the information that can be gleaned from a data set. in contrast, this paper considers both the need to ``desanitize\'\' and the need to support privacy. we consider conflicts between privacy requirements and the needs of analysts examining the redacted data. our goal is to enable an informed decision about the effects of redacting, and failing to redact data. we begin with relationships among the data being examined, including relationships with a known data set and other, additional, external data. by capturing these relationships, desanitization techniques that exploit them can be identified, and the information that must be concealed in order to thwart them can be determined. knowing that, a realistic assessment of whether the information and relationships are already widely known or available will enable the sanitizers to assess whether irreversible sanitization is possible, and if so, what to conceal to prevent desanitization.','Data anonymization and sanitization'
'this work explores issues of computational disclosure control. we examine assumptions in the foundations of traditional problem statements and abstract models. we offer a comprehensive framework, based on the notion of an inference game, that unifies various inference problems by parameterizing their problem spaces. this work raises questions regarding the significance of intractability results. we analyze common structural aspects of inference problems via case studies; these emphasize why explicit policies are needed to specify all social context and ethical values relevant to a problem instance.','Data anonymization and sanitization'
'the line between personal and anonymous information is often unclear. increasingly it falls to lawyers to understand and manage the risks associated with the sharing of \"anonymized\" data sets.','Data anonymization and sanitization'
'privacy becomes a more and more serious concern in applications involving microdata. recently, efficient anonymization has attracted much research work. most of the previous methods use global recoding, which maps the domains of the quasi-identifier attributes to generalized or changed values. however, global recoding may not always achieve effective anonymization in terms of discernability and query answering accuracy using the anonymized data. moreover, anonymized data is often for analysis. as well accepted in many analytical applications, different attributes in a data set may have different utility in the analysis. the utility of attributes has not been considered in the previous methods.in this paper, we study the problem of utility-based anonymization. first, we propose a simple framework to specify utility of attributes. the framework covers both numeric and categorical data. second, we develop two simple yet efficient heuristic local recoding methods for utility-based anonymization. our extensive performance study using both real data sets and synthetic data sets shows that our methods outperform the state-of-the-art multidimensional global recoding methods in both discernability and query answering accuracy. furthermore, our utility-based method can boost the quality of analysis using the anonymized data.','Data anonymization and sanitization'
'in this work, we study how continuous video monitoring and intelligent video processing can be used in eldercare to assist the independent living of elders and to improve the efficiency of eldercare practice. more specifically, we develop an automated activity analysis and summarization for eldercare video monitoring. at the object level, we construct an advanced silhouette extraction, human detection and tracking algorithm for indoor environments. at the feature level, we develop an adaptive learning method to estimate the physical location and moving speed of a person from a single camera view without calibration. at the action level, we explore hierarchical decision tree and dimension reduction methods for human action recognition. we extract important adl (activities of daily living) statistics for automated functional assessment. to test and evaluate the proposed algorithms and methods, we deploy the camera system in a real living environment for about a month and have collected more than 200 hours (in excess of 600 g bytes) of activity monitoring videos. our extensive tests over these massive video datasets demonstrate that the proposed automated activity analysis system is very efficient.','Database activity monitoring'
'','Database activity monitoring'
'multi-agent system (mas) is a system composed of several agents, collectively capable of achieving goals that are difficult to achieve by an individual agent or monolithic system. mas is ideal for a network-like application for its flexibility, distributed nature, and modifiability, without the need for detailed rewriting of the application. in this paper, we have proposed agent based activity monitoring system (abams) for the monitoring of resources over a network, suitable for network of networks; commonly known as can (campus area network). the system is fully autonomous and once initialized with the given rules and domain knowledge abams manages resources on its own with the help of mobile agents.','Database activity monitoring'
'this paper examines an increasingly relevant topic in the multimedia community of wearable devices that record the physical activity of a user throughout a day. while activity and other accelerometry-based data has been shown effective in various multimedia applications -- from context-aware music retrieval to approximating carbon footprint -- the most promising role of these target application for healthcare and personal fitness. recently, several low-cost devices have become available to consumers. in this paper, we perform an evaluation on the most popular devices available on the market (in particular fitbit and nike+) and report our findings in terms of accuracy, type of data provided, available apis, and user experience. this information is useful for researchers considering incorporating these activity-based data streams into their research and for getting a better idea of the reliability and accuracy for use in life-logging and other multimedia applications.','Database activity monitoring'
'appliance load monitoring systems aim to achieve per appliance energy decomposition. such systems however lack automated set-up, which precludes widespread roll-outs. this work illustrates how human supervision can be reduced to a strict minimum; appliance activity states are captured with cheap wireless sensors, which enables accurate automated energy data annotation. this paper demonstrates the accuracy of this technique via an intuitive graphical user interface.','Database activity monitoring'
'in this paper an automated bathroom activity monitoring system based on acoustics is described. the system is designed to recognize and classify major activities occurring within a bathroom based on sound. carefully designed hmm parameters using mfcc features are used for accurate and robust bathroom sound event classification. experiments to validate the utility of the system were performed firstly in a constrained setting as a proof-of-concept and later in an actual trial involving real people using their bathroom in the normal course of their daily lives. preliminary results are encouraging with the accuracy rate for most sound categories being above 84\%. we sincerely believe that the system contributes towards increased understanding of personal hygiene behavioral problems that significantly affect both informal care-giving and clinical care of dementia patients.','Database activity monitoring'
'this paper presents a novel approach for real-time egocentric activity recognition in which component atomic events are characterised in terms of binary relationships between parts of the body and manipulated objects. the key contribution is to summarise, within a histogram, the relationships that hold over a fixed time interval. this histogram is then classified into one of a number of atomic events. the relationships encode both the types of body parts and objects involved (e.g. wrist, hammer) together with a quantised representation of their distance apart and the normalised rate of change in this distance. the quantisation and classifier are both configured in a prior learning phase from training data. an activity is represented by a markov model over atomic events. we show the application of the method in the prediction of the next atomic event within a manual procedure (e.g. assembling a simple device) and the detection of deviations from an expected procedure. this could be used for example in training operators in the use or servicing of a piece of equipment, or the assembly of a device from components. we evaluate our approach (\'bag-of-relations\') on two datasets: \'labelling and packaging bottles\' and \'hammering nails and driving screws\', and show superior performance to existing bag-of-features methods that work with histograms derived from image features [1]. finally, we show that the combination of data from vision and inertial (imu) sensors outperforms either modality alone.','Database activity monitoring'
'','Database activity monitoring'
'','Database activity monitoring'
'the rich sensing ability of smart mobile phones brings an unique opportunity to detect and long-term monitor people\'s physical activities. however, with mobile phone the application has to comply with people\'s usage habit of it and thus capture the right moment to recognize activities, which will potentially cause great in-class variances. as a result, the model potentially becomes complex and costs much computing resources in mobile phone. this paper recognize people\'s physical activities when they place the mobile phone in the pockets near the pelvic region. experiment results show that the accuracy could reach 97.7\%. to reduce the model size, evaluation of each feature attribution contribution for the accuracy is performed. and the result shows that we can cut the feature dimension from 22 to 8 while obtaining the smallest model.','Database activity monitoring'
'cryptographic algorithms, which withstand cryptanalysis after years of rigorous theoretical study and detailed scrutiny have been shown to succumb to attacks that exploit the vulnerabilities in their implementations. therefore, there has been a vast amount of research effort to find potential vulnerabilities in the implementation of cryptographic algorithms, and efficient and effective countermeasures if such vulnerabilities exist. in this paper, we survey side-channel and fault attacks, which are two powerful methods that have been demonstrated to render many implementations effectively broken. while we categorically analyze the attack techniques, possible countermeasures will also be discussed.','Denial-of-service attacks'
'','Denial-of-service attacks'
'this paper addresses the issue of detecting and isolating topology attacks in power networks. a topology attack, unlike a data attack and power injection attack, alters the physical dynamics of the power network by removing bus interconnections. these attacks can manifest as both cyber and physical attacks. a physical topology attack occurs when a bus interconnection is physically broken, while a cyber topology attack occurs when incorrect information about the network topology is transmitted to the system estimator and incorporated as the truth. to detect topology attacks, a stochastic hypothesis testing problem is considered assuming noisy measurements are obtained by periodically sampling a dynamic process described by the networked swing equation dynamics, modified to assume stochastic power injections. a centralized approach to network topology detection and isolation is introduced as a two-part scheme consisting of topology detection followed by topology isolation, assuming a topology attack exists. to address the complexity issues arising with performing centralized detection in large-scale power networks, a decentralized approach is presented that uses only local measurements to detect the presence of a topology attack. simulation results illustrate that both the centralized and decentralized approaches accurately detect and isolate topology attacks.','Denial-of-service attacks'
'hirose and yoshida proposed an authenticated key agreement protocol based on the intractability of the computational diffie-hellman problem. recently, hirose and matsuura pointed out that hirose and yoshida\'s protocol is vulnerable to denial-of-service (dos) attacks. and they proposed two key agreement protocols which are resistant to the dos attacks. their protocols are the first authenticated key agreement protocols resistant to both the storage exhaustion attack and the cpu exhaustion attack. in this paper we show that hirose and matsuura\'s dos-resistant key agreement protocols and hirose and yoshida\'s key agreement protocol are vulnerable to impersonation attacks. we make suggestions for improvements.','Denial-of-service attacks'
'we describe a lattice attack on the digital signature algorithm (dsa) when used to sign many messages, m_i, under the assumption that a proportion of the bits of each of the associated ephemeral keys, y_i, can be recovered by alternative techniques.','Denial-of-service attacks'
'','Denial-of-service attacks'
'in this paper, we propose a formal model of coordinated attacks in which several attackers cooperate towards a common malicious goal. the model investigates both attack planning and vulnerability analysis, thereby providing a uniform approach to system and adversary modelling. in addition, the model is general enough to explain both coordinated and single attacks. in the paper, we define the notion of coordinated-attack graph, propose an algorithm for efficient generation of coordinated-attack graphs, demonstrate how coordinated-attack can be used for vulnerability analysis, and discuss an implementation of a coordinated-attack graph. coordinated-attack graphs can facilitate a wide range of tasks, such as model checking, opponent modelling, intrusion response, sensor configuration, and so forth. in addition, they can be used in robotic warfare, where several intelligent software agents automatically produce and launch coordinated attacks.','Denial-of-service attacks'
'the automobile industry has grown to become an integral part of our everyday life. as vehicles evolve, the primarily mechanical solutions for vehicle control are gradually replaced by electronics and software solutions forming in-vehicle computer networks. an emerging trend is to introduce wireless technology in the vehicle domain by attaching a wireless gateway to the in-vehicle network. by allowing wireless communication, real-time information exchange between vehicles and between infrastructure and vehicles become reality. this communication allows for road condition reporting, decision making, and remote diagnostics and firmware updates over-the-air. however, allowing external parties wireless access to the in-vehicle network creates a potential entry-point for cyber attacks. in this paper, we investigate the security issues of allowing external wireless communication. we use a defense-in-depth perspective and discuss security challenges for each of the prevention, detection, deflection, countermeasures, and recovery layers.','Denial-of-service attacks'
'security protocols are indispensable in secure communication. we give an operational semantics of security protocols in terms of a prolog-like language. with this semantics, we can uncover attacks on a security protocol that are possible with no more than a given number of rounds. though our approach is exhaustive testing, the majority of fruitless search is cut off by selecting a small number of representative values that could be sent by an attacker. hence, the number of scenarios is relatively small and our method is quite practical. furthermore, our method not only reports possible attacks but also describes the attacks in great detail. this description would be very helpful to protocol designers and analyzers.','Denial-of-service attacks'
'this paper proves that several interactive proof systems are zero-knowledge against general quantum attacks. this includes the well-known goldreich-micali-wigderson classical zero-knowledge protocols for graph isomorphism and graph 3-coloring (assuming the existence of quantum computationally concealing commitment schemes in the second case). also included is a quantum interactive protocol for a complete problem for the complexity class of problems having \"honest verifier\" quantum statistical zero-knowledge proofs, which therefore establishes that honest verifier and general quantum statistical zero-knowledge are equal: qszk = qszkhv. previously no non-trivial proof systems were known to be zero-knowledge against quantum attacks, except in restricted settings such as the honest-verifier and common reference string models. this paper therefore establishes for the first time that true zero-knowledge is indeed possible in the presence of quantum information and computation.','Denial-of-service attacks'
'in order to improve the management of copyright in the internet, known as digital rights management, there is the need for a shared language for copyright representation. current approaches are based on purely syntactic solutions, i.e. a grammar that defines a rights expression language. these languages are difficult to put into practise due to the lack of explicit semantics that facilitate its implementation. moreover, they are simple from the legal point of view because they are intended just to model the usage licenses granted by content providers to end-users. thus, they ignore the copyright framework that lies behind and the whole value chain from creators to end-users. our proposal is to use a semantic approach based on semantic web ontologies. we detail the development of a copyright ontology in order to put this approach into practice. it models the copyright core concepts for creation, rights and the basic kinds of actions that operate on content. altogether, it allows building a copyright framework for the complete value chain. the set of actions operating on content are our smaller building blocks in order to cope with the complexity of copyright value chains and statements and, at the same time, guarantee a high level of interoperability and evolvability. the resulting copyright modelling framework is flexible and complete enough to model many copyright scenarios, not just those related to the economic exploitation of content. the ontology also includes moral rights, so it is possible to model this kind of situations as it is shown in the included example model for a withdrawal scenario. finally, the ontology design and the selection of tools result in a straightforward implementation. description logic reasoners are used for license checking and retrieval. rights are modelled as classes of actions, action patterns are modelled also as classes and the same is done for concrete actions. then, to check if some right or license grants an action is reduced to check for class subsumption, which is a direct functionality of these reasoners.','Digital rights management'
'the digital watermark technology and the mobile agent technology play significant roles in the industrial applications in tandem with each other. in recent years, the union of the mobile agent technology and the watermark technology has been intensively investigated. a new architecture is proposed using this integration technology to protect copyright by detecting the watermark on the internet. this architecture comprises four integral blocks: author block (ab), user block (ub), watermark agent block (wab) and mobile agent block (mab). in ab, author embeds his ownership watermark in his work and then registers the same in copyright management server (cms). then cms embeds the second watermark in the digital work produced by watermark agent. this is actually a hidden agent. in ub, user submits detail information about personality and digital work to cms. cms gives user the digital work and user key according to the input information. wab creates the custom-built watermark agents and designs the route strategy. in mab, the watermark agents are dispatched to the suspicious hosts by wab and are examined for their copyright.','Digital rights management'
'because of complexities of the structured finance, the securitization of the assets, distribution of the securities and the protection of the private information among transactions of secirities on it enable networks became serious problems. in this paper, we show that the information capsule for the copyright management could be used to the securitization and its concurrency. we propose the method of the securitization with the information capsule which includes mobile agents. by applying this method to the system that a investor could check the uncertainness of the own securities. for example which obligation is contained or how to estimate the value of the securities.','Digital rights management'
'the internet has spawned a revolution in the way people distribute content and accessservices. at the same time, the availability of broadband and wireless networks has increased, as have the capability and portability of computing and consumer electronic devices. thesefactors have fueled the development of new technologies to automate, manage, and secure content flow and service access over the internet. the recently approved iso standard, mpeg-21 rights expression language (rel), is one such technology. this language is precise, flexible,extensible, and rich in expressing rights. thus, it can support reliable, flexible, and costeffectiveinteroperable digital rights management (drm) systems and applications for electronic commerce and enterprise management of content and services.','Digital rights management'
'the central aspect of digital rights management is rights expression languages (rels) which express and transfer rights from one party to another interoperable format. this paper provides an analysis on rels and defines the main entities model to express rights for database based on xrml.','Digital rights management'
'this paper provides an overview of digital cinema including some of the architectural ideas that have been proposed and are being considered. there are a number of security tools that can be used with the goal of secure delivery of motion picture content to the projector. there are many technical and business constraints that guide these designs. independently, there have been a number of efforts in the watermarking community to attach forensic tracking information to the motion picture content. such information would provide persistent tracking beyond the projector and would be useful in identifying compromised equipment.','Digital rights management'
'most real-life systems delegate responsibilities to different authorities. we apply this idea of delegation to a digital rights management system, to achieve high flexibility without jeopardizing the security. in our model, a hierarchy of authorities issues certificates that are linked by cryptographic means. this linkage establishes a chain of control, identity-attribute-rights, and allows flexible rights control over content. typical security objectives, such as identification, authentication, authorization and access control can be realized. content keys are personalized to detect illegal super distribution. we describe a working prototype, which we develop using standard techniques, such as standard certificates, xml and so forth. we present experimental results to evaluate the scalability of the system. a formal analysis demonstrates that our design is able to detect a form of illegal super distribution.','Digital rights management'
'digital rights management (drm) covers a broad area of intellectual property management and enforcement. drm aims to provide secure and trusted services to control the use and distribution of content. drm technologies have evolved from the primarily enforcement-centric view to a more recent comprehensive value-chain view that manages secure content at all stages of the lifecycle. a fundamental feature of drm systems and standards that are now appearing is the \'\'rights expression language\'\' (rel). the rel is an xml-based language that captures the information required for drm transactions, from the rights holder down to the allowable usages for end users. drm systems need to produce and understand rels to enable the trusted experience. one of the more successful rels is the open digital rights language (odrl). odrl was created in 2000 to meet the needs of a wide range of requirements from different sectors. it has recently been adopted by the open mobile alliance (oma) as the rel used in its drm specifications for mobile content. this paper will provide a brief overview of drm followed by a detailed look at the odrl language and its use of xml. finally, the oma profile of odrl will be reviewed to show how xml-based rels are being used and extended by the community.','Digital rights management'
'digital transactions are usually based on mutual trust. in case of drm (digital rights management) this initial trust is missing on both sides. neither do the content providers trust their clients &#8211; therefore drm was established. nor do the clients trust the content providers and react with not using these systems. the release of an open drm standard by the open mobile alliance (oma) was a first step to increase the trustworthiness of drm. but from the content providers&#8217; perspective a secure implementation for pc platforms was missing. especially the mechanisms to obfuscate and install the device private key which is the security anchor were not established there. this paper shows a software solution for that. a more riskless way to solve this problem is the involvement of trusted computing which is also shown by the authors. finally the authors claim the necessity not to leave the users&#8217; security behind.','Digital rights management'
'','Digital rights management'
'content extraction signatures (ces) enable the selective disclosure of verifiable content from signed documents. we have previously demonstrated a ces extraction policy for fragment grouping to allow the document signer to designate which subsets of the original document are valid subdocuments. extending this ability, we introduce a new &#60;i>hierarchical grouping extraction policy&#60;/i> that is more powerful, and for which the encoding is dramatically smaller, than the existing grouping extraction policy. this new extraction policy maps naturally onto the hierarchically structured documents commonly found in digital libraries. after giving a motivating example involving digital libraries we then conjecture as to how to enrich their functionality through the use of cess. we also show how to implement the new extraction policy using xml signatures with a custom transform along with an improved design for the xml signature structure in order to achieve ces functionality. ','Digital signatures'
'since the blind signature scheme has the unlinkability property, it can be used for untraceable electronic cash systems. the unlinkability property protects the privacy of users; however, it may be abused by criminals, e.g., to launder money or to safely get a ransom. the techniques of fair blind signature are developed to withstand the misuse of unlinkability property. in this paper, we propose not only a user-efficient but also a signer-efficient fair blind signature scheme for untraceable electronic cash. only with the help from a judge or government, the signer or the banker can derive the link between a signature and the instance of the signing protocol which produces the signature when the unlinkability property is abused. comparing with existing fair blind signature schemes, our method greatly reduces the computational load of an on-line judge. hence, the proposed method with an efficient judge can let the signer provide more efficient services to the signature requesters. it is also suitable for the limited computation capacities of users such as smart cards or mobile units. so, the proposed scheme is very useful for e-commerce transactions.','Digital signatures'
'in this paper we present a new type of signature for a group of persons, called a group signature, which has the following properties: (i) only members of the group can sign messages; (ii) the receiver can verify that it is a valid group signature, but cannot discover which group member made it; (iii) if necessary, the signature can be \"opened\", so that the person who signed the message is revealed. the group signatures are a \"generalization\" of the credential/ membership authentication schemes, in which one person proves that he belongs to a certain group. we present four schemes that satisfy the properties above. not all these schemes arc based on the same cryptographic assumption. in some of the schemes a trusted centre is only needed during the setup; and in other schemes, each pason can create the group he belongs to.','Digital signatures'
'signature files provide an efficient access method for text in documents, but retrieval is usually limited to finding documents that contain a specified boolean pattern of words. effective retrieval requires that documents with similar meanings be found through a process of plausible inference. the simplest way of implementing this retrieval process is to rank documents in order of their probability of relevance. in this paper techniques are described for implementing probabilistic ranking strategies with sequential and bit-sliced signature tiles and the limitations of these implementations with regard to their effectiveness are pointed out. a detailed comparison is made between signature-based ranking techniques and ranking using term-based document representatives and inverted files. the comparison shows that term-based representations are at least competitive (in terms of efficiency) with signature files and, in some situations, superior.','Digital signatures'
'we define the mesh signature primitive as an anonymous signature similar in spirit to ring signatures, but with a much richer language for expressing signer ambiguity. the language can represent complex access structures, and in particular allows individual signature components to be replaced with complete certificate chains. because withholding one\'s public key from view is no longer a shield against being named as a possible cosignatory, mesh signatures may be used as a ring signature with compulsory enrollment.we give an efficient construction based on bilinear maps in the common random string model. our signatures have linear size, achieve everlasting perfect anonymity, and reduce to very efficient ring signatures without random oracles as a special case. we prove non-repudiation from a mild extension of the sdh assumption, which we introduce and justify meticulously.','Digital signatures'
'in many real-life situations, massive quantities of signatures have to be issued on cheap passive supports (e.g. paper-based) such as bank-notes, badges, id cards, driving licenses or passports (hereafter ids); while large-scale id replacements are costly and prohibitive, one may reasonably assume that the updating of verification equipment (e.g. off-line border checkpoints or mobile patrol units) is exceptionally acceptable.in such a context, an attacker using coercive means (e.g. kidnapping) can force the system authorities to reveal the infrastructure\'s secret signature keys and start issuing signatures that are indistinguishable from those issued by the authority.the solution presented in this paper withstands such attacks up to a certain point: after the theft, the authority restricts the verification criteria (by an exceptional verification equipment update) in such a way that the genuine signatures issued before the attack become easily distinguishable from the fresher signatures issued by the attacker.needless to say, we assume that at any point in time the verification algorithm is entirely known to the attacker.','Digital signatures'
'','Digital signatures'
'ancient graphical documents are invaluable heritages which have been handed down since generations. they possess both intellectual and spiritual worth for humanity. in this context, many digitization processes have been started, producing very large warehouse of images. these huge amount of data raise the problem of indexing the information in order to make easier navigation in the databases. in the context of a french research program, called madonne, this paper proposes a set of complementary contributions concerning ancient graphic images indexing.','Digital signatures'
'we introduce the notion of sanitizable signatures that offer many attractive security features for certain current and emerging applications. a sanitizable signature allows authorized semi-trusted censors to modify &#8211; in a limited and controlled fashion &#8211; parts of a signed message without interacting with the original signer. we present constructions for this new primitive, based on standard signature schemes and secure under common cryptographic assumptions. we also provide experimental measurements for the implementation of a sanitizable signature scheme and demonstrate its practicality.','Digital signatures'
'','Digital signatures'
'using our alchemy research system as an exemplarsystem, we discuss different approaches to overcomingport-controlled security boundary limitations togeographically distributed computing environments whilemaintaining the inherent security provided by theseboundaries. this discussion is relevant to a number ofparallel/distributed strategies for grid and clusterarchitectures. it also provides insight into generalsecurity concerns in future distributed computingnetworks and how these may best be overcome.','Distributed systems security'
'we present nessi2, the network security simulator, a simulation environment that is based on the service-centric agent platform jiac. it focuses on network security-related scenarios such as attack analysis and evaluation of counter-measures. we introduce the main nessi2 concepts and discuss the motivation for realizing them with agent technology. then, we present the individual components and examples where nessi2 has been successfully applied.','Distributed systems security'
'the tremendous growth of the network-centred (internet and intranet) computing environments requires new architecture for security services. computer crimes are growing rapidly in these environments. in addition, these computing environments are open, and users may be connected or disconnected at any time. this makes computer security a necessity to all computer users. this paper presents a multi-agent system architecture for security services. the main objective of this system is to address some of the shortcomings that are present in contemporary security service systems that focused on providing solutions for specific security issues, such as authentication and authorization. another objective is to provide a relatively complete security service solution to protect hosts and users. the proposed system architecture includes four types of agents: interface, authentication, authorization, and service provider agents (spas). the interface agents interact with the users to fulfill their interests. the authentication agents investigate the validity of using keystroke dynamics to strengthen security. the authorization agents make all decisions regarding who can access which resources and for what purposes. the spas offer different encryption services to different users. this paper provides the agents\' architecture, design and implementations that enable them to cooperate, coordinate, and communicate with each other to provide a secure environment. a prototype of the system is implemented using the java agent development framework.','Distributed systems security'
'scavenged storage systems harness unused disk space from individual workstations the same way idle cpu cycles are harnessed by desktop grid applications like seti@home. these systems provide a promising low cost, high-performance storage solution in certain high-end computing scenarios. however, selecting the security level and designing the security mechanisms for such systems is challenging as scavenging idle storage opens the door for security threats absent in traditional storage systems that use dedicated nodes under a single administrative domain. moreover, increased security often comes at the price of performance and scalability. this paper develops a general threat model for systems that use scavenged storage, presents the design of a protocol that addresses these threats and is optimized for throughput, and evaluates the overheads brought by the new security protocol when configured to provide a number of different security properties.','Distributed systems security'
'one of the more mature instances of a service-oriented architecture is the model known as grid computing. computational grids and data grids are becoming commonplace in certain sectors, yet the style of security they implement is suitable only for a fairly small subset of possible user communities. using some case studies and experience, we describe the existing grid security models, explain why they represent shortcomings for some applications, and describe some emerging architectures, trusted computing and virtualisation, which help address the problems.','Distributed systems security'
'in many exciting multiagent applications--including future battlefields, law enforcement, and commerce--agents must communicate in inherently or potentially hostile environments in which an adversaries disrupt or intercept the communication between agents for malicious purposes, but the wireless ad hoc networks often proposed for these applications are particularly susceptible to attack. intelligent agents must balance network performance with possible harm suffered from an adversary\'s attack, while accounting for the broadcast nature of their communication and heterogenous vulnerabilities of communication links. furthermore, they must do so when the adversary is also actively and rationally attempting to counter their efforts. we address this challenge in this paper by representing the problem as a game between a sender agent choosing communication paths through a network and an adversary choosing nodes and links to attack. we introduce a network-flow-based approach for compactly representing the competing objectives of network performance and security from adversary attack, and provide a polynomial-time algorithm for finding the equilibrium strategy for the sender. through empirical evaluation we show how this technique improves upon existing approaches.','Distributed systems security'
'','Distributed systems security'
'','Distributed systems security'
'security of systems and information has always been a challenge to organisations and industries. many technical solutions including firewalls, encryption and anti-virus software have been used, yet security still remains a problem. these security solutions failures are largely due to the fact that as systems become more complex, a lot of interaction is involved between various actors. some of these interactions usually leave room for security vulnerabilities which are simply not accounted for by the technical security solutions: there are just too many possibilities. my research is focused on this aspect of organisational security. the proposed approach to this involves the monitoring of events for traces of behaviours that may eventually circumvent the security regulations of the organisation. the methodology includes organisational modeling and simulation of self monitoring agents using a normative framework.','Distributed systems security'
'the paper reviews different approaches applied to enable security in grid systems. it analyzes security mechanisms compatible with grid platforms. it focuses on the following aspects of grid security: anomaly detection and security policy verification. petri-net-based model is proposed for access control security analysis in grid systems. that model enhances grid security with trusted \'jobs\' submission and security verification.','Distributed systems security'
'authentication and authorisation infrastructures (aais) are gaining momentum throughout the internet. solutions have been proposed for various scenarios among them academia, grid computing, company networks, and above all ecommerce applications. products and concepts vary in architecture, security features, target group, and usability containing different strengths and weaknesses. in addition security needs have changed in communication and business processes. security on the internet is no longer defined as only security measures for an ecommerce provider against an untrustworthy customer but also vice versa. consequently, privacy, data canniness, and security are demands in this area. the authors define criteria for an ecommerce provider federation using an aai with a maximum of privacy and flexibility. the criteria is derived concentrating on b2c ecommerce applications fulfilling the demands. in addition to best practices found, xacml policies and an attribute infrastructure are deployed. among the evaluated aais are shibboleth, microsoft passport, the liberty alliance framework, and permis.','Domain-specific security and privacy architectures'
'we provide a unified architecture, called space, for secure, privacy-aware, and contextual multimedia systems in organizations. many key and important architectural components already exist which contribute to a unified platform, including the classic data mining, security, and privacy-preserving components in conventional intelligent systems. after presenting an overview of our unified architecture, we focus on the state-of-the-art architectural components for user interaction in future systems - particularly multimedia voice interaction with intelligent systems. this paper shows how user-level conversational data mining (cdm) methods, coupled with biometric security, and enhanced with privacy- awareness, may be used with any web information system architecture. finally, we provide an example of our unified architecture through integrating a knowledge architecture for an e-finance application in the financial services domain. the resulting architectures benefit from added security, privacy-awareness, and contextual filtering at the user-level.','Domain-specific security and privacy architectures'
'data mining is becoming a pervasive technology in several activities as using historical data to predict the success of a marketing campaign, looking for patterns in financial transactions to discover illegal activities or analyzing genome sequences. in this paper we adopt a reference flexible mining architecture able to discover knowledge in a distributed and heterogeneous environment. in the context of security, the information we are seeking is the knowledge of whether a security breach has been experienced, and if the answer is yes, who is the perpetrator. to this purpose, the guide lines of the service oriented architecture, soa, and the orchestration model have been considered as a way to realize a our proposal of knowledge discovery process to intrusion detection.','Domain-specific security and privacy architectures'
'raising awareness and providing guidance to on-line data protection is by all means a crucial issue worldwide. equally important is the issue of applying privacy-related legislation in a coherent and coordinated way. both these topics become even more critical when referring to medical environments and thus to the protection of patients\' privacy and medical data. electronic medical transactions require the transmission of personal and medical information over insecure communication channels like the internet. it is therefore a rather straightforward task to construct &#8220;patient profiles&#8221; that capture the electronic medical behavior of a patient, or even reveal sensitive information in regard with her/his medical history. clearly, the consequence from maintaining such profiles is the violation of the patient\'s privacy. this paper studies medical environments that can support electronic medical transactions or/and the provision of medical information through the web. specifically it focuses on the countermeasures that the various actor categories can employ for protecting the privacy of personal and medical data transmitted during electronic medical transactions.','Domain-specific security and privacy architectures'
'following the events of september 11th, the leaders of developednations have moved quickly to establish new agreements forinternational security cooperation. many of these agreements arebeing forged secretively, and with little democratic oversight.this session discusses the new era of control and surveillance thathas arisen since that tragic day, and what it will mean for ourprivacy and for national security and law enforcement.','Domain-specific security and privacy architectures'
'medsn system for in-home patient monitoring: architecture, privacy and security philip kuryloski, sameer pai, stephen wicker cornell university ithaca, ny 14850 {pjk25, skp27}@cornell.edu, {wicker}@ece.cornell.edu yuan xue vanderbilt university nashville, tn 37235 yuan.xue@vanderbilt.edu','Domain-specific security and privacy architectures'
'','Domain-specific security and privacy architectures'
'security challenges are still among the biggest obstacles when considering the adoption of cloud services. this triggered a lot of research activities, resulting in a quantity of proposals targeting the various cloud security threats. alongside with these security issues, the cloud paradigm comes with a new set of unique features, which open the path toward novel security approaches, techniques, and architectures. this paper provides a survey on the achievable security merits by making use of multiple distinct clouds simultaneously. various distinct architectures are introduced and discussed according to their security and privacy capabilities and prospects.','Domain-specific security and privacy architectures'
'the advent of social networking websites for use in health care has prompted concerns about the risks that such systems pose to the security and privacy of personal health information. in this paper, we survey the research literature, in order to provide a current snapshot of privacy and security safeguards for social network websites. we describe some of the unique features of the health care space, and recommend directions for future research in this relatively new area.','Domain-specific security and privacy architectures'
'','Domain-specific security and privacy architectures'
'the distributed nature of the environment in which privacy and security policies operate requires tools that help enforce consistency of policy rules across different domains. furthermore, because changes to policy rules are required as policies evolve over time, such tools can be used by policy administrators to ensure the consistency of policy changes. in this paper, we describe a number of different policy analysis tools and techniques that we have developed over the years and present them in a unified framework in which both privacy and security policies are discussed. we cover dominance analyses of general policies, conflicts among authorizations and prohibitions, and other analyses of obligations, as well as policy similarity analysis and policy distribution.','Economics of security and privacy'
'commerce is a rapidly emerging application area of ubiquitous computing. in this paper, we discuss the market forces that make the deployment of ubiquitous commerce infrastructures a priority for grocery retailing. we then proceed to report on a study on consumer perceptions of mygrocer, a recently developed ubiquitous commerce system. the emphasis of the discussion is on aspects of security, privacy protection and the development of trust; we report on the findings of this study. we adopt the enacted view of technology adoption to interpret some of our findings based on three principles for the development of trust. we expect that this interpretation can help to guide the development of appropriate strategies for the successful deployment of ubiquitous commerce systems.','Economics of security and privacy'
'','Economics of security and privacy'
'','Economics of security and privacy'
'the third in a series of articles providing basic information on legal issues facing people and businesses that operate in computing-related markets focuses on the responsibility to ensure privacy and data security. the featured web extra is an audio podcast by brian m. gaff and thomas j. smedinghoff, two of the article\'s coauthors.','Economics of security and privacy'
'the concept of a smart grid -- an intelligent and active power distribution network that uses advanced communication technology to collect and use real time operational information for efficient control of the grid -- has become one of the hottest research topics in the areas of information technology and electrical engineering. governments and the private sector have started to invest billions of dollars into this new technology that will not only allow more efficient management of current grids, better load distribution, demand control, up-to-date status monitoring and faster failure recovery, but also promises better integration of new services and applications such as smart homes and intelligent appliances, new energy sources, and ev grids. these features will be made possible by a wide deployment of data collection devices such as embedded sensors, smart meters and communication networks to bring this data into control centres for analysis, as well as automated controls and algorithms for decision making based on the current status information. as more intelligence is built into the electrical grid, the issue of grid security becomes extremely important and must be considered within the broader field of cyber-security. the automated control functions of the smart grid could be manipulated by intruders to gain control of the power distribution networks, steal customer information, or inflict other damages. network-based attacks may be used to disrupt the network, overload part of the grid or disconnect regions. the security of the access devices that provide real-time information must be examined carefully. a major challenge at the moment lies in the variety of options for smart grid communication at both access and core networks. smart meter communication and messaging technologies could be based on dlms/cosem standard or ip-based, using a wired or wireless sensor platform e.g. 6lowpan or ieee802.15.4/zigbee, in a mesh or cluster-based topology, connected using fibers or based on cellular technologies such as lte or wimax. the variety of technologies available poses an enormous challenge in incorporating security and survivability features into the smart grid design. interoperability issues between devices pose further security challenges that must be addressed based on the undergoing standardization works, most notably the ieee p2030 project. overcoming security challenges in smart grid communication will require careful evaluation of the proposed communication technologies and their interoperation. this includes the creation of security test beds, which would allow examination of the survivability of the grid system against a variety of security attacks and further system hardening and the development of integrated simulation environments for the communication network and the power grid. also required is the development of various components of a situational awareness system, such as intrusion detection and prevention systems and network management agents, to monitor and present the common operational picture (cop) of the grid from individual sensors and meters to the main controllers at the network control centers. in addition to the security of grid communication systems, the privacy of communication and consumer data are also extremely important, in particular in the case of smart home/smart grid. this is because detailed consumer data may be collected and analyzed without active participation from the homeowner. the canadian law greatly emphasizes the importance of protecting the privacy of users, and puts that responsibility squarely on the shoulder of data collectors, i.e. grid operators and utility companies in this case. any design for smart grid communication must strike a delicate balance between functionality and privacy. while from an operational point of view it might be beneficial to collect and maintain detailed individual usage information in order to optimize the network operations, this would also cause significant privacy concerns if such data ever fell into the wrong hands or was used for individual identification in cases where consumer protection laws were applied. many questions must be answered, for example: how do we aggregate data and minimize individual user identification without compromising the usefulness of data? how do we to maintain privacy of data along the entire path of the sensors and to the main databases? what is the most secure model for interactions between users and the grid, to monitor and control the trust levels of devices that are connected to the grid? the aim of this workshop was to explore the latest progress and research in the field of smart grid communication security and privacy, and to provide a forum for researchers, students and business experts from both academia and industry, to discuss the latest innovations and future works in the field. the new initiatives by federal and provincial government institutions in north america, including ontario smart grid initiative and british columbia smart metering program, as well as the comprehensive us policy framework for the 21st century grid (released in june 2011) further highlight the urgency and importance of such discussion in academia and industry. considering the broad challenges involved with the design and operation of smart grids, this workshop focused on issues specifically dealing with security and privacy issues. the topics of interest for this workshop included the following: &#8226; smart grid architecture security &#8226; smart grid security risk assessment &#8226; smart grid physical and device security &#8226; mitigating cyber attacks against the smart grid &#8226; intrusion detection for smart grid &#8226; privacy issues in smart metering &#8226; smart grid resilience &#8226; smart grid restoration and failure recovery &#8226; grid access and sensor network security and privacy &#8226; evaluation of smart grid communication protocols &#8226; smart home security and privacy issues &#8226; trust models for smart grid/smart home experts from the business and industry were invited to present an overview of the current trends and challenges in smart grid security and privacy. presentations from the academia provided a glance into the latest academic research for this field. the workshop was concluded with a panel discussion on the topic of information security and privacy that also set the agenda for future workshops of this kind.','Economics of security and privacy'
'death is an uncomfortable subject for many people, and digital systems are rarely designed to deal with this event. in particular, the wide array of existing digital authentication infrastructure rarely deals with gracefully retiring credentials in a uniform fashion. this research paper highlights an emerging paradigm: gracefully dealing with expired digital identities in a secure, privacy-preserving fashion. it examines the confluence of modern browser technology, cloud services, and human factors involved in managing a person\'s digital footprint while they live and retiring it when they die. we contemplate a potential approach to dealing with credentials after death by using cloud computing. we consider the reasons that such an approach may actually provide an opportunity for enhancing authentication security by frustrating identity stealing attacks. we note that this paper is not aimed at trivializing the real grief and loss that people feel, but rather an attempt to understand how security and privacy concerns are shaped by the end of life, with the ultimate goal of easing this transition for friends and family.','Economics of security and privacy'
'the development of transnational computer-communication systems and the associated flows of computer data across international borders have created a number of issues and problems: privacy and security of personal data, non-tariff restrictions, concerns over potential erosion of national sovereignty, protectionism, and so forth. these developments are important to the data processing community in the united states since restrictions may be placed on the systems it develops and the data processing services it offers internationally. this session will address the issues involved in general, then concentrate on privacy protection problems, and finally explore a specific, new problem area---privacy rights of business, industry and other organizations regarding data about themselves.','Economics of security and privacy'
'us president barack obama promised a \"new comprehensive approach\" to cybersecurity and guaranteed to preserve \"personal privacy and civil liberties,\" but the administration has stopped short of committing to the legal changes necessary to protect either information infrastructure or privacy. this tendency to undervalue law as a tool for enhancing both security and individual privacy is shared with other governments. sound cybersecurity policy requires better incentives to secure data and systems, and those incentives will emerge, at least in part, from legal requirements. similarly, serious efforts to protect against cyberthreats will compromise privacy and other civil rights unless those rights are protected by law.','Economics of security and privacy'
'road safety, traffic management, and driver convenience continue to improve, in large part thanks to appropriate usage of information technology. but this evolution has deep implications for security and privacy, which the research community has overlooked so far.','Economics of security and privacy'
'deeply embedded systems present a number of new challenges and opportunities in security. in this essay, we introduce some of them and explore potential ideas for addressing them.','Embedded systems security'
'this paper reports a cryptosystem designed forsecurity of embedded systems. it is based on the theoryof cellular automata(ca). the cellular automata basedcryptosystem(cac) employs a series of transforms - simple,moderatel complex, complex - all generated with differentclasses of ca. the code size of cac can be found tobe lesser than that of an other scheme published in the literature.cryptanalysis of the proposed scheme is reportedalong with similar analysis for two popular systems - desand aes. experimental results confirm that the security ofthe system is significantly better than des and comparableto that of aes. hardware-soft are co-design of cacwith a simple ca hardware is also reported to establish theefficiency in terms of small code size and high speed of execution.','Embedded systems security'
'we propose a method for dynamic security-domain scaling on smps that offers both highly scalable performance and high security for future high-end embedded systems. its most important feature is its highly efficient use of processor resources, accomplished by dynamically changing the number of processors within a security-domain (i.e., dynamically yielding processors to other security-domains) in response to application load requirements. two new technologies make this scaling possible without any virtualization software: (1) self-transition management and (2) unified virtual address mapping. evaluations show that this domain control provides highly scalable performance and incurs almost no performance overhead in security-domains. the increase in oss in binary code size is less than 1.5&percnt;, and the time required for individual state transitions is on the order of a single millisecond. this scaling is the first in the world to make possible the dynamic changing of the number of processors within a security-domain on an arm smp.','Embedded systems security'
'','Embedded systems security'
'security requirements for embedded systems such as consumer devices are becoming stronger. current designs need an isolated environment that stores and processes sensitive data. new hardware technologies are arriving that provide low-cost, high-performance, isolated environments. standard open apis are providing a route to interoperability, defragmentation. and reduced software development costs. securely, flexibly, and efficiently taking advantage of these standards is a complex software design problem. this article is an introduction to one such hardware technology, and a case study of the design of a programmable security software framework. the discussion will be of interest to all types of system designers, from soc to software, because security must be designed into the system from the outset.','Embedded systems security'
'embedded systems present significant security challenges due to their limited resources and power constraints. we propose a novel security architecture for embedded systems (sanes) that leverages the capabilities of reconfigurable hardware to provide efficient and flexible architectural support to both security standards and a range of attacks. this paper shows the efficiency of reconfigurable architecture to implement security primitives within embedded systems. we also propose the use of hardware monitors to detect and defend against attacks. the sanes architecture is based on three main ideas: 1) reconfigurable security primitives, 2) reconfigurable hardware monitors and 3) a hierarchy of security controllers at the primitive, system and executive level. results are presented for a reconfigurable aes security primitive within the ipsec standard and highlight the interest of such a solution.','Embedded systems security'
'the recently discovered \'w32.stuxnet\' worm has drastically changed the perception that systems managing critical infrastructure are invulnerable to software security attacks. here we present an architecture that enhances the security of safety-critical cyber-physical systems despite the presence of such malware. our architecture uses the property that control systems have deterministic real-time) execution behavior to detect an intrusion within 0.6 &#956;s while still guaranteeing the safety of the plant. we also show that even if an attacker is successful (or gains access to the operating system\'s administrative privileges), the overall state of the physical system still remains safe.','Embedded systems security'
'a top-down, multiabstraction layer approach for embedded security design reduces the risk of security flaws, letting designers maximize security while limiting area, energy, and computation costs.','Embedded systems security'
'embedded systems security is a significant requirement in emerging environments, considering the increasing deployment of embedded systems in several application domains. the large number of deployed embedded systems, their limited resources and their increasing complexity render systems vulnerable to an increasing number of threats. additionally, the involvement of sensitive, often private, information and the expectation for safe and dependable embedded platforms lead to strong security requirements, even legal ones, which require new technologies for their provision. in this article, we provide an overview of embedded security issues, used methods and technologies, identifying important challenges in this emerging field.','Embedded systems security'
'','Embedded systems security'
'this paper presents the design of an information- sharing based or server-assisted anti-phishing system. the system follows a client-server architecture and makes decision based on not only client side heuristics but also collective information from multiple clients. when visiting a web site, a client side proxy, installed as a plug-in to a browser, decides on the legitimacy of the web site based on a combination of white list, black list and heuristics. in case the client side proxy does not have sufficient information to make a clear judgment, it reports the suspicious site to a central server which has access to more complete and up to date information and is in a much better position than individual clients to make informed decisions. our system is designed to counter against deceptive phishing as well as dns-hijack attack.','File system security'
'current protection strategies against insider adversaries are expensive, intrusive, not systematically implemented, and operate independently; too often, these strategies are defeated. the authors discuss the development of methods for a systems-based approach to insider security. to investigate insider evolution within an organization, they use system dynamics to develop a preliminary model of the employee life cycle that defines and analyzes the employee population\'s interactions with insider security protection strategies. the authors exercised the model for an example scenario that focused on human resources and personnel security activities&#8212;specifically, prehiring screening and security clearance processes. the model provides a framework for understanding important interactions, interdependencies, and gaps in insider protection strategies. this work provides the basis for developing an integrated systems-based process for building&#8212;that is, designing, evaluating, and operating&#8212;a system for effective insider security.','File system security'
'','File system security'
'sharing network data between unix and nt systems is becoming increasingly important as nt moves into areas previously serviced entirely by unix. one difficulty in sharing data between unix and nt is that their file system security models are quite different. nt file servers use access control lists (acls) that allow permissions to be specified for an arbitrary number of users and groups, while unix nfs servers use traditional unix permissions that provide control only for owner, group, and other. this paper describes a merged model in which a single file system can contain both files with nt-style acls and files with unix-style permissions. for native file service requests (nfs requests to unix-style files and nt requests to nt-style files) the security model exactly matches a unix or nt fileserver. for non-native requests, heuristics allow a reasonable level of access without compromising the security guarantees of the native model.','File system security'
'authorization plays an essential role to ensure the security of a wide variety of computing and it systems such as data management systems, e-trading systems, database transaction systems, etc. this paper aims to propose a high level formal language for specifying and evaluating distributed authorizations with delegation, develop a new method for credential chain discovery, and implement a system prototype for representing and reasoning about access control policies in distributed environments. by applying the new methodology and technology developed from this work, we will be able to design highly secure computing and it systems in many different complex problemdomains.','File system security'
'','File system security'
'a security system for a company network is progressing as a esm (enterprise security management) in an existing security solution foundation. the establishment of the security policy is occupying a very important area in esm of the security system. we tried to analyze the existing esm system for this and designed a security solution structure for enhancing the internal security. we applied implementing directly ids system and tested it. this study examined the structure of security solutions in order to build an enterprise security management system. for this purpose, we analyzed existing enterprise security management systems and, based on the results, proposed a enterprise security management system with reinforced internal security and tested the system. for the test, we used a firewall through log analysis and designed an intra-network using virtual ip system.','File system security'
'computational e-mail systems, which allow mail messages to contain command scripts that automatically execute upon receipt, can be used as a basis for building a variety of collaborative applications. however, their use also presents a serious security problem because a command script from a sender may access/modify receiver\'s private files or execute applications on receiver\'s behalf. existing solutions to the problem either severely restrict i/o capability of scripts, limiting the range of applications that can be supported over computational e-mail, or permit all i/o to scripts, potentially compromising the security of the receiver\'s files. our model, called the intersection model of security, permits i/o for e-mail from trusted senders but without compromising the security of private files. we describe two implementations of our security model: an interpreter-level implementation and an operating systems-level implementation. we discuss the tradeoffs between the two implementations and suggest directions for future work.','File system security'
'','File system security'
'the global information technology (it) industry recognizes the need for standards to improve the quality and consistency of security for it products and services. as such, the international organization for standardization&#x002f; international electrotechnical commission (iso&#x002f;iec) 27000 series is focusing on the requirements, security controls, and implementation guidance for an organization\'s information security management system (isms). this guidance establishes general principles that can be used in various industries and government&semi; however, standardized techniques are also needed to identify, implement, and operate security controls as part of the isms life cycle. the bell labs security framework identifies both the minimal and differentiating security controls by decomposing an it product or service into a layered hierarchy of equipment and facilities groupings and examining the types of activities that occur at each layer in a standardized manner. furthermore, the bell labs security framework security dimensions provide the necessary mechanisms to implement and operate the selected controls. the bell labs security framework enhances the iso&#x002f;iec 27000 series by providing a comprehensive end-to-end approach to implementing it security. &#169; 2007 alcatel-lucent.','File system security'
'we study and further develop two language-based techniques for analyzing security protocols. one is based on a typed process calculus; the other, on untyped logic programs. both focus on secrecy properties. we contribute to these two techniques, in particular by extending the former with a flexible, generic treatment of many cryptographic operations. we also establish an equivalence between the two techniques.','Formal security models'
'existing security models require that information of a given security level be prevented from ``leaking\'\' into lower-security information. high-security applications must be demonstrably free of such leaks, but such demonstration may require substantial manual analysis. other authors have argued that the natural way to enforce these models automatically is with information-flow analysis, but have not shown this to be practicable for general purpose programming languages in current use. modern safety-critical systems can contain software components with differing safety integrity levels, potentially operating in the same address space. this case poses problems similar to systems with differing security levels; failure to show separation of data may require the entire system to be validated at the higher integrity level. in this paper we show how the information flow model enforced by the spark examiner provides support for enforcing these security and safety models. we describe an extension to the spark variable annotations which allows the specification of a security or safety level for each state variable, and an extension to the spark analysis which automatically enforces a given information flow policy on a spark program.','Formal security models'
'','Formal security models'
'security assessment is largely ad hoc today due to its inherent complexity. the existing methods are typically experimental in nature highly dependent of the assessor\'s experience, and the security metrics are usually qualitative. we propose to address the dual problems of experimental analysis and qualitative metrics by developing two complementary approaches for security assessment: (1) analytical modeling, and (2) metrics-based assessment. to avoid experimental evaluation, we put forward a formal model that permits the accurate and scientific analysis of different security attributes and security flaws. to avoid qualitative metrics leading to ambiguous conclusions, we put forward a collection of mathematical formulas based on which quantitative metrics can be derived. the vulnerability analysis model responses to the need for a theoretical foundation for modeling information security, and security metrics are the cornerstone of risk analysis and security management. in addition to the security analysis approach, we discuss security testing methods as well. a relative complete coverage (rcc) principle is proposed along with an example of applying the rcc principle. the innovative ideas proposed in this paper include a hierarchical multi-level modeling approach to modeling vulnerability using model composition and refinement techniques, a data-centric, quantitative metrics mechanism, and multidimensional assessment capturing both process and product elements in a formalized framework.','Formal security models'
'in today&#226;&#128;&#153;s software development process, security related design decisions are rarely made early in the overall process. even if security is considered early, this means that in most cases a more-or-less encompassing security requirement analyses is made, based on this analysis best-practices, ad-hocdesign decisions or individual expertise is used to integrate security during the development process or after weaknesses are found after the deployment. this paper introduces security building block models which are used to build security related components, namely security building blocks. these security building blocks represent concrete security solutions, so called security properties, introduced in other publications of the sec futur project. the goal of this approach is to provide already defined and tested security related software components, which can be used early in the overall development process, to support security-design-decision already while modeling the software-system. the paper shortly describes this new security engineering process with its requirement analysis and definition of security properties and how the security building block model fits into this approach. additionally the security building block model is presented in detail. all artifacts and relationships of the model are described. short examples finish up the paper to show the creation of the security building blocks and their interactions with other software components.','Formal security models'
'in this chapter i present a process algebraic approach to the modelling of security properties and policies. i will concentrate on the concept of secrecy, also known as confidentiality, and in particular on the notion of non-interference. non-interference seeks to characterise the absence of information flows through a system and, as such, is a fundamental concept in information security.a central thesis of these lectures is that, viewed from a process algebraic point of view, the problem of characterising non-interference is essentially equivalent to that of characterising the equivalence of processes. the latter is itself a fundamental and delicate question at the heart of process algebra and indeed theoretical computer science: the semantics of a process is intimately linked to the question of which processes should be regarded as equivalent.we start, by way of motivation and to set the context, with a brief historical background. a much fuller exposition of security policies in the wider sense, embracing properties other than secrecy, can be found in the chapter by pierangela samarati in this volume. we then cover some elements of process algebra, in particular csp (communicating sequential processes), that we need and present a formulation of noninterference, along with some more operational presentations of process algebra, including the idea of bi-simulation. i argue that the classical notion of unwinding found in the security literature is really just bisimulation in another guise.finally, i propose some generalisations of the process algebraic formulations designed to encompass a richer class of policies and examples.','Formal security models'
'in computer security, risk communication refers to informing computer users about the likelihood and magnitude of a threat. efficacy of risk communication depends not only on the nature of the risk, but also on the alignment between the conceptual model embedded in the risk communication and the user\'s mental model of the risk. the gap between the mental models of security experts and non-experts could lead to ineffective risk communication. our research shows that for a variety of the security risks self-identified security experts and non-experts have different mental models. we propose that the design of the risk communication methods should be based on the non-expert mental models.','Formal security models'
'','Formal security models'
'planning information security investment is somewhere between art and science. this paper reviews and compares existing scientific approaches and discusses the relation between security investment models and security metrics. to structure the exposition, the high-level security production function is decomposed into two steps: cost of security is mapped to a security level, which is then mapped to benefits. this allows to structure data sources and metrics, to rethink the notion of security productivity, and to distinguish sources of indeterminacy as measurement error and attacker behavior. it is further argued that recently proposed investment models, which try to capture more features specific to information security, should be used for all strategic security investment decisions beneath defining the overall security budget.','Formal security models'
'digital watermarking, traditionally modeled as communication with side information, is generally considered to have important potential applications in various scenarios such as digital rights managements. however, the current literature mainly focuses on robustness, capacity and imperceptibility. there lacks systematic formal approach in tackling secure issues of watermarking. one one hand, the threat models in many previous works are not sufficiently established, which result in somewhat superficial or even flawed security analysis. on the other hand, there lacks a rigorous model for watermarking in general that allows useful analysis in practice. there has been some efforts in clearing the threat models and formulate rigorous watermarking models. however, there are also many other cases where security issues are lightly or incorrectly treated. in this paper, we survey various security notions and models in previous work, and discuss possible future research directions.','Formal security models'
'blind signatures allow a signer to digitally sign a document without being able to glean any information about the document. in this paper, we investigate the symmetric analog of blind signatures, namely blind message authentication codes (blind macs). one may hope to get the same efficiency gain from blind mac constructions as is usually obtained when moving from asymmetric to symmetric cryptosystems. our main result is a negative one however: we show that the natural symmetric analogs of the unforgeability and blindness requirements cannot be simultaneously satisfied. faced with this impossibility, we show that blind macs do exist (under the one-more rsa assumption in the random oracle model) in a more restrictive setting where users can share common state information. our construction, however, is only meant to demonstrate the existence; it uses an underlying blind signature scheme, and hence does not achieve the desired performance benefits. the construction of an efficient blind mac scheme in this restrictive setting is left as an open problem.*','Hash functions and message authentication codes'
'we propose and investigate the notion of aggregate message authentication codes (macs) which have the property that multiple mac tags, computed by (possibly) different senders on multiple (possibly different) messages, can be aggregated into a shorter tag that can still be verified by a recipient who shares a distinct key with each sender. we suggest aggregate macs as an appropriate tool for authenticated communication in mobile ad-hoc networks or other settings where resource-constrained devices share distinct keys with a single entity (such as a base station), and communication is an expensive resource.','Hash functions and message authentication codes'
'this paper introduces approximate image message authentication codes (imacs) for soft image authentication. the proposed approximate imac survives small to moderate image compression and it is capable of detecting and locating tampering. techniques such as block averaging and smoothing, parallel approximate message authentication code (amac) computation, and image histogram enhancement are used in the construction of the approximate imac. the performance of the approximate imac in three image modification scenarios, namely, jpeg compression, deliberate image tampering, and additive gaussian noise, is studied and compared. simulation results are presented','Hash functions and message authentication codes'
'','Hash functions and message authentication codes'
'we discuss the security of message authentication code (mac) schemes from the viewpoint of differential attack, and propose an attack that is effective against des-mac and feal-mac. the attack derives the secret authentication key in the chosen plaintext scenario. for example, des(8-round)-mac can be broken with 234 pairs of plaintext, while feal8-mac can be broken with 222 pairs. the proposed attack is applicable to any mac scheme, even if the 32-bits are randomly selected from among the 64-bits of ciphertext generated by a cryptosystem vulnerable to differential attack in the chosen plaintext scenario.','Hash functions and message authentication codes'
'aggregate message authentication codes, as introduced by katz and lindell (ct-rsa 2008), combine several macs into a single value, which has roughly the same size as an ordinary mac. these schemes reduce the communication overhead significantly and are therefore a promising approach to achieve authenticated communication in mobile ad-hoc networks, where communication is prohibitively expensive. here we revisit the unforgeability notion for aggregate macs and discuss that the definition does not prevent \"mix-and-match\" attacks in which the adversary turns several aggregates into a \"fresh\" combination, i.e., into a valid aggregate on a sequence of messages which the attacker has not requested before. in particular, we show concrete attacks on the previous scheme. to capture the broader class of combination attacks, we provide a stronger security notion of aggregation unforgeability. while we can provide stateful transformations lifting (non-ordered) schemes to meet our stronger security notion, for the statefree case we switch to the new notion of history-free sequential aggregation. this notion is somewhat between non-ordered and sequential schemes and basically says that the aggregation algorithm is carried out in a sequential order but must not depend on the preceding messages in the sequence, but only on the shorter input aggregate and the local message. we finally show that we can build an aggregation-unforgeable, history-free sequential mac scheme based on general assumptions.','Hash functions and message authentication codes'
'we design an efficient mode of operation on block ciphers, ss-nmac. our mode has the following properties, when instantiated with a block cipher f to yield a variable-length, keyed hash function h:    (1)  mac preservation.  h is a secure message authentication code (mac) with birthday security, as long as f is unpredictable.    (2)  prf preservation.  h is a secure pseudorandom function (prf) with birthday security, as long as f is pseudorandom.    (3)  security against side-channels. as long as the block cipher f does not leak side-channel information about its internals to the attacker, properties (1) and (2) hold even if the remaining implementation of h is completely leaky. in particular, if the attacker can learn the transcript of all block cipher calls and other auxiliary information needed to implement our mode of operation. our mode is the first to satisfy the mac preservation property (1) with birthday security, solving the main open problem of dodis et al. [7] from eurocrypt 2008. combined with the prf preservation (2), our mode provides a hedge against the case when the block cipher f is more secure as a mac than as a prf: if it is false, as we hope, we get a secure variable-length prf; however, even if true, we still \"salvage\" a secure mac, which might be enough for a given application.we also remark that no prior mode of operation offered birthday security against side channel attacks, even if the block cipher was assumed pseudorandom.although very efficient, our mode is three times slower than many of the prior modes, such as cbc, which do not enjoy properties (1) and (3). thus, our work motivates further research to understand the gap between unpredictability and pseudorandomness of the existing block ciphers, such as aes.','Hash functions and message authentication codes'
'in this paper, we propose classes of message authentication codes (mac) based on error correcting-codes. we introduce a new notion of error tolerant forgery of hash messages. these macs allow full error recovery for all applications, while being error-tolerant for less information-sensitive applications. the classes of the keyed hash functions are highly secure, and provide the capabilities of correcting errors on transmission, including burst-errors, which is a typical phenomenon in wireless communications. these classes of hash functions are easily implementable in hardware by means of simple linear feedback shift register structures.','Hash functions and message authentication codes'
'side channel attacks are a serious menace to embedded devices with cryptographic applications which are utilized in sensor and ad hoc networks. in this paper we show that side channel attacks can be applied to message authentication codes, even if the countermeasure is applied to the underlying block cipher. in particular, we show that emac, omac, and pmac are vulnerable to our attack. based on simple power analysis, we show that several key bits can be extracted, and based on differential power analysis, we present selective forgery against these macs. our results suggest that protecting block ciphers against side channel attacks is not sufficient, and countermeasures are needed for macs as well.','Hash functions and message authentication codes'
'chaos functions are mainly used to develop mathematical models of non-linear systems. they have attracted the attention of many mathematicians owing to their extremely sensitive nature to initial conditions and their immense applicability to problems in daily life. in this paper, two widely used chaos functions the logistic equation and the lorenz equation are taken and analyzed. we found that these equations exhibit characteristics, which satisfy the expected properties of message authentication code (mac). we also provide a novel approach of generating mac with higher security but with smaller key size. this is achieved by the design of the algorithm using variable initialization vectors (iv) instead of the constant ivs. variable iv adds strength to the security of the message. the experimental results show that it satisfies the expected characteristics of a message authentication code generation algorithm.','Hash functions and message authentication codes'
'','Information accountability and usage control'
'are designers responsible for all of the uses of the systems they create?','Information accountability and usage control'
'','Information accountability and usage control'
'','Information accountability and usage control'
'','Information accountability and usage control'
'given the ubiquity of data on the web, and the lack of usage restriction enforcement mechanisms, stories of personal, creative and other kinds of data misuses are on the rise. there should be both sociological and technological mechanisms that facilitate accountability on the web that would prevent such data misuses. sociological mechanisms appeal to the data consumer\'s self-interest in adhering to the data provider\'s desires. this involves a system of rewards such as recognition and financial incentives, and deterrents such as prohibitions by laws for any violations and social pressure. bur there is no well-defined technological mechanism for the discovery of accountability or the lack of it on the web. as part of my phd thesis i propose a solution to this problem by designing a web protocol called httpa (accountable http). this protocol will enable data consumers and data producers to agree to specific usage restrictions, preserve the provenance of data transferred from a web server to a client and back to another web server, and more importantly provide a mechanism to derive an `audit trail\' for the data reuse with the help of a trusted intermediary called a `provenance tracker network\'.','Information accountability and usage control'
'electronic health record (ehr) and personal health record (phr) systems could allow patients to better manage their health information and share it to enhance the quality and efficiency of their healthcare. unfortunately, misuse of information stored in ehr and phr systems will create new risks for patients, and we need to empower them to safeguard their health information to avoid problems such as medical identity theft. in this paper, we introduce the notion of accountable use and update of electronic health records and design a patient-centric monitoring system based on it. we develop a system architecture and associated protocols that enable either explicit or implicit patient control over when and how health information is accessed. our approach provides a reasonable solution rather than addressing the more general information flow control problem in distributed systems. we also implement and evaluate a prototype system motivated by a health record sharing scenario based on nhin direct to demonstrate that enhanced accountability can be supported with acceptable performance and integration overheads.','Information accountability and usage control'
'','Information accountability and usage control'
'an important aspect of trust in cloud computing consists in preventing the cloud provider from misusing the user\'s data. in this work-in-progress paper, we propose the approach of data anonymization to solve this problem. as this directly leads to problems of cloud usage accounting, we also propose a solution for anonymous yet reliable access control and accountability based on ring and group signatures.','Information accountability and usage control'
'in this paper we describe our general framework for usage control (ucon) enforcement on grid systems. it allows both grid services level enforcement of ucon as well as fine-grained one at the level of local grid node resources. in addition, next to the classical checks for usage control: checks of conditions, authorizations, and obligations, the framework also includes trust and risk management functionalities. indeed, we show how trust and risk issues naturally arise when considering usage control in grid systems and services and how our architecture is flexible enough to accommodate both notions in a pretty uniform way.','Information accountability and usage control'
'in this paper a meta-model for information flow control is defined using the foundation of barker\'s access control meta-model. the purposes for defining this meta-model is to achieve a more principled understanding of information flow control, to compare information flow control and access control at an abstract level, and to explore how information flow control and access control might be composed to yield a rich new set of ideas and systems for controlling the dissemination of sensitive information. it is shown that it is possible to define a meta-model for information flow control, that such a model is more complex compared to the access control meta-model, and that the meta-models for information flow control and access control can be composed in a conceptually straightforward way.','Information flow control'
'empirical observations of developers editing code revealed that difficulties following control flow relationships led to poor changes, wasted time, and bugs. i am designing a static analysis to compute interprocedural path-sensitive control flow to help developers more quickly and accurately visually answer these common questions about code.','Information flow control'
'we investigate the integration of two approaches to information security: information flow analysis, in which the dependence between secret inputs and public outputs is tracked through a program, and differential privacy, in which a weak dependence between input and output is permitted but provided only through a relatively small set of known differentially private primitives. we find that information flow for differentially private observations is no harder than dependency tracking. differential privacy\'s strong guarantees allow for efficient and accurate dynamic tracking of information flow, allowing the use of existing technology to extend and improve the state of the art for the analysis of differentially private computations.','Information flow control'
'the model of decentralized information flow control (difc) is effective at improving application security and can support rich confidentiality and integrity policies. we describe the design and implementation of dupro, an efficient user-space information flow control framework. dupro adopts software-based fault isolation (sfi) to isolate protection domains within the same process. it controls the end-to-end information flow at the granularity of sfi domains. being a user-space framework, dupro does not require any os changes. since sfi is more lightweight than hardware-based isolation (e.g., os processes), the inter-domain communication and scheduling in dupro are more efficient than process-level difc systems. finally, dupro supports a novel checkpointing-restoration mechanism for efficiently reusing protection domains. experiments demonstrate applications can be ported to dupro with negligible overhead, enhanced security, and with tight control over information flow.','Information flow control'
'numerous sensitive databases are breached every year due to bugs in applications. these applications typically handle data for many users, and consequently, they have access to large amounts of confidential information. this paper describes ifdb, a dbms that secures databases by using decentralized information flow control (difc). we present the query by label model, which introduces new abstractions for managing information flows in a relational database. ifdb also addresses several challenges inherent in bringing difc to databases, including how to handle transactions and integrity constraints without introducing covert channels. we implemented ifdb by modifying postgresql, and extended two application environments, php and python, to provide a difc platform. ifdb caught several security bugs and prevented information leaks in two web applications we ported to the platform. our evaluation shows that ifdb\'s throughput is as good as postgresql for a real web application, and about 1\% lower for a database benchmark based on tpc-c.','Information flow control'
'decentralized information flow control (difc) is an approach to security that allows application writers to control how data flows between the pieces of an application and the outside world. as applied to privacy, difc allows untrusted software to compute with private data while trusted security code controls the release of that data. as applied to integrity, difc allows trusted code to protect untrusted software from unexpected malicious inputs. in either case, only bugs in the trusted code, which tends to be small and isolated, can lead to security violations. we present flume, a new difc model that applies at the granularity of operating system processes and standard os abstractions (e.g., pipes and file descriptors). flume was designed for simplicity of mechanism, to ease difc\'s use in existing applications, and to allow safe interaction between conventional and difc-aware processes. flume runs as a user-level reference monitor onlinux. a process confined by flume cannot perform most system calls directly; instead, an interposition layer replaces system calls with ipcto the reference monitor, which enforces data flowpolicies and performs safe operations on the process\'s behalf. we ported a complex web application (moinmoin wiki) to flume, changingonly 2\% of the original code. performance measurements show a 43\% slowdown on read workloadsand a 34\% slowdown on write workloads, which aremostly due to flume\'s user-level implementation.','Information flow control'
'the architectural design decides the quality and the longevity of the software. gross decomposition of a system into interacting components using proper abstractions for component interaction defines the modularity of the system which in turn decides the values of quality attributes such as performance, reliability, security and modifiability as well as the percentage of design reuse. the decisions of modularization are supported by metrics like cohesion and coupling. in this paper we will focus on the quality attributes, modifiability and evolvability of a system which are overlapping to a large extent and which mainly get affected by the modularization of the system. the principle of encapsulation in object oriented (oo) design overcame the flaws present in structured methodology due to separate data and process components and their interdependencies. but problems in the evolution of oo systems due to crosscutting concerns were resolved using aspect oriented paradigm. the externalization of business logic using rule-based systems also was taken as solution to the evolution of complex software systems. here we focus on the different existing modularization solutions which support the evolvability of a software and the framework cffes (control flow framework for evolving systems) proposed by us.','Information flow control'
'we proposed earlier an optimization approach to flow control where the objective of the control is to maximize the aggregate utility of all sources over their transmission rates. the control mechanism is derived as a gradient projection algorithm to solve the dual problem. the algorithm assumes that a single path serves each source. in this paper, we extend the algorithm to the case where there are multiple paths between sources {destination pair. this allows flow control and routing to be jointly optimized.','Information flow control'
'','Information flow control'
'as companies move towards many-core chips, an efficient on-chip communication fabric to connect these cores assumes critical importance. to address limitations to wire delay scalability and increasing bandwidth demands, state-of-the-art on-chip networks use a modular packet-switched design with routers at every hop which allow sharing of network channels over multiple packet flows. this, however, leads to packets going through a complex router pipeline at every hop, resulting in the overall communication energy/delay being dominated by the router overhead, as opposed to just wire energy/delay.','Information flow control'
'a mobile ad hoc network does not require fixed infrastructure to construct connections among nodes. due to the particular characteristics of mobile ad hoc networks, most existing secure protocols in wired networks do not meet the security requirements for mobile ad hoc networks. most secure protocols in mobile ad hoc networks, such as secure routing, key agreement and secure group communication protocols, assume that all nodes must have pre-shared a secret, or pre-obtained public-key certificates before joining the network. however, this assumption has a practical weakness for some emergency applications, because some nodes without pre-obtained certificates will be unable to join the network. in this paper, a heterogeneous-network aided public-key management scheme for mobile ad hoc networks is proposed to remedy this weakness. several heterogeneous networks (such as satellite, unmanned aerial vehicle, or cellular networks) provide wider service areas and ubiquitous connectivity. we adopt these wide-covered heterogeneous networks to design a secure certificate distribution scheme that allows a mobile node without a pre-obtained certificate to instantly get a certificate using the communication channel constructed by these wide-covered heterogeneous networks. therefore, this scheme enhances the security infrastructure of public key management for mobile ad hoc networks.','Key management'
'while a lot of important information is being sent and received on the internet, the information could be exposed to many threats, and the more the multicast service is various and generalized, the more the service range is widened. when a new member joins in or leaves from the multicast group, the group key, which the existing member used, should be newly updated. the existing method had a problem that the performance was depreciated by the key exchanging. this paper proposes the effective group management mechanism for a secure transmission of the multicast data on the multicast group.','Key management'
'key management for large dynamic groups is adifficult problem because of scalability and security.skdc method is very weak in scalability. logical keyhierarchy (lkh) algorithm proposed in 1997 has greatlyimproved scalability for key management in largedynamic groups. in 1998 one-way-function tree (oft)method appeared to obtain more efficiency than lkh. butoft method has security breaches in collusion attacks. inthis paper a new method will be presented, which is moreefficient than oft and resistant to collusion attacks.','Key management'
'','Key management'
'key establishment plays a central role in authentication and encryption in wireless sensor networks, especially when they are mainly deployed in hostile environments. because of the strict constraints in power, processing and storage, designing an efficient key establishment protocol is not a trivial task. compare with traditional public key cryptography, symmetric key cryptographic with key predistribution mechanism is more suitable for large-scale wireless sensor networks. most of previous solutions have some issues on performance and security capabilities. in this paper, we propose a novel key predistribution model using pre-deployment knowledge to take advantage in terms of network connectivity, resilience against node compromised, memory requirement and energy for transmission.','Key management'
'in secure group communications, the time cost associated with key updates in the events of member join and departure is an important aspect of quality of service, especially in large groups with highly dynamic membership. to achieve better time efficiency, we propose a join-exit-tree (jet) key management framework. first, a special key tree topology with join and exit subtrees is introduced to handle key updates for dynamic membership. then, optimization techniques are employed to determine the capacities of join and exit subtrees for achieving the best time efficiency, and algorithms are designed to dynamically update the join and exit trees. we show that, on average, the asymptotic time cost for each member join/departure event is reduced to o(log (log n)) from the previous cost of o(log n), where n is the group size. our experimental results based on simulated user activities as well as the real mbone data demonstrate that the proposed jet scheme can significantly improve the time efficiency, while maintaining low communication and computation cost, of tree-based contributory key management.','Key management'
'','Key management'
'secure dynamic conferencing (sdc) is a scenario where given a group of participants, any subset of participants can form a privileged subgroup, called a conference, and communicate securely among themselves. the existing sdc schemes belong to two classes: centralized and distributed. the former incurs the single-point of failure, the central point of attack and performance bottleneck. the two existing distributed dynamic conferencing schemes, which are based on public key cryptosystems, are inefficient and imply that anyone, as long as having a pair of public and private keys, can be in the group, thus, lacking the concept of group and the group membership management. in this paper, we first introduce two new concepts based on the well-known key tree scheme: sponsors and co-distributors and then, propose a new distributed dynamic conferencing scheme by designing an efficient algorithm for finding a sponsor or co-distributors. the new scheme enforces group/conference membership management and surpasses all the existing sdc schemes in terms of simplicity,efficiency and scalability.','Key management'
'as evidenced by the proliferation of phishing attacks and keystroke loggers, we know that human beings are not wellequipped to make trust decisions about when to use their passwords or other personal credentials. public key cryptography can reduce this risk of attack, because authentication using pki is designed to not give away sensitive data. however, using private keys on standard platforms exposes the user to \"keyjacking\"; mobile users wishing to use keypairs on an unfamiliar and potentially untrusted workstation face even more obstacles. in this paper we present the design and prototype of porki, a software application for mobile devices that offers an alternative solution to the portable key problem. through the use of temporary keypairs, proxy certificates, and wireless protocols, porki enables a user to employ her pki credentials on any bluetoothenabled workstation, including those not part of her organization\'s network, and even those that might be malicious. moreover, by crafting xacml policy statements that limit the key usage to the workstation\'s trustworthiness level, and inserting these statements into extensions of the proxy certificates, porki provides the user or the relying party with the ability to limit the amount of trust that can be put in the temporary keypair used on that workstation, and thus the scope of a potential compromise.','Key management'
'as group-oriented services become the focal point of ad hoc network applications, securing the group communications becomes a default requirement. in this paper, we address the problem of group access in secure multicast communications for wireless ad hoc networks. we argue that energy expenditure is a scarce resource for the energy-limited ad hoc network devices and introduce a cross-layer approach for designing energy-efficient, balanced key distribution trees to perform key management. to conserve energy, we incorporate the network topology (node location), the \"power proximity\" between network nodes and the path loss characteristics of the medium in the key distribution tree design. we develop new algorithms for homogeneous as well as heterogeneous environments and derive their computational complexity. we present simulation studies showing the improvements achieved for three different but common environments of interest, thus illustrating the need for cross-layer design approaches for security in wireless networks.','Key management'
'post-silicon validation is a necessary step in a design\'s verification process. pre-silicon techniques such as simulation and emulation are limited in scope and volume as compared to what can be achieved on the silicon itself. some parts of the verification, such as full-system functional verification, cannot be practically covered with current pre-silicon technologies. this panel brings together experts from industry, academia, and eda to review the differences and similarities between pre- and post-silicon, discuss how the fundamental aspects of verification are affected by these differences, and explore how the gaps between the two worlds can be bridged.','Logic and verification'
'once a design is both retimed and sequentially optimized, sequential equivalence verification becomes very hard since retiming breaks the equivalence of the retimed sub-blocks although the design equivalence is preserved. this paper presents a novel compositional algorithm to verify sequential equivalence of large designs that are not only retimed but also optimized sequentially and combinationally. with a new notion of conditional equivalence in the presence of retiming, the proposed compositional algorithm performs hierarchical verification by checking whether each sub-block is conditionally equivalent, then checking whether the conditions are justified on their parent block by temporal equivalence. this is the first compositional algorithm handling both retiming and sequential optimizations hierarchically. the proposed approach is completely automatic and orthogonal to any existing sequential equivalence checker. the experimental results show that the proposed algorithm can handle large industrial designs that cannot be verified by the existing methods on sequential equivalence checking.','Logic and verification'
'','Logic and verification'
'','Logic and verification'
'concurrent separation logic provides a way of reasoning about the usage of resources in concurrent programs. proofs in the logic all track the transfer of ownership of portions of memory between concurrent processes, mirroring design principles for concurrent systems programs. this allows the safe treatment of \"daring\" concurrent programs, that access shared memory without explicit protection, outside of critical sections; canonical examples of such daring concurrency are resource managers of various kinds. in this talk i will describe the underpinnings of the concurrent separation logic, andallillustrate it with experimental tools -- smallfoot and space invader -- that are being developed to do automatic proofs with the logic.','Logic and verification'
'this talk gives an overview of meta-programming, with an emphasis on recent developments in extending existing specification and verification technology to meta-programs.','Logic and verification'
'','Logic and verification'
'','Logic and verification'
'','Logic and verification'
'','Logic and verification'
'chaotic mixing based encryption schemes for visual data are shown to be robust to lossy compression as long as the security requirements are not too high. this property facilitates the application of these ciphers in scenarios where lossy compression is applied to encrypted material &#8211; which is impossible in case traditional ciphers should be employed. if high security is required chaotic mixing loses its robustness to compression, still the lower computational demand may be an argument in favor of chaotic mixing as compared to traditional ciphers when visual data is to be encrypted.','Management and querying of encrypted data'
'security and privacy concerns as well as legal considerations force many companies to encrypt the sensitive data in databases. however, storing the data in an encrypted format entails non-negligible performance penalties while processing queries. in this paper, we address several design issues related to querying encrypted data in relational databases. based on our experiments, we propose new and efficient techniques to reduce the cost of cryptographic operations while processing different types of queries. our techniques enable us not only to overlap the cryptographic operations with the io latencies but also to reduce the number of block cipher operations with the help of selective decryption capabilities.','Management and querying of encrypted data'
'this paper describes the design of a censorship-resistant distributed file sharing protocol which has been implemented on top of gnunet, an anonymous, reputation-based network. we focus on the encoding layer of the gnunet file-sharing protocol which supports efficient dissemination of encrypted data as well as queries over encrypted data. the main idea advocated in this paper is that simple cryptographic techniques are sufficient to engineer an efficient data encoding that can make it significantly harder to selectively censor information. our encoding allows users to share files encrypted under descriptive keys which are the basis for querying the network for content. a key property of our encoding is that intermediaries can filter invalid encrypted replies without being able to decrypt the query or the reply. files are stored in small chunks which are distributed and replicated automatically by the gnunet infrastructure. additionally, data files may be stored in plaintext or encrypted form or as a combination of both and encrypted on demand.','Management and querying of encrypted data'
'randomness tests have been employed in encrypted data identification since encrypted data have high randomness. however, the evaluation of various randomness tests is absent. the nist statistical test suite is performed on encrypted and compressed data, which are derived from seven different types of files. and an index, differentiability, is introduced to indicate the ability of a test to identify the encrypted data. results show the effect of differentiability, and suggest that cumulative sums test have the best differentiability. furthermore, some compressed data are similar to encrypted data and the identification of them is a major point of future work.','Management and querying of encrypted data'
'we have designed a das (data as a service) model of e-commerce by using the semi-structured xml data file system instead of traditional database--the general b / s model, which mainly includes the clients&#8217; data encryption, key management on the server and xml encrypted data querying processing on the service provider. the searching method that we connected the xml index&#8216;s skill and deweyencoding [1,2] based on the level to achieve users&#8217; order query processing of xml encrypted data. this project has greatly reduced the system&#8217;s maintenance costs and improved the system&#8217;s capability and operating efficiency. at the same time it can help the enterprise to promote their benefits under the premise of guaranteeing the system&#8217;s security and stability.','Management and querying of encrypted data'
'we consider video sequences that have been encrypted uncompressed. since encryption masks the source, traditional data compression algorithms are rendered ineffective. however, it has been shown that through the use of distributed source-coding techniques, the compression of encrypted data is in fact possible. this means that it is possible to reduce data size without requiring that the data be compressed prior to encryption. indeed, under some reasonable conditions, neither security nor compression efficiency need be sacrificed when compression is performed on the encrypted data (johnson et al., 2004). in this paper we develop an algorithm for the practical lossless compression of encrypted gray scale video. our method is based on considering the temporal correlations in video. this move to temporal dependence builds on our previous work on memoryless sources, and one- and two-dimensional markov sources. for comparison, a motion-compensated lossless video encoder can compress each unencrypted frame of the standard \"foreman&#148; test video sequence by about 57\%. our algorithm can compress compress the same frames, after encryption, by about 33\%.','Management and querying of encrypted data'
'data security is a serious concern when we migrate data to a cloud dbms. database encryption, where sensitive columns are encrypted before they are stored in the cloud, has been proposed as a mechanism to address such data security concerns. the intuitive expectation is that an adversary cannot &#8220;learn&#8221; anything about the encrypted columns, since she does not have access to the encryption key. however, query processing becomes a challenge since it needs to &#8220;look inside&#8221; the data. this tutorial explores the space of designs studied in prior work on processing queries over encrypted data. we cover approaches based on both classic client-server and involving the use of a trusted hardware module where data can be securely decrypted. we discuss the privacy challenges that arise in both approaches and how they may be addressed. briefly, supporting the full complexity of a modern dbms including complex queries, transactions and stored procedures leads to significant challenges that we survey.','Management and querying of encrypted data'
'we propose and evaluate different methods to signal position and size of encrypted rois (regions of interest) in jpeg images. after discussing various design choices regarding the encoding of roi coordinates with a minimal amount of bits, we discuss both, existing and newly proposed approaches to signal the encoded coordinates inside jpeg images. by evaluating the different signalling methods on various data sets, we show that several of our proposed encoding methods outperform jbig in this special use case. furthermore, we show that one of our proposed signalling methods allows length-preserving lossless signalling, i.e., storing roi coordinates in a format-compliant way inside the jpeg images without quality loss or change of file size.','Management and querying of encrypted data'
'there has been considerable interest in querying encrypted data, allowing a &#8220;secure database server&#8221; model where the server does not know data values. this paper shows how results from cryptography prove the impossibility of developing a server that meets cryptographic-style definitions of security and is still efficient enough to be practical. the weaker definitions of security supported by previous secure database server proposals have the potential to reveal significant information. we propose a definition of a secure database server that provides probabilistic security guarantees, and sketch how a practical system meeting the definition could be built and proven secure. the primary goal of this paper is to provide a vision of how research in this area should proceed: efficient encrypted database and query processing with provable security properties.','Management and querying of encrypted data'
'','Management and querying of encrypted data'
'the theses of existonness, compoundness, and polyadness are proved. the consistency of these theses with the reversibility principle is founded. existential foundations of the composition paradigm are constructed.','Mathematical foundations of cryptography'
'','Mathematical foundations of cryptography'
'revolutionary developments which took place in the 1980\'s have transformed cryptography from a semi-scientific discipline to a respectable field in theoretical computer science. in particular, concepts such as computational indistinguishability, pseudorandomness and zero-knowledge interactive proofs were introduced and classical notions as secure encryption and unforgeable signatures were placed on sound grounds. the resulting field of cryptography, reviewed in this survey, is strongly linked to complexity theory (in contrast to \"classical\" cryptography which is strongly related to information theory.','Mathematical foundations of cryptography'
'','Mathematical foundations of cryptography'
'','Mathematical foundations of cryptography'
'','Mathematical foundations of cryptography'
'','Mathematical foundations of cryptography'
'','Mathematical foundations of cryptography'
'','Mathematical foundations of cryptography'
'we propose three theories, which can be regarded as attempts to characterize and establish guaranteed properties of wireless networks: (i) how and to what extent can we deliver packets with hard delay bounds? (ii) how and to what extent can we synchronize clocks in wireless networks? (iii) can we develop provably secure protocols for the entire life-cycle of wireless networks that also optimize a utility measure while operating in a hostile environment? for the first problem, consider an access point serving several clients over unreliable wireless links. suppose packets arrive for/from the clients, with each packet having a hard deadline, after which it is dropped. we characterize precisely the mix of delivery ratios, channel unreliabilities and hard deadline that the access point can guarantee, under some models. for the second problem, consider a wireless network where clocks at the nodes are linear, though with different rates (skews) and offsets. nodes can exchange packets with their neighbors, with direction dependent delays. we characterize precisely to what extent clocks can and cannot be synchronized and delays determined. under a random model the end-to-end error can be kept bounded irrespective of network size. concerning the third problem, traditionally, wireless protocols have been developed to provide performance. as attacks are identified, the protocols are fortified against the identified vulnerabilities. however, holistic guarantees are not provided against other attacks. we seek to reverse this paradigm. we propose a provable approach that guarantees the protocol suite is secure when the nodes are subject to certain assumptions. the protocols take a set of good nodes mingled with unknown malicious nodes from primordial birth to an operating network, while attaining min-max of a utility function. the maximization is over protocols announced and followed by the good nodes, and the minimization is over all behaviors of the malicious nodes. further, the malicious nodes are reduced to either cooperating or jamming. [joint work with vivek borkar, nikolaos freris, scott graham, i-hong hou, yih-chun hu, jonathan ponniah and roberto solis].','Mobile and wireless security'
'a configuration of unit cubes in three dimensions with integer coordinates is called an &lt;em&gt;animal&lt;/em&gt; if the boundary of their union is homeomorphic to a sphere. shermer discovered several animals from which no single cube may be removed such that the resulting configurations are also animals [14]. here we obtain a dual result: we give an example of an animal to which no cube may be added within its minimal bounding box such that the resulting configuration is also an animal. we also present two &lt;em&gt;o&lt;/em&gt;(&lt;em&gt;n&lt;/em&gt;)-time algorithms for determining whether a given configuration of &lt;em&gt;n&lt;/em&gt; unit cubes is an animal.','Penetration testing'
'','Penetration testing'
'with the publication of ieee 1574.4 guide for design, operation, and integration of distributed resource island systems with electric power systems, there is an increasing amount of attention on not only the design and operations of microgrids, but also on the proper operation and testing of these systems. this standard provides alternative approaches and good practices for the design, operation, and integration of microgrids. this includes the ability to separate from and reconnect to part of the utility grid while providing power to the islanded power system. this presentation addresses the industry need to develop standardized testing and evaluation procedures for microgrids in order to assure quality operation in the grid connected and islanded modes of operation.','Penetration testing'
'software testing is any activity aimed at evaluating an attribute or capability of a program or system and determining that it meets its required results. although crucial to software quality and widely deployed by programmers and testers, software testing still remains an art, due to limited understanding of the principles of software. the difficulty in software testing stems from the complexity of software: we can not completely test a program with moderate complexity. testing is more than just debugging. the purpose of testing can be quality assurance, verification and validation, or reliability estimation. testing can be used as a generic metric as well. correctness testing and reliability testing are two major areas of testing. software testing is a trade-off between budget, time and quality.','Penetration testing'
'','Penetration testing'
'a process is an important concept in modern software development. only when the activities are organized in process descriptions, can these be communicated, followed, observed, and improved. the basis for understanding what testing is, is therefore the understanding of the testing process. this paper presents the general concept of a process, and expands on this to present a suggestion for a generic testing process. for each of the activities in this generic process this paper presents suggestions for detailed activities, meant to serve as inspiration for further work on a truly generic testing process.','Penetration testing'
'let &#934; be a set of general boolean functions on n variables, such that each function depends on exactly k variables, and each variable can take a value from [1,d]. we say that &#934; is &#949;-far from satisfiable, if one must remove at least &#949;nk functions in order to make the set of remaining functions satisfiable. our main result is that if &#934; is &#949;-far from satisfiable, then most of the induced sets of functions, on sets of variables of size c(k,d)&#949;2, are not satisfiable, where c(k,d) depends only on k and d. using the above claim, we obtain similar results for k-sat and k-naeq-sat.assume we relax the decision problem of whether an instance of one of the above mentioned problems is satisfiable or not, to the problem of deciding whether an instance is satisfiable or &#949;-far from satisfiable. while the above decision problems are np-hard, our result implies that we can solve their relaxed versions, that is, distinguishing between satisfiable and &#949;-far from satisfiable instances, in randomized constant time.from the above result we obtain as a special case, previous results of alon and krivelevich [3] and of czumaj and sohler [8], concerning testing of graphs and hypergraphs colorability. we also discuss the problem of testing whether a graph g can be d-colored, such that it does not contain any copy of a colored graph from a fixed, given set of colored graphs.','Penetration testing'
'as a direct result of the awe-inspiring changes in lsi technology, an 8-bit microcomputer today has an equivalent of 8000 transistors-which for testing purposes are quite inaccessible, said tudor finch of bell labs in his introduction to session 8. finch, who chaired the session, went on to point out that this level of integration forces one to resort to more indirect techniques-e.g., techniques applying stimuli to input pins and observing the corresponding responses. the stimuliresponse behavior for a fault-free chip then characterizes its fault-free signaturere -iations indicate a failure condition. these deviations long with the stimuli test patterns can then be exhaustively catalogued for each and every specific fault condition to yield what is referred to as a \"fault-dictionary.\" the problem, observed finch, is that the sheer combinatorics of the situation yield literally astronomical numbers, thus excluding exhaustive characterizing techniques. this circumstance has motivated the development of algorithmic as well as heuristic techniques, which for specific fault-models given the correct behavior of a circuit automatically generates all fault-signatures. the relationship of test vectors or patterns required per gate versus circuit complexity was illustrated trated by t. finch as shown in the figure.','Penetration testing'
'','Penetration testing'
'many security managers face a dilemma, or at least think they do. their live networks are too important to risk damage and downtime through rigorous stress testing. but if they do not test the networks properly, they are unlikely to stand up under a real attack, which could prove far more damaging.','Penetration testing'
'how to protect sensible resources is an important issue in the development of web service applications. this paper presents a formal model for resource protections, aiming at statically analyzing and verifying that the applications use these resources in a valid manner, i.e., obeying all the protection policies. the policies are logical properties of resource usage behaviors. the usage behaviors are extracted from the execution of web services by a type and effect system, and represented as concurrent regular expressions. after a suitable transformation, the expressions can be checked for validity by model-checking tools. web service applications use the resources correctly if their concurrent regular expressions are verified valid. the analysis result shows our approach can improve system performances in comparison with runtime checkers, e.g., execution monitors.','Privacy protections'
'on the 21st december, 2001 australia introduced a new privacy law titled the privacy amendment private sector act, which extended the privacy act (1988) to apply to private sector companies. previous literature has examined the gap between self-regulation and legislation, but no research had investigated australia\'s transformation from self-regulation to legislation. the methodology used in this paper employs a triangulation method, which includes literature research into the background of privacy in australia and research into the privacy amendment (private sector) act 2000 itself, a longitudinal web site assessment and questionnaire. the study also outlines whether the new law brought into force at the end of 2001 meets government and consumer expectations in australia, and how this method of privacy protection compares with different privacy protection methods such as (i)self-regulation, found within the us, and (ii)legislation found within the eu.','Privacy protections'
'first page of the article','Privacy protections'
'this paper introduces yarra, a conservative extension to c to protect applications from non-control data attacks. yarra programmers specify their data integrity requirements by declaring critical data types and ascribing these critical types to important data structures. yarra guarantees that such critical data is only written through pointers with the given static type. any attempt to write to critical data through a pointer with an invalid type (perhaps because of a buffer overrun) is detected dynamically. we formalize yarra\'s semantics and prove the soundness of a program logic designed for use with the language. a key contribution is to show that yarra\'s semantics are strong enough to support sound local reasoning and the use of a frame rule, even across calls to unknown, unverified code. we evaluate a prototype implementation of a compiler and runtime system for yarra by using it to harden four common server applications against known non-control data vulnerabilities. we show that yarra defends against these attacks with only a negligible impact on their end-to-end performance.','Privacy protections'
'we present a recursive algorithm to update a kripke model so as to satisfy a formula of the computation-tree logic (ctl). recursive algorithms for model update face a difficulty: deleting (adding) transitions from (to) a kripke model to satisfy a universal (an existential) subformula may dissatisfy some existential (universal) subformulas. our method employs protected models to overcome this difficulty. we demonstrate our algorithm with a classical example of automatic synthesis described by emerson and clarke in 1982. from a dummy model, where the accessibility relation is the identity relation, our algorithm can efficiently generate a model to satisfy a specification of mutual exclusion in a variant of ctl. such a variant extends ctl with an operator that limits the out-degree of states. we compare our method with a generateand-test algorithm and outline a proof of soundness and completeness for our method.','Privacy protections'
'we propose several new methods to protect the scalar multiplication on an elliptic curve against differential analysis. the basic idea consists in transforming the curve through various random morphisms to provide a non-deterministic execution of the algorithm. the solutions we suggest complement and improve the state-of-the-art, but also provide a practical toolbox of efficient countermeasures. these should suit most of the needs for protecting implementations of crypto-algorithms based on elliptic curves.','Privacy protections'
'the paper reveals the solution for the protection of inputs and outputs of embedded systems and their mathematical description. these are the problems of galvanic separation, restrictions disturbing voltage, limit reduction of signals and signal verification of loaded value. they dealt galvanic isolators and limiters limit on another principle. the work compares different methods of input and output. another part is a summary of methods for operations related to the evaluation of the accuracy of the capture inputs before further processing. other parts of the thesis is a mathematical description of the behavior of protection of inputs and outputs. to find the causes of nonlinearity limit limiters and analog galvanic isolators. the linear optocouplers are designed to the partial non-linearity caused by the method used and participation components. the research work was performed to financial support of grant reg. no iga/32/fai/11/d.','Privacy protections'
'photo sharing has become a popular feature of many online social networking sites. many of the photo sharing applications on these sites, allow users to annotate photos with those who are in them. a number of researchers have examined the social uses and privacy issues of online photo sharing sites, but few have explored the privacy issues of photo sharing in social networks. in this paper, we begin by examining some of our findings from a series of focus groups on photo privacy in the social networking domain. we then devise a new mechanism to enhance photo privacy based on these findings.','Privacy protections'
'the number of web threats increased in large measure in the last few years. it is not related to the pc based threats only, new operating systems of handy devices are in danger as well. in this paper testing methods of web threat protections are discussed. a unique anti-malware testing procedure has been developed under the aegis of checkvir lab. this testing procedure can provide actual comparative test results of anti-malware solutions automatically for the it user community on the web and in addition other manually or semi-automatically executed tests can provide more details about the knowledge of the tested products as well. these methods can provide results soon after the new version of a particular version of an anti-malware solution is released. the real-time automatic testing is based on a set of dedicated pcs continuously checking the possible updates and they are dealing with executing the predefined testing jobs.','Privacy protections'
'the sensitivities of the bits of low weight codewords inturbo codes to noises are discussed firstly and theresult is that the lower weight the higher sensitivity.theoretical analysis shows that through the protectionson such key-bits which have high sensitivities; theminimal weight of the codes can be improvedapparently. the simulations also give out the supportedresults: the ber could be highly improved at higheb/n0 for code rates both 1/3 and 1/2.','Privacy protections'
'quantum cryptography offers a promising unbreakable cryptographic solution as it ensures perfect secrecy in applications such as quantum key distribution and bit commitment. the focus of this paper is to trace the development of quantum key distribution protocols and discuss the state of the art and open issues in this field. although the protocols have mathematically proven to be totally secure, it must be emphasized that any real world implementation suffers the limitations of physical devices which can serve as vulnerability for an eavesdropper to exploit. in this paper, we survey the quantum key distribution protocols which exist in literature and explore the attacks to which they are vulnerable. we highlight the main implementation bottlenecks related with each implementation and the solutions proposed thereof.','Privacy-preserving protocols'
'private information retrieval (pir) protocols allow users to learn data items stored at a server which is not fully trusted, without disclosing to the server the particular data element retrieved. several pir protocols have been proposed, which provide strong guarantees on user privacy. nevertheless, in many application scenarios it is important to protect the database as well. in this paper, we investigate the amount of data disclosed by the the most prominent pir protocols during a single run. we show that a malicious user can stage attacks that allow an excessive amount of data to be retrieved from the server. furthermore, this vulnerability can be exploited even if the client follows the legitimate steps of the pir protocol, hence the malicious request can not be detected and rejected by the server. we devise mechanisms that limit the pir disclosure to a single data item.','Privacy-preserving protocols'
'privacy-preserving reconciliation protocols on ordered sets are protocols that solve a particular subproblem of secure multiparty computation. here, each party holds a private input set of equal size in which the elements are ordered according to the party\'s preferences. the goal of a reconciliation protocol on these ordered sets is then to find all common elements in the parties\' input sets that maximize the joint preferences of the parties. in this paper, we present two main contributions that improve on the current state of the art. first, we propose two new protocols for privacy-preserving reconciliation and prove their correctness and security properties. we implement and evaluate our protocols as well as two previously published multi-party reconciliation protocols. our implementation is the first practical solution to reconciliation problems in the multi-party setting. our comparison shows that our new protocols outperform the original protocols. the basic optimization idea is to reduce the highest degree polynomial in the protocol design. second, we generalize privacy-preserving reconciliation protocols, i. e., relaxing the input constraint from totally ordered input sets of equal size to pre-ordered input sets of arbitrary size.','Privacy-preserving protocols'
'it is often the case in applications of cryptographic protocols that one party would like to determine a practical upper-bound on the physical distance to the other party. for instance, when a person conducts a cryptographic identification protocol at an entrance to a building, the access control computer in the building would like to be ensured that the person giving the responses is no more than a few meters away. the \"distance bounding\" technique we introduce solves this problem by timing the delay between sending out a challenge bit and receiving back the corresponding response bit. it can be integrated into common identification protocols. the technique can also be applied in the three-party setting of \"wallets with observers\" in such a way that the intermediary party can prevent the other two from exchanging information, or even developing common coinflips.','Privacy-preserving protocols'
'in the article problem of securing distributed biometric authentication system was discussed. after introduction to biometric domain, the key issues connected with secure communication in distributed authentication environments were briefly described. in this work a concept of enhancement of biometric authentication system by application of quantum protocols was formulated. two quantum protocols were analyzed. obtained experimental results enable further development of research work in chosen area.','Privacy-preserving protocols'
'wireless sensor networks are increasingly being integrated into the modern automobile with the intention of improving safety and reducing costs. however, it has been shown that the first widely-deployed and mandatory in-car sensor networks --tire pressure monitoring systems (tpmss) -- employ no cryptographic algorithms for protecting their wireless communication, incurring serious security and privacy risks for consumers. in this paper, we design and evaluate cost-effective secure communication protocols that can resolve the privacy and security vulnerabilities of existing tpmss as well as other forthcoming in-car wireless sensor networks. our implementation on arduino platforms shows that the proposed protocol incurs modest overhead and is applicable to resource-constrained embedded systems.','Privacy-preserving protocols'
'we address the problem of data linkage and data extraction across database tables of sensitive information about individuals, in an environment of constraints on organisations\' ability to share data and a need to protect individuals\' privacy and confidentiality. we propose several privacy-preserving data linkage and data extraction protocols. our first protocol enables data linkage across separate database tables, without requiring any identifying information to be revealed to any party outside the originating data source. our second protocol enables the extraction of a cohort of individuals\' data from a data source, without revealing the membership of any individual in that cohort to the data source. we describe a variation of the first protocol which enables data sources to generate common pseudonyms without revealing any identifying information to any party, and show how the protocols are applicable for any number of data sources.','Privacy-preserving protocols'
'distance-bounding protocols prevent man-in-the-middle attacks by measuring response times. the four attacks such protocols typically address, recently formalized in [10], are: (1) mafia fraud, where the adversary must impersonate to a verifier in the presence of an honest prover; (2) terrorist fraud, where the adversary gets some offline prover support to impersonate; (3) distance fraud, where provers claim to be closer to verifiers than they really are; and (4) impersonations, where adversaries impersonate provers during lazy phases. durholz et al. [10] also formally analyzed the security of (an enhancement of) the kim-avoine protocol [14]. in this paper we quantify the security of the following well-known distance-bounding protocols: hancke and kuhn [13], reid et al. [16], the swiss-knife protocol [15], and the very recent proposal of yang et al. [17]. concretely, our main results show that (1) the usual terrorist-fraud countermeasure of relating responses to a long-term secret key may enable socalled key-learning mafia fraud attacks, where the adversary flips a single time-critical response to learn a key bit-by-bit; (2) though relating responses may allow mafia fraud, it sometimes enforces distance-fraud resistance by thwarting the attack of boureanu et al. [5]; (3) none of the three allegedly terrorist-fraud resistant protocols, i.e. [15, 16, 17], is in fact terrorist fraud resistant; for the former two schemes this is a matter of syntax, attacks exploiting the strong formalization of [10]; the attack against the latter protocol of [17], however, is almost trivial; (4) unless key-update is done regardless of protocol completion, the protocol of yang et al. is vulnerable to denial-of-service attacks. in light of our results, we also review definitions of terrorist fraud, arguing that, while the strong model in [10] may be at the moment more appropriate than mere intuition, it could be too strong to capture terrorist attacks.','Privacy-preserving protocols'
'attacks against encrypted protocols have become increasingly popular and sophisticated. such attacks are often undetectable by the traditional intrusion detection systems (idss). additionally, the encrypted attack-traffic makes tracing the source of the attack substantially more difficult. in this paper, we address these issues and devise a mechanism to trace back attackers against encrypted protocols. in our efforts to combat attacks against cryptographic protocols, we have integrated a traceback mechanism at the monitoring stubs (mss), which were introduced in one of our previous works. while we previously focused on strategically placing monitoring stubs to detect attacks against encrypted protocols, in this work we aim at equipping mss with a traceback feature. in our approach, when a given ms detects an attack, it starts tracing back to the root of the attack. the traceback mechanism relies on monitoring the extracted features at different mss, i.e., in different points of the target network. at each ms, the monitored features over time provide a pattern which is compared or correlated with the monitored patterns at the neighboring mss. a high correlation value in the patterns observed by two adjacent mss indicates that the attack traffic propagated through the network elements covered by these mss. based on these correlation values and a prior knowledge of the network topology, the system can then construct a path back to the attacking hosts. the effectiveness of the proposed traceback scheme is verified by simulations.','Privacy-preserving protocols'
'security protocols are indispensable in secure communication. we give an operational semantics of security protocols in terms of a prolog-like language. with this semantics, we can uncover attacks on a security protocol that are possible with no more than a given number of rounds. though our approach is exhaustive testing, the majority of fruitless search is cut off by selecting a small number of representative values that could be sent by an attacker. hence, the number of scenarios is relatively small and our method is quite practical. furthermore, our method not only reports possible attacks but also describes the attacks in great detail. this description would be very helpful to protocol designers and analyzers.','Privacy-preserving protocols'
'human artists typically have a personal signature, by which their individual authorship can be recognized. modernist artists tried to avoid such idiosyncracies, focussing on abstract structure instead-and welcomed computers, accordingly. but even those computer artists who have deliberately tried to lose their signature have not managed to do so. perhaps evolutionary methods might help? reasons are discussed both for believing and for doubting that evolutionary art could be wholly free from personal signatures.','Pseudonymity, anonymity and untraceability'
'a method of integrating user authentication with anonymity and untraceability is presented based on the secret-key certificate and the algebraic structure of error-correcting codes. authentication protocol proposed here provides a means for the authentication server to avoid the requirement of maintaining a secure database of user secrets. especially, since the proposed protocol uses a symmetric-key cryptography and eliminates the key management problem, it is efficient and convenient for both the hardware-limited users and the authentication server.','Pseudonymity, anonymity and untraceability'
'','Pseudonymity, anonymity and untraceability'
'there are numerous works on the privacy and the security problems for rfid systems. however, many of them have failed due to the lack of formal security proof. in the literature, there are a few formal models that consider forward untraceability. in asiacrypt 2007, vaudenay presented an new security and privacy model for rfid that combines early models to more understandable one. in this paper, we revisit vaudenay\'s model and modify it by considering the notion of forward untraceability. our modification considers all message flows between rfid reader and tags before and after compromising secrets of tag. we analyze some rfid schemes claiming to provide forward untraceability and resistance to server impersonation. for each scheme, we exhibit attacks in which a strong adversary can trace the future interactions of the tag and impersonate the valid server to the tag. further, we show that a previously proposed attack claiming to violate forward untraceability of an existing rfid scheme does not violate forward untraceability.','Pseudonymity, anonymity and untraceability'
'authentication and key agreement protocols are the essential guardians of the distributed applications. they help the servers and users establish mutual trust and create secure communication channels. in this paper, we propose an authentication and key agreement scheme that is secure and has low communication and computation costs. besides its efficiency, the most significant feature of the scheme is to provide initiator untraceability which completely conceals the users\' identities from all eavesdropping adversaries. the scheme is suitable to be used in mobile services and e-commerce applications due to its low costs on communication and computation as well as its untraceability feature.','Pseudonymity, anonymity and untraceability'
'in this paper, we study the concept of privacy-preserving multi-service subscription systems. with such system, service providers can propose to their customers, by the way of a subscription, several distinct services that users can access while being anonymous. we moreover study how users can be untraceable w.r.t. the service provider during the subscription process, in such a way that it is additionally possible to make profiling on the users\' customs. this permits the service provider to propose some advertisements to users while protecting the privacy of the latter, even this may be seen as contradictory. we also propose concrete instantiations, based on signature schemes with extensions from camenisch and lysyanskaya.','Pseudonymity, anonymity and untraceability'
'','Pseudonymity, anonymity and untraceability'
'this paper presents a generic and simple transformation that adds traceability to an anonymous encryption scheme. we focus on the case of honest senders, which finds applications in many real-life scenarios. advantageously, our transformation can be applied to already deployed public-key infrastructures. two concrete implementations are provided.','Public key encryption'
'one of needham and schroeder\'s proposed signature authentication protocols is shown to fail when there is a possibility of compromised keys: this invalidates one of the applications of their technique. a more elaborate mechanism is proposed which does not require a network clock, but does require a third party to the transaction. the latter approach is shown to be reliable in a fairly strong sense.','Public key encryption'
'','Public key encryption'
'in certificateless public key encryption (cl-pke), the private key generator (pkg) keeps a master secret key to generate a partial private key corresponding to a user\'s identity. together with a secret value generated by the user, a full private key can be constructed for decryption. traditional security model for cl-pke assumes that (i) both the master secret key of the pkg and the full private key of the user under attack are in absolute secrecy; and (ii) the attacker can only obtain either the target user\'s secret value without any partial knowledge of the partial private key or vice versa. however, the advancement of practical side-channel attacks enable attackers to obtain partial information of both keys easily, making the above assumption invalid. in this paper, we give the first leakage-resilient cl-pke. we consider different leakage conditions for type i (third party attackers) and type ii (honest-but-curious pkg) attackers, following the classification in traditional cl-pke. we give a concrete construction in the composite order bilinear group. we prove the security of our scheme in the standard model, overcoming some technical difficulties in the security proofs for both type i and type ii attackers of cl-pke.','Public key encryption'
'lossy trapdoor functions (ltfs) was introduced by peikert and waters in 2008. the importance of the ltfs was justified by their numerous cryptographic applications, like the construction of injective one-way trapdoor functions, cca-secure public-key encryption, etc. however, little research on application of ltfs to key-leakage resilient public-key encryption was done. in this article we introduce a new variant of ltfs featuring leakage-resilience, namely lrltfs and give a realization of lrltfs with leakage rate 1/&#920;(&#954;) (where &#954; is the security parameter) under the decisional diffie-hellman (ddh) assumption. we further improve the leakage rate to 1-o(1) over a composite-order group in which the decisional composite residuosity (dcr) assumption holds. we also introduce a new notion of key-leakage attacks, which we call weak key-leakage attacks, for bridging the adaptive and non-adaptive key-leakage attacks in the setting of public-key cryptosystem. in this model, the leakage adversary only gets a part of public key before accessing to a leakage oracle. we show that lrltfs imply public-key encryption schemes secure against chosen-ciphertext weak key-leakage attacks in a black-box sense.','Public key encryption'
'','Public key encryption'
'in this talk, we discuss properties of public key encryption schemes which are derived from group signatures. abdalla and warinschi (icics 2004) and ohtake et al. (africacrypt 2009) already showed that it is possible to construct a chosen-ciphertext secure public key encryption from an arbitrary group signature scheme in a black-box manner. by extending these results, we further show that if the underlying group signature scheme has some special property, then its converted public key encryption scheme also yields a special property which is inherited from the underlying group signature.','Public key encryption'
'a timed-release cryptosystem allows a sender to encrypt a message so that only the intended recipient can read it only after a specified time. we formalize the concept of a secure timed-release public-key cryptosystem and show that, if a third party is relied upon to guarantee decryption after the specified date, this concept is equivalent to identity-based encryption; this explains the observation that all known constructions use identity-based encryption to achieve timed-release security. we then give several provably-secure constructions of timed-release encryption: a generic scheme based on any identity-based encryption scheme, and two more efficient schemes based on the existence of cryptographically admissible bilinear mappings. the first of these is essentially as efficient as the boneh-franklin identity-based encryption scheme, and is provably secure and authenticated in the random oracle model; the final scheme is not authenticated but is provably secure in the standard model (i.e., without random oracles).','Public key encryption'
'the rsa method is used for the interchange of secret messages via insecure channels. it is elegant in theory and fast and reliable in practice. applications are in the field of communication networks.the method is initialized by choosing some suitable large prime numbers. encrypting and decrypting of a message are done by modular arithmetic. the modulus is a large integer (e.g. 200 decimal digits). any attempt to break the system amounts to factoring the modulus.the basic operations that are needed for an implementation are fundamental for any computer algebra package, e.g. sac-2 offers them in an suitable way. this project therefore requires only little programming, but some insight into theory and the ability to find and put together existing components. an optional part of the project deals with the security of the method.','Public key encryption'
'this paper describes some recently successful results in the cmos vlsi implementation of public-key data encryption algorithms. architectural details, circuits, and prototype test results are presented for rsa encryption and multiplication in the finite field gf(2m). these designs emphasize high throughput and modularity. an asynchronous modulo multiplier is described which permits a significant improvement in rsa encryption throughput relative to previously described synchronous implementations.','Public key encryption'
'in the article problem of securing distributed biometric authentication system was discussed. after introduction to biometric domain, the key issues connected with secure communication in distributed authentication environments were briefly described. in this work a concept of enhancement of biometric authentication system by application of quantum protocols was formulated. two quantum protocols were analyzed. obtained experimental results enable further development of research work in chosen area.','Security protocols'
'attacks against encrypted protocols have become increasingly popular and sophisticated. such attacks are often undetectable by the traditional intrusion detection systems (idss). additionally, the encrypted attack-traffic makes tracing the source of the attack substantially more difficult. in this paper, we address these issues and devise a mechanism to trace back attackers against encrypted protocols. in our efforts to combat attacks against cryptographic protocols, we have integrated a traceback mechanism at the monitoring stubs (mss), which were introduced in one of our previous works. while we previously focused on strategically placing monitoring stubs to detect attacks against encrypted protocols, in this work we aim at equipping mss with a traceback feature. in our approach, when a given ms detects an attack, it starts tracing back to the root of the attack. the traceback mechanism relies on monitoring the extracted features at different mss, i.e., in different points of the target network. at each ms, the monitored features over time provide a pattern which is compared or correlated with the monitored patterns at the neighboring mss. a high correlation value in the patterns observed by two adjacent mss indicates that the attack traffic propagated through the network elements covered by these mss. based on these correlation values and a prior knowledge of the network topology, the system can then construct a path back to the attacking hosts. the effectiveness of the proposed traceback scheme is verified by simulations.','Security protocols'
'','Security requirements'
'security and efficiency of rekeying are crucialrequirements for multicast key management. however,the two requirements pull in different directions andbalancing them to meet the application needs is still anopen issue. in this paper we introduce a hybrid keytree scheme to balance security, namely the resistanceto collusion, and the efficiency. the resistance tocollusion is measured by an integer parameter. thecommunication and the storage requirements for thecontroller depend on this parameter too, and theydecrease as the resistance to collusion is relaxed. weanalytically evaluate the efficiency of our scheme andcompare with the previous work. the results show that ourscheme allows a fine-tuning of security requirements versusefficiency requirements at run-time, that is not possible withthe previous key management schemes.','Security requirements'
'as technology continues to evolve, so do different entities that threaten the security of this technology. we believe that in order to build dependable software security should be treated just as any other important aspect of a system; to do this we must emphasize it at the beginning of our development cycle and be able to carry these security requirements down the cycle. we focus on a technique known as the common criteria, which allows for the development of security requirements. we extend the capabilities of common criteria beyond the requirements phase, to allow us to take security requirements into further stages of the cycle. in this paper we describe ccarch, a technique accompanied by a set of tools, that takes common criteria expressed security requirements to the architectural level. our approach aids in making the usage of common criteria more beneficial and applicable.','Security requirements'
'text-based passwords are still the most commonly used authentication mechanism in information systems. we took advantage of a unique opportunity presented by a significant change in the carnegie mellon university (cmu) computing services password policy that required users to change their passwords. through our survey of 470 cmu computer users, we collected data about behaviors and practices related to the use and creation of passwords. we also captured users\' opinions about the new, stronger policy requirements. our analysis shows that, although most of the users were annoyed by the need to create a complex password, they believe that they are now more secure. furthermore, we perform an entropy analysis and discuss how our findings relate to nist recommendations for creating a password policy. we also examine how users answer specific questions related to their passwords. our results can be helpful in designing better password policies that consider not only technical aspects of specific policy rules, but also users\' behavior in response to those rules.','Security requirements'
'','Security requirements'
'managing the development of software requirements can be a complex and difficult task. the environment is often chaotic. as analysts and customers leave the project, they are replaced by others who drive development in new directions. as a result, inconsistencies arise. newer requirements introduce inconsistencies with older requirements. the introduction of such requirements inconsistencies may violate stated goals of development. in this article, techniques are presented that manage requirements document inconsistency by managing inconsistencies that arise between requirement development goals and requirements development enactment. a specialized development model, called a requirements dialog meta-model, is presented. this meta-model defines a conceptual framework for dialog goal definition, monitoring, and in the case of goal failure, dialog goal reestablishment. the requirements dialog meta-model is supported in an automated multiuser world wide web environment, called dealscribe. an exploratory case study of its use is reported. this research supports the conclusions that: 1) an automated tool that supports the dialog meta-model can automate the monitoring and reestablishment of formal development goals, 2) development goal monitoring can be used to determine statements of a development dialog that fail to satisfy development goals, and 3) development goal monitoring can be used to manage inconsistencies in a developing requirements document. the application of dealscribe demonstrates that a dialog meta-model can enable a powerful environment for managing development and document inconsistencies.','Security requirements'
'modern enterprise systems have to comply to regulations such as basel iii resulting in complex security requirements. these requirements need to be modeled at design-time and enforced at runtime. moreover, modern enterprise systems are often business-process driven, i.e., the system behavior is described as high-level business processes that are executed by a business process execution engine. consequently, there is a need for an integrated and tool-supported methodology that allows for specifying and enforcing compliance and security requirements for business process-driven enterprise systems. in this paper, we present a tool chain supporting both the design-time modeling as well as the run-time enforcement of security requirements for business process-driven systems.','Security requirements'
'','Security requirements'
'','Security requirements'
'security and privacy, accountability and anonymity, transparency and unobservability: these terms and more are vital elements for defining the overall security requirements---and, thus, security measurability criteria---of systems. however, these distinct yet related concepts are often substituted for one another in our discussions on securing trustworthy systems and services. this is damaging since it leads to imprecise security and trust requirements. consequently, this results in poorly defined metrics for evaluating system security. this paper proposes a trust-terms ontology, which maps out and defines the various components and concepts that comprise ict security and trust. we can use this ontology tool to gain a better understanding of their trust and security requirements and, hence, to identify more precise measurability criteria.','Security requirements'
'npo is one of the most important sectors in national economy, which has made great contribution to economic development. perfect incentive mechanism on npo performance management is the important guarantee of improving performance management level. however, present npo incentive mechanism has the problems of npo material incentive to employees is insufficient, npo inner motivation to employees is inadequate and npo tax incentive is imperfect, which hidden the healthy development of npo in a certain degree. put forward suggestions of trying hard to improve npo income, increasing inner incentive to npo employees and perfecting tax incentive to npo on the basis of theoretical and npo incentive mechanism analysis.','Side-channel analysis and countermeasures'
'recently, major portal sites are suffering from a number of attacks and it is growing exponentially. july 2009, there has been system failure on government sites and some of the major portal sites due to the ddos (distributed denial of service) attack. moreover, portal sites are exploited by a cross-site scripting vulnerability in 2010. to solve these problems, each portal site made an effort to eliminate the security vulnerability of the website and to protect personal information such as id and password. however, portal sites still have the security vulnerabilities against arp (address resolution protocol) poisoning attack and the certificate spoofing attack. in this paper, we show the results of our penetration test and present the countermeasures on the arp (address resolution protocol) poisoning attack and the certificate spoofing attack.','Side-channel analysis and countermeasures'
'with the rapid development of chinese economy, flight delay becomes more and more serious and brings a lot of complaints and emergency situations. the present situation of flight delay in our country and various influencing factors of flight delay is analyzed. related statistic characteristics of flight delay are given by mathematical analysis and computation. some measures to reduce flight delay are put forward from an integrated and systematic viewpoint, which is related to air traffic control departments, airlines and airports.','Side-channel analysis and countermeasures'
'environmental logistics is the new mode of cycle logistics, which promotes the healthy development of the economy and consumer life. it changes the original one-way function relationship between economic development and logistics or consumption life and logistics. it also reduces the damage logistics brings to the environment. it is an important aspect in the economic development of low-carbon cycle. at present, environmental logistics is regarded as the development focus of modern logistics industry throughout the world. therefore the corresponding environmental logistics policies and regulations are introduced to lay the foundation for sustainable development of environmental logistics. in china, it has also gradually aroused the wide concern of the industry, and is considered the important trend of china\'s future logistics industry development, but it is still in its infancy and faces many bottlenecks in the development constraints. this article analyzes the promotion difficulties of environmental logistics and tries to find out the way to break through.','Side-channel analysis and countermeasures'
'in recent research it turned out that boolean verification of digital signatures in the context of ws-security is likely to fail: if parts of a soap message are signed and the signature verification applied to the whole document returns true, then nevertheless the document may have been significantly altered.in this paper, we provide a detailed analysis on the possible scenarios that enable these signature wrapping attacks. derived from this analysis, we propose a new solution that uses a subset of xpath instead of id attributes to point to the signed subtree, and show that this solution is both efficient and secure.','Side-channel analysis and countermeasures'
'smes ??small and medium enterprises?? have become an important force in promoting the national economy, but in the business of crisis and instability in the international situation as a whole case of stage financing has become the most important issues to smes. for this purpose, the authors analyzed the status of financing for the sustainable development of smes, and gave the corresponding countermeasures and suggestions.','Side-channel analysis and countermeasures'
'in the engineering implementation of ason control plane, rsvp_te protocol is widely used. based on the specific analysis of reliability mechanisms of rsvp_te protocol, security problems that rsvp_te protocol may confront with are discussed, and the corresponding countermeasures are proposed in the paper.','Side-channel analysis and countermeasures'
'nowadays, the music industry is moving from analog to digital. most of the big portal sites provide commercial online music streaming services. in this paper, we analyzed the security of the korean commercial online music streaming services which are provided by the korea\'s major portal sites(dosirak, cyworld, and naver). we will show attacks on commercial online music streaming services that lead to an infringement of the copyright and we will also propose countermeasures for online commercial music streaming services.','Side-channel analysis and countermeasures'
'in this paper, we address issues related to the modeling, analysis, and countermeasures of worm attacks on the internet. most previous work assumed that a worm always propagates itself at the highest possible speed. some newly developed worms (e.g., &#8220;atak&#8221; worm) contradict this assumption by deliberately reducing the propagation speed in order to avoid detection. as such, we study a new class of worms, referred to as self-disciplinary worms. these worms adapt their propagation patterns in order to reduce the probability of detection, and eventually, to infect more computers. we demonstrate that existing worm detection schemes based on traffic volume and variance cannot effectively defend against these self-disciplinary worms. to develop proper countermeasures, we introduce a game-theoretic formulation to model the interaction between the worm propagator and the defender. we show that an effective integration of multiple countermeasure schemes (e.g., worm detection and forensics analysis) is critical for defending against self-disciplinary worms. we propose different integrated schemes for fighting different self-disciplinary worms, and evaluate their performance via real-world traffic data.','Side-channel analysis and countermeasures'
'in order to improve the present situation of university intellectual property rights operating management and to enhance the scientific research innovation ability, this paper analyzes the present situation and the existing problems of university intellectual property rights operating management. based on it some measures are put forward such as improving the leaderships\' understanding of the work of intellectual property rights, doing university technology transfer works well, strengthening system construction of intellectual property management and establishing patent special funds. these measures\'s implementations ensure the high efficiency of the university intellectual property operating management.','Side-channel analysis and countermeasures'
'social web has changed the concept of leisure time. as a result street neighbors have been replaced by e-neighbors and walls have become e-walls to share ideas and gossips. despite so many advantages we cannot ignore potential threats to user privacy and security. in order to be extremely usable, such systems should have strict security and privacy policies in place. in this paper the authors focus on \"facebook\" to understand privacy and security problems by carrying out a web based survey. based on the findings from empirical data the authors propose different enhancements for the improvement of user privacy and potential threats to user account security.','Social aspects of security and privacy'
'the lack of trust is identified as the key concern for consumers in the ecommerce environment. service providers attempt to address this concern by implementing public key infrastructure (pki) systems for online security and privacy and to enhance user confidence. much research has focused on the technical implementation of online security and privacy systems. this paper discusses social and cultural influence as critical elements of a trusted online service environment. it suggests a mechanism for enhancing trust in e-commerce that takes account of these influences.','Social aspects of security and privacy'
'the globalisation of biotechnology brings not only new economic prospects but also new risks. the development of international bio-safety guidelines is essential.','Social aspects of security and privacy'
'user interaction for interactive tv (itv) services becomes a critical aspect in the design of new itv and internet protocol tv (iptv) offers. new services like social tv, direct image and data up- and download from the set-top box, connectivity of the itv system to the pc and other mobile media devices, pose the question on how to support security, privacy, and personalization. to investigate the (sometimes na&#239;ve) concepts of users of what changes an interactive tv system will bring in terms of security and privacy for their living room behaviors, and how to best support these aspects in the user interaction, an ethnographic study with 40 households (126 participants) was conducted. we investigated users\' assumptions and ideas on the concepts of security, privacy and personalization using a combination of playful probing and an interview at the start and the end of the study. findings show that households are not aware of possible problems in terms of security and privacy, before the introduction of interactive tv. this work presents user generated ideas on how to improve security aspects on the tv, user\'s perception on identification mechanisms for tv services, and summarizes ethnographic insights found in the study.','Social aspects of security and privacy'
'','Social aspects of security and privacy'
'observance of laws on the protection of minors is a serious problem at places with internet access. we examine protection mechanisms in view of the internet\'s open and dynamic nature and claim that those based on deterrence and restoration should be preferred to purely preventive ones. we then present a strategy adequate for secondary schools. the technical part relies on adaptable filter and logging components, which check and log requested web-pages. in a feed-back loop a human auditor provides more information to the system, thus increasing the effectiveness. on the other hand, separation of powers among auditors ensures the users\' privacy.','Social aspects of security and privacy'
'radio frequency identification &#40;rfid&#41; has recently received a lot of attention as an augmentation technology in manufacturing, scm, and retail inventory control. however, widespread deployment of rfid tags may create new threats to security and privacy of individuals and organisations. this paper gives an overview of all types of rfid privacy and security problems and its countermeasures.','Social aspects of security and privacy'
'smart cards are replacing traditional magnetic cards for payment transactions. one of the main reasons is enhanced security capabilities that can be built in a smart card. with the high popularity of web technology, there is a trend towards smart cards being used as an electronic wallet for payment transactions on internet. most of the related work of smart card payment transactions concentrates only on the security aspects of hardware/firmware, encryption method and key management, or they only propose the online shopping protocol for uni-directional payment transaction based on the scenery of exact payment from the customer to merchant during business activity.we developed a prototype system called \"smartflow\" to demonstrate these kinds of business activities on internet by smart card. the main focus of this paper is to present the framework of \"smartflow\" and some important security and privacy issues for bi-directional payment transaction with change among more than two parties involved business activity. further, we present the application of downloading software license key from internet into smart card in smartflow environment. we have already implemented a prototype \"smartflow\" system with these functionalities.','Social aspects of security and privacy'
'','Social aspects of security and privacy'
'','Social aspects of security and privacy'
'the real and growing threats to critical infrastructure demand that it professionals remain vigilant.','Social network security and privacy'
'as the sophistication of wiretapping technology grows, so too do the risks it poses to our privacy and security.','Social network security and privacy'
'','Social network security and privacy'
'','Social network security and privacy'
'we present a practical scheme for internet-scale collaborative analysis of information security threats which provides strong privacy guarantees to contributors of alerts. wide-area analysis centers are proving a valuable early warning service against worms, viruses, and other malicious activities. at the same time, protecting individual and organizational privacy is no longer optional in today\'s business climate. we propose a set of data sanitization techniques and correlation, while maintaining privacy for alert contributors. our approach is practical, scalable, does not rely on trusted third parties or secure multiparty computation schemes, and does not require sophisticated schemes, and does not require sophisticated key management.','Social network security and privacy'
'death is an uncomfortable subject for many people, and digital systems are rarely designed to deal with this event. in particular, the wide array of existing digital authentication infrastructure rarely deals with gracefully retiring credentials in a uniform fashion. this research paper highlights an emerging paradigm: gracefully dealing with expired digital identities in a secure, privacy-preserving fashion. it examines the confluence of modern browser technology, cloud services, and human factors involved in managing a person\'s digital footprint while they live and retiring it when they die. we contemplate a potential approach to dealing with credentials after death by using cloud computing. we consider the reasons that such an approach may actually provide an opportunity for enhancing authentication security by frustrating identity stealing attacks. we note that this paper is not aimed at trivializing the real grief and loss that people feel, but rather an attempt to understand how security and privacy concerns are shaped by the end of life, with the ultimate goal of easing this transition for friends and family.','Social network security and privacy'
'','Social network security and privacy'
'','Social network security and privacy'
'as the internet is used to a greater extent in business, issues of protection and privacy will have more importance. users and organizations must have the ability to control reads and writes to network accessible information, they must be assured of the integrity and confidentiality of the information accessed over the net, and they must have a means to determine the security, competence, and honesty of the commercial service providers with which they interact. they must also be able to pay for purchases made on the network, and they should be free from excessive monitoring of their activities. this paper discusses characteristics of the internet that make it difficult to provide such assurances and surveys some of the techniques that can used to protect users of the network','Social network security and privacy'
'this paper describes a collaborative structured demonstration of reverse engineering tools that was presented at a working session at wcre 2001 in stuttgart, germany. a structured demonstration is a hybrid tool evaluation technique that combines elements from experiments, case studies, technology demonstrations, and benchmarking. the essence of the technique is to facilitate learning about software engineering tools using a common set of tasks. the collaborative experience discussed at wcre involved several peer and complementary technologies that were applied in concert to solve a real life reverse engineering problem. for the most part, the tool developers themselves applied their own tools to this problem. preliminary results have shown to the research community that we still have much to learn about our tools and how they can be applied as part of a reverse engineering and reengineering process. consequently, the participants agreed to continue participation in this demonstration beyond the wcre event.','Software reverse engineering'
'this paper presents a process for the reengineering of computer-based control systems, and describes tools that automate portions of the process. the intermediate representation (ir) for capturing features of computer-based systems during reverse engineering is presented. a novel feature of the ir is that it incorporates the control system software architecture, a view that enables information to be captured at five levels of granularity: the program level, the task level, the package level, the subprogram level, and the statement level. a reverse engineering toolset that constructs the ir from ada programs, displays the ir, and computes metrics for concurrency, communication and object-orientedness is also presented.','Software reverse engineering'
'reverse engineering aims at extracting many kinds of information from existing software and using this information for system renovation and program understanding. the goal of this full day wcre\'05 workshop is to identify methods and techniques for reverse engineering from software to requirements (retr).','Software reverse engineering'
'in this paper, we present a framework for reverse engineeringallowing the integration and interaction of differentanalysis and visualization tools. the framework architecturethat we propose uses a dynamic type system to guaranteethe proper exchange of data between the tools and aset of wrapper classes to handle their communication. thisallows for an easy and secure integration of tools that haveoriginally not been designed to work together. in this sense,existing tools can be (re-)used and integrated. as a proofof concept we also present our own instantiation of the proposedframework architecture.','Software reverse engineering'
'','Software reverse engineering'
'','Software reverse engineering'
'during software evolution, programmers devote most of their effort to the understanding of the structure and behavior of the system. for object-oriented code, this might be particularly hard, when multiple, scattered objects contribute to the same function. design views offer an invaluable help, but they are often not aligned with the code, when they are not missing at all.this tutorial describes some of the most advanced techniques that can be employed to reverse engineer several design views from the source code. the recovered diagrams, represented in uml (unified modeling language), include class, object, interaction (collaboration and sequence), state and package diagrams. a unifying static code analysis framework used by most of the involved algorithms is presented at the beginning of the tutorial. a single running example is referred all over the presentation. trade-offs (e.g., static vs. dynamic analysis), limitations and expected benefits are also discussed.','Software reverse engineering'
'this is a questionnaire on program understanding and reverse engineering. it may be filled out manually or on-line. the results of the questionnaire will be used to guide the research of the two authors, both of whom are ph.d. students working in this area. copies of the resulting report will be mailed to all who participate, and a summary of the results will be published in an appropriate forum.','Software reverse engineering'
'a great number of existing xml documents in various domain such as electrical business have to be maintained in order to constantly adapt to a dynamically changing environment to keep pace with business needs. a dtd or xml schema in its current textual form commonly lacks clarity and readability, which makes the maintenance process tedious and error-prone. this paper presents an approach to reverse engineering the xml documents to conceptual model, which makes the xml documents more close to real world and business needs, let the designers quickly gain a picture of the overall structure of xml documents in order to improve its quality, increase the maintainability and reusability. in this paper, the conceptual model is described by uml class diagram, a three-level model is defined, and a novel approach for extracting various structure and semantic information from existing dtd is given, especially the inheritance structure can be inferred from the dtd structure.','Software reverse engineering'
'','Software reverse engineering'
'agile software development has been used by industry to create a more flexible and lean software development process, i.e making it possible to develop software at a faster rate and with more agility during development. there are however concerns that the higher development pace and lack of documentation are creating less secure software. we have therefore looked at three known security engineering processes, microsoft sdl, cigatel touchpoints and common criteria and identified what specific security activities they performed. we then compared these activities with an agile development process that is used in industry. developers, from a large telecommunication manufacturer, were interviewed to learn their impressions on using these security activities in an agile development process. we produced a security enhanced agile development process that we present in this paper. this new agile process use activities from already established security engineering processes that provide the benefit the developers wanted but did not hinder or obstruct the agile process in a significant way.','Software security engineering'
'this paper presents lessons learned and observations noted about the state of security-engineering practices by three information security practitioners with different perspectives - two in industry and one in academia. all authors have more than 20-years experience in this field and two were former members of the us national computer security center during the early days of the trusted computer system evaluation criteria and the strong promotion of trusted operating systems that accompanied the release of that document. in the last 20 years, it has been argued that security-engineering practices have not kept pace with the escalating threats to information systems. much has occurred since that time - new security paradigms, failure of evaluated products to emerge into common use, new systemic threats, and an increased awareness of the risk faced by information systems. this paper presents an empirical view of lessons learned in security-engineering, experiences in applying the trade, and observations made about the successes and failures of security practices and technology. this work was sponsored in part by nsf grant.','Software security engineering'
'e-health is a health care system which is supported by electronic process and communication. the information that is kept in the system must be accurate. in case of false information, it may cause harm to human life. so this system needs more security to protect the credential information. e-health system is the most security sensitive process handled electronically. the highest achievable security is never too much for an e-health system. so when system is being built, tasks such as security requirements elicitation, specification and validation are essential to assure the quality of the resulting secure e-health system. by considering the security requirements as functional requirements in the requirement phase, the completeness of security requirements for e-health system can be developed. in this paper we propose model oriented security requirements engineering (mosre) framework in the early phases of e-health system development, to identify assets, threats and vulnerabilities. this helps in standardizing the security requirements for secure e-health system without any security issues.','Software security engineering'
'high performance computing (hpc) within the department of defense (dod) is vital to the execution of the research and development mission. however, users must access these resources using interfaces that are complicated, specialized, primitive, and opaque. ezhpc was developed to address this complexity. the ezhpc project is interesting in that it abstracts for the user many of the technical details and tools related to hpc within the dod high performance computing modernization program (hpcmp) while providing a web-based application that allows user access to hpc resources. leveraging open-source, widely available technology, the ezhpc security model addresses the dod security requirements for authentication, confidentiality, availability, and integrity. this paper discusses an architecture that employs end-to-end encryption, open-source technologies, message authentication codes, and authentication of users via the hpcmp-modified implementation of the massachusetts institute of technology kerberos. this architecture provides ezhpc the flexibility to reduce the technology barrier to those unfamiliar with interactive use of hpc systems while providing significant capability to more advanced users. the subsequent abstraction of hpc resources constitutes a security model for any web-enabled application programming interface.','Software security engineering'
'the purpose of this research paper is to illustrate the industrial and federal need for information systems security engineering (isse) in order to build information assurance (ia) into a system rather than the current costly practice of fixing systems after production. extensive research was performed by collecting information from throughout the world wide web to include sites such as the national security agency\'s homepage, the information assurance technical framework homepage, the workshop for application of engineering principles to system security design, as well as many others. this research realized the following findings: (1) ia is dangerously left out of systems engineering processes; (2) a consortium from academia, industry and the federal government have formalized isse and its processes; (3) federally sponsored and industrially sponsored professional certifications exist for security engineers practicing isse; (4) isse, however, is not greatly used today due to a lack of understanding and a perceived high cost; (5) end-users are beginning to understand ia and are calling for more secure systems. this paper was written to illustrate a way forward, a method to bring isse to the frontlines of systems engineering and bring to life a notional concept of designing for security. this paper does not provide quantitative analyses as to the benefits of isse vs. the initial up front costs; however, further research should be accomplished in the future to address this. in conclusion, i recommend that isse must be identified as a critical component of the systems engineering lifecycle and be properly utilized to ensure that future products meet the ia demands of the end user. to achieve this, academia must build degree programs to educate isse and incorporate isse into existing degree programs; industry and the federal government must both embrace these principles and apply these techniques to their post-production, active engineering as well as new program developments.','Software security engineering'
'we propose a set of quantitative metrics to empirically evaluate security quality levels on an open (software) engineering service bus (engsb) platform.','Software security engineering'
'requirements engineering, a vital component in successful project development, often neglects sufficient attention to security concerns. further, industry lacks a useful model for incorporating security requirements into project development. studies show that upfront attention to security saves the economy billions of dollars. industry is thus in need of a model to examine security and quality requirements in the development stages of the production lifecycle.in this paper, we examine a methodology for both eliciting and prioritizing security requirements on a development project within an organization. we present a model developed by the software engineering institute\'s networked systems survivability (nss) program, and then examine two case studies where the model was applied to a client system. the nss program continues to develop this useful model, which has proven effective in helping an organization understand its security posture.','Software security engineering'
'','Software security engineering'
'we present tool-support for checking uml models and c code against security requirements. a framework supports implementing verification routines, based on xmi output of the diagrams from uml case tools, and on control flow generated from the c code. the tool also supports weaving security aspects into the code generated from the models. advanced users can use this open-source framework to implement verification routines for the constraints of self-defined security requirements. we focus on a verification routine that automatically verifies crypto-based software for security requirements by using automated theorem provers.','Software security engineering'
'there are a number of critical factors driving security in web engineering. these include: economic issues, people issues, and legislative issues. this paper presents the argument that a security improvement approach (sia), which can be applied to different web engineering development processes, is essential to successfully addressing web application security. in this paper, the criteria that any sia will have to address, for a web engineering process, are presented. the criteria are derived with supporting empirical evidence based on an in-depth security survey conducted within a fortune 500 financial service sector organization and supporting literature. the contribution of this paper is two fold. the criteria presented in this paper can be used to assess the security of an existing web engineering process and also to guide security improvement initiatives in web engineering.','Software security engineering'
'in this paper we present a new kind of cryptanalytic attack which utilizes bugs in the hardware implementation of computer instructions. the best known example of such a bug is the intel division bug, which resulted in slightly inaccurate results for extremely rare inputs. whereas in most applications such bugs can be viewed as a minor nuisance, we show that in the case of rsa (even when protected by oaep), pohlig-hellman, elliptic curve cryptography, and several other schemes, such bugs can be a security disaster: decrypting ciphertexts on &lt;em&gt;any&lt;/em&gt;computer which multiplies &lt;em&gt;even one pair of numbers&lt;/em&gt;incorrectly can lead to full leakage of the secret key, sometimes with a single well-chosen ciphertext.','Spoofing attacks'
'just as errors in sequential programs can lead to security exploits, errors in concurrent programs can lead to concurrency attacks. questions such as whether these attacks are feasible and what characteristics they have remain largely unknown. in this paper, we present a preliminary study of concurrency attacks and the security implications of real world concurrency errors. our study yields several interesting findings. for instance, we observe that the exploitability of a concurrency error depends on the duration of the timing window within which the error may occur. we further observe that attackers can increase this window through carefully crafted inputs. we also find that four out of five commonly used sequential defenses become unsafe when applied to concurrent programs. based on our findings, we propose new defense directions and fixes to existing defenses.','Spoofing attacks'
'','Spoofing attacks'
'it is a general belief among the designers of block-ciphers that even a relatively weak cipher may become very strong if its number of rounds is made very large. in this paper we describe a new generic known- (or sometimes chosen-) plaintext attack on product ciphers, which we call the slide attack and which in many cases is independent of the number of rounds of a cipher. we illustrate the power of this new tool by giving practical attacks on several recently designed ciphers: treyfer, wake-rofb, and variants of des and blowfish.','Spoofing attacks'
'confining a program during its execution so that it can\'t leak information to other programs is an old concern. recently, several researchers succeeded in fingerprinting distant machines by measuring temperature side effects on clocks. but can temperature also leak secrets in a computer or a chip? we started by implementing a covert channel between two processes (a sender and a receiver) running on the same machine. producing heat is simple: all the sender must do is launch massive calculations. to sense temperature in the machine, we considered three options: fan-based solutions, built-in sensors; and faults as heat detectors.','Spoofing attacks'
'we present template attacks, the strongest form of side channel attack possible in an information theoretic sense. these attacks can break implementations and countermeasures whose security is dependent on the assumption that an adversary cannot obtain more than one or a limited number of side channel samples. they require that an adversary has access to an identical experimental device that he can program to his choosing. the success of these attacks in such constraining situations is due manner in which noise within each sample is handled. in contrast to previous approaches which viewed noise as a hindrance that had to be reduced or eliminated, our approach focuses on precisely modeling noise, and using this to fully extract information present in a single sample. we describe in detail how an implementation of rc4, not amenable to techniques such as spa and dpa, can easily be broken using template attacks with a single sample. other applications include attacks on certain des implementations which use dpa-resistant hardware and certain ssl accelerators which can be attacked by monitoring electromagnetic emanations from an rsa operation even from distances of fifteen feet.','Spoofing attacks'
'in the early days of the commercial web, there were enough vulnerabilities in web servers and operating systems that black hats could easily exploit these infrastructural elements. however, as things progressed, software developers began hardening these components against attack. eventually, this caused attackers to shift their focus towards web-based applications, especially those using dynamic scripting, such as php. in the first of two articles exploring attacks on web applications, david watson, leader of the uk honeynet project, explores the evolution of web based attacks, charting how we reached our current point. he also examines how the constant drive towards new functionality hindered the security process as companies attempted to cram new features into products at the expense of secure development. web browsers, web servers and http were relative latecomers to the underlying infrastructure of the embryonic internet. they began life between 1989 and 1992 as simple tools for sharing static content between servers and clients using hyperlinks. their ease of use, combined with the development of extensions to support dynamic content, helped to make web interfaces commonplace. web application development became increasingly rapid and often included application code to extend core service functionality. this was at odds to the standard approach of other relatively mature network services.','Spoofing attacks'
'there are many factors that can dictate the success of a piece of malware. these include how and to whom it is delivered, how it is executed, how rapidly it propagates and how successfully it evades detection. the first two of these describe the process of threat delivery and execution, which are perhaps the most influential factors in the success of a threat. traditionally cybercriminals have used email as their preferred vector of attack, employing various social engineering tactics in order to entice the recipient into executing the malicious attachment. as companies have become more aggressive in blocking email content, criminals have shifted their attentions, and are now firmly focused on the web.','Spoofing attacks'
'this article will be the first in a series that will review web application security issues and provide suggestions on how to avoid the classic pitfalls. this particular article will discuss code injection and specifically cross site scripting. injection into application elements other than the web server and the client (i.e. sql injection) will be discussed in later articles.','Spoofing attacks'
'traditional anti-tampering system can not guarantee its own security. in order to reduce tampering and ensure the integrity of web pages and web server security and the stability of operation, the paper analyzes the current anti-tampering technology of several webs, and introduces the model three-threading technology based on the existing anti-tampering technology and the multi-ring structure of the node page tamper-resistant model. it can protect against tampering with the system. this paper describes the structures, working mechanism and the implementation process in details. the study illustrates that the security of the system on the website is guaranteed and the efficiency of the server is improved.','Tamper-proof and tamper-resistant designs'
'tamper-resistant software has been studied as techniques to protect algorithm or secret data. there are many ways to realize tamper-resistant software including the method of making software hard to read. so far, no objective and quantitative method is known for evaluating tamper-resistant software. most of known evaluation methods require involvement of human being. that means their evaluation results deeply depend on the skill and subjectivity of human. therefore, it has been expected to devise an objective and quantitative evaluation method in place of subjective evaluation methods. in this paper we propose a new such method to measure how hard to read. the basic idea is to use the parse tree of a compiler for a programming language, and evaluate depth and weights of the tree for a code. we give some experimental results to examine its effectiveness.','Tamper-proof and tamper-resistant designs'
'although there have been attempts to develop code transformations that yield tamper-resistant software, no reliable software-only methods are known. this paper studies the hardware implementation of a form of execute-only memory (xom) that allows instructions stored in memory to be executed but not otherwise manipulated. to support xom code we use a machine that supports internal compartments---a process in one compartment cannot read data from another compartment. all data that leaves the machine is encrypted, since we assume external memory is not secure. the design of this machine poses some interesting trade-offs between security, efficiency, and flexibility. we explore some of the potential security issues as one pushes the machine to become more efficient and flexible. although security carries a performance penalty, our analysis indicates that it is possible to create a normal multi-tasking machine where nearly all applications can be run in xom mode. while a virtual xom machine is possible, the underlying hardware needs to support a unique private key, private memory, and traps on cache misses. for efficient operation, hardware assist to provide fast symmetric ciphers is also required.','Tamper-proof and tamper-resistant designs'
'tamper-resistant software (trs) consists of two functional components: tamper detection and tamper response. although both are equally critical to the effectiveness of a trs system, past research has focused primarily on the former, while giving little thought to the latter. not surprisingly, many successful breaks of commercial trs systems found their first breaches at the relatively na&#239;ve tamper-response modules. in this paper, we describe a novel tamper-response system that evades hacker detection by introducing delayed, probabilistic failures in a program. this is accomplished by corrupting the program\'s internal state at well-chosen locations. our tamper-response system smoothly blends in with the program and leaves no noticeable traces behind, making it very difficult for a hacker to detect its existence. the paper also presents empirical results to demonstrate the efficacy of our system.','Tamper-proof and tamper-resistant designs'
'this paper investigates secure ways to interact with tamper-resistant hardware leaking a strictly bounded amount of information. architectural support for the interaction mechanisms is studied and performance implications are evaluated. the interaction mechanisms are built on top of a recently-proposed secure processor ascend[ascend-stc12]. ascend is chosen because unlike other tamper-resistant hardware systems, ascend completely obfuscates pin traffic through the use of oblivious ram (oram) and periodic oram accesses. however, the original ascend proposal, with the exception of main memory, can only communicate with the outside world at the beginning or end of program execution; no intermediate information transfer is allowed. our system, stream-ascend, is an extension of ascend that enables intermediate interaction with the outside world. stream-ascend significantly improves the generality and efficiency of ascend in supporting many applications that fit into a streaming model, while maintaining the same security level.simulation results show that with smart scheduling algorithms, the performance overhead of stream-ascend relative to an insecure and idealized baseline processor is only 24.5\%, 0.7\%, and 3.9\% for a set of streaming benchmarks in a large dataset processing application. stream-ascend is able to achieve a very high security level with small overheads for a large class of applications.','Tamper-proof and tamper-resistant designs'
'while hardware design focuses on creating minimally-sized circuits, this paper proposes that security-centric designs require a departure from this mentality. the need for built-in protection mechanisms at all levels of design is paramount to providing cost-effective secure systems. we focus on the high-level design of sequential circuits by targeting finite state machines (fsms) and their vulnerability to non-invasive, side channel based, attacks. the unconventional paradigm shift needed is justified by showing that conventional, minimalism-based, fsm synthesis and encodings allow direct correlation between state/transitions and hamming models. a two-fold method, involving structural modifications and specific encoding strategies, is proposed for side-channel secure fsm (s*fsms). preliminary high-level simulations show the effectiveness and potential for security driven s*fsm synthesis methods to mitigate the relationship between attack models and underlying hardware implementations.','Tamper-proof and tamper-resistant designs'
'we specify a hardware architecture that supportstamper-resistant software by identifying an \"idealized\"model, which gives the abstracted actions available to asingle user program. this idealized model is compared toa concrete \"actual\" model that includes actions of an adversarialoperating system. the architecture is verified byusing a finite-state enumeration tool (a model checker) tocompare executions of the idealized and actual models. inthis approach, software tampering occurs if the system canenter a state where one model is inconsistent with the other.in performing the verification, we detected an replay attackscenario and were able to verify the security of our solutionto the problem. our methods were also able to verifythat all actions in the architecture are required, as well ascome up with a set of constraints on the operating system toguarantee liveness for users.','Tamper-proof and tamper-resistant designs'
'','Tamper-proof and tamper-resistant designs'
'due to limited available memory (of the order of kilobytes) on embedded devices (such as smart cards), we undertake an approach of partitioning the whole program when it does not fit in the memory. the program partitions are downloaded from the server on demand into the embedded device just before execution. we devise a method of partitioning the code and data of the program such that no information regarding the control flow behavior of the program is leaked out. this property is called tamper resistance and it is very important for secure embedded devices such as smart cards which could hold sensitive information and/or carry out critical computation such as financial transactions. a preliminary solution to this problem was proposed in our earlier work [1]. this work proposes a new and more comprehensive solution to the problem. first, we propose a new policy which is based on keeping nothing in terms of partitions on the smart card. this policy is unlike the one in previous work which mandated keeping partitions in memory to which control flow was guaranteed to return. based on this new policy, a new partitioning algorithm is proposed for minimal safe partitions which reduces their memory requirements over previous work. the drawback of this new policy is however lower execution speed due to frequent communication encountered. in order to not significantly degrade performance, we propose caching frequently executed functions on the smart card without violation of tamper resistance. a framework is designed to determine the set of functions to be cached in conjunction with specific minimal safe partitions. further reduction in memory requirements is achieved due to the data partitioning.the decrease in memory footprint over the previous method is 27\% for code memory and 32.4\% for data memory on average. the speed-up over the old method is quite significant when applied to whole programs in large benchmarks (500 times on average). the conclusion is that previous method [1] is not suitable as a whole program partitioning strategy whereas the new proposed method is a viable solution.','Tamper-proof and tamper-resistant designs'
'in recent years, many have suggested to apply encryption in the domain of software protection against malicious hosts. however, little information seems to be available on the implementation aspects or cost of the different schemes. this paper tries to fill the gap by presenting our experience with several encryption techniques: bulk encryption, an on-demand decryption scheme, and a combination of both techniques. our scheme offers maximal protection against both static and dynamic code analysis and tampering. we validate our techniques by applying them on several benchmark programs of the cpu2006 test suite. and finally, we propose a heuristic which trades off security versus performance, resulting in a decrease of the runtime overhead.','Tamper-proof and tamper-resistant designs'
'in this paper, we introduce argumentation frameworks with necessities (afns), an extension of dung\'s argumentation frameworks (afs) taking into account a necessity relation as a kind of support relation between arguments (an argument is necessary for another). we redefine the acceptability semantics for these extended frameworks and we show how the necessity relation allows a direct and easy correspondence between a fragment of logic programs (lps) and afns. we introduce then a further generalization of afns that extends the necessity relation to deal with sets of arguments. we give a natural adaptation of the acceptability semantics to this new context and show that the generalized frameworks allow to encode arbitrary logic programs.','Trust frameworks'
'','Trust frameworks'
'','Trust frameworks'
'\"frameworks\" is an interactive installation trying to enhance the experience of an architectural space. this installation is about generating warmth to medium like walls of an architectural space that would otherwise be considered a cold and insignificant space. through this installation, we witness an opportunity to interact with the walls to add a new meaning to that space. this installation showcases how interactivity can be explored for opening up new possibilities to create more immersive experiences. the artists have tried to achieve this immersive experience by developing a game, where a virtual character plays with walls and their depth, in an actual physical space. a game is considered as one of the most engaging mediums. the artist tries to take a simple game and transform it into a more engaging and immersive experience primarily because of the scale and the seamless integration of the architectural spaces and structures into the game play. the installation will be designed in response to specific spaces or public sites selected by the artist.','Trust frameworks'
'this article is a collection of papers collected from the nonprofits that run three critical frameworks in use by it organizations around the globe&amp;#x2013;the project management institute, capability maturity model integration, and the innovation value institute. these papers outline the current state and future direction of these organizations and their frameworks.','Trust frameworks'
'','Trust frameworks'
'abstract argument frameworks have been used for various applications within multi-agent systems, including reasoning and negotiation. different argument frameworks make use of different inter-argument relations and semantics to identify some subset of arguments as coherent, yet there is no easy way to map between these frameworks; most commonly, this is done manually according to human intuition. in response, in this paper, we show how a set of arguments described using dung\'s or nielsen\'s argument frameworks can be mapped from and to an argument framework that includes both attack and support relations. this mapping preserves the framework\'s semantics in the sense that an argument deemed coherent in one framework is coherent in the other under a related semantics. interestingly, this translation is not unique, with one set of arguments in the support based framework mapping to multiple argument sets within the attack only framework. additionally, we show how eaf can be mapped into a subset of the argument interchange format (aif). by using this mapping, any other argument framework using this subset of aif can be translated into a daf while preserving its semantics.','Trust frameworks'
'in this paper, we extend dung\'s seminal argument framework to form a probabilistic argument framework by associating probabilities with arguments and defeats. we then compute the likelihood of some set of arguments appearing within an arbitrary argument framework induced from this probabilistic framework. we show that the complexity of computing this likelihood precisely is exponential in the number of arguments and defeats, and thus describe an approximate approach to computing these likelihoods based on monte-carlo simulation. evaluating the latter approach against the exact approach shows significant computational savings. our probabilistic argument framework is applicable to a number of real world problems; we show its utility by applying it to the problem of coalition formation.','Trust frameworks'
'frameworks are large building blocks of systems, encapsulating the commonalities of a family of applications. for reuse of these common features, frameworks are instantiated by smaller-sized components, plugins, to specific products. however, the framework instantiation process is often difficult, because not all aspects of the interplay of the framework and its plugins can be captured by standard type systems. application developers instantiating a framework often fail to develop correct applications. thus, this paper surveys several typical framework instantiation problems. a simple facet-based classification of the problems is given. it is shown how the different problem classes are related to phases of the software process and how they can be tackled appropriately. finally, the paper derives several research challenges, in particular, the challenge to define appropriate framework instantiation languages.','Trust frameworks'
'inquisitive semantics (cf. groenendijk, 2008) provides a formal framework for reasoning about information exchange. the central logical notion that the semantics gives rise to is compliance. this paper presents an algorithm that computes the set of compliant responses to a given initiative. the algorithm is sound and complete. the implementation is accessible online via www.illc.uva.nl/inquisitive-semantics.','Trusted computing'
'sometimes it is worthwhile to view college and university computing centers as an outsider. it is amazing that between large and small, public and private, one sees many similarities. most colleges and universities have their own hardware or access to computing power from a local network. there are statistical and graphical packages to maintain, documentation to write and maintain, users to consult with on a first come/first serve basis and at a higher level, reading rooms and bookstores. there are many common software products such as basic, fortran (including watfiv), pl/i, spss, apl, the biomedical data programs, imsl, calcomp, and of course star trek. in 1974, i had the opportunity to participate in a working group considering the issue of future services of university computing centers at the conference for academic computing center directors, snowmass, colorado. that working group left a lasting impression on me. recently, i have spent many restless hours assembling my ideas of the future in computing services at my own institution and then planning the orchestration of the changes.','Trusted computing'
'computers, like cars, are often at least half empty, and this observation led to the latest big idea in it. if idle compute cycles and empty storage systems could be exploited, there would be a huge new source of processing power that could be tapped, particularly by smaller enterprises that could not afford equivalent it resources of their own. the big hurdle is security, given that the idea of grid computing is to range outside your domain to find capacity to run applications that may involve confidential data and put intellectual property at risk.','Trusted computing'
'embedding computers into our environment is perhaps not only a job for computer scientist and engineers. we propose to understand the computer as a material for design as means to invite artists, architect, and designers to participate in envisioning how and where the computational power can be used. we will invite the conference attendees to (once again) think about how to bridge the so-called gap between computational and material properties but this time using a material rather than the traditional information centric perspective. the invitation is extended through hands-on experiences with our two samples of computational composites.','Trusted computing'
'in this paper, we propose a metabolic computing model for realizing a sustainable information system. we think that this metabolic computing model has both high fault tolerance and sustainability. we also propose a realistic architecture of the metabolic computing model. a metaboloid is a processing unit in this architecture. a set of metaboloids is organized as a mesh-connected norma (no remote memory access). however, the network may change because of metabolism. therefore, metaboloids have to achieve homeostasis to manage running tasks. we propose two new algorithms in this paper, namely bubbling and drifting.','Trusted computing'
'','Trusted computing'
'there\'s a revolution coming in the field of computing. and it\'s coming from the smallest of all places: the subatomic particles that form the basis of all matter.','Trusted computing'
'','Trusted computing'
' social computing research focuses on online social behavior and using artifacts derived from it for providing recommendations and other useful community knowledge. unfortunately, some of that behavior and knowledge incur societal costs, particularly with regards to privacy, which is viewed quite differently by different populations as well as regulated differently in different locales. but clever technical solutions to those challenges may impose additional societal costs, e.g., by consuming substantial resources at odds with green computing, another major area of societal concern. we propose a new crosscutting research area, societal computing, that focuses on the technical tradeoffs among computational models and application domains that raise significant societal issues. this dissertation, advised by prof. gail kaiser, will focus on privacy concerns in the context of societal computing and will aim to address research topics such as design patterns and architectures for privacy tradeoffs, better understanding of users\' privacy requirements so that tradeoffs with other areas such as green computing can be dealt with in a more effective manner, and better visualization techniques for making privacy and its tradeoffs more understandable. ','Trusted computing'
'a qualitative change in the scaling of semiconductor technology has ended the performance scaling of the single-thread processors that have been used as the building blocks for high-performance computers for the last decade and has made computers of all scales power limited. in today\'s power-limited regime, efficient high-performance computers must be built from throughput processors, processors, like gpus, that are optimized for sustained performance per unit power --- rather than for single-thread performance. this talk will discuss some of the challenges and opportunities in the architecture and programming of future throughput processors. in these processors, performance derives from parallelism and efficiency derives from locality. parallelism can take advantage of the plentiful and inexpensive arithmetic units in a throughput processor. without locality, however, bandwidth quickly becomes a bottleneck. communication bandwidth, not arithmetic is the critical resource in a modern computing system that dominates cost, performance, and power. this talk will discuss exploitation of parallelism and locality with examples drawn from the imagine and merrimac projects, from nvidia gpus, and from three generations of stream programming systems.','Trusted computing'
'logical methods can be used to design, implement, and analyze mechanisms for enforcing security and privacy policies. this article summarizes some significant results in this area, discusses the potential for wider adoption of these methods, and highlights remaining challenges that are being addressed by ongoing research.','Usability in security and privacy'
'death is an uncomfortable subject for many people, and digital systems are rarely designed to deal with this event. in particular, the wide array of existing digital authentication infrastructure rarely deals with gracefully retiring credentials in a uniform fashion. this research paper highlights an emerging paradigm: gracefully dealing with expired digital identities in a secure, privacy-preserving fashion. it examines the confluence of modern browser technology, cloud services, and human factors involved in managing a person\'s digital footprint while they live and retiring it when they die. we contemplate a potential approach to dealing with credentials after death by using cloud computing. we consider the reasons that such an approach may actually provide an opportunity for enhancing authentication security by frustrating identity stealing attacks. we note that this paper is not aimed at trivializing the real grief and loss that people feel, but rather an attempt to understand how security and privacy concerns are shaped by the end of life, with the ultimate goal of easing this transition for friends and family.','Usability in security and privacy'
'because it is increasingly difficult if not impossible to define the perimeter that separates the trusted inside from the untrusted outside, many security and privacy mechanisms no longer work in an online world.','Usability in security and privacy'
'','Usability in security and privacy'
'in recent years, many countries have adopted electronic passport systems, which merge the technologies of radio frequency identification &#40;rfid&#41; or contactless smart card and biometric identification. this paper describes various security issues applied to electronic passports and analyses the technology being implemented, with case studies of electronic passport systems in various countries. the case studies include the security measures that each country has taken to secure the electronic passports, as well as how the breach of security takes place. this has implications on how security can be improved as more countries are adopting electronic passports.','Usability in security and privacy'
'wireless communication is continuing to make inroads into many facets of society and is gradually becoming more and more ubiquitous. while in the past wireless communication (as well as mobility) was largely limited to the first and last transmission hops, today\'s wireless networks are starting to offer purely wireless, often mobile, and even opportunistically connected operation. the purpose of this article is to examine security and privacy issues in some new and emerging types of wireless networks, and attempt to identify directions for future research.','Usability in security and privacy'
'sensor networks offer economically viable solutions for a variety of applications. for example, current implementations monitor factory instrumentation, pollution levels, free-way traffic, and the structural integrity of buildings. other applications include climate sensing and control in office buildings and home environmental sensing systems for temperature, light, moisture, and motion.','Usability in security and privacy'
'adding digital intelligence and two-way functionalities to the power grid is one of the most flourishing topics in both academic and public institution communities. efficiency, improved reliability and safety are the benefits promised by the new smart grid at the price of privacy and security challenges which are only in part similar to the security issues of it networks. we survey the current grid architecture and the relation among the smart grid operators to analyze the security and privacy threats which needs to be addressed to secure the smart grid digital infrastructure.','Usability in security and privacy'
'over the past several years, social networking sites have arisen to facilitate social interactions on the internet while revolutionizing how online users interact with others. most social networking sites offer the basic features of online interaction, communication, and interest sharing, letting individuals create online profiles that other users can view. unfortunately, current trends in social networks indirectly require users to become system and policy administrators to protect their online contents. social networks\' security and privacy requirements still aren\'t well understood or fully defined. nevertheless, it\'s clear that they\'ll be quite different from classic security and privacy requirements because social networks involve user-centric concerns and allow multiple users to specify security policies on shared data. so, we must bring a depth of security experience from multiple security domains and technologies to this field, as well as a breadth of knowledge about social networks.','Usability in security and privacy'
'within the next year, travelers from dozens of nations may be carrying a new form of passport in response to a mandate by the united states government. the e-passport, as it is sometimes called, represents a bold initiative in the deployment of two new technologies: radio-frequency identification (rfid) and biometrics. important in their own right, e-passports are also the harbinger of a wave of next-generation id cards: several national governments plan to deploy identity cards integrating rfid and biometrics for domestic use. we explore the privacy and security implications of this impending worldwide experiment in next-generation authentication technology. we describe privacy and security issues that apply to e-passports, then analyze these issues in the context of the international civil aviation organization (icao) standard for e-passports.','Usability in security and privacy'
'in this paper we discuss ongoing work in an ambitious darpa funded effort to develop new ways of achieving cyber security. the broad approach taken by the project is for the network to be self-aware and to self-adapt in order to dodge attacks. after all attempts to avoid attack have been applied, some attacks will still get through. in critical systems, it is not always the best or practical thing to do to shut down the network under attack. in this paper we describe how trust in cyber assets can be modeled and used to support self-adaptive techniques in continuing to operate important computational functions in a network that is actively under attack. the paper, in particular describes the trust modeling system that attempts to maintain a model of trust for networked resources using a combination of two basic ideas: conditional trust (based on conditional preference (cp-nets) and the principle of maximum entropy (pme).','Virtualization and security'
'this article briefly describes the introduction and evolution of information security management systems (isms), their application and the introduction of national and regulatory requirements to protect information and how these regulations may be mapped into an isms.','Virtualization and security'
'a tradeoff is a situation that involves losing one quality or aspect of something in return for gaining another quality or aspect. speaking about the tradeoff between performance and security indicates that both, performance and security, can be measured, and that to increase one, we have to pay in terms of the other. while established metrics for performance of systems exist this is not quite the case for security. in this chapter we present standard performance metrics and discuss proposed security metrics that are suitable for quantification. the dilemma of inferior metrics can be solved by considering indirect metrics such as computation cost of security mechanisms. security mechanisms such as encryption or security protocols come at a cost in terms of computing resources. quantification of performance has long been done by means of stochastic models. with growing interest in the quantification of security stochastic modelling has been applied to security issues as well. this chapter reviews existing approaches in the combined analysis and evaluation of performance and security. we find that most existing approaches take either security or performance as given and investigate the respective other. for instance [34] investigates the performance of a server running a security protocol, while [21] quantifies security without considering the cost of increased security. for special applications, mobile ad-hoc networks in [5] and the email system in [32] we will see that models exist which can be used to explore the performance-security tradeoff. to illustrate general aspects of the security-performance tradeoff we set up a simple generalised stochastic petri net (gspn) model that allows us to study both, performance and security and especially the tradeoff between both. we formulate metrics, such as cost and an abstract combined performance and security measure that explicitly express the tradeoff and we show that system parameters can be found that optimise those metrics. these parameters are optimal for neither performance nor security, but for the combination of both.','Virtualization and security'
'we propose a model and definition for anonymous (group) identification that is well suited for rfid systems. this is based on the definition of juels and weis of strong privacy for rfid tags, where we add requirements for completeness and soundness. we also propose a weaker and more realistic definition of privacy. for the case where tags hold independent keys, we prove a conjecture by juels and weis, namely in a strongly private and sound rfid system using only symmetric cryptography, a reader must access virtually all keys in the system when reading a tag. it was already known from work by molnar, soppera and wagner that when keys are dependent, the reader only needs to access a logarithmic number of keys, but at a cost in terms of privacy: for that system, privacy is lost if an adversary corrupts just a single tag. we propose protocols offering a new range of tradeoffs between security and efficiency. for instance, the number of keys accessed by a reader to read a tag can be significantly smaller than the number of tags while retaining soundness and privacy, as long as we assume suitable limitations on the adversary.','Virtualization and security'
'modern critical infrastructures have command and control systems. these command and control systems are commonly called supervisory control and data acquisition (scada). in the past, scada system has a closed operational environment, so these systems were designed without security functionality. nowadays, as a demand for connecting the scada system to the open network growths, the study of scada system security is an issue. a key-management scheme is critical for securing scada communications. numerous key-management structures for scada also have been suggested. 11770-2 mechanism 9 key establishment protocol has been used in scada communication however a security proof for the 11770-2 mechanism 9 protocol is needed. the purpose of this paper is to provide a general overview about scada system, and its related security issues. furthermore, we try to investigate the importance of key management protocol and the need of formal security poof.','Virtualization and security'
'planning information security investment is somewhere between art and science. this paper reviews and compares existing scientific approaches and discusses the relation between security investment models and security metrics. to structure the exposition, the high-level security production function is decomposed into two steps: cost of security is mapped to a security level, which is then mapped to benefits. this allows to structure data sources and metrics, to rethink the notion of security productivity, and to distinguish sources of indeterminacy as measurement error and attacker behavior. it is further argued that recently proposed investment models, which try to capture more features specific to information security, should be used for all strategic security investment decisions beneath defining the overall security budget.','Virtualization and security'
'security properties such as confidentiality and authenticity may be considered in terms of the flow of messages within a network. to the extent that this characterisation is justified, the use of a process algebra such as communicating sequential processes (csp) seems appropriate to describe and analyse them\'. this paper explores ways in which security properties may be described as csp specifications, how security mechanisms may be captured, and how particular protocols designed to provide these properties may be analysed within the csp framework. the paper is concerned with the theoretical basis for such analysis. a sketch verification of a simple example is carried out as an illustration.','Virtualization and security'
'scalable trusted computing seeks to apply and extend the fundamental technologies of trusted computing to large-scale systems. to provide the functionality demanded by users, bootstrapping a trusted platform is but the first of many steps in a complex, evolving mesh of components. the bigger picture involves building up many additional layers to allow computing and communication across large-scale systems, while delivering a system retaining some hint of the original trust goal. not to be lost in the shuffle is the most important element: the system\'s human users. unlike 40 years ago, they cannot all be assumed to be computer experts, under the employ of government agencies which provide rigorous and regular training, always on tightly controlled hardware and software platforms. it seems obvious that the design of scalable trusted computing systems necessarily must involve, as an immutable design constraint, realistic expectations of the actions and capabilities of normal human users. experience shows otherwise. the security community does not have a strong track record of learning from user studies, nor of acknowledging that it is generally impossible to predict the actions of ordinary users other than by observing (e.g., through user experience studies) the actions such users actually take in the precise target conditions. we assert that because the design of scalable trusted computing systems spans the full spectrum from hardware to software to human users, experts in all these areas are essential to the end-goal of scalable trusted computing.','Virtualization and security'
'desktop virtualisation is attracting a lot of interest from companies of all sizes, as they face the challenges of managing user environments and keeping large numbers of desktops up to date. however, this technology is still very new, and the security of these virtual desktop environments is often not considered when implementing pilot schemes or production projects. so what are the areas to be aware of when looking at desktop virtualisation, and what steps should you take to keep these environments secure?','Virtualization and security'
'','Virtualization and security'
'network attacks often employ scanning to locate vulnerable hosts and services. fast and accurate detection of local scanners is key to containing an epidemic in its early stage. existing scan detection schemes use statically determined detection criteria, and as a result do not respond well to traffic perturbations. we present two adaptive scan detection schemes, success based (sb) and failure based (fb), which change detection criteria based on traffic statistics. we evaluate the proposed schemes analytically and empirically using network traces. against fast scanners, the adaptive schemes render detection precision similar to the traditional static schemes. for slow scanners, the adaptive schemes are much more effective, both in terms of detection precision and speed. sb and fb have non-linear properties not present in other schemes. these properties permit a lower sustained scanning threshold and a robustness against perturbations in the background traffic.','Vulnerability scanners'
'we develop a novel technique for authenticating physical documentsby using random, naturally occurring imperfections in paper texture.to this end, we devised a new method for measuring thethree-dimensional surface of a paper without modifying the documentin any way, using only a commodity scanner.from this physicalfeature, we generate a concise fingerprint that uniquely identifiesthe document.our method is secure against counterfeiting, robustto harsh handling, and applicable even before any content is printedon a page.it has a wide range of applications, including detectingforged currency and tickets, authenticating passports, and haltingcounterfeit goods.on a more sinister note, document identificationcould be used to de-anonymize printed surveys and to compromise thesecrecy of paper ballots.','Vulnerability scanners'
'','Vulnerability scanners'
'','Vulnerability scanners'
'the detection and tracking of moving objects is an essential task in robotics. the cmu-ri navlab group has developed such a system that uses a laser scanner as its primary sensor. we will describe our algorithm and its use in several applications. our system worked successfully on indoor and outdoor platforms and with several different kinds and configurations of two-dimensional and three-dimensional laser scanners. the applications vary from collision warning systems, people classification, observing human tracks, and input to a dynamic planner. several of these systems were evaluated in live field tests and shown to be robust and reliable. &#x00a9; 2012 wiley periodicals, inc. &#169; 2013 wiley periodicals, inc.','Vulnerability scanners'
'','Vulnerability scanners'
'','Vulnerability scanners'
'this article will introduce the security threats most prevalent today, explain the reasoning for automated scanning and why the software solutions are not always as perfect as you would desire.','Vulnerability scanners'
'','Vulnerability scanners'
'','Vulnerability scanners'
'software tend to be omnipresent in all modern systems. it often manipulates critical resources which interests pirates and need to be secured. given the fact that most common software attacks can\'t be stopped or detected using conventional security mechanisms, malicious intruders try hack into systems by exploiting a software vulnerability. vulnerabilities result from the use of traditional development processes - not focusing on security concerns - and the lack of necessary knowledge and guidance on how to produce secure software. they include implementation bugs such as buffer overflows and design flaws such as inconsistent error handling. several efforts are undertaken, to improve secure software engineering, however, developers still miss or misuse acquired knowledge due to domain immaturity, newness of the field, process complexity and absence of environments supporting such development. this paper presents our approach addressing software application security issues through its development process using a strategy oriented process model. the main feature of the proposed process model is that it provides a two level guidance: 1) a strategic guidance helping the developer to choose one among a compilations of the existing methods, standards and best practices and 2) a tactic guidance helping the developer to achieve his selection. this process model is easily extensible and allows building customized processes adapted to the context, the developer\'s finalities and the product state.','Web application security'
'the security services within applications have received recent attention. it has been suggested that this may be the only way to increase overall information system assurance in an era where ict governance and compliance have taken on new force and the use of commodity level ict products for critical information systems continues. while it has been argued that an application can be no more secure than its underlying computer subsystems, security at the application layer was always envisaged as playing a major role, e.g. in the \"open systems interconnection (osi)\" security model. at a time when \"end-user\" programming is being advocated, the needs and parameters of security education and training are rapidly changing, and increased threats from global internet connection are rapidly rising, there is a need to reconsider security schemes at the application level. this paper examines current trends in application design, development, deployment and management and evaluates these against known system vulnerabilities and threats.','Web application security'
'as security professionals we have a good handle on securing our perimeters, yet security compromises continue to rise. hackers have found a new attack vector and are successfully exploiting it. application exploits are to blame for this rise in security compromises and security professionals need to identify and secure the application. while risk cannot be completely eliminated, a strong application security program can identify and mitigate these risks to a more manageable level. organizational support, framework selection, and adherence to compliance and regulatory requirements are vital to the success of the program and the security of your applications. if you lack any of these elements the program will fail. there are many frameworks to choose from, so careful consideration must be taken to ensure the right framework is chosen for your organization. a successful application security program will be fully integrated within the sdlc. it will enable your organization to identify and remediate risks with applications. if implanted and executed effectively it will also meet the requirements for fisma compliance.','Web application security'
'business-driven development and management of secure applications and solutions is emerging as a key requirement in the realization of an on demand enterprise. in a given enterprise, individuals acting in various roles contribute to the modeling, development, deployment, and management of the security aspects of a business application. we look at the business-application life cycle and propose a policy-driven approach overlaid on a model-driven paradigm for addressing security requirements. our approach suggests that security policies are to be modeled using policies and rule templates associated with business processes and models, designed and implemented through infrastructure-managed or application-managed environments based on modeled artifacts, deployed into an infrastructure and potentially customized to meet the security requirements of the consumer, and monitored and managed to reflect a consistent set of policies across the enterprise and all layers of its application infrastructure. we use a pragmatic approach to identify intersection points between the platform-independent modeling of security policies and their concrete articulation and enforcement. this approach offers a way to manage and monitor systems behavior for adherence and compliance to policies. monitoring may be enabled through both information technology (it) and business dashboards. systematic approaches to connect business artifacts to implementation artifacts help implement business policies in system implementations. best practices and security usage patterns influence the design of reusable and customizable templates. because interoperability and portability are important in service-oriented architecture (soa) environments, we list enhancements to standards (e.g., business process execution language [bpel], unified modeling languagetm [uml&#174;]) that must be addressed to achieve an effective life cycle.','Web application security'
'with the incoming of information era, web-based service has been developed rapidly and offered more and more business. these &#8220;open&#8221;, and widely &#8220;web enabled&#8221; applications are subject to greater and greater levels and types of attacks as hackers exploit vulnerabilities within the software like sql injection and cross site scripts (xss) attack. in this paper, we proposed a type of novel embedded markov model (emm) to detect different web application attacks, monitor the on-line user behavior and defend the malevolent user promptly. comparing to previous web application attacks detecting approaches, our emm approach can not only detect user&#8217;s invalidated input errors but also find out the unreasonable page transition behavior. by detecting unreasonable page transition, we can immediately defend the malevolent or silly user behavior to avoid the further web system failures and sensitive information disclosure. furthermore, we implement an on-line user behavior surveillance system and use the real web traffic to evaluate the performance of our system. the experiment results show that our proposed emm method can discover the abnormal behavior of malevolent user and detect the invalidated input attacks like sql injection, xss and string buffer overflow attacks.','Web application security'
'web applications are important, ubiquitous distributed systems whose current security relies primarily on server-side mechanisms. this paper makes the end-to-end argument that the client and server must collaborate to achieve security goals, to eliminate common security exploits, and to secure the emerging class of rich, cross-domain web applications referred to as web 2.0. in order to support end-to-end security, web clients must be enhanced. we introduce mutation-event transforms: an easy-to-use client-side mechanism that can enforce even fine-grained, application-specific security policies, and whose implementation requires only straightforward changes to existing web browsers. we give numerous examples of attractive, new security policies that demonstrate the advantages of end-to-end web application security and of our proposed mechanism.','Web application security'
'in previous work we have presented the zenturio experimentmanagement system for performance and parameterstudies of parallel and distributed applications on clusterand grid architectures. in this paper we describe experiencesof an on-going work, targeting the implementationof zenturio on top of the open grid services architecture(ogsa). we analyse the opportunities offered by aweb services toolkit to develop grid services as required byogsa and compare them with the solutions offered by theopen grid services infrastructure (ogsi) specification. issuesregarding proxy management, service lifecycle, uddiservice repository, firewall management, factory and registryservices, service throughput, and security are comparativelyanalysed in both implementations.','Web application security'
'non-functional descriptions of web services and busi- ness rules play an important role in specification and analy- sis of the security constraints of web services. as existing approaches do not provide logic and semantic model for the web services security constraints, sharing and reason- ing over them are infeasible. the proposal builds upon the project akt\'s1 work in defining a semantic web constraint interchange format (cif), which itself builds on the pro- posed semantic web rule language (swrl). the main contributions of this paper are a new ontology for representing security constraints as policy and a seman- tic policy framework for the management of the policies; we also show the possibility to integrate the business rules and non-functional descriptions into policy specification by means of converting them into constraint satisfaction prob- lem (csp) using cif.','Web application security'
'integrating security throughout the life cycle can improve overall web application security. with adetailed review of the steps involved in applying security-specific activities throughout the softwaredevelopment life cycle, the author walks practitioners through effective, efficient application design,development, and testing.','Web application security'
'web application development is a large and growing area of employment for computer science graduates. while our graduates have learned how to design and implement web applications that work correctly with expected inputs, few have learned how to design and implement software that is secure against common web application vulnerabilities. the most common security vulnerabilities in software are cross-site scripting, sql injection, and php include bugs. all three problems are web application vulnerabilities. these vulnerabilities can allow attackers to access applications without permission, obtain sensitive information like credit card or social security numbers, and steal merchandise or transfer funds from commercial web sites. the number of vulnerabilities discovered each year has increased at an exponential rate since 2000. in this tutorial, we will describe how attackers exploit common web application vulnerabilities and show live demonstrations of such attacks. we will show participants how to teach their students to design and write secure code that is immune to these attacks. the tutorial will present resources that participants can use to incorporate web application security into programming, database, web development, and information security courses.','Web application security'
'this paper introduces an asynchronous optimistic certified email protocol, with stateless recipients, that relies on key chains to considerably reduce the storage requirements of the trusted third party. the proposed protocol thereby outperforms the existing schemes that achieve strong fairness. the paper also discusses the revocation of compromised keys as well as practical considerations regarding the implementation of the protocol.','Web protocol security'
'','Web protocol security'
'most organizations have an extensive set of security requirements, established for commercial firms through complex interactions of business goals, government regulations, and insurance requirements. meeting these requirements has been time consuming and error prone, because there haven\'t been standardized, automated ways of performing all of the tasks and reporting on results. another obstacle has been the lack of interoperability across security tools. to overcome these deficiencies and reduce security administration costs, the national institute of standards and technology developed the security content automation protocol (scap).','Web protocol security'
'the portal security transaction protocol (pstp) is a new signature technology that adds signature semantics to one-time password technology. pstp was developed to secure transactions in the financial services industry; however, pstp may be applicable to signatures in other spaces. pstp technology provides high signature strength of mechanism without requiring asymmetric key pairs deployed to client machines. pstp provides cryptographic after-the-fact evidence of a transaction event in a secured log.','Web protocol security'
'cryptographic protocol designers work incrementally. having achieved some goals for confidentiality and authentication in a protocol &#928;1 , they transform it to a richer &#928;2 to achieve new goals. but do the original goals still hold? more precisely, if a goal formula &#915; holds whenever &#928;1 runs against an adversary, does a translation of &#915; hold whenever &#928;2 runs against it? we prove that a transformation preserves goal formulas if a labeled transition system for analyzing &#928;1 simulates a portion of an lts for analyzing &#928;2 , while preserving progress in that portion. thus, we examine the process of analyzing a protocol &#928;. we use ltss that describe &lt;em&gt;our&lt;/em&gt; activity when &lt;em&gt;analyzing&lt;/em&gt; &#928;, not that of the principals &lt;em&gt;executing&lt;/em&gt; &#928;. each analysis step considers--for an observed message reception--what earlier transmissions would explain it. the lts then contains a transition from a fragmentary execution containing the reception to a richer one containing an explaining transmission. the strand space protocol analysis tool cpsa generates some of the ltss used.','Web protocol security'
'security protocol participants are software and/or hardware agents that are -- as with any system -- potentially vulnerable to failure. protocol analysis should extend not just to an analysis of the protocol specification, but also to its implementation and configuration in its target environment. however, an in-depth formal analysis that considers the behaviour and interaction of all components in their environment is not feasible in practice. this paper considers the analysis of protocol deployment rather than implementation. instead of concentrating on detailed semantics and formal verification of the protocol and implementation, we are concerned more with with the ability to trace, at a practical level of abstraction, how the protocol deployment, that is, the configuration of the protocol components, relate to each other and the overall protocol goals. we believe that a complete security verification of a system is not currently achievable in practice and seek some degree of useful feedback from an analysis that a particular protocol deployment is reasonable.','Web protocol security'
'query-response based protocols between a client and a server such as ssl, tls, ssh are asymmetric in the sense that the querying client and the responding server play different roles, and for which there is a need for two-way linkability between queries and responses within the protocol. we are motivated by the observation that though results exist in other related contexts, no provably secure scheme has been applied to the setting of client-server protocols, which differ from conventional communications on the above points. we show how to secure the communication of queries and responses in these client-server protocols in a provably secure setting. in doing so, we propose a new primitive: a query-response encapsulation scheme; we give an instantiation, and we demonstrate how this primitive can be used for our purpose. in our proof of secure encapsulation, we show how to preserve the notion of \"local-security\".','Web protocol security'
'we propose a class of protocol transformations, which can be used to (1) develop (families of) security protocols by refinement and (2) abstract existing protocols to increase the efficiency of verification tools. we prove the soundness of these transformations with respect to an expressive security property specification language covering secrecy and authentication properties. our work clarifies and significantly extends the scope of earlier work in this area. we illustrate the usefulness of our approach on a family of key establishment protocols.','Web protocol security'
'the ansi t10 object-based storage devices (osd) standard is a new standard. it evolves the storage interface from fixed size blocks to variable size objects and includes an integrated security protocol that protects storage. this paper presents the requirements, the design tradeoffs, and the final security protocol as defined in the standard. the resulting protocol is based on a secure capability-based model, enabling fine-grained access control that protects both the entire storage device and individual objects from unauthorized access. the protocol defines three methods of security based on the applications\' requirements. furthermore, the protocol\'s key management algorithm allows keys to be changed quickly, without disrupting normal operations. finally, the protocol is currently being enhanced for version 2.0 of the ansi t10 osd standard; future extensions will include data-encryption and access-control on sections of storage objects.','Web protocol security'
'we describe a formal approach to the analysis of security aspects of an identity federation protocol for web services in convergent networks. this network protocol was proposed by telecom italia as a solution to allow end users to access services on the web through different access networks without explicitly providing any credentials, while the service providers can trust the user\'s identity information provided by the access networks and access some user data. as a first step towards a full-blown formal security analysis of the protocol, we specify three user scenarios in the process algebra crypto-ccs and verify the vulnerability of one of these specifications w.r.t. a man-in-the-middle attack with the model checker pamochsa.','Web protocol security'
