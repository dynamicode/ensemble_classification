research in data sanitization (including anonymization) emphasizes ways to prevent an adversary from desanitizing data. most work focuses on using mathematical mappings to sanitize data. a few papers examine incorporation of privacy requirements, either in the guise of templates or prioritization. essentially these approaches reduce the information that can be gleaned from a data set. in contrast, this paper considers both the need to ``desanitize'' and the need to support privacy. we consider conflicts between privacy requirements and the needs of analysts examining the redacted data. our goal is to enable an informed decision about the effects of redacting, and failing to redact data. we begin with relationships among the data being examined, including relationships with a known data set and other, additional, external data. by capturing these relationships, desanitization techniques that exploit them can be identified, and the information that must be concealed in order to thwart them can be determined. knowing that, a realistic assessment of whether the information and relationships are already widely known or available will enable the sanitizers to assess whether irreversible sanitization is possible, and if so, what to conceal to prevent desanitization.