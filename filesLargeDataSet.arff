@relation _Users_mani_Courses_CS6604DigitalLibraries_Project_codeBase_mani_filesLargeDataSet

@attribute text string
@attribute @@class@@ {'Access control',Authorization,'Block and stream ciphers','Browser security','Cryptanalysis and other attacks','Data anonymization and sanitization','Database activity monitoring','Denial-of-service attacks','Digital rights management','Digital signatures','Distributed systems security','Domain-specific security and privacy architectures','Economics of security and privacy','Embedded systems security','File system security','Formal security models','Hardware reverse engineering','Hardware-based security protocols','Hash functions and message authentication codes','Information accountability and usage control','Information flow control','Information-theoretic techniques','Intrusion detection systems','Key management','Logic and verification','Malicious design modifications','Malware and its mitigation','Management and querying of encrypted data','Mathematical foundations of cryptography','Mobile and wireless security','Mobile platform security','Multi-factor authentication','Penetration testing','Privacy protections','Privacy-preserving protocols','Pseudonymity, anonymity and untraceability','Public key encryption','Security protocols','Security requirements','Side-channel analysis and countermeasures','Social aspects of security and privacy','Social network security and privacy','Software reverse engineering','Software security engineering','Spoofing attacks','Tamper-proof and tamper-resistant designs','Trust frameworks','Trusted computing','Usability in security and privacy','Virtualization and security','Vulnerability scanners','Web application security','Web protocol security'}

@data
'locking protocols for multi-level transactions have been studied since the very beginning. more recently, a hybrid concurrency control protocol for multi-level transactions called fopl has been developed. it employs access lists on the database objects and forward oriented commit validation. the basic test on all levels is based on the reordering of the access lists.so far a detailed analysis of fopl \'s benefits is missing. this paper describes a testbed for multi-level transactions which allows to measure transaction throughput and rollback frequency. the testbed allows to use a mix of strict two-phase locking and fopl on up to 4 levels. the tests work on randomly generated multi-level transactions on the basis of pages, records and virtual database objects on higher levels.a comparison of multi-level concurrency control protocols','Access control'
'the edca mechanism of the ieee 802.11 standard has been designed to support, among others, video traffic. this mechanism relies on a number of parameters whose configuration is left open by the standard. although there are some recommended values for these parameters, they are fixed independent of the wlan conditions, which results in suboptimal performance. following this observation, a number of approaches in the literature have been devised to set the edca parameters based on an estimation of the wlan conditions. however, these previous approaches are based on heuristics and hence do not guarantee optimized performance. in this article we propose a novel algorithm to adjust the edca parameters to carry video traffic which, in contrast to previous approaches, is sustained on mathematical foundations that guarantee optimal performance. in particular, our approach builds upon (i) an analytical model of the wlan performance under video traffic, used to derive the optimal point of operation of edca, and (ii) a control theoretic designed mechanism which drives the wlan to this point of operation. via extensive simulations, we show that the proposed approach performs optimally and substantially outperforms the standard recommended configuration as well as previous adaptive proposals.a control theoretic scheme for efficient video transmission over ieee 802.11e edca wlans','Access control'
'we propose a qos based call admission control (cac) scheme designed for multiclass cellular networks. the scheme allocates the portions of the channel capacity to individual traffic classes, and by monitoring the call arrival patterns periodically updates the share of each class. for each traffic class, it treats new and handoff calls separately to keep the drop probability of handoff calls as small as possible to maintain the user satisfaction at a high level. our modeling study and analysis show that the cac mechanism is capable of both satisfying the quality of service requirements of multiple traffic classes while optimizing the system utilization.a dynamic call admission control scheme for optimization with qos provisioning in multiclass cellular networks','Access control'
'a dynamic key-lock-pair access control scheme','Access control'
'in this paper we present k-vac -- a key-value access control model for modern non-relational data stores. this model supports specification and enforcement of access control policies at different levels of resource hierarchy, such as a column family, a row, or a column. the policies can be based on contents of the key-value store and they may also include context information. through a case-study example we demonstrate the capabilities of this system.a fine-grained access control model for key-value systems','Access control'
'reflective database access control (rdbac) is a model in which a database privilege is expressed as a database query itself, rather than as a static privilege contained in an access control list. rdbac aids the management of database access controls by improving the expressiveness of policies. however, such policies introduce new interactions between data managed by different users, and can lead to unexpected results if not carefully written and analyzed. we propose the use of transaction datalog as a formal framework for expressing reflective access control policies. we demonstrate how it provides a basis for analyzing certain types of policies and enables secure implementations that can guarantee that configurations built on these policies cannot be subverted.a formal framework for reflective database access control policies','Access control'
'provenance is a directed acyclic graph that explains how a resource came to be in its current form. traditional access control does not support provenance graphs. we cannot achieve all the benefits of access control if the relationships between the data and their sources are not protected. in this paper, we propose a language that complements and extends existing access control languages to support provenance. this language also provides access to data based on integrity criteria. we have also built a prototype to show that this language can be implemented effectively using semantic web technologies.a language for provenance access control','Access control'
'attribute based access control (abac) grants accesses to services based on the attributes possessed by the requester. thus, abac differs from the traditional discretionary access control model by replacing the &#60;i>subject&#60;/i> by a set of attributes and the &#60;i>object&#60;/i> by a set of services in the access control matrix. the former is appropriate in an identity-less system like the internet where subjects are identified by their characteristics, such as those substantiated by certificates. these can be modeled as attribute sets. the latter is appropriate because most internet users are not privy to method names residing on remote servers. these can be modeled as sets of service options. we present a framework that models this aspect of access control using logic programming with set constraints of a computable set theory [dppr00]. our framework specifies policies as stratified constraint flounder-free logic programs that admit primitive recursion. the design of the policy specification framework ensures that they are consistent and complete. our abac policies can be transformed to ensure faster runtimes.a logic-based framework for attribute based access control','Access control'
'reflective database access control (rdbac) is a model in which a database privilege is expressed as a database query itself, rather than as a static privilege in an access control matrix. rdbac aids the management of database access controls by improving the expressiveness of policies, enabling enforcement at the database level rather than at the application level. this in turn facilitates the creation of new applications without the need for duplicating security enforcement in each application. past work has proposed the use of the transaction datalog (td) language as a theoretical basis for rdbac. we present a case study for a medical database using td. this case study includes a wide range of access patterns for which rdbac provides a simple method for formulating policies, demonstrating the flexibility of rdbac as well as the practicality and scalability of using such a system in real-world applications that require non-trivial policy definitions on large data sets.a medical database case study for reflective database access control','Access control'
'security policy enforcement is instrumental in preventing the unauthorized disclosure of sensitive data, protecting the integrity of vital data, mitigating the likelihood of fraud, and ultimately enabling the secure sharing of information. in accessing a given resource, policy may dictate, for example that a user has a need-to-know, is appropriately cleared, is competent, has not already performed a different operation on the same resource, the resource was previously accessed by a different user, is incapable of accessing other enterprise resources, or is capable of accessing an object or any copy of the object while performing a specific task. currently, there exist a rich set of formal security models that can translate organizational policies. a small sample of well documented policies include, avors of discretionary access control (dac), mandatory access control (mac), role-based access control (rbac), orcon, chinese wall, and history-based separation of duty. enterprise policies that are designed to protect resources are also ad-hoc in nature. as a major component of any operating system or application, access control mechanisms come in a wide variety of forms, each with their individual method for authentication, access control data constructs for specifying and managing policy, and functions for making access control decisions and enforcement of policies. of the numerous recognized access control policies, today\'s oss rigidly limit enforcement to a small subset of known policies. policies are also routinely accommodated through the implementation of access control mechanisms within applications. prominent among these applications are database management systems, but these applications can also include a number of smaller applications such as enterprise calendars, time and attendance, and workflow management. essentially, any application that requires a user\'s authentication, typically also affords an independent access control service. not only do these applications further aggravate identity and privilege management problems, applications can also undermine policy enforcement objectives. for instance, although a file management system may narrowly restrict user access to a specific file, chances are the content of that file can be copied to an attachment or a message and mailed to anyone in the organization, or for that matter, the world. in consideration of these issues an important question is raised - does a meta model exist that can serve as a unifying framework for specifying and comprehensively enforcing any access control policy? some may argue that convergence towards a meta model is already underway. for example, rbac, and xacml have been shown effective in their specification and enforcement of access control policies and have been applied in providing interoperable protection. is rbac fundamental to access control and can it eventually be extended and tinkered with to accommodate any policy? rbac has already been shown to be able to be configured to enforce both dac and mls. and, since rbac was formally proposed in the early and mid 90\'s a large number of extensions to the rbac model have been proposed to accommodate a wide variety of policy issues and applications. the question here is - are these extensions getting closer to a meta model or are we making it up as we go along. at sacmat 2005, nist had proposed an access control framework, referred to as the policy machine (pm) that has been shown to accommodate a wide variety of access control policies including dac, mac, and rbac. since that publication the pm has been refined and to demonstrate its viability in specifying and enforcing a wide variety of attribute-based policies, nist has developed a reference implementation. however, some have suggested that the basic relations of the pm are similar to that of rbac and that its other policy appeasing relations and functions could be applied in extending the rbac model. in addressing the interoperability problem and the policy exibility problem the xacml policy specification language has been growing in recognition and its use. can this approach to access control be adopted or can it evolve as the meta model? xacml\'s current focus is on providing access control that is interoperable among applications. as currently specified and applied xacml has does not deal with all types of objects, for example files in an operating system. it is not comprehensive (e.g., it would not prevent the leakage of a sensitive object to an unauthorized principle through copying and past to an email message that could be sent to anyone in the world). in addition to discussions related to the above technologies, this panel will address two fundament questions. what practical good can the existence of a meta model provide? and, is it even possible for a meta model to be developed given the large diversity and types of access control policies?a meta model for access control','Access control'
'due to the ability to construct a large-scale sensing system by the cooperative behaviors of multiple sensor nodes, sensor networks are expected to be applied to many applications such as environmental monitoring. on the other hand, with the development of robotics technology in recent years, there has been many studies on sensors with a moving function (mobile sensors). in this paper, we propose an effective mobile sensor control method for sparse sensor networks. our method uses two types of sensor nodes, fixed node and mobile node. the data acquired by nodes are accumulated on a fixed node before transferred to the sink node. in addition, our method transfers the accumulated data efficiently by constructing the communication route of multiple mobile nodes between fixed nodes. we also conducted simulation experiments to evaluate the performance of our method.a mobile sensor control method for sparse sensor networks','Access control'
'in order to provide a general access control methodology for parts of xml documents, we propose combining role-based access control as found in the role graph model, with a methodology originally designed for object-oriented databases. we give a description of the methodology, showing how different access modes, xpath expressions and roles can be combined, and how propagation of permissions is handled. given this general approach, a system developer can design a complex authorization model for collections of xml documents.a role-based approach to access control for xml databases','Access control'
'access and use control using externally controlled reference monitors','Access control'
'we address the problem of defining access control policies that may be used in the evaluation of requests made by client actors, in the course of e-trading, to perform actions on the resources maintained by an e-collective. an e-collective is a group of agents that may act individually or in conjunction with other agents to satisfy a client\'s request to act. our principal contribution to this key problem is to define formally an access control model in terms of which policies may be specified for helping to ensure that only legitimate forms of client actions are performed in the course of engaging in e-trading. we call this model the action control model. in action control, the notion of intentional, empowered, authorized actions, that may be performed individualistically or jointly with other agents, and in a manner that is consistent with a group ethos, is the basis for specifying a set of permissives. a permissive is a generalization of the notion of a permission (as the latter term is usually interpreted in access control). in addition to the formal definition of the action control model, we give examples of action control policy specifications and we describe a candidate implementation and performance measures.access control by action control','Access control'
'many access control policy languages, e.g., xacml, allow a policy to contain multiple sub-policies, and the result of the policy on a request is determined by combining the results of the sub-policies according to some policy combining algorithms (pcas). existing access control policy languages, however, do not provide a formal language for specifying pcas. as a result, it is difficult to extend them with new pcas. while several formal policy combining algebras have been proposed, they did not address important practical issues such as policy evaluation errors and obligations; furthermore, they cannot express pcas that consider all sub-policies as a whole (e.g., weak majority or strong majority). we propose a policy combining language pcl, which can succinctly and precisely express a variety of pcas. pcl represents an advancement both in terms of theory and practice. it is based on automata theory and linear constraints, and is more expressive than existing approaches. we have implemented pcl and integrated it with sun\'s xacml implementation. with pcl, a policy evaluation engine only needs to understand pcl to evaluate any pca specified in it.access control policy combining','Access control'
'access control with role attribute certificates','Access control'
'access-control software','Access control'
'we introduce a generalization of role-based access control that we call the action-status access control (asac) model. the asac model addresses certain shortcomings with rbac models when applied in distributed computing contexts. the asac model is based on the notion of status, and a nonmonotonic theory of access control that is founded upon the notions of events, actions and times. the approach allows automatic changes to be made to policy requirements and agent authorizations that may be based, in part, on an agent\'s intentional behaviors.action-status access control','Access control'
'adaptive rate control, if properly employed, is an effective mechanism to sustain acceptable levels of quality of service (qos) in wireless networks where channel and traffic conditions vary over time. in this paper we present an adaptive rate (source and channel) control mechanism, developed as part of an adaptive resource allocation and management (aram) algorithm, for use in direct broadcast satellite (dbs) networks. the algorithm performs admission control and dynamically adjusts traffic source rate and forward error correction (fec) rate in a co-ordinated fashion to satisfy qos requirements. to analyze its performance, we have simulated the adaptive algorithm with varying traffic flows and channel conditions. the traffic flow is based on a variable bit rate (vbr) source model that represents motion picture expert group (mpeg) traffic fluctuations while the dbs channel model is based on a two-state additive white gaussian noise (awgn) channel. for measures of performance, the simulator quantifies throughput, frame loss due to congestion during transmission as well as qos variations due to channel (fec) and source (mpeg compression and data transmission) rate changes. to show the advantage of the adaptive fec mechanism, we also present the performance results when fixed fec rates are employed. the results indicate significant throughput and/or quality gains are possible when the fec/source pairs are adjusted properly in co-ordination with source rate changes.adaptive rate control and qos provisioning in direct broadcast satellite networks','Access control'
'this paper investigates the performance of tcp-friendly rate control (tfrc) to control the transmission rate of scalable video streams when used in a mobile network. the streams are encoded using the scalable video coding (svc) extension of the h.264/avc standard. adding or removing the layers is decided based on the tfrc during varying channel conditions of the mobile network. we conduct simulations in various realistic use cases, evaluate and compare the performance with and without tfrc-based adaptation. the results show significant improvements in terms of lower loss rate, delay, required buffer size and less playback interruption.adaptive video streaming over a mobile network with tcp-friendly rate control','Access control'
'priority based link-bandwidth partitioning is required to support wireless multimedia services, having diverse qos (delay, throughput) requirements, in mobile ad hoc networks with multimedia nodes. a new class of service disciplines, termed &#8220;batch and prioritize&#8221; or bp admission control (ac), is proposed. the bp algorithms use the delay tolerance of applications to batch requests in time slots. bandwidth assignment is made either at the end of the slot, or during the slot, on a priority basis. analytical and simulation models are developed to quantify the performance of the bp schemes. the results are compared with those obtained for a first-come-first-served (fcfs) service discipline. the class of bp schemes trade off the delay and loss tolerance of applications to improve the net carried traffic on the link. further, such schemes enable an easy implementation for adaptive prioritization, where the degree of precedence given to an application varies with offered load and the link capacity.admission control with priorities','Access control'
'this paper presents an approach for formally specifying and enforcing security policies on web service implementations. networked services in general, and web services in particular, require extensive amounts of code to ensure that clients respect site-integrity constraints. we provide a language by which these constraints can be expressed and enforced automatically, portably and efficiently. security policies in our system are specified in a language based on temporal logic, and are processed by an enforcement engine to yield site and platform-specific access control code. this code is integrated with a web server and platform-specific libraries to enforce the specified policy on a given web service. our approach decouples the security policy specification from service implementations, provides a mandatory access control model for web services, and achieves good performance. we show that up to 22\% of the code in a traditional web service module is dedicated to security checking functionality, including checks for client sequencing and parameter validation. we show that our prototype language implementation, webguard, enables web programmers to significantly reduce the amount of security checking code they need to develop manually. the quality of the code generated by webguard from formal policy specifications is competitive with the latency of handcrafted code to within a few percent.an access control language for web services','Access control'
'the link interference and multi-hop characters make wireless mesh network (wmn) performance can not be tuned well by local information. we propose an analytical flow control scheme (afcs) based on the optimal bandwidth allocation model that is specifically designed for the unique characteristics of wmn. the original contribution incorporated in a flow contention model of wmn based on maximal link interference region. with the contention constraint, the optimality of bandwidth allocation can be achieved by maximizing the aggregated utility across all flows. we then deduce a practical equation for adjusting sending rate from the bandwidth allocation model. based on the simplified analytical equation and the congestion notification message mechanism, we propose an equation-based flow control framework for real-time traffic in wmn. simulation results have shown the afcs can improve the capacity of the wmn by 40\% and alleviate the unfairness and delay jitter greatly. the fast response, fairness and steadiness characters of the afcs are helpful for rigorous real-time traffic and dynamic wireless network.an analytical flow control scheme for real-time traffic in wireless mesh network','Access control'
'congestion causes packet loss which in turn drastically decreases network performance and throughput. as sensors are energy constraint so it is a decisive task to detect congestion and perform congestion control. additionally, varieties of application have different requirement (i.e. delay, link utilization, and packet loss). in this paper we proposed an application priority based rate control algorithm to mitigate congestion in sensor network. this approach also maintains an interactive queue management scheme so that requirements of different application can be fulfilled. to ensure varieties application priority, concept of intra queue priority and inter queue priority are evolved. node priority based hop by hop rate adjustment is also proposed here to ensure high link utilization. finally experimental outputs have demonstrated the effectiveness of this task and show a noticeable performance in terms of energy analysis and throughput of the network.an approach for congestion control in sensor network using priority based application','Access control'
'this work presents a new approach for topology control (tc) in wireless sensor networks (wsn) devised for indoor scenarios with obstacles and no feedback mechanisms. the technique is supported by two novel metrics, namely blockage rate and useful area rate, applicable to environments with well-defined obstacles described by site specific propagation models. simulation results in some realistic scenarios showed that the technique allows to relate connectivity with transmission power levels and to identify critical transmission power levels. also, it is shown that the technique is equivalent to an approximated mean value in terms of connectivity aspects.an approach for wireless sensor networks topology control in indoor scenarios','Access control'
'the advent of emerging technologies such as web services, service-oriented architecture, and cloud computing has enabled us to perform business services more efficiently and effectively. however, we still suffer from unintended security leakages by unauthorized actions in business services while providing more convenient services to internet users through such a cutting-edge technological growth. furthermore, designing and managing web access control policies are often error-prone due to the lack of effective analysis mechanisms and tools. in this paper, we represent an innovative policy anomaly analysis approach for web access control policies. we focus on xacml (extensible access control markup language) policy since xacml has become the de facto standard for specifying and enforcing access control policies for various web-based applications and services. we introduce a policy-based segmentation technique to accurately identify policy anomalies and derive effective anomaly resolutions. we also discuss a proof-of-concept implementation of our method called xanalyzer and demonstrate how efficiently our approach can discover and resolve policy anomalies.anomaly discovery and resolution in web access control policies','Access control'
'we present blockrate, a wireless bitrate adaptation algorithm designed for blocks, or large contiguous units of transmitted data, as opposed to small packets. in contrast to state-of-the-art algorithms that can either have the amortization benefits of blocks or high responsiveness to underlying channel conditions of packets, blockrate has both. our evaluation shows that blockrate achieves up to 2.8&#215; goodput improvement in a variety of mobility scenarios.anticipatory wireless bitrate control for blocks','Access control'
'in ieee 802.11 wireless networks, the downlink delay rises as the number of voip nodes increases while the uplink delay remains small due to the same chance of media access between nodes and the access point (ap). this degrades the capacity and qos of voip significantly. therefore, we introduce adaptive priority control (apc) to balance the downlink and uplink delay of voip traffic at the mac layer, by giving to the ap a higher transmission priority, which is adaptively decided according to the uplink and downlink traffic volume. and, we verify through theoretical analysis that apc is an optimal method to balance the uplink and downlink delay.balancing uplink and downlink delay of voip traffic in wlans using adaptive priority control (apc)','Access control'
'ieee 802.11 wireless local area networks (wlans) technologies are being considered as potential turnkey solutions for constructing infrastructures of wireless mesh networks (wmns). with known experiments and simulation results, performances at the transport layer, including the transmission control protocol (tcp) and user datagram protocol (udp), have not been doing satisfactorily for connections passing through multiple hops using 802.11 technologies. data throughput usually drops significantly when a connection has to go through more than three wireless hops. since udp is a simple connectionless protocol, its performance failure indicates that the design of 802.11 medium access control (mac) protocol may not be competent to handle traffic on the wireless mesh networks. in this paper, a multi-radio multi-channel wireless mesh network architecture with a novel mac protocol design is proposed. the mac protocol extends the request-to-send/clear-to-send message pair in 802.11 for channel reservation operations. only two stations at an instant can have the sole possession of a specific channel with an announced duration. during this duration, the two stations can establish a connection for sending multiple segments to each other, if needed. this per-hop reservation concept can further be deployed into multi-hop wireless networks, and high throughput data rate should be achievable for multi-hop tcp and udp connections.channel control for multi-radio multi-channel wireless mesh networks','Access control'
'recently, the notion of the semantic web has been introduced to define a machine-interpretable web targeted for automation, integration and reuse of data across different applications. under the semantic web, web pages are annotated by concepts that are formally defined in ontologies along with the relationships among them. as information pertaining to different concepts has varying access control requirements, in this paper, we propose an access control model for the semantic web that is capable of specifying authorizations over concepts defined in ontologies and enoforcing them upon data instances annotated by the concepts. it is important to note that semantic relationships among concepts play a key role in making access control decisions. this is because, based on the relationship, one may infer information contained in one concept node from that of the other. therefore, we first identify the important domain-independent relationships among concepts, categorize them and propose propagation policies based on these categories of relationships. in particular, we allow propagation of authorizations based on the semantic relationships among concepts to prevent illegal inferences. we then show how concept-level security polices can be represented in an owl-based access control language. finally, we demonstrate how users\' requests can be handled under our access control model. our concept-level model is especially suitable for the specification and administration of access control over semantically related web data under the semantic web even if they conform to different dtds or use different tag names.concept-level access control for the semantic web','Access control'
'the ieee 802.11p standard specifies the phy and mac layer operations for transmitting and receiving periodic broadcast messages for vehicular safety. many studies have identified issues with the csma based ieee 802.11p mac at high densities of devices, mainly reflected by low packet reception rate. in this paper, we make an interesting observation that with increasing density, the ieee 802.11p mac tends towards an aloha-type behavior where concurrent transmissions by close-by devices are not prevented. this behavior can lead to poor packet reception rate even for vehicles in close neighborhood. many efforts have been made to address the ieee 802.11p mac issues to provide better performance for dsrc safety applications, including the introduction of decentralized congestion control (dcc) algorithm to etsi standards in europe. in this paper, we evaluate the performance of the proposed dcc algorithm and observe that the nominal parameters in dcc are unsuitable in many scenarios. using transmit power control as an example, we develop a simple rule within the dcc framework that can significantly improve the safety packet reception performance with increasing densities. the dcc algorithms are fully compatible with the ieee 802.11p standards and asynchronous in nature. a parallel approach to handle high device densities is a slotted synchronous mac, where time is slotted based on gps synchronization and each transmitter contends for a set of recurring time slots (or channels) with periodicity matching the required safety message periodicity. as compared to the per-packet based contention scheme as in csma defined in ieee 802.11, such a scheme is much better suited for periodic safety broadcast. in this paper, we design a standard compliant tdm overlay on top of the mac layer that can significantly improve the packet reception performance. combined with a distributed resource selection protocol, the synchronous mac can discover even more neighboring devices than the improved asynchronous approach, making dsrc safety applications more reliable.congestion control for vehicular safety','Access control'
'in this paper, we study the performance of utility maximization congestion control over multihop csma-based networks. we consider decoupled vs. joint design of congestion control and medium access and consider unmodified mac protocols such as ieee 802.11. networks employing such mac protocols incur flow starvation both without congestion control and with existing tcp-based congestion control. we develop a framework to study key issues in such networks that are not incorporated by prior models, yet are critical to the performance of congestion control algorithms. we study the role of data transmission capacity that is location dependent and, even more, unknown. we show that for the case of consistent channel state, a single globally optimal data transmission capacity does not exist. moreover, for the case of inconsistent channel state that arises due to the carrier sense mechanism itself, a data transmission capacity that provides convergence to perfectly fair rates does not exist, i.e., the congestion control algorithm converges to incorrect rates. we study the impact of inter-node collaboration within a contention region, and show that collaboration can alleviate these problems and ensure convergence to fair rates. finally, we compare the performance of congestion control in a collaborative network with the performance of tcp, and show that tcp starves some flows, whereas congestion control with collaboration removes starvation, provides significantly better fairness, and achieves 17\% higher aggregate throughput.congestion control in csma-based networks with inconsistent channel state','Access control'
'this paper looks at the problem of designing medium access algorithm for wireless networks with the objective of providing high throughput and low delay performance to the users, while requiring only a modest computational effort at the transmitters and receivers. additive inter-user interference at the receivers is an important physical layer characteristic of wireless networks. today\'s wi-fi networks are based upon the abstraction of physical layer where inter-user interference is considered as noise leading to the \'collision\' model in which users are required to co-ordinate their transmissions through carrier sensing multiple access (csma)-based schemes to avoid interference. this, in turn, leads to an inherent performance trade-off [1]: it is impossible to obtain high throughput and low delay by means of low complexity medium access algorithm (unless p=np). as the main result, we establish that this trade-off is primarily due to treating interference as noise in the current wireless architecture. concretely, we develop a simple medium access algorithm that allows for simultaneous transmissions of users to the same receiver by performing joint decoding at receivers, over time. for a receiver to be able to decode multiple transmissions quickly enough, we develop appropriate congestion control where each transmitter maintains a \"window\" of undecoded transmitted data that is adjusted based upon the \"feedback\" from the receiver. in summary, this provides an efficient, low complexity \"online\" code operating at varying rate, and the system as a whole experiences only small amount of delay (including decoding time) while operating at high throughput.congestion control meets medium access','Access control'
'recently, compound tcp has come to be regarded as the most promising transport layer protocol for high-speed and long-distance networks. in our previous work, we have shown that the loss-based congestion control mechanism performed by compound tcp does not achieve fairness in throughput. in order to solve this problem, we have proposed compound tcp+. this protocol decreases the loss window size when it is anticipated that a packet loss might occur, without an actual packet loss. using simple simulations, we have shown that compound tcp+ connections have high fairness in wireless lans. however, in our previous work, we have not examined the congestion control scheme of compound tcp+, and the performance of compound tcp+ has been evaluated only by means of simple simulations. in this paper, we evaluate the congestion control scheme of compound tcp+ in wireless lans. we consider three forms of the congestion control scheme, and show that a linear decrease in the loss window size is good when the network is not in a state of serious congestion that causes a packet loss. furthermore, we implement compound tcp+ on a linux kernel, and show its effectiveness by evaluating its performance in an experimental network environment.congestion control scheme of compound tcp+ in wireless lans','Access control'
'content-dependent access control, where the access decisions depend upon the value of an attribute of the object itself, is required in many applications. however problems arise in an object-based environment, because obtaining the value of an object\'s attribute requires an operation upon the object. we discuss the conceptual and performance implications of introducing content-dependent access control, and suggest how the problems can be avoided in some cases by using a domain-based approach to access control.content-dependent access control','Access control'
'accesses that are not permitted by implemented policy but that share similarities with accesses that have been allowed, may be indicative of access-control policy misconfigurations. identifying such misconfigurations allows administrators to resolve them before they interfere with the use of the system. we improve upon prior work in identifying such misconfigurations in two main ways. first, we develop a new methodology for evaluating misconfiguration prediction algorithms and applying them to real systems. we show that previous evaluations can substantially overestimate the benefits of using such algorithms in practice, owing to their tendency to reward predictions that can be deduced to be redundant. we also show, however, that these and other deductions can be harnessed to substantially recover the benefits of prediction. second, we propose an approach that significantly simplifies the use of misconfiguration prediction algorithms. we remove the need to hand-tune (and empirically determine the effects of) various parameters, and instead replace them with a single, intuitive tuning parameter. we show empirically that this approach is generally competitive in terms of benefit and accuracy with algorithms that require hand-tuned parameters.discovering access-control misconfigurations','Access control'
'it is well known that power control can help to improve spectrum utilization in cellular wireless systems. however, many existing distributed power control algorithms do not work well without an effective connection admission control (cac) mechanism, because they could diverge and result in dropping existing calls when an infeasible call is admitted. in this work, based on a system parameter defined as the discriminant, we propose two distributed cac algorithms for a power-controlled system. under these cac schemes, an infeasible call is rejected early, and incurs only a small disturbance to existing calls, while a feasible call is admitted and the system converges to the pareto optimal power assignment. simulation results demonstrate the performance of our algorithms.distributed admission control for power-controlled cellular wireless systems','Access control'
'in this paper, we study jointly the problems of rate control and contention resolution in multi-cell wlans based on the ieee 802.11 mac protocol in the presence of hidden terminals. we adopt the network utility maximization (num) framework to formulate the problem as a nonlinear optimization problem. unlike previous approaches that require maximal weight scheduling or target simple mac protocols without hidden terminals, our problem formulation considers a realistic ieee 802.11-based mac layer model. the resulting formulation being non-convex, we propose a simple scheme to transform this non-convex problem into a convex one. we derive a simple distributed algorithm to solve the problem via lagrangian dual decomposition. our algorithm does not require exchanges of topology or spatial information about the mobile clients between aps. practical issues like random backoff, carrier sensing, frame retransmission and optimal contention window (cw) setting are considered in our design. the effectiveness, accuracy and convergence speed of our analytically derived algorithm are verified via numerical experiments and simulations.distributed rate control and contention resolution in multi-cell ieee 802.11 wlans with hidden terminals','Access control'
'we investigate the cost of changing access control policies dynamically as a response action in computer network defense. we compare and contrast the use of access lists and capability lists in this regard, and develop a quantitative feel for the performance overheads and storage requirements. we also explore the issues related to preserving safety properties and trust assumptions during this process. we suggest augmentations to policy specifications that can guarantee these properties in spite of dynamic changes to system state. using the lessons learned from this exercise, we apply these techniques in the design of dynamic access controls for dynamic environments.dynamic access control','Access control'
'in this paper, we present a mandatory access control system that uses input from multiple stakeholders to compose policies based on runtime information. in the emerging open cell phone system environment, many devices run software whose access permissions depends on multiple stakeholders, such as the device owner, the service provider, the application owner, etc., rather than a single system administrator. however, current access control administration remains as either discretionary, allowing the running and perhaps compromised process to administer permissions, or mandatory, requiring a system administrator to know all permissions for all possible legal runs. a key problem is that users may download arbitrary programs to their devices, requiring that the system contain such programs while allowing some reasonable functionality. however, such programs may need access to permissions that in combination with other conflicting permissions may lead to an attack, such as allowing voice-over-ip calls. in our approach, we use a \"soft\" sand-boxing mechanism to first contain such processes, request the stakeholder to authorize operations outside the sandbox that are not prohibited by policy, and maintain a runtime execution role for the process to identify its access state to the stakeholders. we define a proxy policy server that caches and combines stakeholder policies to make such access decisions. our framework was implemented by modifying the selinux module and using a remote proxy policy server, although a local proxy policy server is also possible. we incur a 0.288 ts performance overhead only when stakeholders need to be consulted, and new permissions are cached.dynamic mandatory access control for multiple stakeholders','Access control'
'due to small size of sensor nodes deployed in wireless sensor networks (wsns), energy utilization is a key issue. poor channel conditions lead to retransmissions and hence, result in energy wastage. error control strategies are usually utilized to accommodate channel impairments like noise and fading in order to optimize energy consumption for network lifetime enhancement. meanwhile, cooperative communication also emerges to be an appropriate candidate to combat the effects of channel fading. energy efficiency of cooperative scheme when applied with automatic repeat request (arq), hybrid-arq (harq) and forward error correction (fec) is investigated in this work. moreover, the expressions for energy efficiency of direct transmission, single relay cooperation and multi relay cooperation are also derived. in all, our work is focused towards energy optimal communication in wsns. our results show that error control strategies along with the cooperative scheme significantly enhances system performance in the form of energy optimization.energy aware error control in cooperative communication in wireless sensor networks','Access control'
'as users store and share more digital content at home, effective access control becomes increasingly important. one promising mechanism for helping non-expert users create accurate access policies is reactive policy creation, in which users can update their policy dynamically in response to access requests that cannot otherwise succeed. an earlier study suggested that reactive policy creation may be a good fit for file access control at home. to test this theory, we designed and piloted an experience sampling study in which participants used a simulated reactive access control system for a week. preliminary results suggest a neutral to positive response to using this kind of system and indicate that reactive policy creation may help meet users\' need for dynamic, contextual policy decisions.exploring reactive access control','Access control'
'access control models are usually static, i.e, permissions are granted based on a policy that only changes seldom. especially for scenarios in health care and disaster management, a more flexible support of access control, i.e., the underlying policy, is needed. break-glass is one approach for such a flexible support of policies which helps to prevent system stagnation that could harm lives or otherwise result in losses. today, break-glass techniques are usually added on top of standard access control solutions in an ad-hoc manner and, therefore, lack an integration into the underlying access control paradigm and the systems\' access control enforcement architecture. we present an approach for integrating, in a fine-grained manner, break-glass strategies into standard access control models and their accompanying enforcement architecture. this integration provides means for specifying break-glass policies precisely and supporting model-driven development techniques based on such policies.extending access control models with break-glass','Access control'
'we discuss the protection requirements of a distributed storage service comprising a two-level hierarchy of storage servers with value-adding service layers above them. a flexible and extensible access control mechanism is required. our scheme uses access control lists (acls) to allow fine grained expression of policy together with capabilities for efficient runtime access after a once-off acl check. our capabilities are principal- specific and transient and their design ensures that access to objects is via the correct service hierarchy; for example, a directory object may only be manipulated via a directory service. the implementation of this protection is stateless at the servers above the storage service. the scheme also provides a convenient means to delegate rights for an object, temporarily, to an unprivileged server, for example a print-server. the fact that our capabilities are short-lived alleviates the requirement for selective revocation and crash recovery.extensible access control for a hierarchy of servers','Access control'
'recent studies on operational wireless lans (wlans) have shown that user load is often unevenly distributed among wireless access points (aps). this unbalanced load results in unfair bandwidth allocation among users. we observe that the unbalanced load and unfair bandwidth allocation can be greatly alleviated by intelligently associating users to aps, termed association control, rather than having users greedily associate aps of best received signal strength.in this study, we present an efficient algorithmic solution to determine the user-ap associations that ensure max-min fair bandwidth allocation. we provide a rigorous formulation of the association control problem that considers bandwidth constraints of both the wireless and backhaul links. our formulation indicates the strong correlation between fairness and load balancing, which enables us to use load balancing techniques for obtaining near optimal max-min fair bandwidth allocation. since this problem is np-hard, we present algorithms that achieve a constant-factor approximate max-min fair bandwidth allocation. first, we calculate a fractional load balancing solution, where users can be associated with multiple aps simultaneously. this solution guarantees the fairest bandwidth allocation in terms of max-min fairness. then, by utilizing a rounding method we obtain an efficient integral association. in particular, we provide a 2-approximation algorithm for unweighted greedy users and a 3-approximation algorithm for weighted and bounded-demand users. in addition to bandwidth fairness, we also consider time fairness and we show it can be solved optimally. we further extend our schemes for the on-line case where users may join and leave. our simulations demonstrate that the proposed algorithms achieve close to optimal load balancing and max-min fairness and they outperform commonly used heuristic approaches.fairness and load balancing in wireless lans using association control','Access control'
'much previous work has examined the wireless power control problem using tools from game theory, an economic concept which describes the behavior of interdependent but non-cooperative users. in this paper, we expand these ideas to the antecedent process of deciding which users may participate in the network, i.e. the admission control problem. in particular, we propose three distinct pricing schemes for influencing users as they make their participation decisions. we fully characterize the equilibria induced by each and then test their performance in a simulated, wireless environment. our preliminary results show that these schemes have the potential to produce high quality outcomes in an incentive-compatible way.game-based admission control for wireless systems','Access control'
'we investigate multi-channel transmission schemes for packetized wireless data networks. the transmitting unit transmits concurrently in several orthogonal channels (for example, distinct fdma bands or cdma codes) with randomly fluctuating interference and there is a global constraint on the total power transmitted across all channels at any time slot. incoming packets to the transfer are queued up in separate buffers, depending on the channel they are to be transmitted in. in each time slot, one packet can be transmitted in each channel from its corresponding queue. the issue is how much power to transmit in each channel, given the interference in it and the packet backlog, so as to optimize various power and delay costs associated with the system. we formulate the general problem taking a dynamic programming approach. through structural decomposition of the problem, we design practical novel algorithms for allocating power to various channels under the global constraint.globally constrained power control across mulitple channels in wireless data networks','Access control'
'h-mac is a novel time division multiple access (tdma) based mac protocol designed for body sensor networks (bsns). it improves energy efficiency by exploiting human heartbeat rhythm information to perform time synchronization for tdma. heartbeat rhythm is inherent in every human body and can be detected in a variety of biosignals. therefore, biosensors in bsns can extract the heartbeat rhythm from their sensory data. moreover, all the rhythms represented by peak sequences are naturally synchronized since they are driven by the same source, the heartbeat. by following the rhythm, wireless biosensors can achieve time synchronization without having to turn on their radio to receive periodic timing information from a central controller, so that energy cost for time synchronization can be completely avoided and the lifetime of network can be prolonged. an active synchronization recovery scheme is also developed, in which two resynchronization procedures are implemented. the algorithms are verified using real world data from mit-bih multi-parameter database mimic.heartbeat driven medium access control for body sensor networks','Access control'
'femtocells are low-power cellular base stations that can improve indoor coverage and offload data traffic from the macrocell network. nevertheless, access control that has strong impacts on interference mitigation is still an important issue for the femtocell network. in this paper, we consider a two-tier orthogonal frequency division multiple access (ofdma) femtocell network that contains one macrocell base station (mbs), several femtocell access points (faps) and femtocell user equipments (fues). in particular, fues can get better service by selecting faps properly according to their reported channel information and pay for the resource they use. however, since channel information is private, fues cannot know the truth. so faps have incentive to report exaggerated information to win greater opportunity to be selected by fues and gain more payments. we propose a joint access control and subchannel allocation (jacsa) scheme to solve the truth-telling problem and provide a suboptimal subchannel allocation solution. further analysis shows that by adding a transfer payment to each fap, the maximum expected total payment can only be obtained when all faps report the true information. moreover, the subchannel allocation solution can achieve near optimal performances with low computational complexity compared with the optimal subchannel allocation (osa) scheme.joint access control and subchannel allocation scheme for ofdma femtocell network using a truthful mechanism','Access control'
'in this paper, we propose a scheme to minimize power consumption and guarantee delay requirement in wireless sensor networks. based on queue and delay information, we propose a joint controller, which controls both sleeping time and queue threshold in a distributed manner. also, we provide a delay notification mechanism which helps nodes determine an appropriate queue threshold according to the application-specific delay requirements. we analyze the adaptive behavior of the proposed method using the first-moment approximation. based on control theory, we derive conditions for system stability. asymptotic analysis shows that our proposed algorithm guarantees end-to-end delay requirement by controlling parameters of local nodes. simulation results indicate that the proposed scheme outperforms existing scheduling protocols in terms of delay and power consumption.joint queue and sleep control for energy-efficiency and delay guarantees in wireless sensor networks','Access control'
'ieee 802.11 wireless networks perform poorly in the presence of large traffic volumes. measurements have shown that packet collisions and interference can lead to degraded performance to the extent that users experience unacceptably low throughput, which can ultimately lead to complete network breakdown [12]. an admission control framework that limits network flows can prevent network breakdown and improve the performance of throughput and delay-sensitive multimedia applications. in this paper, we present a measurement-driven admission control scheme that leverages wireless characteristics for intelligent flow control in a static wireless network. experiments on the 25 node ucsb meshnet show that the proposed admission control scheme can enhance network performance such that the qos requirements of real time applications, such as voip, can be met.measurement-driven admission control on wireless backhaul networks','Access control'
'cognitive wireless ad hoc is highly focused as a promising future wireless technology. however, cognitive wireless ad hoc networks have some difficulties of neighbor discovery and routing. for helping neighbor discovery and routing in the cognitive wireless ad hoc networks, we introduce a common link control radio (clcr) that is a common active wireless system of cognitive terminals. in addition, we propose a novel cognitive ad hoc routing protocol based on a common link control radio (clcr) called a minimum weight routing protocol. we show that the proposed routing protocol can greatly reduce communication overhead in cognitive wireless ad hoc networks.minimum weight routing based on a common link control radio for cognitive wireless ad hoc networks','Access control'
'this paper presents call admission control and bandwidth reservation schemes in wireless cellular networks that have been developed based on assumptions more realistic than existing proposals. in order to guarantee the handoff dropping probability, we propose to statistically predict user mobility based on the mobility history of users. our mobility prediction scheme is motivated by computational learning theory, which has shown that prediction is synonymous with data compression. we derive our mobility prediction scheme from data compression techniques that are both theoretically optimal and good in practice. in order to utilize resource more efficiently, we predict not only the cell to which the mobile will handoff but also when the handoff will occur. based on the mobility prediction, bandwidth is reserved to guarantee some target handoff dropping probability. we also adaptively control the admission threshold to achieve a better balance between guaranteeing handoff dropping probability and maximizing resource utilization. simulation results show that the proposed schemes meet our design goals and outperform the static-reservation and cell-reservation schemes.mobility-based predictive call admission control and bandwidth reservation in wireless cellular networks','Access control'
'the proposed scheme utilizes local information and a predictive model to provide feedback control commands for power adjustments. a sufficient condition that ensures system stability is obtained. in addition, the bound of the received carrier-to-interference ratio (cir) is derived in the presence of short-term fading. the bound of the received cir is shown to be a function of the power control step size and the target cir. simulation results were obtained to verify the theoretical derivations.on the convergence of adaptive power control algorithm for cellular systems','Access control'
'the evolution of information systems sees an increasing need of flexible and sophisticated approaches for the automated detection of anomalies in security policies. one of these anomalies is redundancy, which may increase the total cost of management of the policies and may reduce the performance of access control mechanisms and of other anomaly detection techniques. we consider three approaches that can remove redundancy from access control policies, progressively reducing the number of authorizations in the policy itself. we show that several problems associated with redundancy are np-hard. we propose exact solutions to two of these problems, namely the minimum policy problem, which consists in computing the minimum policy that represents the behaviour of the system, and the minimum irreducible policy problem, consisting in computing the redundancy-free version of a policy with the smallest number of authorizations. furthermore we propose heuristic solutions to those problems. we also present a comparison between the exact and heuristics solutions based on experiments that use policies derived from bibliographical databases.on the notion of redundancy in access control policies','Access control'
'one of the challenging issues in the energy-constrained ad~hoc wireless networks is to find ways that increase their lifetime. squeezing maximum energy from the battery of the nodes of these networks requires the communication protocols to be designed such that they are aware of the state of the batteries. traditional mac protocols for ad~hoc networks are designed without considering the battery state. major contributions of this paper are: (a) a novel distributed battery aware medium access control (bamac(k)) protocol that takes benefit of the chemical properties of the batteries, to provide fair scheduling and increased network and node lifetime through uniform discharge of batteries, (b) a discrete time markov chain analysis for batteries of the nodes of ad~hoc wireless networks, and (c) a thorough comparative study of our protocol with ieee 802.11 and dwop (distributed wireless ordering protocol) mac protocols. the key idea proposed in this paper is to piggy-back nodes\' battery-state information with the packets sent by the nodes by means of which the nodes are scheduled to ensure a uniform battery discharge. we model the operation of the battery using a discrete time markovian chain. using the theoretical analysis, we calculate lifetime of the battery in terms of maximum number of packets that a node can transmit before its battery drains fully. extensive simulations have shown that our protocol extends the battery lifetime consuming 96\% and 60\% less percentage nominal capacity spent per packet transmission compared to the ieee 802.11 and the dwop mac protocols, respectively. in general, performance results show that bamac(k) outperforms ieee 802.11 and dwop mac protocols, in terms of power consumption, fairness, and lifetime of the nodes. we have also analyzed the factors that influence the uniform discharge of batteries and their lifetime.on using battery state for medium access control in ad hoc wireless networks','Access control'
'in tdma-based point-to-multipoint rural wireless deployments, co-located base station radios and sector antennas are used to increase base station capacity. to achieve maximum capacity with limited availability of non-overlapping wireless channels, we need to operate as many radios as possible from different sectors on the same channel. however, operating co-located radios on the same channel can result in substantial interference especially with the current practice of operating all radios at maximum power. we investigate techniques that increase network throughput by eliminating this interference. to this end we formulate an lp optimization problem that maximizes throughput by computing optimal transmit schedules, optimal allocation of clients to base station radios, and optimal radio power levels. our results suggest that there is a large gap between currently-used and optimal strategies, creating opportunities for simple, practical algorithms to address these issues. our techniques are equally applicable to both wifi based networks as well as other point-to-multipoint technologies such as wimax.optimal scheduling and power control for tdma based point to multipoint wireless networks','Access control'
'optimality of greedy power control and variable spreading gain in multi-class cdma mobile networks','Access control'
'this panel will address the following question. does an increase in the granularity of access control systems produce a measurable reduction in risk and help meet the goals of the organization, or is the cost prohibitively high? after decades of access control research, products, and practice, there has been a trend towards more complex access control policies and models that more finely restrict (or allow) access to resources. this allows policy administrators to more closely specify any high level abstract policy they may have in mind, or accurately enforce regulations such as hippa, sox, or pci. the end goal is to allow only those actions that are desirable in hindsight, or via an approach to which bishop et al. refer as the oracle policy. as the expressive power of access control models can vary, an administrator may need a more powerful model to specify the high level policy they need for their particular application. it is not uncommon for new models to add new key-attributes, data-sources, features, or relations to provide a richer set of tools. this has resulted in an explosion of new one-off models in the literature, few of which make their way to real products or deployment. to increase the expressive power of a model, increase its granularity, reduce the complexity of administration and to answer desirable security queries such as safety, a plethora of new concepts have been added to access control models. to name a few: groups and roles; hierarchies and constraints; parameterized permissions; exceptions; time and location of users and resources; relationships between subjects; attributes of subjects, objects, and actions; information flow; conflict of interest classes; obligations; trust, benefit, and risk; workflows; delegation; situational awareness and context; and so on. all of these constructs build to a meta-model, as barker observes. this granularity has resulted in many novel and useful findings, new algorithms, and challenging open research issues, but poses potential problems as well. with granularity often comes complexity which manifests itself in specifying policies, managing and maintaining policies over time, and auditing logs to ensure compliance. this panel will discuss issues surrounding the problem of complexity in access control. from designing and specifying new models, designing enforcement mechanisms on real-world systems, policy lifecycle, and the role of analytics from automatically generating policies to auditing logs. so, is this complexity worth it? does increasing the granularity produce a measurable reduction in the risk to sensitive resources and protect the goals of the organization or is the cost prohibitively high? can we ever truly specify a \"correct\" and \"complete\" policy, which may be too dynamic and require the interpretation of the courts to decide, especially when policies are intended to enforce ambiguous regulations. finally, at what cost should we strive for a perfect, fine-grained policy? should more resources be places on recovery from security breaches than on prevention? should we be \"going for mean time to repair equals zero rather than mean time between failure equals infinity.\"panel on granularity in access control','Access control'
'random medium-access-control (mac) algorithms have played an increasingly important role in the development of wired and wireless local area networks (lans) and yet the performance of even the simplest of these algorithms, such as slotted-aloha, are still not clearly understood. in this paper we provide a general and accurate method to analyze networks where interfering users share a resource using random mac algorithms. we show that this method is asymptotically exact when the number of users grows large, and explain why it also provides extremely accurate performance estimates even for small systems. we apply this analysis to solve two open problems: (a) we address the stability region of non-adaptive aloha-like systems. specifically, we consider a fixed number of buffered users receiving packets from independent exogenous processes and accessing the resource using aloha-like algorithms. we provide an explicit expression to approximate the stability region of this system, and prove its accuracy. (b) we outline how to apply the analysis to predict the performance of adaptive mac algorithms, such as the exponential back-off algorithm, in a system where saturated users interact through interference. in general, our analysis may be used to quantify how far from optimality the simple mac algorithms used in lans today are, and to determine if more complicated (e.g. queue-based) algorithms proposed in the literature could provide significant improvement in performance.performance of random medium access control, an asymptotic approach','Access control'
'in this paper, we propose a subscription-based policy control framework that implements a subscription-centered approach for policy control for ip multimedia subsystem (ims), defined by 3rd generation partnership project (3gpp), release 7. the framework also enables flexible policy definitions based on the subscriber\'s profile at the application level. in addition, the framework provides functionalities of organizing the subscription data, identifying the policy, stipulating the policy control process, interpreting, managing and enforcing the corresponding policies. the main objective of this framework is to qualify the subscribers and thus, enhance the network customization reflected by the capabilities of defining various flexible policies based on different subscriber policy control requirements.policy control framework for ip multimedia subsystem','Access control'
'the application of policy in telecommunicationsnetworks is still in its initial stages. since networkelements lack native policy mechanisms, the impact ofusing policy, instead of snmp or cli basedconfiguration, is not clearly understood. this paperpresents a \"policy control model\" and self-organisingcapabilities that network elements mustexhibit to realise the policy potential for makingservice management simpler and more efficient. in thiscontext, we discuss the importance of roles, propose aseparation of functionality that makes policy solutionsadaptable to diverse operational environments, andstudy the benefits of self-organising network elementsobtaining their policy configurations through astateless pull mechanism.policy control model','Access control'
'this paper presents a novel access control framework reducing the access control problem to a traditional decision problem, thus allowing a policy designer to reuse tools and techniques from the decision theory. we propose here to express, within a single framework, the notion of utility of an access, decisions beyond the traditional allowing/denying of an access, the uncertainty over the effect of executing a given decision, the uncertainty over the current state of the system, and to optimize this process for a (probabilistic) sequence of requests. we show that an access control mechanism including these different concepts can be specified as a (partially observable) markov decision process, and we illustrate this framework with a running example, which includes notions of conflict, critical resource, mitigation and auditing decisions, and we show that for a given sequence of requests, it is possible to calculate an optimal policy different from the naive one. this optimization is still possible even for several probable sequences of requests.quantitative access control with partially-observable markov decision processes','Access control'
'nowadays, remarkable development of information technologies makes information sharing and distributing possible in smooth water than ever. it enables to obtain information anytime and anywhere and provides technologies for superior services based on situation information. also, it enables machines to grasp the meaning of information under semantic web technologies and new information paradigm. these technologies have been varied by information approach and acquisition method than ever before. however, illegal attacker behind development that uses better tools gives rise to side effects. current systems can be intruded by a number of different ways of them. therefore, system safety and privacy protection is being threatened by the invaders. there is access control technology for database system among the security technologies against this menace. recently, security technique is carrying out researches of situation intelligence, privacy, and xml access etc. in order to correspond with new computing environments. in this paper, we propose extended role based access control (rbac) in semantic web. extended model can dynamically authorizes user\'s permission under context component. also, we propose concept-enforcement model based on semantic web. owl defines the terms used to describe and represent an area of knowledge. proposed model enable to access semantic execution by suggested model even though it doesn\'t correspond with security policy.rbac-based access control for privacy preserving in semantic web','Access control'
'pervasive environment is a post-desktop model of human-computer interaction in which information processing has been thoroughly integrated into everyday object and activities. in there environment access control is a critical issue, with many aspects relating to the establishment, authorization and enforcement of policies that protect the resources from adversaries. recently, many researches have been worked methods that have access to resources in ubiquitous applications. however they are inadequate to meet the requirement for privacy safeguard and dynamic changes. in this paper we propose how to protect sensitive data and present extended role based access control (rbac) that respond to all the requirements for privacy control. we also show how proposed model preserves safety properties in spite of dynamic changes to access control permission.rbac-based access control for privacy protection in pervasive environments','Access control'
'the relationship-based access control (rebac) model was recently proposed as a general-purpose access control model. it supports the natural expression of parameterized roles, the composition of policies, and the delegation of trust. fong proposed a policy language that is based on modal logic for expressing and composing rebac policies. a natural question is whether such a language is representationally complete, that is, whether the language is capable of expressing all rebac policies that one is interested in expressing. in this work, we argue that the extensive use of what we call relational policies is what distinguishes rebac from traditional access control models. we show that fong\'s policy language is representationally incomplete in that certain previously studied relational policies are not expressible in the language. we introduce two extensions to the policy language of fong, and prove that the extended policy language is representationally complete with respect to a well-defined subclass of relational policies.relationship-based access control policies and their policy languages','Access control'
'access control policy is typically defined in terms of attributes, but in many applications it is more natural to define permissions in terms of relationships that resources, systems, and contexts may enjoy. the paradigm of relationship-based access control has been proposed to address this issue, and modal logic has been used as a technical foundation. we argue here that hybrid logic -- a natural and well-established extension of modal logic -- addresses limitations in the ability of modal logic to express certain relationships. we identify a fragment of hybrid logic to be used for expressing relationship-based access-control policies, show that this fragment supports important policy idioms, and demonstrate that it removes an exponential penalty in existing attempts of specifying complex relationships such as \"at least three friends\". we also capture the previously studied notion of relational policies in a static type system.relationship-based access control','Access control'
'the access matrix is a useful model for understanding the behaviour and properties of access control systems. while the matrix is rarely implemented, access control in real systems is usually based on access control mechanisms, such as access control lists or capabilities, that have clear relationships with the matrix model. in recent times a great deal of interest has been shown in role based access control (rbac) models. however, the relationship between rbac models and the access matrix is not clear. in this paper we present a model of rbac based on the access matrix which makes the relationships between the two explicit. in the process of constructing this model, some fundamental similarities between certain capability models and rbac are revealed.role-based access control and the access control matrix','Access control'
'role-based acess control','Access control'
'unlike traditional multihop forwarding among stationary sensor nodes, use of mobile devices for data collection in wireless sensor networks has recently been gathering more attention. the use of mobility significantly reduces the energy consumption at sensor nodes, elongating the functional lifetime of the network. however, a drawback is an increased data delivery latency. reducing the latency through optimizing the motion of data mules is critical for this approach to thrive. in this article, we focus on the problem of motion planning, specifically, determination of the speed of the data mule and the scheduling of the communication tasks with the sensors. we consider three models of mobility capability of the data mule to accommodate different types of vehicles. under each mobility model, we design optimal and heuristic algorithms for different problems: single data mule case, single data mule with periodic data generation case, and multiple data mules case. we compare the performance of the heuristic algorithm with a naive algorithm and also with the multihop forwarding approach by numerical experiments. we also compare one of the optimal algorithms with a previously proposed method to see how our algorithm improves the performance and is also useful in practice. as far as we know, this study is the first of a kind that provides a systematic understanding of the motion planning problem of data mules.speed control and scheduling of data mules in sensor networks','Access control'
'role based access control (rbac) is a widely used access control paradigm. in large organizations, the rbac policy is managed by multiple administrators. an administrative role based access control (arbac) policy specifies how each administrator may change the rbac policy. it is often difficult to fully understand the effect of an arbac policy by simple inspection, because sequences of changes by different administrators may interact in unexpected ways. arbac policy analysis algorithms can help by answering questions, such as user-role reachability, which asks whether a given user can be assigned to given roles by given administrators. allowing roles and permissions to have parameters significantly enhances the scalability, flexibility, and expressiveness of arbac policies. this paper defines parbac, which extends the classic arbac97 model to support parameters, and presents an analysis algorithm for parbac. to the best of our knowledge, this is the first analysis algorithm specifically for parameterized arbac policies. we evaluate its efficiency by analyzing its parameterized complexity and benchmarking it on case studies and synthetic policies.symbolic reachability analysis for parameterized administrative role based access control','Access control'
'the evolution of wireless transmission technology enables users to have multimedia services anywhere in the digital home network. in this paper, we propose a handoff control scheme called computing powerless handoff (cp-handoff) for devices without computation capability, such as the light-weight bluetooth (lw-bt) headset. the main concern of the cp-handoff is that the client device doesn\'t have the computation capability. the main function of the computing powerless client device is to send/receive signals and play the audio data. thus, how to have the handoff control when a computing powerless client device is switched from one access point to another access point becomes a problem to be resolved. a typical example is in the bt wireless network environment. the lw-bt headset is dedicated for audio applications and doesn\'t have extra computation capability. the cp-handoff puts all handoff computation overhead on the master bt device, such that the lw-bt headset is unaware of the handoff. based on the proposed cp-handoff control scheme, a ubiquitous audio access platform (uaap) is proposed and developed to allow users to access audio files using the lw-bt headset in the digital home network when they are roaming. the evaluation results show that the proposed cp-handoff only needs 1.17s to prepare the avdtp session handoff and 0.38s for the avdtp session handoff. the transmission of audio packets to the lw-bt headset is only suspended for 0.38s. the audio packet buffering scheme and retransmission mechanism ensure that the lw-bt headset can resume lost audio packets during handoff.the handoff control scheme for computing powerless devices and its applications in the digital home network','Access control'
'we propose an approach to topology control based on the principle of maintaining the number of neighbors of every node equal to or slightly below a specific value k. the approach enforces symmetry on the resulting communication graph, thereby easing the operation of higher layer protocols. to evaluate the performance of our approach, we estimate the value of k that guarantees connectivity of the communication graph with high probability. we then define k-neigh, a fully distributed, asynchronous, and localized protocol that follows the above approach and uses distance estimation. we prove that k-neigh terminates at every node after a total of 2n messages have been exchanged (with n nodes in the network) and within strictly bounded time. finally, we present simulations results which show that our approach is about 20\% more energy-efficient than a widely-studied existing protocol.the k-neigh protocol for symmetric topology control in ad hoc networks','Access control'
'this paper proposes to utilize virtual backbone to handle control messages in ad hoc networks. the virtual backbone is built by using the minimum connected dominating set (mcds) on a graph. the first part of this paper presents a new algorithm to construct the mcds. the construction of the mcds is formulated using the linear programming approach. we compared the performance of this procedure with those other previous approaches, and we find that our approach is less complex and gives the nearest solution to the optimal one. the second part of this paper presents different techniques of diffusion in ad hoc networks such as flooding, clustering, mp relay, and backbone based on mcds, etc. the flooding technique is simple and efficient, but it is expensive in term of bandwidth, and causes excessive flows of message etc. simulation results show that the approach of virtual backbone based mcds outperforms flooding and mp relay.virtual backbone based on mcds for topology control in wireless ad hoc networks','Access control'
'conducting enterprise-wide vulnerability assessment (va) on a regular basis plays an important role in assessing an enterprise\'s information system security status. however, an enterprise network is usually very complex, divided into different types of zones, and consisting of hundreds of hosts in the networks. the complexity of it systems makes va an extremely time-consuming task for security professionals. they are seeking for an automated tool that helps monitor and manage the overall vulnerability of an enterprise. this paper presents a novel methodology that provides a dashboard solution for managing enterprise level vulnerability. in our methodology, we develop a multi-layer tree based model to describe enterprise vulnerability topology. then we apply a client/server structure to gather vulnerability information from enterprise resources automatically. finally a set of well-defined metric formulas is applied to produce a normalized vulnerability score for the whole enterprise. as a prototype, we developed the implementation of our methodology, evmat, an enterprise vulnerability management and assessment tool, to test our method. experiments on a small e-commerce company and a small it company demonstrate the great potentials of our tool for enterprise-level security.a multi-layer tree model for enterprise vulnerability management',Authorization
'this paper presents three encoding strategies based on digital logic for steganography on voice over ip (voip), which aim to enhance the embedding transparency. differing from previous approaches, our strategies reduce the embedding distortion by improving the similarity between the cover and the covert message using digital logical transformations, instead of reducing the amount of the substitution bits. therefore, by contrast, our strategies will improve the embedding transparency without sacrificing the embedding capacity. of these three strategies, the first one adopts logical operations, the second one employs circular shifting operations, and the third one combines the operations of the first two. all of them are evaluated through comparing their prototype implementations with some existing methods in a prototypical covert communication system based on voip (called stegvoip). the experimental results show that the proposed strategies can effectively enhance the embedding transparency while maintaining the maximum embedding capacity.digital logic based encoding strategies for steganography on voice-over-ip',Authorization
'home office',Authorization
'the security of information systems is a serious issue because computer abuse is increasing. it is important, therefore, that systems analysts and designers develop expertise in methods for specifying information systems security. the characteristics found in three generations of general information system design methods provide a framework for comparing and understanding current security design methods. these methods include approaches that use checklists of controls, divide functional requirements into engineering partitions, and create abstract models of both the problem and the solution. comparisons and contrasts reveal that advances in security methods lag behind advances in general systems development methods. this analysis also reveals that more general methods fail to consider security specifications rigorously.information systems security design methods',Authorization
'information security is very important nowadays. every it system needs protection mechanisms for stability and safety of work. to solve this task, there are proposed a variety of security-providing solutions, but most of them are very expensive and non-systematic. the paper discusses up-to-date techniques implemented for security aims and addresses to the technique of security control based on settings monitoring of variable program components of the trusted information environment. there is proposed a formal basis of security control based on finding the security settings which provide the system with stability and integrity. the specified technique allows proposing a schema of dynamic security and integrity control system which provides an automated process of security assurance and management. these security control technique and system extend security-relevant approaches making security reachable, permanent, and effective.permanent protection of information systems with method of automated security and integrity control',Authorization
'security certification includes assessing an information system to verify its compliance with diverse, pre-selected security controls. the goal of certification is to identify where controls are implemented correctly and where they are violated, creating potential vulnerability risks. certification complexity is magnified in software composed of systems of systems where there are limited formal methodologies to express management policies, given a set of security control properties, and verify them against the interaction of the participating components and their individual security policy implementations. in this paper, we extend context unity, a formal, distributed, and context aware coordination language to support policy controls. the new language features enforce security controls and provide a means to declare policy specifics in a manner similar to declaring variable types. we use these features in a specification to show how verifying system compliance with selected security controls, such as those found in the nist sp800-53 document, can be accomplished.security policy foundations in context unity',Authorization
'this paper presents a unique summer project for a group of undergraduate students and high school computer teachers to gain research experiences in the area of cybersecurity. the students and teachers were selected from the participants in the nsf reu and ret programs at the host institution. through the research on security testing of a real-world online banking system, the students and teachers have not only learned about the cutting-edge security testing techniques, but also made publishable contributions to the research base. the two collaborating graduate assistants served as an immediate role model for the undergraduates and an indirect role model for high school students through the teachers. with the help from the graduate assistants, the students and teachers were able to work effectively toward achieving their research objectives. the internal competition helped the participants get a better sense of achievement and satisfaction. the research experiences also prepared the teachers with the necessary knowledge for introducing cybersecurity topics (e.g., secure programming) into future classroom activity. as such, the project described in this paper provides a model summer program for undergraduate and/or k-12 teachers to gain research experiences.software security testing of an online banking system',Authorization
'the lessons of the great chicago flood of 1992',Authorization
'in this paper, we discuss the creation of a student blue team to assist campus organizations with security incident response. we also explore approaches for establishing a relationship with university information technology staff, informing blue team members of professional and ethical responsibilities, and aiding system administrators with incident response and system hardening. finally, we discuss the benefits to students taking part in these activities, as well as their contributions to improving an organizations security posture.training cyber-defense and securing information assets using student blue teams',Authorization
'why 32-bit desktops need virus protection',Authorization
'your place or theirs&#8212;is onsite service worth it?: you\'d be surprised at what the guarantee doesn\'t cover. ask tough questions first',Authorization
'designing symmetric ciphers based on chaotic maps or cellular automata has a long but rarely successful history. in this paper, we examine some symmetric ciphers based on chaotic maps and cellular automata, and indicate how to reconcile design techniques for these primitives with current methodologies.a critique of some chaotic-map and cellular automata-based stream ciphers','Block and stream ciphers'
'saarinen recently proposed a chosen iv statistical attack, called the d-monomial test, and used it to find weaknesses in several proposed stream ciphers. in this paper we generalize this idea and propose a framework for chosen iv statistical attacks using a polynomial description. we propose a few new statistical attacks, apply them on some existing stream cipher proposals, and give some conclusions regarding the strength of their iv initialization. in particular, we experimentally detected statistical weaknesses in some state bits of grain-128 with full iv initialization as well as in the keystream of trivium using an initialization reduced to 736 rounds from 1152 rounds. we also propose some stronger alternative initialization schemes with respect to these statistical attacks.a framework for chosen iv statistical analysis of stream ciphers','Block and stream ciphers'
'to enhance the security and reliability of the widely&#45;used stream ciphers, a novel mesh check&#45;sum abft scheme for stream ciphers is developed. by utilising the ready&#45;made arithmetic unit in stream ciphers, single and multiple errors can be detected and corrected in a cheap way. to meet different requirements in practical applications, 4&#45;d mesh check&#45;sum abft scheme is proposed which can be applied to rc4 or other stream ciphers. the 2&#45;d mesh check&#45;sum abft scheme is able to detect and correct single error with high efficiency. the 4&#45;d mesh check&#45;sum abft scheme is capable of correcting up to three errors located randomly in an n&#45;element matrix with acceptable computation and bandwidth overhead. the workload can be remarkably reduced when most communications are error&#45;free. our scheme also provides one&#45;to&#45;one mapping between index and check&#45;sum, so that error can be located and recovered by easier logic and simpler operation.a mesh check&#45;sum abft scheme for stream ciphers','Block and stream ciphers'
'we propose a new pseudorandom generator based on linear feedback shift registers (lfsr) and feedback with carry shift registers (fcsr).we then present a variant of this generator which can used for a self-synchronizing stream cipher.a new class of stream ciphers combining lfsr and fcsr architectures','Block and stream ciphers'
'in 1980 m. hellman presented a cryptanalytic time-memory trade-off which reduces computational time by distributing possible key solution space n between memory m and time t, by using a pre-computed table, for block ciphers. this method was applied to stream ciphers by a. biryukov and a. shamir with consideration of known output data d. recently, p. oechslin described a technique which makes an improvement to hellman&#8217;s original idea in computational time with introducing the rainbow chain model for block ciphers. in this paper, we present the application of the rainbow chain model as a time-memory trade-off attack for stream ciphers.a new cryptanalytic time-memory trade-off for stream ciphers','Block and stream ciphers'
'we present an algebraic attack approach to a family of irregularly clock-controlled bit-based linear feedback shift register systems. in the general set-up, we assume that the output bit of one shift register controls the clocking of other registers in the system and produces a family of equations relating the output bits to the internal state bits. we then apply this general theory to four specific stream ciphers: the (strengthened) stop-and-go generator, the alternating step generator, the self-decimated generator and the step1/step2 generator. in the case of the strengthened stop-and-go generator and of the self-decimated generator, we obtain the initial state of the registers in a significantly faster time than any other known attack. in the other two situations, we do better than or as well as all attacks but the correlation attack. in all cases, we demonstrate that the degree of a functional relationship between the registers can be bounded by two. finally, we determine the effective key length of all four systems.algebraic attacks on clock-controlled stream ciphers','Block and stream ciphers'
'hermes8 [6,7] is one of the stream ciphers submitted to the ecrypt stream cipher project (estream [3]). in this paper we present an analysis of the hermes8 stream ciphers. in particular, we show an attack on the latest version of the cipher (hermes8f), which requires very few known keystream bytes and recovers the cipher secret key in less than a second on a normal pc. furthermore, we make some remarks on the cipher\'s key schedule and discuss some properties of ciphers with similar algebraic structure to hermes8.an analysis of the hermes8 stream ciphers','Block and stream ciphers'
'this paper proposes and analyzes an approach for design of stream ciphers based on joint computing over random and secret data. feasibility of encryption/ decryption computation when the ciphertext involve pure random data is shown. the core element of the proposed approach for stream ciphering is a pseudo-random embedding of the random bits into the ciphertext and this embedding plays role of a homophonic encoding. the initial ciphertext with the embedded random bits is further on intentionally degraded by its exposure to a moderate noise which can be modelled as the binary symmetric channel effect. a security evaluation of the proposed approach implies that its security appears as a consequence of hardness of the lpn problem, as well. the developed design has potential of providing that complexity of recovering the secret key in the known plaintext attack scenario is close to the complexity of recovering the secret key via the exhaustive search, i.e. close to the maximal possible one for the given size of the secret key. the proposed approach can be considered as a trade-off between the increased security and decreased communications efficiency which in a number of scenarios appears as a suitable one.an approach for stream ciphers design based on joint computing over random and secret data','Block and stream ciphers'
'in stream ciphers, the ratio of performance to the security is the most important issue. however, the s-boxes used in a stream cipher can become a bottleneck of speed due to use of large memory, difficulty in hardware realization and more processing. this paper proposes an s-box construction that is easy to implement both in hardware and software. the proposed s-box is efficient in speed, parameterized and scalable with excellent security properties. it also provides a designer with the flexibility to trade-off among speed, area and the security properties. the security analysis has been performed on the s-box. the security properties are found to be comparable with the existing standards.an efficient, parameterized and scalable s-box for stream ciphers','Block and stream ciphers'
'cube attacks were introduced in dinur and shamir (2009) as a cryptanalytic technique that requires only black box access to the underlying cryptosystem. the attack exploits the existence of low degree polynomial representation of a single output bit (as a function of the key and plaintext bits) in order to recover the secret key. although cube attacks can be applied in principle to almost any cryptosystem, most block ciphers iteratively apply a highly non-linear round function (based on sboxes or arithmetic operations) a large number of times which makes them resistant to cube attacks. on the other hand, many stream ciphers (such as trivium (de canni&#232;re and preneel 2008)), are built using linear or low degree components and are natural targets for cube attacks. in this paper, we describe in detail how to apply cube attacks to stream ciphers in various settings with different assumptions on the target stream cipher and on the data available to the attacker.applying cube attacks to stream ciphers in realistic scenarios','Block and stream ciphers'
'the f-fcsr stream cipher family has been presented a few years ago. apart from some flaws in the initial propositions, corrected in a later stage, there are no known weaknesses of the core of these algorithms. two variants, f-fcsr-h and f-fcsr-16, were proposed in the estream project, and f-fcsr-h v2 is one of the ciphers selected for the estream portfolio. in this paper we present a new and severe cryptanalytic attack on the f-fcsr stream cipher family. we give the details of the attack when applied to f-fcsr-h v2 and f-fcsr-16. the attack requires a few mbytes of received sequence, and the complexity is low enough to allow the attack to be performed on a single pc within seconds.breaking the stream ciphers f-fcsr-h and f-fcsr-16 in real time','Block and stream ciphers'
'this work shows that sequences generated by a class of linear cellular automata equal output sequences of certain nonlinear sequence generators a simple modelling process for obtaining the automata from a partial description of such generators is here described furthermore, a method that uses the linearity of these cellular models for reconstructing some deterministic bits of the keystream sequence is presented.concatenated automata in cryptanalysis of stream ciphers','Block and stream ciphers'
'correlation attacks on stream ciphers','Block and stream ciphers'
'power analysis attacks as side channel analysis techniques of cryptographic devices have been mounted against block ciphers and public key but rarely against stream ciphers. there are no reports on correlation power analysis (cpa) attack against stream ciphers so far. this paper proposes a novel cpa against synchronous stream ciphers. then we present two experiments of cpa attacks on stream ciphers a5/1 and e0. the experimental results indicate that cpa of synchronous stream ciphers is feasible.correlation power analysis attack against synchronous stream ciphers','Block and stream ciphers'
'we describe a cryptanalytical technique for distinguishing some stream ciphers from a truly random process. roughly, the ciphers to which this method applies consist of a \"non-linear process\" (say, akin to a round function in block ciphers), and a \"linear process\" such as an lfsr (or even fixed tables). the output of the cipher can be the linear sum of both processes. to attack such ciphers, we look for any property of the \"non-linear process\" that can be distinguished from random. in addition, we look for a linear combination of the linear process that vanishes. we then consider the same linear combination applied to the cipher\'s output, and try to find traces of the distinguishing property.in this report we analyze two specific \"distinguishing properties\". one is a linear approximation of the non-linear process, which we demonstrate on the stream cipher snow. this attack needs roughly 295 words of output, with work-load of about 2100. the other is a \"low-diffusion\" attack, that we apply to the cipher scream-0. the latter attack needs only about 243 bytes of output, using roughly 250 space and 280 time.cryptanalysis of stream ciphers with linear masking','Block and stream ciphers'
'the ssc2 is a fast software stream cipher designed for wireless handsets with limited computational capabilities. it is the only one stream cipher which is special designed aim to energy efficient cryptography for wireless sensor networks in recent years open literatures. in this paper, the improved guess-and-determine attacks on both lfsr and lagged-fibonacci half-ciphers of the ssc2 stream cipher are proposed. and some open problems about designing energy efficient stream cipher are discussed.cryptanalysis of the energy efficient stream ciphers ssc2','Block and stream ciphers'
'stream cipher is an important device of the gsm system. a secure stream cipher is based on the secure key generator. lo and chen proposed a new key generator for the gsm network. based on the basic architecture of their key generator, they designed three stream ciphers for different security levels of the gsm network. they claimed that the output of their key generator has a long period and satisfies randomness. consequently, they claimed that the stream ciphers are secure. this paper shows that their key generator and stream ciphers have some weakness. their stream cipher architectures are not secure.cryptanalysis on stream ciphers for gsm networks','Block and stream ciphers'
'this paper presents a new attack called decimation attack of most stream ciphers. it exploits the property that multiple clocking (or equivalently d-th decimation) of a lfsr can simulate the behavior of many other lfsrs of possible shorter length. it yields then significant improvements of all the previous known correlation and fast correlation attacks. a new criterion on the length of the polynomial is then defined to resist to the decimation attack. simulation results and complexity comparison are detailed for ciphertext only attacks.decimation attack of stream ciphers','Block and stream ciphers'
'pseudonoise sequences generated by linear feedback shift registers [1] with some nonlinear combining function have been proposed [2]-[5] for cryptographic applications as running key generators in stream ciphers. in this correspondence it will be shown that the number of trials to break these ciphers can be significantly reduced by using correlation methods. by comparison of computer simulations and theoretical results based on a statistical model, the validity of this analysis is demonstrated. rubin [6] has shown that it is computationally feasible to solve a cipher proposed by pless [2] in a known plaintext attack, using as few as 15 characters. here, the number of ciphertext symbols is determined to perform a ciphertext-only attack on the pless cipher using the correlation attack. our conclusion from the analysis is that the pseudonoise generator\'s output sequence and the sequences generated by the linear feedback shift registers should be uncorrelated. this leads to constraints for the nonlinear combining function to be used.decrypting a class of stream ciphers using ciphertext only','Block and stream ciphers'
'py and pypy are efficient array-based stream ciphers designed by biham and seberry. both were submitted to the estream competition. this paper shows that py and pypy are practically insecure. if one key is used with about 216ivs with special differences, with high probability two identical keystreams will appear. this can be exploited in a key recovery attack. for example, for a 16-byte key and a 16-byte iv, 223chosen ivs can reduce the effective key size to 3 bytes. for a 32-byte key and a 32-byte iv, the effective key size is reduced to 3 bytes with 224chosen ivs. py6, a variant of py, is more vulnerable to these attacks.differential cryptanalysis of the stream ciphers py, py6 and pypy','Block and stream ciphers'
'side-channel attacks on block ciphers and public key algorithms have been discussed extensively. however, there is only sparse literature about side-cannel attacks on stream ciphers. the few existing references mainly treat timing [8] and template attacks [10], or provide a theoretical analysis [6], [7] of weaknesses of stream cipher constructions. in this paper we present attacks on two focus candidates, trivium and grain, of the estream stream cipher project. the attacks exploit the resynchronization phase of ciphers. a novel concept for choosing initial value vectors is introduced, which totally eliminates the algorithmic noise of the device, leaving only the pure side-channel signal. this attack allows to recover the secret key with a small number of samples and without building templates. to prove the concept we apply the attack to hardware implementations of the ciphers. for both stream ciphers we are able to reveal the complete key.differential power analysis of stream ciphers','Block and stream ciphers'
'this paper presents a new type of distinguisher for the shrinking generator and the alternating-step generator with known feedback polynomial and for the multiplexor generator. for the former the distinguisher is more efficient than existing ones and for the latter it results in a complete breakdown of security. the distinguisher is conceptually very simple and lends itself to theoretical analysis leading to reliable predictions of its probability of success.distinguishing stream ciphers with convolutional filters','Block and stream ciphers'
'guess-and-determine (gd) attacks have recently been proposed for the effective analysis of word-oriented stream ciphers. this paper discusses gd attacks on clock-controlled stream ciphers, which use irregular clocking for a non-linear function. the main focus is the analysis of irregular clocking for gd attacks. we propose gd attacks on a typical clock-controlled stream cipher aa5, and calculate the process complexity of our proposed gd attacks. in the attacks, we assume that the clocking of linear feedback shift registers (lfsrs) is truly random. an important consideration affecting the practicality of these attacks is the question of whether these assumptions are realistic. because in practice, the clocking is determined by the internal states. we implement miniature ciphers to evaluate the proposed attacks, and show that they are applicable. we also apply the gd attacks to other clock controlled stream ciphers and compare them. finally, we discuss some properties of gd attacks on clock-controlled stream ciphers and the effectiveness of the clock controllers. our research results contain information that are useful in the design of clock-controlled stream ciphers.experimental analysis of guess-and-determine attacks on clock-controlled stream ciphers','Block and stream ciphers'
'fast correlation attacks on certain stream ciphers','Block and stream ciphers'
'in this paper, some new results are presented on the selective discrete fourier spectra attack introduced first as the r&#x00f8;njom-helleseth attack and the modifications due to r&#x00f8;njom, gong, and helleseth. the first part of this paper fills some gaps in the theory of analysis in terms of the discrete fourier transform (dft). the second part introduces the new fast selective dft attacks, which are closely related to the fast algebraic attacks in the literature. however, in contrast to the classical view that successful algebraic cryptanalysis of lfsr-based stream cipher depends on the degree of certain annihilators, the analysis in terms of the dft spectral properties of the sequences generated by these functions is far more refined. it is shown that the selective dft attack is more efficient than known methods for the case when the number of observed consecutive bits of a filter generator is less than the linear complexity of the sequence. thus, by utilizing the natural representation imposed by the underlying lfsrs, in certain cases, the analysis in terms of dft spectra is more efficient and has more flexibility than classical and fast algebraic attacks. consequently, the new attack imposes a new criterion for the design of cryptographic strong boolean functions, which is defined as the spectral immunity of a sequence or a boolean function.fast discrete fourier spectra attacks on stream ciphers','Block and stream ciphers'
'in this paper, the hardware implementations of six representative stream ciphers are compared in terms of performance, consumed area and the throughput-to-area ratio. the stream ciphers used for the comparison are zuc, snow3g, grain v1, mickey v2, trivium and e0. zuc, snow3g and e0 have been used for the security part of well known standards, especially wireless communication protocols. in addition, grain v1, mickey v2 and trivium are currently selected as the final portfolio of stream ciphers for profile 2 (hardware) by the estream project. the designs were implemented by using vhdl language and for the hardware implementations a fpga device was used. the highest throughput has been achieved by snow3g with 3330mbps at 104mhz and the lowest throughput has been achieved by e0 with 187mbps at 187mhz. also, the most efficient cipher for hardware implementation in terms of throughput-to-area ratio is mickey v2 cipher while the worst cipher for hardware implementation is grain v1.fpga-based performance analysis of stream ciphers zuc, snow3g, grain v1, mickey v2, trivium and e0','Block and stream ciphers'
'differential cryptanalysis is probably the most popular tool for chosen plaintext attacks on block ciphers. it also applies to chosen iv attacks on stream ciphers, but here, high order differential attacks have been surprisingly successful, namely on nlfsr-based constructions. most approaches have been developed in terms of the algebraic normal form of boolean functions. prominent examples are the d-monomial test, cube attacks, and cube testers. we review the various techniques and translate them into the terminology of high order derivatives introduced by lai. the unified view points out similarities between seemingly different approaches and naturally suggests generalizations and refinements such as conditional differential cryptanalysis.high order differential attacks on stream ciphers','Block and stream ciphers'
'in this paper, we surveyed hc-128 and hc-256 as methods for protecting the distribution of digital images in an efficient and secure way. we proposed the hongjun cipher (hc) image encryption algorithm based on column-wise raster scanning of the plain image. then, we performed a series of tests and some comparisons to justify the efficiency of surveyed algorithms for image encryption. these tests included key space analysis, visual test and histogram analysis, randomness analysis, information entropy, encryption quality, correlation analysis, differential analysis, sensitivity analysis and performance analysis. based on all analysis and experimental results, it can be concluded that the two variants of hc scheme are efficient, feasible and trustworthy to be adopted for image encryption.image encryption using hc-128 and hc-256 stream ciphers','Block and stream ciphers'
'in this paper, implementation and analysis of three different versions of pseudorandom bit generators (prbg) based on elliptic curves over prime and binary fields is presented. implementations are carried out so that the algorithms could be compared in terms of time complexity and sequences could be compared in terms of periodicity, since the periodicity of all the generated streams are not available in literature. based on the results of implementation and analysis, the pseudorandom bit generators (prbg) most suitable for software and hardware realisations of stream cipher are identified. the software implementations of prbg are carried out using mathematica and the implementations in vhdl are done using the altera quartus iiv6.0 simulation software. the montgomery\'s point multiplication method has also been discussed and implemented for comparison with the conventional point multiplication algorithm. together with this, faster software algorithms for field inversion and point counting are discussed.implementation and analysis of stream ciphers based on the elliptic curves','Block and stream ciphers'
'we motivate and describe a mode of operation hem (resp., them) that turns a n-bit blockcipher into a variable-input-length cipher (resp., tweakable cipher) that acts on strings of [n..2n&#8722;1] bits. both hem and them are simple and intuitive and use only two blockcipher calls, while prior work at least takes three. we prove them secure in the sense of strong prp and tweakable strong prp, assuming the underlying blockcipher is a strong prp.length-doubling ciphers and tweakable ciphers','Block and stream ciphers'
'linear feedback shift registers (lfsrs) are used as building blocks for many stream ciphers, wherein, an n-degree primitive connection polynomial is used as a feedback function to realize an n-bit lfsr. this paper shows that such lfsrs are susceptible to power analysis based side channel attacks (sca). the major contribution of this paper is the observation that the state of an n-bit lfsr can be determined by making o(n) power measurements. interestingly, neither the primitive polynomial nor the value of n be known to the adversary launching the proposed attack. the paper also proposes a simple countermeasure for the sca that uses n additional flipflops.lfsr based stream ciphers are vulnerable to power attacks','Block and stream ciphers'
'linear feedback shift register (lfsr) based stream ciphers are popular because of their low hardware implementation costs. the nonlinear combination generators and clock-controlled generators are two very commonly used schemes in lfsr based stream ciphers. fpga implementation of these two schemes has been done to obtain an idea about their hardware complexity. the fast correlation attack and edit distance attack, are among the fastest of the reported attacks on the nonlinear combination generators and clock-controlled generators respectively. these two attacks have been implemented. this paper compares the time for successful cryptanalytic attacks on both systems so as to compare their levels of security and hardware complexity.linear feedback shift register based stream ciphers','Block and stream ciphers'
'a general stream cipher with memory in which each cipher-text symbol depends on both the current and previous plaintext symbols, as well as each plaintext symbol depends on both the current and previous ciphertext symbols, is pointed out. it is shown how to convert any keystream generator into a stream cipher with memory and their security is discussed. it is proposed how to construct secure self-synchronizing stream ciphers, keyed hash functions, hash functions, and block ciphers from any secure stream cipher with memory. rather new and unusual designs can thus be obtained, such as the designs of block ciphers and (keyed) hash functions based on clock-controlled shift registers only.modes of operation of stream ciphers','Block and stream ciphers'
'to enhance the security and reliability of thewidely-used stream ciphers, a 2-d and a 3-d mesh-knightalgorithm based fault tolerant (abft) schemes for streamciphers are developed which can be universally applied to rc4 and other stream ciphers. based on the ready-made arithmetic unit in stream ciphers, the proposed 2-d abft scheme is able to detect and correct any simple error, and the 3-d mesh-knight abft scheme is capable of detecting andcorrecting up to three errors in a data matrix with liner computation and bandwidth overhead. the proposed schemes provide one-to-one mapping between data index and check sum group so that error can be located and recovered by easier logic and simple operations.multiple dimensional fault tolerant schemes for crypto stream ciphers','Block and stream ciphers'
'in cryptology we commonly face the problem of finding an unknown key &lt;emphasis fontcategory=\"sansserif\"&gt;k&lt;/emphasis&gt; from the output of an easily computable keyed function &lt;em&gt;f&lt;/em&gt; (&lt;em&gt;c&lt;/em&gt; ,&lt;emphasis fontcategory=\"sansserif\"&gt;k&lt;/emphasis&gt; ) where the attacker has the power to choose the public variable &lt;em&gt;c&lt;/em&gt; . in this work we focus on self-synchronizing stream ciphers. first we show how to model these primitives in the above-mentioned general problem by relating appropriate functions &lt;em&gt;f&lt;/em&gt; to the underlying ciphers. then we apply the recently proposed framework presented at africacrypt\'08 by fischer &lt;em&gt;et. al.&lt;/em&gt; for dealing with this kind of problems to the proposed t-function based self-synchronizing stream cipher by klimov and shamir at fse\'05 and show how to deduce some non-trivial information about the key. we also open a new window for answering a crucial question raised by fischer &lt;em&gt;et. al.&lt;/em&gt; regarding the problem of finding weak iv bits which is essential for their attack. new directions in cryptanalysis of self-synchronizing stream ciphers','Block and stream ciphers'
'the stream ciphers py, py6 designed by biham and seberry were promising candidates in the ecrypt-estream project because of their impressive speed. since their publication in april 2005, a number of cryptanalytic weaknesses of the ciphers have been discovered. as a result, a strengthened version pypy was developed to repair these weaknesses; it was included in the category of \'focus ciphers\' of the phase ii of the estream competition. however, even the new cipher pypy was not free from flaws, resulting in a second redesign. this led to the generation of three new ciphers tpypy, tpy and tpy6. the designers claimed that tpy would be secure with a key size up to 256 bytes, i.e., 2048 bits. in february 2007, sekar et al. published an attack on tpy with 2281 data and comparable time. this paper shows how to build a distinguisher with 2275 key/ivs and one outputword per each key (i.e., the distinguisher can be constructed within the design specifications); it uses a different set of weak states of the tpy. our results show that distinguishing attacks with complexity lower than the brute force exist if the key size of tpy is longer than 275 bits. furthermore, we discover a large number of similar bias-producing states of tpy and provide a general framework to compute them. the attacks on tpy are also shown to be effective on py.new weaknesses in the keystream generation algorithms of the stream ciphers tpy and py','Block and stream ciphers'
'in this paper we investigate nonlinear equivalence of stream ciphers over a finite field, exemplified by the pure lfsr-based filter generator over f2. we define a nonlinear equivalence class consisting of filter generators of length n that generate a binary keystream of period dividing 2n-1, and investigate certain cryptographic properties of the ciphers in this class. we show that a number of important cryptographic properties, such as algebraic immunity and nonlinearity, are not invariant among elements of the same equivalence class. it follows that analysis of cipher-components in isolation presents some limitations, as it most often involves investigating cryptographic properties that vary among equivalent ciphers. thus in order to assess the resistance of a cipher against a certain type of attack, one should in theory determine the weakest equivalent cipher and not only a particular instance. this is however likely to be a very difficult task, when we consider the size of the equivalence class for ciphers used in practice; therefore assessing the exact cryptographic properties of a cipher appears to be notoriously difficult.nonlinear equivalence of stream ciphers','Block and stream ciphers'
'we study the observability of a permutation on a finite set by a complex-valued function. the analysis is done in terms of the spectral theory of the unitary operator on functions defined by the permutation. any function f can be written uniquely as a sum of eigenfunctions of this operator; we call these eigenfunctions the eigencomponents of f. it is shown that a function observes the permutation if and only if its eigencomponents separate points and if and only if the function has no nontrivial symmetry that preserves the dynamics. some more technical conditions are discussed. an application to the security of stream ciphers is discussed.observability of permutations, and stream ciphers','Block and stream ciphers'
'here i suggest a design criterion for the choice of connection-polynomials in lfsr-based stream-cipher systems. i give estimates of orders of magnitude of the sparse-multiples of primitive-polynomials. i show that even for reasonable degrees (degrees of the order of 100) of primitive connection-polynomials the degrees of their sparse-multiples are \"considerably higher\".on choice of connection-polynominals for lfsr-based stream ciphers','Block and stream ciphers'
'in this paper, we examine the effectiveness of clock control in protecting stream ciphers from a distinguishing attack, and show that this form of control is effective against such attacks. we model two typical clock-controlled stream ciphers and analyze the increase in computational complexity for these attacks due to clock control. we then analyze parameters for the design of clock-controlled stream ciphers, such as the length of the lfsr used for clock control. by adopting the design criteria described in this paper, a designer can find the optimal length of the clock-control sequence lfsr.on effectiveness of clock control in stream ciphers','Block and stream ciphers'
'in this paper, the complexity of applying a guess and determine attack to so-called linear feedback shift register (lfsr)-based stream ciphers is analyzed. this family of stream ciphers uses a single or several lfsr and a filtering function f : gf(2)n &#8594; gf(2)m to generate the blocks of m &#8805; 1 keystream bits at the time. in difference to a classical guess and determine attack, a method based on guessing certain bits in order to determine the remaining secret key/state bits, our approach efficiently takes advantage of the reduced preimage space for relatively large m and at the same time employing the design structure of the cipher. several variations of the algorithm are derived to circumvent the sensitivity of attack to the input data, n, m and the key length. in certain cases, our attack outperforms classical algebraic attacks [10]; these being considered as one of the most efficient cryptanalyst tools for this type of ciphers. a superior performance of our attack over algebraic attacks is demonstrated in case the filtering function belongs to the extended maiorana-mcfarland class.on guess and determine cryptanalysis of lfsr-based stream ciphers','Block and stream ciphers'
'stream ciphers play an important role in symmetric cryptology because of their suitability in high speed applications where block ciphers fall short. a large number of fast stream ciphers or pseudorandom bit generators (prbg&#39;s) can be found in the literature that are based on arrays and simple operations such as modular additions, rotations and memory accesses (e.g. rc4, rc4a, py, py6, isaac etc.). this paper investigates the security of array-based stream ciphers (or prbg&#39;s) against certain types of distinguishing attacks in a unified way. we argue, counter-intuitively, that the most useful characteristic of an array, namely, the association of array-elements with unique indices, may turn out to be the origins of distinguishing attacks if adequate caution is not maintained. in short, an adversary may attack a cipher simply exploiting the dependence of array-elements on the corresponding indices. most importantly, the weaknesses are not eliminated even if the indices and the array-elements are made to follow uniform distributions separately. exploiting these weaknesses we build distinguishing attacks with reasonable advantage on five recent stream ciphers (or prbg&#39;s), namely, py6 (2005, biham et al.), ia, isaac (1996, jenkins jr.), ngg, gghn (2005, gong et al.) with data complexities 268.61, 232.89, 216.89, 232.89 and 232.89 respectively. in all the cases we worked under the assumption that the key-setup algorithms of the ciphers produced uniformly distributed internal states. we only investigated the mixing of bits in the keystream generation algorithms. in hindsight, we also observe that the previous attacks on the other array-based stream ciphers (e.g. py, etc.), can also be explained in the general framework developed in this paper. we hope that our analyses will be useful in the evaluation of the security of stream ciphers based on arrays and modular addition.on the (in)security of stream ciphers based on arrays and modular addition','Block and stream ciphers'
'f-fcsr-h v2 is one of the 8 final stream ciphers in the estream portfolio. however, it was broken by m. hell and t. johansson at asiacrypt 2008 by exploiting the bias in the carry cells of a galois fcsr. in order to resist this attack, at sac 2009 f. arnault $et \\ al.$ proposed the new stream cipher f-fcsr-h v3 based upon a ring fcsr. m. hell and t. johansson only presented experimental results but no theoretical results for the success probability of their powerful attack against f-fcsr-h v2. and so far there are no analytical results of f-fcsr-h v3. this paper discusses the probability distribution of the carry cells of f-fcsr-h v2 and f-fcsr-h v3. we build the probability model for the carry cells of the two stream ciphers and prove that the consecutive output sequence of a single carry cell is a homogeneous markov chain and the inverse chain is also a homogeneous markov chain. we also prove that the probability of l consecutive outputs of a single carry cell to be zeros is (1/2)&#183;(3/4)l&#8722;1, which is a weakness of the carry cells of f-fcsr-h v2 and f-fcsr-h v3, noticing that (1/2)&#183;(3/4)l&#8722;1&gt;2&#8722;l for l&gt;1. fcsr is a finite-state automata, so its distribution is stable. based on this fact, we construct a system of equations using the law of total probability, and present a theoretical probability of breaking f-fcsr-h v2 by solving the equations. applying this technique to f-fcsr-h v3, we obtain that the probability of all the 82 carry cells of f-fcsr-h v3 to be zeros at the same clock is at least 2&#8722;64.29, which is much higher than 2&#8722;82. this is another weakness of the carry cells of f-fcsr-h v3. our results provide theoretical support to m.hell and t.johansson\'s cryptanalysis of f-fcsr-h v2 and establish a theoretical foundation for further cryptanalysis of f-fcsr-h v3.on the probability distribution of the carry cells of stream ciphers f-fcsr-h v2 and f-fcsr-h v3','Block and stream ciphers'
'the ease of programming offered by the cuda programming model attracted a lot of programmers to try the platform for acceleration of many non-graphics applications. cryptography, being no exception, also found its share of exploration efforts, especially block ciphers. in this contribution we present a detailed walk-through of effective mapping of hc-128 and hc-256 stream ciphers on gpus. due to inherent inter-s-box dependencies, intra-s-box dependencies and a high number of memory accesses per keystream word generation, parallelization of hc series of stream ciphers remains challenging. for the first time, we present various optimization strategies for hc-128 and hc-256 speedup in tune with cuda device architecture. the peak performance achieved with a single data-stream for hc-128 and hc-256 is 0.95 gbps and 0.41 gbps respectively. although these throughput figures do not beat the cpu performance (10.9 gbps for hc-128 and 7.5 gbps for hc-256), our multiple parallel data-stream implementation is benchmarked to reach approximately 31 gbps for hc-128 and 14 gbps for hc-256 (with 32768 parallel data-streams). to the best of our knowledge, this is the first reported effort of mapping hc-series of stream ciphers on gpus.optimized gpu implementation and performance analysis of hc series of stream ciphers','Block and stream ciphers'
'design of key generator is the core process in stream cipher. this paper discusses the security of the lfsr and nonlinear combining functions as two core components of key generator. the models studied in this paper include linear shift register sequence, geffe sequence, shrinking feedback sequence and widespread to the general nonlinear combining sequence. improvement advice is proposed for cryptographist after each case of study to obtain stream cipher of higher strength.research on the security of key generator in stream ciphers','Block and stream ciphers'
'in some applications for synchronous stream ciphers, the risk of loss of synchronization cannot be eliminated completely. in these cases frequent resynchronization or resynchronization upon request may be necessary. in the paper it is shown that this can lead to significant deterioration of the cryptographic security. a powerful general attack on nonlinearly filtered linear (over z2) systems is presented. this attack is further refined to efficiently cryptanalyze a linear system with a multiplexer as output function.resynchronization weaknesses in synchronous stream ciphers','Block and stream ciphers'
'sayers and ciphers','Block and stream ciphers'
'scan chain based attacks are a kind of side channel attack, which targets one of the most important feature of today\'s hardware - the test circuitry. design for testability (dft) is a design technique that adds certain testability features to a hardware design. on the other hand, this very feature opens up a side channel for cryptanalysis, rendering crypto-devices vulnerable to scan-based attack. our work studies scan attack as a general threat to stream ciphers and arrives at a general relation between the design of stream ciphers and their vulnerability to scan attack. finally, we propose a scheme which we show to thwart the attacks and is more secure than other contemporary strategies. scan based side channel attacks on stream ciphers and their counter-measures','Block and stream ciphers'
'chaotic communication schemes aim to provide security over the conventional communication schemes. the sensitivity of chaotic systems/maps to their initial conditions and the parameters is used to introduce the security, where the latter is used as the secret key. the applicability of conventional chaotic systems/maps in communication channels with significant noise and multi-path is limited. symbolic dynamics (sd) based methods have been shown to provide high quality synchronization (hqs). in this paper, a new digital chaotic communication scheme, which utilizes the sd based synchronization, is proposed. this is similar to a self-synchronizing stream cipher where synchronization information is provided periodically. for the proposed scheme, a theoretical expression for the upper bound of the bit error rate (ber) is derived. numerical simulations are carried out to assess the ber performances of the system in awgn and multi-path channels. some security aspects of the proposed system are also studied.self-synchronizing chaotic stream ciphers','Block and stream ciphers'
'feedback with carry shift registers (fcsrs) are a promising alternative to lfsrs for the design of stream ciphers. most of the fcsr-based stream ciphers use a galois representation. in this case, the control of a single bit leads to the control of the feedback values. this particular property was exploited to break most of the existing proposals. recently, a new representation for fcsr automata was presented. this representation is a generalization of both galois and fibonacci representations. in this representation any cell can be used for a feedback for any other cell. with a good choice for the parameters, those new fcsr automatas are resistant to the previous attacks and the internal diffusion is significantly improved. using this approach, a new hardware oriented version of f-fcsr has been recently proposed.in this paper, we propose a new design for fcsrs suitable for software applications. using this approach, we present a new version of x-fcsr-128 suitable for software applications which is really efficient in software.software oriented stream ciphers based upon fcsrs in diversified mode','Block and stream ciphers'
'solving a class of stream ciphers','Block and stream ciphers'
'according to the network security problem of the wireless sensor networks, there are several severe challenges, such as limited processing power, storage, bandwidth, and energy. the stream cipher to be suitable for wireless sensor networks is proposed. and it composed of a 128-length linear feedback shift register (lfsr) and an 11-variable boolean function. the stream cipher algorithm is written with 8051 assembly code, and compared with the rc5 and a5. through the comparison, the efficiency is satisfied. the code size of the stream cipher used in the wireless sensor networks is 578 bytes and the execution time is 0.024642 second and it faster than rc5 and a5 and uses less code size. then the statistic testing and the known attack methods are used to test the security of the designed stream cipher. and the results can prove that its security is very high and suitable for wireless sensor networks.stream ciphers on wireless sensor networks','Block and stream ciphers'
'replacing random permutations by random functions for the update of a stream cipher introduces the problem of entropy loss. to assess the security of such a design, we need to evaluate the entropy of the inner state. we propose a new approximation of the entropy for a limited number of iterations. subsequently, we discuss two collision attacks which are based on the entropy loss. we provide a detailed analysis of the complexity of those two attacks as well as of a variant using distinguished points.stream ciphers using a random update function','Block and stream ciphers'
'synchronous stream ciphers are commonly used in applications with high throughput requirements or on hardware devices with restricted resources. well known stream ciphers are a5/1, used in gsm, rc4, used in ssl, or e0 as specified in bluetooth, but also some block cipher modes of operation. a review of the development of stream ciphers is given which starts with classical designs and is directed to modern dedicated stream ciphers as in the european noe estream project. the history of stream ciphers is rich in new proposals followed by devastating breaks, e.g., by statistical or algebraic attacks. differential cryptanalysis is probably the most popular tool for chosen plaintext attacks on block ciphers. it also applies to the initialization step in stream ciphers, but here, high order differential attacks are shown to be surprisingly successful, namely on constructions based on linear and nonlinear feedback shift registers. the process of designing and cryptanalyzing stream ciphers has not only resulted in a number of building blocks for stream ciphers: similar components turn out to be useful as well in the design of lightweight block ciphers, hash functions and in algorithms for authenticated encryption.stream ciphers, a perspective','Block and stream ciphers'
'stream ciphers','Block and stream ciphers'
'for security applications in wireless sensor networks (wsns), choosing best algorithms in terms of energy-efficiency and of small-storage requirements is a real challenge because the sensor networks must be autonomous. in [22], the authors have benchmarked on a dedicated platform some block-ciphers using severalmodes of operations and have deduced the best block cipher to use in the context of wsns. this article proposes to study on a dedicated platform of sensors some stream ciphers. first, we sum-up the security provided by the chosen stream ciphers (especially the ones dedicated to software uses recently proposed in the european project ecrypt, workpackage estream [27]) and presents some implementation tests performed on the platform [16].survey and benchmark of stream ciphers for wireless sensor networks','Block and stream ciphers'
'the differential cryptanalysis and design of natural stream ciphers','Block and stream ciphers'
'the key and iv setup algorithms of both hc-256 and hc-128 cipher are cryptanalyzed in this paper. both ciphers are software-efficient stream ciphers, proposed as the candidates of ecrypt stream cipher project and selected into the final portfolio. key-schedule is one of the most important parts of designing a security cipher. the weak key initialization process is a fatal potential weakness of a cipher. the analysis results show that the master keys \\emph{k} of hc-256 and hc-128 can be recovered from the states \\emph{p} and \\emph{q} easily. therefore, the key and iv setup algorithms is not perfect enough. final, an enhanced algorithm is proposed base on subkey addition as block ciphers, to improve the security of stream ciphers.the key and iv setup of the stream ciphers hc-256 and hc-128','Block and stream ciphers'
'at fse 2004 two new stream ciphers vmpc and rc4a have been proposed. vmpc is a generalisation of the stream cipher rc4, whereas rc4a is an attempt to increase the security of rc4 by introducing an additional permuter in the design. this paper is the first work presenting attacks on vmpc and rc4a. we propose two linear distinguishing attacks, one on vmpc of complexity 254, and one on rc4a of complexity 258. we investigate the rc4 family of stream ciphers and show some theoretical weaknesses of such constructions.two linear distinguishing attacks on vmpc and rc4a and weakness of rc4 family of stream ciphers','Block and stream ciphers'
'two stream ciphers','Block and stream ciphers'
'in the paper we extend known results studying the application of cas for stream ciphers. we illustrate the notion of weak keys in such a cryptosystem and describe the experiments related to its implementation on micro-controllers.weak key analysis and micro-controller implementation of ca stream ciphers','Block and stream ciphers'
'\"cooperative security\"','Browser security'
'many systems execute untrusted programs in virtual machines (vms) to limit their access to system resources. sun introduced the java vm in 1995, primarily intended as a lightweight platform for execution of untrusted code inside web pages. more recently, microsoft developed the .net platform with similar goals. both platforms share many design and implementation properties, but there are key differences between java and .net that have an impact on their security. this paper examines how .net\'s design avoids vulnerabilities and limitations discovered in java and discusses lessons learned (and missed) from java\'s experience with security..net security','Browser security'
'in this paper we introduce active security, a new methodology which introduces programmatic control within a novel feedback loop into the defense infrastructure. active security implements a unified programming environment which provides interfaces to (i) protect the infrastructure under common attack scenarios (e.g., configure a firewall), (ii) sense the current state of the infrastructure through a wide variety of information, (iii) adjust the configuration of the infrastructure at run time based on sensed information, (iv) collect forensic evidence on-demand, at run-time for attribution, and (v) counter the attack through more advanced mechanisms such as migrating malicious code to a quarantined system. we built an initial prototype that extends the floodlight software-defined networking controller to automatically interface with the snort intrusion detection system to detect anomalies, the linux memory extractor to collect forensic evidence at run-time, and the volatility parsing tool to extract an executable from physical memory and analyze information about the malware (which can then be used by the active security system to better secure the infrastructure).active security','Browser security'
'this paper revisits the conventional notion of security, and champions a paradigm shift in the way that security should be viewed: we argue that the fundamental notion of security should naturally be one that actively aims for the root of the security problem: the malicious (human-terminated) adversary. to that end, we propose the notion of adversarial security where non-malicious parties and the security mechanism are allowed more activeness; we discuss framework ideas based on factors affecting the (human) adversary, and motivate approaches to designing adversarial security systems. indeed, while security research has in recent years begun to focus on human elements of the legitimate user as part of the security system\'s design e.g. the notion of ceremonies; our adversarial security notion approaches general security design by considering the human elements of the malicious adversary.adversarial security','Browser security'
'federated identity management is often viewed by corporations as a solution to support secure online commerce by synthesising complex and fragmented user information into a single entity. however previous research (satchell et al 2006) has revealed a new set of end user needs for the design of identity management systems. this paper explores these needs from an identity management provider perspective, finds both alignment and divergence in needs and identifies a generational shift as a major cause of the differing needs. whilst x and y generations do not react strongly to concerns about digital identity theft or misappropriation of information, they seek to create and control their digital representations to be streamlined, portable across domains and revealing elements of their real life identity. there is still a considerable challenge for providers who must look beyond \'security\' and \'authentication\' to include \'user control\', \'synthesis\', \'portability\' and \'personalisation\' in the design of their systems.beyond security','Browser security'
'blanket security','Browser security'
'captchas have been widely used across the internet to defend against undesirable or malicious bot programs. in this article, the authors describe the security of a captcha reported in a recent peer-reviewed paper and deployed on the internet. they show that although this scheme was effectively resistant to one of the best optical character recognition programs on the market, they could break it with a success rate of higher than 90 percent by using a simple but novel attack. in contrast to early work that relied on sophisticated computer vision or machine learning algorithms, they used simple pattern recognition algorithms that exploited fatal design errors. the main contribution of their work is that simply counting the pixels in a captcha\'s characters can be a very powerful attack.captcha security','Browser security'
'misconceptions about cloud computing security threaten to slow its adoption.cloud security','Browser security'
'with the growing requirements for protection generated by legislation such as the 1974 privacy act, the increasing complexity of computer and data communications applications, and increasing awareness regarding computer vulnerabilities, the discipline of computer security is achieving independent recognition. current data processing literature is a rich source of information. articles and papers regarding security, design of software protection, operational practices and auditing number in the thousands. most of them are very narrow in scope or so general that they are of little use.computer security','Browser security'
'in the cyber security domain, we have been collecting \'big data\' for almost two decades. the volume and variety of our data is extremely large, but understanding and capturing the semantics of the data is even more of a challenge. finding the needle in the proverbial haystack has been attempted from many different angles. in this talk we will have a look at what approaches have been explored, what has worked, and what has not. we will see that there is still a large amount of work to be done and data mining is going to play a central role. we\'ll try to motivate that in order to successfully find bad guys, we will have to embrace a solution that not only leverages clever data mining, but employs the right mix between human computer interfaces, data mining, and scalable data platforms. traditionally, cyber security has been having its challenges with data mining. we are different. we will explore how to adopt data mining algorithms to the security domain. some approaches like predictive analytics are extremely hard, if not impossible. how would you predict the next cyber attack? others need to be tailored to the security domain to make them work. visualization and visual analytics seem to be extremely promising to solve cyber security issues. situational awareness, large-scale data exploration, knowledge capture, and forensic investigations are four top use-cases we will discuss. visualization alone, however, does not solve security problems. we need algorithms that support the visualizations. for example to reduce the amount of data so an analyst can deal with it, in both volume and semantics.cyber security','Browser security'
'the need for computer security has grown every year since the creation of computer systems, but yet with such a high demand, there are many systems that are, by no means, adecuetly protected.this paper will discuss the three classes of vulnerabilities of security. the three classes are: those that threaten the physical integrity of the computer installation and its data, those that threaten the loss or compromise of the data from outside the computersite, and those that threaten loss or compromise of data from inside the computer site.the chief physical risk to a computer site is fire, acts of sabotage, industrial accidents, natural disasters, and mechanical or electrial malfunction of the computer system. outside threats are those people who do not work for a particular firm, but yet wish to gain information about it that is not readily accessible to them. inside threats come from employees who wish to compromise the computer system weither for gain, accident, or past time. each one of these topics will be dealt withthe next part of the paper deals with the subject of setting up a security program. the first step in this subject is the study to access the probability of an event occuring, and determining it as either fatal to the business, very serious, moderately serious, relatively unimportant, or seriousness unknown. some of the security techniques of checks and tests on the system are then discused.data security','Browser security'
'database security','Browser security'
'the department of homeland security (dhs) shifted the focus of airport security in 2004 to incorporate the need to continuously and rapidly adapt security to shifting threats. mitre is developing a dynamic security airport simulation as part of a mitre-sponsored research project in which attacker and defense behavior in the airport environment are modeled. the simulation accepts threat vectors (path-weapon combinations) from other software or the user and models the performance of the airport defense against those threat vectors. the simulation includes two intelligent agents: the attacker and the defense. these agents model the behavior of those two entities; their logic includes both decision making and learning.dynamic security','Browser security'
'edi security','Browser security'
'voters\' trust in elections comes from a combination of the mechanisms and procedures we use to record and tally votes, and from confidence in election officials\' competence and honesty. electronic voting systems pose considerable risks to both the perception and reality of trustworthy elections.election security','Browser security'
'the convergence between spam and viruses has given rise to a far more insidious problem than we\'ve previously experienced. until recently spam and viruses have been talked about in isolation, viewed as something entirely separate, both with their own unique set of issues and accompanying detection techniques. if only... convergence came about as spammers began to lose the initiative when highly effective filtering systems were developed to block the delivery of most of the unsolicited email being sent to businesses. they have not wasted their time in turning their attention elsewhere. today firms are faced with viruses such as lovegate, fizzer, bugbear, mimail, mydoom, all designed with the same primary objective in mind: to infect as many machines as possible so that they could collectively be used to send more spam. \'spam supermarkets\' are available, with the creator of the latest variant selling access to his collective mass of infected machines to the highest bidder. armed with knowledge of the enemy, companies now need to implement best practices that help protect their business from rapidly evolving malicious attacks. the convergence between spam and viruses has given rise to a far more insidious problem than we\'ve previously experienced. historically, spam and viruses have been talked about in isolation, viewed as something entirely different, both with their own unique set of issues and accompanying detection techniques.email security','Browser security'
'technological, economic, social and political drivers will foster demand for embedded security.embedded security','Browser security'
'\'stolen laptop compromises privacy of national institute of health study subjects,\' amnews, april 12, 2008 \'sensitive data \'lost by councils\',\' bbc, april 13, 2008 \'mod laptop stolen from mcdonalds,\' bbc, april 12, 2008 \'a laptop stolen everyday in ahmedabad,\' times of india, april 10, 2008 \'data loss prompts security move,\' bbc, april 9, 2008 \'lawmaker\'s health data on stolen laptop,\' the guardian, april 3, 2008employee security','Browser security'
'this article forms the second part of an investigation into the controls that organisations should consider enforcing on endpoint computing devices such as desktops and laptops. the controls are based on a research project performed by the information security forum that identified ten key areas that organisations need to focus on.endpoint security','Browser security'
'because of the power and sophistication of vulnerability-exploitation efforts, security requires a thorough schooling in technology, but that is far from enough. security, or more accurately, relative trust, is not a \"thing\" but an end result that comes only from critical thinking.enhancing security','Browser security'
'as we mature and move toward \"enlightened security,\" this magazine will explore why we do what we do and what we know about cause and effect. we will also broaden content to address dependability and policy, and to apply contributions that other disciplines can make to our understanding of security, privacy, and reliability. the featured web extra identifies and thanks the reviewers who served our publication in 2012.enlightened security','Browser security'
'to survive in the business, especially the online, world, an enterprise requires an effective security infrastructure. however, enterprises should bear in mind that there is often also a legal obligation to keep certain data or information secure, emanating from legislation such as intellectual property, data protection or trade secret legislation or from contract. a number of technological measures already exist to provide security in the online world, such as encryption, electronic signatures and privacy enhancing technologies. however, one often encounters a number of legal constraints to the use of these measures through, for example, restrictions on the export and use of cryptography, difficulties and doubts of the legal recognition of electronic signatures, and different national rules on certification services. if electronic commerce is to grow, there is need for more international co-operation as well as mutual recognition of, and more liberalisation of, such regulations.enterprise security','Browser security'
'sociological study of normal routine behavior informs us of the mechanisms through which safety and social order are maintained. from ethnographic studies of public sites such as the new york subway system, we learn how workers\' routines fit or do not fit into official security-era policies coming from above. suggestions are provided for concrete mechanisms likely to enhance security while providing collateral benefits of enhanced efficiency and pleasure to members of the public.everyday security','Browser security'
'the time for security excuses is over.focus: security','Browser security'
'one of the most important, and most overlooked, ways that we learn is by \"social learning\"&#x0097;by hearing stories from our friends. stories are a valuable and underutilized tool for helping people learn how to make better and more secure decisions.folk security','Browser security'
'in any enterprise, there are many aspects to security, and they apply to different divisions of the enterprise: manufacturing, shipping, sales, administration, etc. those of us who work with computers know that we have to think about security just as much as those working in other divisions have to. in each different area, there are threats or dangers which must be protected against. some of these threats may be specific to a particular department while others are common to several different departments. for each particular threat there may several different measures that can be taken to protect against it. we can divide all measures into three categories: physical, operational and administrative. when we examine each of these categories, we find that they all apply to the security of every division and department of an enterprise, including that of the computer department.global security','Browser security'
'grid technology has being widely accepted in distributed resources sharing and high performance computing cross multi administrative domains. in this paper, we analysis the security issues in grid computing environments, and propose a security framework for vegagos[1] which is a service oriented architecture middleware developed for the china national grid. we address mutual authentication using certificate with digital signature. we address authorization through combining vo level access control decision and resource level enforcement. communication security is guaranteed by tls/ssl at transport level and ws-security at message level. this security framework has been implemented in vegagos and deployed in china national grid environment.gos security','Browser security'
'currently, members of the grid community are working within the global grid forum to articulate their security requirements, and to fill the gaps that are perceived to exist within industry-wide standardization efforts. probably the main difference between the grid community\'s requirements and those seen in the rest of the industry is the focus on how we can express, communicate and enforce the different security policies while crossing adminstrative boundaries.this presentation will discuss some grid specific use cases to emphasize this difference in focus, briefly discuss the open grid services architecture that provides the framework in which we try to address these requirements, enumerate how security aspects can be addressed by existing standards and where the gaps are, and specifically show some details on our current work to address-many of our distributed authorization problems.grid security','Browser security'
'\"homeland security\" is a major concern for governments worldwide, which must protect their populations and the critical infrastructures that support them. information technology plays an important role in many homeland security initiatives. on one hand, it can help mitigate risk and enable effective responses to disasters of natural or human origin. yet, its suitability for this role is plagued by questions ranging from dependability concerns to the risks that some technologies, such as surveillance, profiling and data aggregation, pose to privacy and civil liberties. on the other hand, information technology is itself an infrastructure to be protected.this includes not only the internet and financial infrastructure but also the complexsystems that control critical infrastructure such as energy, transportation, and manufacturing.homeland security','Browser security'
'this paper will present a method for disseminating mobile agents throughout a network in order to protect that network. this method is based on the immune concept of inflammation, which provides greater blood flow to areas of the body in danger. it also presents a prototype implementation of the model, and analyses that prototype.inflammation security','Browser security'
'we analyze the strategic interactions among end-users and between end-users and attackers in mass and targeted attacks. in mass attacks, precautions by end-users are strategic substitutes. this explains the inertia among users in taking precautions even in the face of grave potential consequences. generally, information security can be addressed from two angles - facilitating end-user precautions and enforcement against attackers. we show that, enforcement is more effective as an all-round policy to enhance information security.facilitating user precautions leads to increased precautions and increased end-user demand, which have conflicting effects on the total harm suffered by end-users. hence, reduced form estimates of the impact of facilitating precautions may over- or under- estimate the impact, depending on which effect is stronger. further, in targeted attacks, the outcome of interaction between users and attackers depends on the specific cost functions. attackers may target low-valuation users as they take fewer precautions.information security','Browser security'
'it security is by its nature a fire fighting activity, never mind what some of the pundits say. it is impossible to anticipate every conceivable threat in advance, so inevitably security is to some extent reactive. but there are more and less intelligent ways both to tackle blazes when they occur and to minimise their impact, whether they were started deliberately or by accident.integrated security','Browser security'
'java security','Browser security'
'some consider unmaintainable code a tool that provides job security. at the low level, you can obtain such code through incorrect or inconsistent formatting, naming, and commenting. complex and gratuitous coupling, lack of assertions, and failure to use a language\'s type system can further complicate the picture. at a higher level, deep and wide class hierarchies, lack of cohesion, and unhelpful package relationships can hinder maintainability. finally, at the development process level, lack of version control, subpar build-and-release procedures, a lacking testing infrastructure, and the hiring of mediocre developers will hammer the last nails into a project\'s maintainability coffin.job security','Browser security'
'the keyboard is the most widely used input device in the computing environment. authentication systems also usually need keyboards to acquire user passwords. even though a perfect crypto technology is deployed into the authentication system, it becomes useless if the password from the keyboard is stolen as a plain text form. therefore, several solutions have been introduced to protect passwords from keyboard. however, these solutions have not been sustainable to protect passwords, due to some compromising vulnerabilities. this paper explores recent critical vulnerabilities to the keyboard security and technologically analyzes various offensive and defensive aspects of the keyboard interfaces and their security mechanisms.keyboard security','Browser security'
'there exist legal structures defining the exclusive rights of authors, and means for licensing portions of them to others in exchange for appropriate obligations. we propose an analogous approach for security, in which portions of exclusive security rights owned by system stakeholders may be licensed as needed to others, in exchange for appropriate security obligations. copyright defines exclusive rights to reproduce, distribute, and produce derivative works, among others. we envision exclusive security rights that might include the right to access a system, the right to run specific programs, and the right to update specific programs or data, among others. such an approach uses the existing legal structures of licenses and contracts to manage security, as copyright licenses are used to manage copyrights. at present there is no law of &#8220;security right&#8221; as there is a law of copyright, but with the increasing prevalence and prominence of security attacks and abuses, of which stuxnet and flame are merely the best known recent examples, such legislation is not implausible. we discuss kinds of security rights and obligations that might produce fruitful results, and how a license structure and approach might prove more effective than security policies.licensing security','Browser security'
'linux security has been a hot issue ever since the operating system was born as the open source successor to unix, but the debate has intensified with its growing commercial popularity.linux security','Browser security'
'the relative absence of malware targeting macintosh systems has meant that the platform has come to enjoy a reputation for being significantly safer than windows-based pcs. however, malware is only a subset of the threat landscape facing users online, and it is therefore important that the perceived safety of the mac environment is not generalised too far. this article examines the situation, beginning with consideration of how security is used in the marketing of the platform, and whether this may engender misconceptions on the part of users. it also considers the apparent lack of priority towards ensuring that those wanting to run security software on their macs can actually do so (a problem illustrated by the release of mac os x 10.6, which caused incompatibility with a number of leading antivirus packages, and in some cases took several weeks to resolve). the discussion then moves to consider the extent to which mac users really require such protection, highlighting that while the malware risk to mac os x itself might be low, the mac can still have a role in combating the wider malware problem, and users still need to be protected against various other online threats such as phishing. thus, while users can often consider themselves safer on a mac, security still needs to be a firm issue on their agenda. there is now an increased public awareness of online threats, and with users\' concerns in mind, security features are consequently mentioned in an increasing array of advertising materials as factors that might sway would-be purchasers. a prominent example of this trend is apple\'s marketing campaign for the mac, which has long used security as one of the major selling points against competing pcs (with the fact that the system \'\'doesn\'t get pc viruses\'\' having been listed as the headline reason for choosing the platform, as shown in figure 1b).mac security','Browser security'
'most organizations have an extensive set of security requirements, established for commercial firms through complex interactions of business goals, government regulations, and insurance requirements. meeting these requirements has been time consuming and error prone, because there haven\'t been standardized, automated ways of performing all of the tasks and reporting on results. another obstacle has been the lack of interoperability across security tools. to overcome these deficiencies and reduce security administration costs, the national institute of standards and technology developed the security content automation protocol (scap).managing security','Browser security'
'to become a legitimate science, computer security requires metrics. however, metrics are the one thing most lacking in our current understanding of computer security. computer security metrics can be based on computational complexity or on economic or biological metaphors, or they can be empirical. any successful metric must address multiple layers of security.measuring security','Browser security'
'mhs security','Browser security'
'fueled by widespread adoption of employee-owned devices in the workplace and the explosion of mobile applications, mobile device security is under heavy debate in both the academic and industry security communities. businesses and government agencies are struggling to find some sense of control at a time when employee-owned devices now access some of the most sensitive data in an organization. various approaches and solutions have been proposed, ranging from device-based intrusion detection systems, execution isolation through application sandboxing and bare metal hypervisors, ontology-based firewalls, behavior-based detection, to cloud-based protection through the use of vpn technology. the challenge of heterogeneous hardware and software platforms, such as ios vs. android os, adds yet another layer of complexity to creating a comprehensive solution. the authors provide an overview of the current threats based on data collected from observing the interaction of 75 million users with the internet. extrapolating this data gives an insight into what threats wait on the horizon.mobile security','Browser security'
'this paper discusses a theory of covert channels called mode security. the general idea isto organize the state transitions of a multilevel state machine into distinct sets called modes. roughly speaking, each machine mode is totally secure when considered in isolation of all other modes. covert channels can therefore only occur when the machine executes a mode change decision; thus the name, mode security. the claim that all covert channels ina mode secure system are connected with mode change decisions can be justified by a generalization of the turing test model of non-information flow.mode security','Browser security'
'first, multilateral security and its potential are introduced. then protection goals as well as their synergies and interferences are described. after pointing out some basic facts about security technology in general, a structured overviewo f technologies for multilateral security is given. an evaluation of the maturity and effectiveness of these technologies shows that some should be applied immediately, while others need quite a bit of further research and development. finally, a vision for the future is given.multilateral security','Browser security'
'for the first two decades in the nascent computer-security field (1970s and 80s), everyone focused on multilevel security. why did the focus all but disappear? was there a major failure in multilevel security? is there less need for multilevel security today? if there still is a need, how might we address it?multilevel security','Browser security'
'in this talk i will described issues related to securing multimedia content. in particular i will discuss why tradition security methods, such as cryptography, do not work. i believe that perhaps too has been promised and not enough has been delivered with respect to multimedia security. i will overview research issues related to data hiding, digital rights management systems, media forensics, and describe how various applications scenarios impact security issues.multimedia security','Browser security'
'firewalls are crucial elements in enforcing network security policies. they have been widely deployed for securing private networks but, their configuration remains complex and error prone. during the last years, many techniques and tools have been proposed to correctly configure firewalls. however, most of existing works are informal and do not take into account the global performance of the network or other qualities of its services (qos). in this paper we introduce a formal approach allowing to formally and optimally configure a network so that a given security policy is respected and by taking into account the qos.network security','Browser security'
'nt security','Browser security'
'debate continues to rage over security outsourcing and it remains an emotive issue in many enterprises while managed services and outsourcing have been in vogue in other it sectors, there has been reluctance among large organizations to outsource their security functions. this article examines the views on this topic and the growing realisation that it does not have to be a simple either/or choice.outsourced security','Browser security'
'this paper describes the history of the design of the password security scheme on a remotely accessed time-sharing system. the present design was the result of countering observed attempts to penetrate the system. the result is a compromise between extreme security and ease of use.password security','Browser security'
'personal digital assistants (pdas) are now commonplace in every type of business. they extend the central computing power of an organization to the roaming worker within the office environment and in the wider world. the delivery of multiple connectivity methods such as wireless or return-to-base technologies has enabled a much expanded roaming computing ability.pda security','Browser security'
'pim security','Browser security'
'the secure \"pairing\" of wireless devices based on out-of-band communication is an established research direction. unfortunately, this approach is prone to human errors that lead to man-in-the-middle attacks. to address this and to better motivate users, this paper proposes the use of computer games for pairing. games make the pairing process enjoyable and engaging, thus improving its usability and security. the technical contribution of this work is a new pairing system called \"alice says.\" this is a game that achieves pairing and is based on the memory game simon. we also discuss the design and implementation of alice says. on a broader note, this paper also points to other security problems that are currently lacking optimal solutions and suggests how games and entertainment can be applied to improve them.playful security','Browser security'
'the capital costs alone of providing a dedicated fully resilient network, in addition to storage, backup, remote access or virtual platform, can be excessive. customers would not see a return on investment by deploying a dedicated infrastructure which would then be vastly under-used.preserving security','Browser security'
'proactive security sounds at first sight like just another marketing gimmick to persuade customers to sign for up for yet another false dawn. after all proactivity is surely just good practice, protecting in advance against threats that are known about, like bolting your back door just in case the burglar comes. to some proactive security is indeed just a rallying call, urging it managers to protect against known threats, and avoid easily identifiable vulnerabilities. all too often for example desktops are not properly monitored allowing users to unwittingly expose internal networks to threats such as spyware. similarly remote execution can be made the exception rather than the default, making it harder for hackers to co-opt internal servers for their nefarious ends.proactive security','Browser security'
'strong, machine-checked security proofs of operating systems have been in the too hard basket long enough. they will still be too hard for large mainstream operating systems, but even for systems designed from the ground up for security they have been counted as infeasible. there are high-level formal models, nice security properties, ways of architecting and engineering secure systems, but no implementation level proofs yet, not even with the recent verification of the sel4 microkernel. this needs to change.provable security','Browser security'
'is more always better? is conventional wisdom always the right guideline in the development of security policies that have large opportunity costs? is the evaluation of security measures after their introduction the best way? in the past, these questions were frequently left unasked before the introduction of many public security measures. in this paper we put forward the new paradigm that agent-based simulations are an effective and most likely the only sustainable way for the evaluation of public security measures in a complex environment. as a case-study we provide a critical assessment of the power of telecommunications data retention (tdr), which was introduced in most european countries, despite its huge impact on privacy. up to now it is unknown whether tdr has any benefits in the identification of terrorist dark nets in the period before an attack. the results of our agent-based simulations suggest, contrary to conventional wisdom, that the current practice of acquiring more data may not necessarily yield higher identification rates.public security','Browser security'
'to inform the design of security policy, task models of password behaviour were constructed for different user groups-computer scientists, administrative staff and students. these models identified internal and external constraints on user behaviour and the goals for password use within each group. data were drawn from interviews and diaries of password use. analyses indicated password security positively correlated with the sensitivity of the task, differences in frequency of password use were related to password security and patterns of password reuse were related to knowledge of security. modelling revealed computer scientists viewed information security as part of their tasks and passwords provided a way of completing their work. by contrast, admin and student groups viewed passwords as a cost incurred when accessing the primary task. differences between the models were related to differences in password security and used to suggest six recommendations for security officers to consider when setting password policy.rational security','Browser security'
'radio frequency identification (rfid) systems have become popular for automated identification and supply chain applications. this article describes the technical fundamentals of rfid systems and the associated standards. specifically, we address the security and privacy aspects of this relatively new and heterogeneous radio technology. we discuss the related security requirements, the threats and the implemented mechanisms. then the current security and privacy proposals and their enhancements are presented. finally we discuss the role of this technology in ubiquitous computing.rfid security','Browser security'
'for too long security has been an afterthought in the software development lifecycle (sdlc) with critical security flaws being uncovered either just prior to deployment when they endanger promised release dates, or worse, after an application has been deployed. this is no longer acceptable. security can be affordable and achievable if integrated properly at each key phase of the sdlc.security & sdlc','Browser security'
'security - introduction','Browser security'
'for several centuries, alchemists searched for the philosopher\'s stone-the elusive substance that could transmute common lead into gold. over the past 50 years, a new kind of alchemy has emerged. its philosopher\'s stone is the computing technology that can transmute ordinary individuals into singular experts in a specific domain. editor in chief george cybenko looks at this phenomena in the security field.security alchemy','Browser security'
'history tells us that julius cesar shifted each letter in his messages to his generals by three places in the alphabet. the generals knew to shift back by three letters to read the message. securing information for transfer between entities requires an agreement on how the information will be protected. modern science and technology has brought more advanced methods of protecting information, but the basic need for an agreement between entities desiring to communicate securely still exists. in modern terms this agreement is a security association (sa). there are varying definitions of a security association in current standards and this paper attempts to clarify, these definitions. security protocols requiring security associations as well as emerging protocols that establish and manage security associations are presented. if future global interoperability is to be provided securely one of the first building blocks will be the ability to negotiate and establish security associations. therefore, issues that must be resolved for future global interoperability are discussed. our work to create a network security research environment for future global needs is also presented.security associations','Browser security'
'security audits are now part of the regulatory landscape for many organisations, and it becomes particularly important when dealing with third party providers such as outsourcers. service contracts must be complemented by documentation clearly explaining the nature of such audits, and the expectations from them. under such agreements, individuals within the customer and the outsourcing provider must understand the role is that they play in supporting such security audits. making reference to best practices and international standards, dario forte examines the assignment and control procedures for outsourcer authorisation profiles, assessing their potential impact on data confidentiality, integrity and availability and on service levels, identifying potential weaknesses in control measures and the corresponding corrective actions that should be taken. laws in various countries require multi-level audits, especially for information systems managed wholly or partially via service contract with outsourcers. the documentation describing the logical security controls and methodology for protecting data integrity in the context of such services comprises a series of annexes to the service contracts, as well as a security manual which is usually necessary for setting forth policies and procedures. both company and outsourcer personnel must be assigned authorisation profiles that are appropriate to their roles in the provision of service.security audits','Browser security'
'this article will look at how it security needs to be seen as an employees ongoing development and how organizations should educate their workforce using ongoing systematic and proactive user awareness programs. in this article i will focus on simple steps to educate a workforce about it security at all levels in the office, to make sure every single person in the workplace understands the important role they play in defending attacks and keeping their organization secure.security awareness','Browser security'
'financial directors often ask if the investments made in security solutions are worth it. sometimes, on the other hand, they are surprised at the low budgets allocated for security. is there a point at which the budget is spot on? money spent on office cleaning shows visible results, so how do we go about assessing if the money invested in security is really being used to good effect?security budgeting','Browser security'
'with security becoming an important concern for both users as well as designers of large-scale software systems, it is necessary to introduce security considerations very early in the system development life-cycle namely in the modeling phase itself. but the main problem in the widespread adoption of security modeling has been that representations of even moderate-size systems consume so much memory (due to the infamous state space explosion problem) that designers are loathe to spend time increasing the complexity of their models by introducing security aspects in the design phase itself. in this paper we propose a technique called security check which entails taking small units of a system, putting them in a \"security harness\" that exercises relevant executions appropriately within the unit, and then model checking these more tractable units. for most systems whose security requirements are localized to individual system components or interactions between small numbers of components, security check offers a means of coping with state explosion. another major benefit of security check is that it enables us to detect system vulnerabilities even when the attack behavior is not known. and for known attack patterns security check can provide models of suspicious behavior which can then be used for intrusion detection at a later stage.security check','Browser security'
'it\'s been said that the internet is a global community made of all the users on the network. like any community, there are businesses conducting commerce, individuals going about their daily lives, and even a few bad actors. but unlike our physical communities, there are no police cars roaming the neighbourhoods looking for these bad actors. there aren\'t even boundaries that help law enforcement activities. at the end of the day, this global community without boundaries means that every enterprise has to be on the lookout for not just the security of their own systems, but also the security of the community as a whole.security citizen','Browser security'
'while healthcare organizations strive to increase control of network access, clinicians need unencumbered access to data. daily, clinicians make unconscious decisions to be in compliance with the security measures, or to live with a certain level of insecurity to get their job done. the unanticipated consequences of these decisions can unintentionally lead to suboptimal outcomes. to attain a favorable outcome in security implementations, some research has recommended taking a holistic approach with a strong sociotechnical perspective to security system design. to help understand what this means, a 15-month ethnographic study followed the implementation of a single sign-on system in a regional hospital. the findings revealed that security system designers must address user behavior to reach an optimal level of assurance. in addition, they suggest that managing a certain level of insecurity within the environment\'s constraints might be more effective than deploying expensive or invasive security mechanisms.security dilemma','Browser security'
'security documentation','Browser security'
'in large-scale systems, partitioning keys among higher-level structures becomes necessary. establishing and managing these structures as security domains requires an underlying infrastructure to securely provide local services.security domains','Browser security'
'the topic of this proposed special session is security education with an emphasis on increasing the number of schools and programs that teach it. we will review the current status of security education as it is taught within the cs discipline including funding, curriculum standards and government programs. we will also address misconceptions people may have regarding teaching security and how these views hinder the development of programs and curriculum. another area we cover which we feel is critical to increasing participation in security education is resources for teaching security including existing curriculum, expert assistance from centers of excellence schools and other resources. the format of the session will consist of both presentation periods with at least half the session devoted to participant question and answer. session participants will be encouraged to share experiences and common concerns to the benefit of everyone attending.security education','Browser security'
'millions of security events are generated daily from network devices, operating systems, applications, and even security solutions themselves. these events are typically siloed within files and databases or sent via syslog. much attention is paid within organisations to preventing security events, but some incidents will inevitably still occur. when this happens, organisations must be prepared with an events management strategy. the more that this can be automated, the better.security events','Browser security'
'a review of practical unix & internet security, 3rd edition by simson garfinkel, gene spafford, and alan schwartz.security fundamentals','Browser security'
'there is a huge demand for wireless sensors and rfid tags for remote surveillance and tracking. however, in order for such technologies to gain wide acceptance in industry, there needs to be strong security integrated into them. traditional cryptographic schemes are infeasible due to hardware, computation, and power constraints. to that end, we introduce a new security paradigm, namely security fusion. in this approach, strong security properties are synthesized from weaker point-to-point properties, thereby minimizing the resource requirements at each node without compromising the system-level security. in this paper, we describe the concept of security fusion and give a motivation example using finite state machines.security fusion','Browser security'
'the issue of security, which is considered to be the final hurdle to be overcome in achieving interoperability of us army computer systems, is discussed. in particular, the problem of transferring data across security boundaries is examined. the primary concern is to defend against unauthorized disclosure of high-side (e.g. top secret) data to low-side (e.g. secret) users. this might be caused by high-side errors, by malicious high-side software, or by active penetration from the low side. other concerns include defending against modification or destruction of high-side data as well as denial of service to high-side users. typical current solutions are described. the use of a security guard (also known as a security filter), i.e. a device or set of controls that mediates data transfers across security boundaries, to prevent leakage and penetration is examined. lessons from past failures with guards and experience with an operational guard are discussed. although military applications are addressed, the security issues raised are also thought to apply in the commercial sectorsecurity guards','Browser security'
'www (world wide web) has incurred the problem that users are not necessarily provided with the information they want to receive, at a time when the amount of information is explosively increasing. therefore, tim berners-lee proposed the semantic web as the new web paradigm to solve this problem. but there is always a security problem in semantic web such as www and the study about this is insufficient. therefore, the authors of this paper propose that the security intelligence system should be used to present semantic information using ontology and prevents that users flow out the information. this system is an acm(access control matrix) based access control model basically, and it is a system that prevent information leakage by user&#8217;s deliberation and by user&#8217;s mistake. it can be also used for www.security intelligence','Browser security'
'recent experience in computer security has illustrated the susceptibility of numerous operating systems to hostile penetration. successful penetrations have been directed at manufacturers\' conventional operating systems as well as special \"secure\" versions that have been the subjects of exhaustive efforts to find and fix all potential security problems. while formal reports are understandably hard to come by, it appears that the effort required to \"break\" any operating system and obtain access to any information it stores (at any time and without detection) is in the range two to four man-months. in contrast, the effort expended in futile attempts to prevent such penetration may be as much as two orders of magnitude greater (several man-years or more).security kernels','Browser security'
'have your systems been configured to write events to the system logs? most people would answer \'\'yes\'\'. are they using the standard default settings? the answer to this will vary. how often are the logs examined? most people would answer only \'\'when there is a problem\'\'.security logs','Browser security'
'information systems have been used for many years to analyze problems and compare options in a managed environment. the introduction of computer and information security systems into such an environment is a typical example of a situation to which an information systems approach can be applied. in this paper, we examine the issues peculiar to implementation of security in a healthcare environment, looking specifically at one such specially designed system, sim-ethics, which takes a participational approach.security management','Browser security'
'threat analysis and mitigation, both essential for corporate security, are time consuming, complex and demand expert knowledge. we present an approach for simulating threats to corporate assets, taking the entire infrastructure into account. using this approach effective countermeasures and their costs can be calculated quickly without expert knowledge and a subsequent security decisions will be based on objective criteria. the ontology used for the simulation is based on landwehr&#39;s [alrl04] taxonomy of computer security and dependability.security ontology','Browser security'
'security policy','Browser security'
'we relate two models of security protocols, namely the linear logic or multiset rewriting model, and the classical logic, horn clause representation of protocols. more specifically, we show that the latter model is an abstraction of the former, in which the number of repetitions of each fact is forgotten. this result formally characterizes the approximations made by the classical logic model.security protocols','Browser security'
'as dynamic ram scaling approaches its physical limit, phase-change memory is the most mature and well-studied option for potential dram replacement. however, malicious wear-out attacks can exploit pcm\'s limited write endurance. to address this, a low-cost wear-leveling scheme can dynamically randomize the data addresses across the entire address space and obfuscate their actual locations from users and system software.security refresh','Browser security'
'the premier us science funding agency, the national science foundation, has just released details of 33 research projects worth $14.3 million running over one to five years under its $30 million cyber trust programme.security research','Browser security'
'\'\'security is everyone\'s responsibility.\'\' it security professionals in halls around the world echo this phrase. it sounds reasonable on the face of it; keeping our systems secure is such a pervasive problem that it\'s best tackled by everyone in the organisation. by spreading the responsibility around, we stand a fighting chance against an enemy that always seems one step ahead.security responsibility','Browser security'
'a formal approach to managing and mitigating security risks in the software life cycle is requisite to developing software that has a higher degree of assurance that it is free of security defects which pose risk to the computing environment and the organization. due to its criticality, security should be integrated as a formal approach in the software life cycle. both a software security checklist and assessment tools should be incorporated into this life cycle process and integrated with a security risk assessment and mitigation tool. the current research at jpl addresses these areas through the development of a software security assessment instrument (ssai) and integrating it with a defect detection and prevention (ddp) risk management tool.security risks','Browser security'
'security solutions','Browser security'
'this paper gives a security and specification-oriented semantics for systems. the semantic model is derived from that for the trace model of hoare\'s communicating sequential processes[ho85] and is used to define various security concepts, such as multi-level secure system, trusted users and integrity. we indicate how implementations of secure systems may be derived from their specifications.security specifications','Browser security'
'there is a need for a broad range of it security standards and technical guidelines to support cybersecurity at both the national and international levels. however a number of standards are available or under development. these spawn from various bodies including the national institute of standards & technology (nist), the internet engineering task force (ietf), and iso, the international organization for standardisation. dr walter fumy, vice president of information technology security at siemens filters through some of the standard variations that are shaping the it security field. some areas such as cryptography are well established as many algorithms and techniques have been standardised. but there is no internationally recognized information security management (isms) system standard. inevitably, establishing information technology security standards means playing catch-up with the technology and the ingenuity of the people who attack it systems. but it must be done.security standards','Browser security'
'this position paper proposes a research agenda for the field of security testing. it gives a critical account of the state of the art as seen by a practitioner and identifies questions that research failed to answer so far, or failed to answer in such a way that it would have had an impact in the real world. three categories of research problems are proposed: theory of vulnerabilities, theory of security testing, and tools and techniques. 1. about this paper the science of security testing is still in its infancy. this paper proposes a research agenda for this field. it does so from a very specific perspective: that of a tester who, being aware of the lack of a scientific basis of his work, has to and wants to assess the security level of software systems on the basis of testing. what such a tester needs is not research papers but useful tools that optimize the work that is already being done in various labs around the world. the key underlying assumption of this paper is therefore that research should take an approach similar to what a usability engineer would do when designing a tool: first understand the task, then design solutions and tools. hence the title, turning practice into theory. this paper contains no original research whatsoever. rather, it is a position paper and conveys the author\'s opinion on the subject. the author has a background in applied research and practical security testing, which may explain some of the views expressed here. primarily, the present paper collects problems the author encountered during several years of testing and evaluating systems for their security. secondarily it presents a number of observations how security testing is approached today, none of which should be taken for more than anecdotal evidence, though. the remainder of this paper is organized as follows: section 2 outlines the author\'s conception of securitysecurity testing','Browser security'
'the author discusses the problem of how a security specialist should think. in particular, such a person should know how to evaluate complex systems and look for vulnerabilities created by interactions. it\'s hard to do, and even harder to teach.security think','Browser security'
'security watch','Browser security'
security,'Browser security'
'the most complex and important relationship in the cyber world is that between a company\'s top management and security management. until recently, security was viewed mainly as a question of image, something completely unrelated to real business. when business needs and security needs were brought into the same conversation, it was usually in a conflictual manner. however, the last ten years have seen an increasing number of security incidents and the enactment of more and more laws and regulations to address the issue. this has inevitably led to a gradually deepening awareness on the part of both private and public organisations that effective security measures are a fundamental requisite for business integrity and for compliance with the law.selling security','Browser security'
'as short message service (sms) is now widely use as business tool, it security has become a major concern for business organizations and customers. there is a need for an end to end sms encryption in order to provide a secure medium for communication. this paper evaluates rsa, elgamal and elliptic curve encryption techniques using random sms messages of various sizes to measure their encryption and decryption time. the experimental results are presented to show the effectiveness of each algorithm and to choose the most suitable algorithm for sms encryption.sms security','Browser security'
'this paper introduces a multi-agent belief revision algorithm that utilizes knowledge about reliability or trustworthiness of information sources to evaluate incoming information and the sources providing that information. it also allows an agent to learn the trustworthiness of other agents using (1) dissimilarity measures (measures that show how much incorrect information from a particular information source) calculated from the proposed belief revision processes (direct trust revision) and/or (2) communicated trust information from other agents (recommended trust revision). a set of experiments are performed to validate and measure the performance of the proposed trust revision approaches. the performance (frequency response and correctness) of the proposed algorithm is analyzed in terms of delay time (the time required for the step response of an agent\'s belief state to reach 50 percent of the ground truth value), maximum overshoot (the largest deviation of the belief value over the ground truth value during the transient state), and steady-state error (deviation of the belief value after the transient state). the results show a design trade off in better responsiveness to system configuration or environmental changes versus resilience to noise. an agent designer may either (1) select one of the trust revision algorithms proposed or (2) use both of them to achieve better performance at the cost of system resource such as computation power and communication bandwidth.soft security','Browser security'
'as users of many different types of software, one of the last things we think about when deciding which is best for the task at hand is the security of the software. this of course depends on the type of software as well. if it is banking or tax software then, security is of concern. however, in most cases the thought rarely crosses the average users mind. that is until something happens to the system or information the system may hold. this way of thinking is very similar to the way some software is created. the software\'s features, usability, and interface are normally at the forefront. on the other hand the software\'s security is just an afterthought for many developers. while software is developed in a very intricate process, it does not currently stress security. in this paper, we will describe different approaches that have been recently researched to help create more secure software. although these approaches will be introduced separately, using a combination of two or all three together would be the most secure combination.software security','Browser security'
'many see us returning to an \'information centric\' view, where chief information officers are assigned the task of protecting company information, beyond just knowing where it is stored. it is important to have control over the physical location of information for compliance and strategy reasons.storage & security','Browser security'
'tetra is a digital trunked radio standard for the private/professional mobile radio (pmr) market. bt\'s airwave service, a nation-wide mobile radio service designed to serve the needs of the police and other public safety organisations in england, wales and scotland, is based on tetra and the inherent security of tetra is an essential element of the service. this paper describes the basic security mechanisms in tetra and the various security services that are delivered using them.tetra security','Browser security'
'tmn security','Browser security'
'in existing universal mobile telecommunications system(umts) system, it is easy to introduce various attacks into core network. in this paper, we propose an identity verification scheme in the rrc connection establishment phase. to avoid the illegal user accessing the system, each user should submit an accessing network credential while they apply for the radio resource. our investigations have shown that our strategy can prevent various hostile attacks and save plenty of resources.umts security','Browser security'
'multi-factor authentication involves the use of more than one mode in authentication processes and is typically employed to increase security compared to a fixed password (knowledge-based mode). this research compared three different ebanking authentication processes, a two-layer password (1-factor) method and two alternative 2-factor solutions. the 2-factor processes used one-time-passcodes (otps) delivered either via a small, single-use device or by text message to a mobile phone. the three authentication methods were compared in a repeated-measures experiment with 141 participants. three user groups were balanced in the experiment to investigate the effect of experience (current users of the service) on perceptions of usability and security. attitudes toward usability and observations were taken for each process. other data gathered quality ratings, preferences and ranked comparisons regarding convenience and security issues. both 2-factor methods scored significantly higher than the 1-factor method for ebanking authentication usability metrics overall, but experienced users gave higher scores to the 1-factor method they currently use. overall preferences were spread evenly between the three methods. however, the majority of the participant sample perceived the 1-factor method they had most experience with as being the most secure and most convenient option. the results offer insight into customer attitudes important in their selection of authentication options: convenience, personal ownership and habitual experience of processes.usable security','Browser security'
'as people start depending more on technology and the internet they are opening themselves up to new risks. in this project, we specifically investigated wireless router interfaces to understand the needs of users when they configure security. two studies were conducted: a baseline study comparing the interfaces of two routers on the market and a study comparing a prototype and the linksys interface. the baseline study showed that there was no difference between the current interfaces. we then conducted a controlled experiment with a prototype that gave visual feedback. the prototype showed significant improvement in level of security achieved.useable security','Browser security'
'this article deals with the problem of security application on the h.323 videoconferencing. the h.323 communication uses many protocols and messages that are sent by the tcp and the udp protocols. the data are provided by h.323 terminals and several control applications needed for videoconference administration. some of these data transmissions need to be protected by encryption. in this article we show how to do this in a very easy way, which is independent of any of the participating h.323 terminals and other control applications. this independence guarantees easy deployment of this approach on any existing videoconference system.videoconference security','Browser security'
'continuing our series on security voip, we look at best security practices for organizations deploying ip telephony in anger. practices are graded as \'desirable\', \'optional\' etc. but one isn\'t: keeping the voice and data traffic separate through the use of 802.1q vlans. that\'s firmly in the \'mandatory\' category. when securing network infrastructure, one technique is vital, according to the author: employ separate voice and data vlans. this has several advantages. amongst them, the inherent isolation provided by vlans ensures that \'inter-vlan traffic\' is under management control and that network attached pcs cannot initiate a direct attack on voice components. when defining threats, it is imperative that there is a holistic approach to it security, so that the voice system is included in overall security risk analysis and best practices are applied as deemed appropriate. these are aligned to data system security measures, as a minimum. in practical terms this would typically include deep packet inspection techniques, robust wireless security mechanisms, and endpoint security on servers and hosts. by definition voip traffic is vulnerable to the same threats as data traversing the ip network. the most common threats are from dos attacks, malware and deliberate intrusion. we continue our voip series exploring how best to tackle them.voip security','Browser security'
'this paper proposes a theory of watermarking security based on a cryptanalysis point of view. the main idea is that information about the secret key leaks from the observations, for instance, watermarked pieces of content, available to the opponent. tools from information theory (shannon\'s mutual information and fisher\'s information matrix) can measure this leakage of information. the security level is then defined as the number of observations the attacker needs to successfully estimate the secret key. this theory is applied to two common watermarking methods: the substitutive scheme and the spread spectrum-based techniques. their security levels are calculated against three kinds of attack. the experimental work illustrates how blind source separation (especially independent component analysis) algorithms help the opponent exploiting this information leakage to disclose the secret carriers in the spread spectrum case. simulations assess the security levels derived in the theoretical part of the paper.watermarking security','Browser security'
'web service (ws)-related security standards provide a comprehensive framework to develop and implement security systems for ws environments. these standards are often broad in scope, and the price of this generality is a high level of complexity. this in turn can lead to potential security problems. web services are a powerful set of technologies that can be used to build service-oriented architectures (soa). current security mechanisms are not flexible, efficient and manageable enough for soa-associated collaboration patterns and business models. this demands adequate security mechanisms respecting the specific requirements of web service-based soa.web security','Browser security'
'wireless networks are desirable to many organisations because they increase workforce flexibility and save cabling costs. in older offices, listed buildings, and businesses based in the outdoors, wireless maybe the only way to provide network access to all parts of the workplace environment. some businesses even provide wireless networks to reach out to visitors and customers to entice them into their premises and allow them to work from there. wireless networks will enable the coming myriad of tablet and slate machines to function as seamless internet terminals. they are here to stay.wifi security','Browser security'
'wireless security','Browser security'
'it happens every year. summer security conventions bring computer professionals and hackers from around the world to a handful of venues. premier among these gatherings are the blackhat and defcon, conventions held in late july under the hot sun of las vegas. and each year new security tools are released, old tools are given new life, and new vulnerabilities are announced. this year was no exception. the weeklong security fest saw dozens of new tools including a handful of wireless utilities. this new round of wireless security tools represents a new level of sophistication and danger in the wireless security arena.wirless security','Browser security'
'a look back at wireless technology through the years.wlan security','Browser security'
'workshop: security','Browser security'
'code injection vulnerabilities continue to prevail. attacks of this kind such as stack buffer overflows and heap buffer overflows account for roughly half of the vulnerabilities discovered in software every year. the research presented in this paper extends earlier work in the area of code injection attack detection in unix environments. it presents a framework for detecting new or previously unseen code injection attacks in a heterogeneous networking environment and compares code injection attack and detection strategies used in the unix and windows environments. the approach presented is capable of detecting both obfuscated and clear text attacks, and is suitable for implementation in the windows environment. a prototype intrusion detection system (ids) capable of detecting code injection attacks, both clear text attacks and obfuscated attacks, which targets windows systems is presented.a framework for detecting network-based code injection attacks targeting windows and unix','Cryptanalysis and other attacks'
'side channels attacks (sca) are very effective and low cost methods to extract secret information from supposedly secure cryptosystems. differential power analysis (dpa) and differential electromagnetic analysis (dema) are among the most cited attack types. the traditional synchronous design flow used to create such systems favors the leakage of information that enables attackers to draw correlations between data processes and circuit power consumption or electromagnetic radiations. by using well known analysis techniques these correlations may allow that an attacker retrieve secret cryptographic keys. in recent years, several countermeasures against sca have been proposed. globally asynchronous locally synchronous (gals) and fully asynchronous design methods appear as alternatives to design tamper resistant cryptosystems. however, according to previous works they use to achieve this with significant area, throughput, latency and power penalties. this paper proposes a new gals pipeline architecture for the data encryption standard (des) that explores the trade-off between circuit area and robustness. robustness is enhanced by replicating the des hardware structure in asynchronously communicating module instances, coupled with self-varying operating frequencies. designs prototyped on fpgas using the proposed technique and submitted to dema attacks presented promising robustness against attacks and throughput superior to previously reported results.a gals pipeline des architecture to increase robustness against dpa and dema attacks','Cryptanalysis and other attacks'
'today distributed denial of service (ddos) attacks are causing major problems to conduct online business over the internet. recently several schemes have been proposed on how to prevent some of these attacks, but they suffer from a range of problems, some of them being impractical and others not being effective against these attacks. in this paper, we propose a controller-agent model that would greatly minimize ddos attacks on internet. with a new packet marking technique and agent design our scheme is able to identify the approximate source of attack (nearest router) with a single packet even in case of attack with spoofed source addresses. our scheme is invoked only during attack times, is able to process the victims traffic separately without disturbing other traffic, is able to establish different attack signatures for different attacking sources, can prevent the attack traffic at the nearest router to the attacking system, has fast response time, is simple in its implementation and can be incrementally deployed. hence we believe that the scheme proposed in this paper seems to be a promising approach to prevent distributed denial of service attacksa practical method to counteract denial of service attacks','Cryptanalysis and other attacks'
'nvd and exploit-db are the de facto standard databases used for research on vulnerabilities, and the cvss score is the standard measure for risk. on open question is whether such databases and scores are actually representative of attacks found in the wild. to address this question we have constructed a database (ekits) based on the vulnerabilities currently used in exploit kits from the black market and extracted another database of vulnerabilities from symantec\'s threat database (sym). our final conclusion is that the nvd and edb databases are not a reliable source of information for exploits in the wild, even after controlling for the cvss and exploitability subscore. an high or medium cvss score shows only a significant sensitivity (i.e. prediction of attacks in the wild) for vulnerabilities present in exploit kits (ekits) in the black market. all datasets exhibit a low specificity.a preliminary analysis of vulnerability scores for attacks in wild','Cryptanalysis and other attacks'
'this paper analyzes malicious activity collected from a test-bed, consisting of two target computers dedicated solely to the purpose of being attacked, over a 109 day time period. we separated port scans, icmp scans, and vulnerability scans from the malicious activity. in the remaining attack data, over 78\% (i.e., 3,677 attacks) targeted port 445, which was then statistically analyzed. the goal was to find the characteristics that most efficiently separate the attacks. first, we separated the attacks by analyzing their messages. then we separated the attacks by clustering characteristics using the k-means algorithm. the comparison between the analysis of the messages and the outcome of the k-means algorithm showed that 1) the mean of the distributions of packets, bytes and message lengths over time are poor characteristics to separate attacks and 2) the number of bytes, the mean of the distribution of bytes and message lengths as a function of the number packets are the best characteristics for separating attacks.a statistical analysis of attack data to separate attacks','Cryptanalysis and other attacks'
'this paper presents a simulation comparing various resampling procedures for estimating classification error rate. the simulations were done for small sample sizes, for the two-class and three-class problems.application of bootstrap and other resampling techniques','Cryptanalysis and other attacks'
'the public key cryptosystem based on rank error correcting codes (the gpt cryptosystem) was proposed in 1991. several attacks against this system were published, including gibson\'s attacks and recent overbeck\'s attacks. in this paper, we improve the gpt system by more careful choice of parameters to withstand these attacks.attacks and counter-attacks on the gpt public key cryptosystem','Cryptanalysis and other attacks'
'we systematically study the design of image recognition captchas (ircs) in this paper. we first review and examine all existing ircs schemes and evaluate each scheme against the practical requirements in captcha applications, particularly in large-scale real-life applications such as gmail and hotmail. then we present a security analysis of the representative schemes we have identified. for the schemes that remain unbroken, we present our novel attacks. for the schemes for which known attacks are available, we propose a theoretical explanation why those schemes have failed. next, we provide a simple but novel framework for guiding the design of robust ircs. then we propose an innovative irc called cortcha that is scalable to meet the requirements of large-scale applications. it relies on recognizing objects by exploiting the surrounding context, a task that humans can perform well but computers cannot. an infinite number of types of objects can be used to generate challenges, which can effectively disable the learning process in machine learning attacks. cortcha does not require the images in its image database to be labeled. image collection and captcha generation can be fully automated. our usability studies indicate that, compared with google\'s text captcha, cortcha allows a slightly higher human accuracy rate but on average takes more time to solve a challenge.attacks and design of image recognition captchas','Cryptanalysis and other attacks'
'in this article we present some weaknesses in the rc4 cipher and their cryptographic applications. especially we improve the attack described by fluhrer, mantin, shamir (in: selected areas in cryptography, 2001) in such a way, that it will work, if the weak keys described in that paper are avoided. a further attack will work even if the first 256 byte of the output remain unused. finally we show that variants of the rc4 algorithm like ngg and rc4a are also vulnerable by these techniques.attacks on the rc4 stream cipher','Cryptanalysis and other attacks'
'the influence of computer technology on the human activities has greatly increased during the last three decades, due to major developments in the vlsi technology. however this widespread use of computer equipments has generated computer a considerable increase of computer crimes. to reduce this problem it is necessary to carried out a network analysis using the computer network traffic. however the increase of network traffic is huge, doing the analysis of traffic data complicated. thus it is required to develop an effective and automatic algorithm to carry out the traffic network analysis, facilitating i such way the expert forensic work. this paper proposes a network analysis algorithm using recurrent neural network that can analyze computer network attacks facilitating the evidence extraction. proposed algorithm can reduce time and cost of forensic analysis.attacks recognition using recurrent neural network','Cryptanalysis and other attacks'
'we compare the performance of three types of neural network-based ensemble techniques to that of a single neural network. the ensemble algorithms are two versions of boosting and committees of neural networks trained independently. for each of the four algorithms, we experimentally determine the test and training error curves in an optical character recognition (ocr) problem as both a function of training set size and computational cost using three architectures. we show that a single machine is best for small training set size while for large training set size some version of boosting is best. however, for a given computational cost, boosting is always best. furthermore, we show a surprising result for the original boosting algorithm: namely, that as the training set size increases, the training error decreases until it asymptotes to the test error rate. this has potential implications in the search for better training algorithms.boosting and other ensemble methods','Cryptanalysis and other attacks'
'bound variables and other anaphors','Cryptanalysis and other attacks'
'pin minimization is an important issue for massively parallel architectures because the number of processing elements that can be placed on a chip, board, or chassis is often pin limited. a d-dimensional bused hypercube interconnection network is presented that allows nodes to simultaneously (in one clock tick) exchange data across any dimension using only d+1 ports per node rather than 2d. despite this near two-to-one reduction, the network also allows nodes that are two dimensions apart to simultaneously exchange data; as a result, certain routings can be performed in nearly half the time. the network is shown to be a special case of a general construction in which any set of d permutations can be performed, in one clock tick, using only d+1 ports per node. a lower-bound technique is also presented and used to establish the optimality of the network, as well as that of several other new bused networks.bused hypercubes and other pin-optimal networks','Cryptanalysis and other attacks'
'we describe several software side-channel attacks based on inter-process leakage through the state of the cpu&#8217;s memory cache. this leakage reveals memory access patterns, which can be used for cryptanalysis of cryptographic primitives that employ data-dependent table lookups. the attacks allow an unprivileged process to attack other processes running in parallel on the same processor, despite partitioning methods such as memory protection, sandboxing and virtualization. some of our methods require only the ability to trigger services that perform encryption or mac using the unknown key, such as encrypted disk partitions or secure network links. moreover, we demonstrate an extremely strong type of attack, which requires knowledge of neither the specific plaintexts nor ciphertexts, and works by merely monitoring the effect of the cryptographic process on the cache. we discuss in detail several such attacks on aes, and experimentally demonstrate their applicability to real systems, such as openssl and linux&#8217;s dm-crypt encrypted partitions (in the latter case, the full key can be recovered after just 800 writes to the partition, taking 65 milliseconds). finally, we describe several countermeasures for mitigating such attacks.cache attacks and countermeasures','Cryptanalysis and other attacks'
'catts, computer assistants, and other training tactics','Cryptanalysis and other attacks'
'this paper describes an empirical research study to characterize attackers and attacks against targets of opportunity. a honey net infrastructure was built and deployed over 167 days that leveraged three different honey pot configurations and a ssh-based authentication proxy to attract and follow attackers over several weeks. a total of 211 attack sessions were recorded and evidence was collected at each stage of the attack sequence: from discovery to intrusion and exploitation of rogue software. this study makes two important contributions: 1) we introduce a new approach to measure attacker skills, and 2) we leverage keystroke profile analysis to differentiate attackers beyond their ip address of origin.characterizing attackers and attacks','Cryptanalysis and other attacks'
'this article describes the results of a network analysis based on the citation among communication journals and those academic disciplines that are cited by those journals labeled as \"communication\" by the web of science. the results indicate that the journals indexed solely as communication rather than those also tagged as another social science are more central in the citation network. further, a cluster analysis of the cited disciplines revealed three groupings, a micro psychological cluster, a macro socio-political group and a woman\'s studies clique. a two-mode network analysis found that the most central communication journals cited multiple clusters, while the peripheral journals cited only one, suggesting that the structure of influence on the field of communication is more complex than suggested by park and leydesdorff (scientometrics 81(1):157---175, 2009). also, the results indicate that the macro cluster is about twice as influential as the micro cluster, rather than as park and leydesdorff suggest that psychology is the discipline\'s primary influence.citations among communication journals and other disciplines','Cryptanalysis and other attacks'
'logical attacks on smart cards have been used for many years, but their attack potential is hindered by the processes used by issuers to verify the validity of code, in particular bytecode verification. more recently, the idea has emerged to combine logical attacks with a physical attack, in order to evade bytecode verification. we present practical work done recently on this topic, as well as some countermeasures that can be put in place against such attacks, and how they can be evaluated by security laboratories.combined attacks and countermeasures','Cryptanalysis and other attacks'
'conways rats and other reversals','Cryptanalysis and other attacks'
'the galois/counter mode (gcm) of operation has been standardized by nist to provide single-pass authenticated encryption. the ghash authentication component of gcm belongs to a class of wegman-carter polynomial hashes that operate in the field gf(2128). we present message forgery attacks that are made possible by its extremely smooth-order multiplicative group which splits into 512 subgroups. gcm uses the same block cipher key k to both encrypt data and to derive the generator h of the authentication polynomial for ghash. in present literature, only the trivial weak key h=0 has been considered. we show that ghash has much wider classes of weak keys in its 512 multiplicative subgroups, analyze some of their properties, and give experimental results on aes-gcm weak key search. our attacks can be used not only to bypass message authentication with garbage but also to target specific plaintext bits if a polynomial mac is used in conjunction with a stream cipher. these attacks can also be applied with varying efficiency to other polynomial hashes and macs, depending on their field properties. our findings show that especially the use of short polynomial-evaluation macs should be avoided if the underlying field has a smooth multiplicative order.cycling attacks on gcm, ghash and other polynomial macs and hashes','Cryptanalysis and other attacks'
'dancing orchids and other art','Cryptanalysis and other attacks'
'new forms of internet attacks, such as sql slammer,have become increasingly sophisticated. although codedin a simple way, the sql slammer worm propagated allover the world at an extremely high speed in a short periodof time, rendering it impossible for humans to counterit using manual intervention. in this paper, we proposea security framework called japonica to detect and respondto unknown attacks at the early stage through the dynamicorchestration of prevention, detection, and responsemechanisms. we identify important requirements to supportthe proposed framework and corresponding system entities.also, we describe our model using colored petri netsto discover a uniform message exchange format among theentities. one unique characteristic of japonica is an activeresponse coordinator and we demonstrate its feasibilityin a proof-of-concept prototype, utilizing a honeypot as anactive entity. our results indicate that japonica can successfullyprevent the spread of sql slammer without humanintervention. we are currently extending the framework tocounter other forms of sophisticated internet attacks.defeating internet attacks using risk awareness and active honeypots','Cryptanalysis and other attacks'
'irregular and unpredictable events are increasingly important design elements of several of the latest business intelligence technologies, such as complex event processing (cep), business performance management (bpm), and the real-time enterprise (rte). theories of individual and organizational learning from irregular events - exceptions, interruptions, surprises, accidents, and so on - tend to conform to one of two contradictory patterns. i draw these theories together to understand the complementary processes by which learners derive knowledge and insight from irregular events. i identify contingency factors that bias learners toward one of the two cognitive modes - incorporation of multiple events into a generalized understanding, or expansion of individual events into rich analytical conversations - and propose \"exception design\" levers by which bpm dashboard implementers can adjust these factors and influence the way users create knowledge in business intelligence. a pilot experiment lends some support to the hypothesis that frequency and ambivalence are designable aspects of exceptions that affect learning. the author seeks feedback and suggestions for a more effective research design.exceptions and other rare and irregular events','Cryptanalysis and other attacks'
'encryption algorithms that use the same secret key for encryption and decryption (also known as block ciphers) allow confidential information to be protected and accessible only by legitimate parties who have knowledge of that secret key. before the public can be comfortable with using a block cipher, it needs to gain public trust on its level of security. over the years, the approach has been somewhat ad hoc where security of a cipher is generally taken to be resistance against some commonly known cryptanalytic attacks, though in parallel some researchers began to introduce sound design theory related to the resistance of a cipher against particular types of attacks. the commutative diagram (cd) cryptanalysis was formalized at fse 2004 as a framework for expressing certain kinds of attacks on block ciphers. being able to use this to unify the different types of attacks in one common framework is one of its main advantages. it was also left as an open problem to extend the framework to incorporate more attacks namely the slide, boomerang, amplified boomerang/rectangle and square attacks. in this paper, we show how to model these attacks with the cd framework.extending commutative diagram cryptanalysis to slide, boomerang, rectangle and square attacks','Cryptanalysis and other attacks'
'passive network monitors, known as telescopes or darknets, have been invaluable in detecting and characterizing malware outbreaks. however, as the use of such monitors becomes commonplace, it is likely that malware will evolve to actively detect and evade them. this paper highlights the threat of simple, yet effective, evasive attacks that undermine the usefulness of passive monitors. our results raise an alarm to the research and operational communities to take proactive countermeasures before we are forced to defend against similar attacks appearing in the wild. specifically, we show how lightweight, coordinated sampling of the ip address space can be used to successfully detect and evade passive network monitors. equally troubling is the fact that in doing so attackers can locate the &#8220;live&#8221; ip space clusters and divert malware scanning solely toward active networks. we show that evasive attacks exploiting this knowledge are also extremely fast, overtaking the entire vulnerable population within seconds.fast and evasive attacks','Cryptanalysis and other attacks'
'fault induction attacks are a serious concern for designers of secure embedded systems. an ideal solution would be a generic circuit transformation that would produce circuits that are robust against fault induction attacks. we develop a framework for analyzing the security of systems against single fault attacks and apply it to a recent proposed method (dual-rail encoding) for generically securing circuits against single fault attacks. ultimately, we find that the method does not hold up under our threat models: n-bit cryptographic keys can be extracted from the device with roughly n trials. we conclude that secure designs should incorporate explicit countermeasures to either directly address or attempt to invalidate our threat models.fault attacks on dual-rail encoded systems','Cryptanalysis and other attacks'
'two hundred and sixty-three subjects each gave examples for one of five geographic categories: geographic features, geographic objects, geographic concepts, something geographic, and something that could be portrayed on a map. the frequencies of various responses were significantly different, indicating that the basic ontological terms feature, object, etc., are not interchangeable but carry different meanings when combined with adjectives indicating geographic or mappable. for all of the test phrases involving geographic, responses were predominantly natural features such as mountain, river, lake, ocean, hill. artificial geographic features such as town and city were listed hardly at all for geographic categories, an outcome that contrasts sharply with the disciplinary self-understanding of academic geography. however, geographic artifacts and fiat objects, such as roads, cities, boundaries, countries, and states, were frequently listed by the subjects responding to the phrase something that could be portrayed on a map. in this paper, we present the results of these experiments in visual form, and provide interpretations and implications for further research.features, objects, and other things','Cryptanalysis and other attacks'
'the notions of weak stationarity of stochastic sequences and homogeneity of linear systems are modified in an appropriate way. using the theory of hypergroups a couple of results are derived for this class of stochastic sequences resp. linear systems.graph theoretic and other models','Cryptanalysis and other attacks'
'although motivated by both usability and security concerns, the existing literature on click-based graphical password schemes using a single background image (e.g., passpoints) has focused largely on usability. we examine the security of such schemes, including the impact of different background images, and strategies for guessing user passwords. we report on both short- and long-term user studies: one lab-controlled, involving 43 users and 17 diverse images, and the other a field test of 223 user accounts. we provide empirical evidence that popular points (hot-spots) do exist for many images, and explore two different types of attack to exploit this hot-spotting: (1) a \"human-seeded\" attack based on harvesting click-points from a small set of users, and (2) an entirely automated attack based on image processing techniques. our most effective attacks are generated by harvesting password data from a small set of users to attack other targets. these attacks can guess 36\% of user passwords within 231 guesses (or 12\% within 216 guesses) in one instance, and 20\% within 233 guesses (or 10\% within 218 guesses) in a second instance. we perform an image-processing attack by implementing and adapting a bottom-up model of visual attention, resulting in a purely automated tool that can guess up to 30\% of user passwords in 235 guesses for some instances, but under 3\% on others. our results suggest that these graphical password schemes appear to be at least as susceptible to offline attack as the traditional text passwords they were proposed to replace.human-seeded attacks and exploiting hot-spots in graphical passwords','Cryptanalysis and other attacks'
'in this article, we present improved related-key attacks on the original desx, and desx+, a variant of the desx with its pre-and post-whitening xor operations replaced with addition modulo 264. compared to previous results, our attack on desx has reduced text complexity, while our best attack on desx+ eliminates the memory requirements at the same processing complexity.improved related-key attacks on desx and desx+','Cryptanalysis and other attacks'
'at crypto \'85, desmedt and odlyzko described a chosen-ciphertext attack against plain rsa encryption. the technique can also be applied to rsa signatures and enables an existential forgery under a chosen-message attack. the potential of this attack remained untapped until a twitch in the technique made it effective against two very popular rsa signature standards, namely iso/iec 9796-1 and iso/iec 9796-2. following these attacks, iso/iec 9796-1 was withdrawn and iso/iec 9796-2 amended. in this paper, we explain in detail desmedt and odlyzko\'s attack as well as its application to the cryptanalysis of iso/iec 9796-2.index calculation attacks on rsa signature and encryption','Cryptanalysis and other attacks'
'copyrights have dominated the legal treatment of works of authors to the detriment of the theory of public domain treatment after a limited period of time. the imbalance should be addressed, as should the handling of rights in a digital regime where copying and distribution is trivially achieved. recoding of rights and rights transfers for all works could be part of the solution to managing rights in a digital environment.internet and the other ip','Cryptanalysis and other attacks'
'traditional intrusion detection systems (ids) detect attacks by comparing current behavior to signatures of known attacks. one main drawback is the inability of detecting new attacks which do not have known signatures. in this paper we propose a learning algorithm that constructs models of normal behavior from attack-free network traffic. behavior that deviates from the learned normal model signals possible novel attacks. our ids is unique in two respects. first, it is nonstationary, modeling probabilities based on the time since the last event rather than on average rate. this prevents alarm floods. second, the ids learns protocol vocabularies (at the data link through application layers) in order to detect unknown attacks that attempt to exploit implementation errors in poorly tested features of the target software. on the 1999 darpa ids evaluation data set [9], we detect 70 of 180 attacks (with 100 false alarms), about evenly divided between user behavioral anomalies (ip addresses and ports, as modeled by most other systems) and protocol anomalies. because our methods are unconventional there is a significant non-overlap of our ids with the original darpa participants, which implies that they could be combined to increase coverage.learning nonstationary models of normal network traffic for detecting novel attacks','Cryptanalysis and other attacks'
'building on the work of others is the only way to make substantial progress in any field. yet computer programming continues as a cottage industry because programmers insist on reinventing tools for each new application. what we must encourage is a way of packaging programs so that they can be perceived as standard tools, each performing its specialized task sufficiently well that there is seldom any need felt to duplicate its function.minicompilers, preprocessors and other tools','Cryptanalysis and other attacks'
'smartphones\' features are great, but with the power they provide, there\'s also a threat. smartphones are becoming a target of attackers in the same way pcs have been for many years. this article examines the security models of two popular smart phone operating systems: apple\'s ios and google\'s android.mobile attacks and defense','Cryptanalysis and other attacks'
'cloud computing is a resulting technology from many fields of computing. the concept core of cloud computing is to get services and processing capacity over the internet. this technology reduces cost, increase storage, automate systems, and introduce flexibility and mobility of information. many technologies have been emerged from the cloud computing such as mobile cloud computing. mobile cloud computing is a combination between mobile computing and cloud computing, aims at providing optimal services for mobile users. because mobile computing includes using computers during the movement from place to place to provide users with their maximum need, they have the ability to access other computer, other digital and portable devices around them. the emergence of, nearly similar, technology that deal with this issue is called ubiquitous computing. ubiquitous computing implies making the digital devices available while they are effectively invisible to users. its aim is to break away from the desktop computing to provide computational services to a user when and where required. these technologies are limited and encountered several challenges. mobile cloud computing will support these technologies and solve most of these challenges. in this paper, we survey the resulted technology mobile cloud computing, and we explore different technologies that preceded it such as cloud computing, mobile computing and ubiquitous computing.mobile cloud computing and other mobile technologies','Cryptanalysis and other attacks'
'back in 2007, hasegawa discovered a novel cross-site scripting (xss) vector based on the mistreatment of the backtick character in a single browser implementation. this initially looked like an implementation error that could easily be fixed. instead, as this paper shows, it was the first example of a new class of xss vectors, the class of mutation-based xss (mxss) vectors, which may occur in innerhtml and related properties. mxss affects all three major browser families: ie, firefox, and chrome. we were able to place stored mxss vectors in high-profile applications like yahoo! mail, rediff mail, openexchange, zimbra, roundcube, and several commercial products. mxss vectors bypassed widely deployed server-side xss protection techniques (like html purifier, kses, htmllawed, blueprint and google caja), client-side filters (xss auditor, ie xss filter), web application firewall (waf) systems, as well as intrusion detection and intrusion prevention systems (ids/ips). we describe a scenario in which seemingly immune entities are being rendered prone to an attack based on the behavior of an involved party, in our case the browser. moreover, it proves very difficult to mitigate these attacks: in browser implementations, mxss is closely related to performance enhancements applied to the html code before rendering; in server side filters, strict filter rules would break many web applications since the mxss vectors presented in this paper are harmless when sent to the browser. this paper introduces and discusses a set of seven different subclasses of mxss attacks, among which only one was previously known. the work evaluates the attack surface, showcases examples of vulnerable high-profile applications, and provides a set of practicable and low-overhead solutions to defend against these kinds of attacks.mxss attacks','Cryptanalysis and other attacks'
'networked technologies and other stories','Cryptanalysis and other attacks'
'online education, social media and other topics','Cryptanalysis and other attacks'
'operations on hypermaps, and other automorphisms','Cryptanalysis and other attacks'
'particles, pragmatic and other','Cryptanalysis and other attacks'
'phishing is a form of online identity theft employing both social engineering and technical subterfuge to steal user credentials such as usernames and passwords. targeted data sources include especially web pages, email spam, domain names. mounting a phishing attacks may take several ways but the popular one takes the form of a phishing message arrives in the user mailbox pretending to be from a bank, directing the user to a web page and asking him to enter his credentials, but the web page is not one actually associated with the bank. in this paper, we focus on the web site phishing, in which available solutions are based either on providing early warning of suspicious activity and rapid response or on the use of tls (transport layer security). we present the tls-srp (secure remote password) and tls-psk (pre shared key) protocols and we demonstrate how these two solutions can be useful to reduce the web site phishing threats.phishing attacks and solutions','Cryptanalysis and other attacks'
'in this paper, we describe two attacks on ieee 802.11 based wireless lans. the first attack is an improved key recovery attack on wep, which reduces the average number of packets an attacker has to intercept to recover the secret key. the second attack is (according to our knowledge) the first practical attack on wpa secured wireless networks, besides launching a dictionary attack when a weak pre-shared key (psk) is used. the attack works if the network is using tkip to encrypt the traffic. an attacker, who has about 12-15 minutes access to the network is then able to decrypt an arp request or response and send 7 packets with custom content to network.practical attacks against wep and wpa','Cryptanalysis and other attacks'
'program committee and other reviewers','Cryptanalysis and other attacks'
'due to their high practical impact, cross-site scripting (xss) attacks have attracted a lot of attention from the security community members. in the same way, a plethora of more or less effective defense techniques have been proposed, addressing the causes and effects of xss vulnerabilities. noscript, and disabling scripting code in non-browser applications such as e-mail clients or instant messengers. as a result, an adversary often can no longer inject or even execute arbitrary scripting code in several real-life scenarios. in this paper, we examine the attack surface that remains after xss and similar scripting attacks are supposedly mitigated by preventing an attacker from executing javascript code. we address the question of whether an attacker really needs javascript or similar functionality to perform attacks aiming for information theft. the surprising result is that an attacker can also abuse cascading style sheets (css) in combination with other web techniques like plain html, inactive svg images or font files. through several case studies, we introduce the so called scriptless attacks and demonstrate that an adversary might not need to execute code to preserve his ability to extract sensitive information from well protected websites. more precisely, we show that an attacker can use seemingly benign features to build side channel attacks that measure and exfiltrate almost arbitrary data displayed on a given website. we conclude this paper with a discussion of potential mitigation techniques against this class of attacks. in addition, we have implemented a browser patch that enables a website to make a vital determination as to being loaded in a detached view or pop-up window. this approach proves useful for prevention of certain types of attacks we here discuss.scriptless attacks','Cryptanalysis and other attacks'
'in spite of the use of standard web security measures (ssl/tls), users enter sensitive information such as passwords into fake web sites. such fake sites cause substantial damages to individuals and corporations. in this work, we identify several vulnerabilities of browsers, focusing on security and identification indicators. we present improved security and identification indicators, as we implemented in trustbar, a browser extension we developed. with trustbar, users can assign a name or logo to identify ssl/tls-protected sites; if users did not assign a name or logo, trustbar identifies protected sites by the name or logo of the site, and by the certificate authority (ca) who identified the site. we present usability experiments which compared trustbar\'s indicators to the basic indicators available in most browsers (padlock, url, and https prefix), and some relevant secure-usability principles.security and identification indicators for browsers against spoofing and phishing attacks','Cryptanalysis and other attacks'
'sets and other types','Cryptanalysis and other attacks'
'when designing or analyzing applications or infrastructures with high reliability, safety, security, or survivability demands, the fundamental questions are: what is required of the application and can the infrastructure support these requirements. in the design and analysis of fault-tolerant systems, fault models have served us well to describe the theoretical limits. but with the inclusion of malicious acts, the direct application of fault models has exposed limited applicability. however, we can take advantage of the powerful fault models if we defer their direct application from the events that lead to faults, that is, the fault causes, and instead focus on the effects. this way one can avoid questions referring to the meaning of fault models in the context of previously unsuitable faults like trojan horses or denial of service (dos) attacks. instead, we can use fault models at the level of abstraction where the application maps on the infrastructure. in this paper fault models are discussed in the context of system survivability and malicious act. it is shown that these models can be used to balance the demands put on the application and the capabilities of the underlying infrastructure. active and imposed fault descriptions are defined that allow to match the mechanisms that provide survivability to the application with the infrastructure-imposed limitations. by defining a system as a collection of functionalities, individual functionalities and their associated fault descriptions can be analyzed in isolation.surviving attacks and intrusions','Cryptanalysis and other attacks'
'the high-performance computing and communications program has been attacked for having vague and dubious goals. partly because of this perception, 65 members of the house of representatives cast votes against extending it. we need concrete measures of hpcc progress without such narrowly defined goals. measuring performance by high flops rates, speedup, and hardware efficiency can take us further from the solution to scientific problems, not closer. this paradox is especially pronounced for \"grand challenge\" and \"teraflops computing\". the author considers how we need a practical way to define and communicate ends-based performance of an application, not means-based measures such as teraflops or double precision. human productivity issues such as development time and cost and the quality of the knowledge we obtain should be the basis of our performance metrics.teraflops and other false goals','Cryptanalysis and other attacks'
'the security of encryption algorithms dependsheavily on the computational infeasibility ofexhaustive key-space searches. the rc4 cipher,utilized primarily in the area of datacommunications, is being used in this paper as atest case for determining the effectiveness ofexhaustive key-searches implemented on fpgasusing a network on chip (noc) designarchitecture. preliminary results show that anetwork of key-checker units implemented on axilinx xc2v1000 fpga using the celoxica dk2design tools can exploit the speed and parallelismof hardware such that the entire key-space of a 40-bit rc4 encryption can be searched in minutes. furthermore, it has been found that the clock rateof the circuit diminishes as the number of key-checkerunits increases. future work is proposed tofind a method for predicting an optimal balancebetween the size of the network (# of key-checkerunits) and the clock rate in order to maximizeperformance.the effectiveness of brute force attacks on rc4','Cryptanalysis and other attacks'
'the most and other radio telescopes','Cryptanalysis and other attacks'
'by carefully measuring the amount of time required tm perform private key operalions, attackers may be able to find fixed diffie-hellman exponents, factor rsa keys, and break other cryptosystems. against, a valnerable system, the attack is computationally inexpensive and often requires only known ciphertext. actual systems are potentially at risk, including cryptographic tokens, network-based cryptosystems, and other applications where attackers can make reasonably accurate timing measurements. techniques for preventing the attack for rsa and diffie-hellman are presented. some cryptosystems will need to be revised to protect against the attack, and new protocols and algorithms may need to incorporate measures to prevenl timing attacks.timing attacks on implementations of diffie-hellman, rsa, dss, and other systems','Cryptanalysis and other attacks'
'a type flaw attack on a security protocol is an attack where a field that was originally intended to have one type is subsequently interpreted as having another type. in the paper, we describe some typical examples of type flaw attack and give a type flaw attack that was found on the gdoi protocol. we then analysize the limitations of recent tagging technique of preventing type flaw attack. finally, a method against type flaw attack by checking the length of message is proposed.type flaw attacks and prevention in security protocols','Cryptanalysis and other attacks'
'using prompt and other predicasts databases','Cryptanalysis and other attacks'
'y2k and other software noncrises','Cryptanalysis and other attacks'
'recent research studied the problem of publishing microdata without revealing sensitive information, leading to the privacy-preserving paradigms of k-anonymity and l-diversity. k-anonymity protects against the identification of an individual\'s record. l-diversity, in addition, safeguards against the association of an individual with specific sensitive information. however, existing approaches suffer from at least one of the following drawbacks: (i) l-diversification is solved by techniques developed for the simpler k-anonymization problem, causing unnecessary information loss. (ii) the anonymization process is inefficient in terms of computational and i/o cost. (iii) previous research focused exclusively on the privacy-constrained problem and ignored the equally important accuracy-constrained (or dual) anonymization problem. in this article, we propose a framework for efficient anonymization of microdata that addresses these deficiencies. first, we focus on one-dimensional (i.e., single-attribute) quasi-identifiers, and study the properties of optimal solutions under the k-anonymity and l-diversity models for the privacy-constrained (i.e., direct) and the accuracy-constrained (i.e., dual) anonymization problems. guided by these properties, we develop efficient heuristics to solve the one-dimensional problems in linear time. finally, we generalize our solutions to multidimensional quasi-identifiers using space-mapping techniques. extensive experimental evaluation shows that our techniques clearly outperform the existing approaches in terms of execution time and information loss.a framework for efficient data anonymization under privacy and accuracy constraints','Data anonymization and sanitization'
'published data is prone to privacy attacks. sanitization methods aim to prevent these attacks while maintaining usefulness of the data for legitimate users. quantifying the trade-off between usefulness and privacy of published data has been the subject of much research in recent years. we propose a pragmatic framework for evaluating sanitization systems in real-life and use data mining utility as a universal measure of usefulness and privacy. we propose a definition for data mining utility that can be tuned to capture the needs of data users and the adversaries\' intentions in a setting that is specified by a database, a candidate sanitization method, and privacy and utility concerns of data owner. we use this framework to evaluate and compare privacy and utility offered by two well-known sanitization methods, namely k-anonymity and &#949;-differential privacy, when uci\'s \"adult\" dataset and the weka data mining package is used, and utility and privacy measures are defined for users and adversaries. in the case of k-anonymity, we compare our results with the recent work of brickell and shmatikov (kdd 2008), and show that using data mining algorithms increases their proposed adversarial gains.a practice-oriented framework for measuring privacy and utility in data sanitization systems','Data anonymization and sanitization'
'the need for information security and privacy awareness increases as technology advances and becomes more widely used in everyday life. information storages such as hard disk drives, cds, and usb flash memories are commonly used today, yet the majority of the users neglects to be concerned about the privacy of the information they store on these devices. it is important to know how these storage devices operate and how to permanently delete data from them. disk sanitization plays a major role in ensuring information security and hence it is the focus of this paper.a study of information privacy and data sanitization problems','Data anonymization and sanitization'
'discovering frequent patterns in large databases is one of the most studied problems in data mining, since it can yield substantial commercial benefits. however, some sensitive patterns with security considerations may compromise privacy. in this paper, we aim to determine appropriate balance between need for privacy and information discovery in frequent patterns. a novel method to modify databases for hiding sensitive patterns is proposed in this paper. multiplying the original database by a sanitization matrix yields a sanitized database with private content. in addition, two probabilities are introduced to oppose against the recovery of sensitive patterns and to reduce the degree of hiding non-sensitive patterns in the sanitized database. the complexity analysis and the security discussion of the proposed sanitization process are provided. the results from a series of experiments performed to show the efficiency and effectiveness of this approach are described.an efficient sanitization algorithm for balancing information privacy and knowledge discovery in association patterns mining','Data anonymization and sanitization'
'data collection agencies publish sensitive data for legitimate purposes, such as research, marketing and etc. data publishing has attracted much interest in research community due to the important concerns over the protection of individuals privacy. as a result several sanitization mechanisms with different notions of privacy have been proposed. to be able to measure, set and compare the level of privacy protection, there is a need to translate these different mechanisms to a unified system. in this paper, we propose a novel information theoretic framework for representing a formal model of a mechanism as a noisy channel and evaluating its privacy and utility. we show that deterministic publishing property that is used in most of these mechanisms reduces the privacy guarantees and causes information to leak. the great effect of adversary\'s background knowledge on this metric is concluded. we also show that using this framework we can compute the sanitization mechanism\'s preserved utility from the point of view of a data user. by using the specifications of a popular sanitization mechanism, k-anonymity, we analytically provide a representation of this mechanism to be used for its evaluation.an information theoretic privacy and utility measure for data sanitization mechanisms','Data anonymization and sanitization'
'we study the problem of privacy-preservation in social networks. we consider the distributed setting in which the network data is split between several data holders. the goal is to arrive at an anonymized view of the unified network without revealing to any of the data holders information about links between nodes that are controlled by other data holders. to that end, we start with the centralized setting and offer two variants of an anonymization algorithm which is based on sequential clustering (sq). our algorithms significantly outperform the sangreea algorithm due to campan and truta which is the leading algorithm for achieving anonymity in networks by means of clustering. we then devise secure distributed versions of our algorithms. to the best of our knowledge, this is the first study of privacy preservation in distributed social networks. we conclude by outlining future research proposals in that direction.anonymization of centralized and distributed social networks by sequential clustering','Data anonymization and sanitization'
'preserving individual privacy when publishing data is a problem that is receiving increasing attention. thanks to its simplicity the concept of k-anonymity, introduced by samarati and sweeney [1], established itself as one fundamental principle for privacy preserving data publishing. according to the k-anonymity principle, each release of data must be such that each individual is indistinguishable from at least k-1 other individuals. in this article we tackle the problem of anonymization of moving objects databases. we propose a novel concept of k-anonymity based on co-localization, that exploits the inherent uncertainty of the moving object\'s whereabouts. due to sampling and imprecision of the positioning systems (e.g., gps), the trajectory of a moving object is no longer a polyline in a three-dimensional space, instead it is a cylindrical volume, where its radius @d represents the possible location imprecision: we know that the trajectory of the moving object is within this cylinder, but we do not know exactly where. if another object moves within the same cylinder they are indistinguishable from each other. this leads to the definition of (k,@d)-anonymity for moving objects databases. we first characterize the (k,@d)-anonymity problem, then we recall nwa (neverwalkalone), a method that we introduced in [2] based on clustering and spatial perturbation. starting from a discussion on the limits of nwa we develop a novel clustering method that, being based on edr distance [3], has the important feature of being time-tolerant. as a consequence it perturbs trajectories both in space and time. the novel method, named w4m (waitforme), is empirically shown to produce higher quality anonymization than nwa, at the price of higher computational requirements. therefore, in order to make w4m scalable to large datasets, we introduce two variants based on a novel (and computationally cheaper) time-tolerant distance function, and on chunking. all the variants of w4m are empirically evaluated in terms of data quality and efficiency, and thoroughly compared to their predecessor nwa. data quality is assessed both by means of objective measures of information distortion, and by more usability oriented measure, i.e., by comparing the results of (i) spatio-temporal range queries and (ii) frequent pattern mining, executed on the original database and on the (k,@d)-anonymized one. experimental results over both real-world and synthetic mobility data confirm that, for a wide range of values of @d and k, the relative distortion introduced by our anonymization methods is kept low. moreover, the techniques introduced to make w4m scalable to large datasets, achieve their goal without giving up data quality in the anonymization process.anonymization of moving objects databases by clustering and perturbation','Data anonymization and sanitization'
'sharing healthcare data has become a vital requirement in healthcare system management; however, inappropriate sharing and usage of healthcare data could threaten patients&#8217; privacy. in this article, we study the privacy concerns of sharing patient information between the hong kong red cross blood transfusion service (bts) and the public hospitals. we generalize their information and privacy requirements to the problems of centralized anonymization and distributed anonymization, and identify the major challenges that make traditional data anonymization methods not applicable. furthermore, we propose a new privacy model called lkc-privacy to overcome the challenges and present two anonymization algorithms to achieve lkc-privacy in both the centralized and the distributed scenarios. experiments on real-life data demonstrate that our anonymization algorithms can effectively retain the essential information in anonymous data for data analysis and is scalable for anonymizing large datasets.centralized and distributed anonymization for high-dimensional healthcare data','Data anonymization and sanitization'
'many applications of social networks require identity and/or relationship anonymity due to the sensitive, stigmatizing, or confidential nature of user identities and their behaviors. recent work showed that the simple technique of anonymizing graphs by replacing the identifying information of the nodes with random ids does not guarantee privacy since the identification of the nodes can be seriously jeopardized by applying background based attacks. in this paper, we investigate how well an edge based graph randomization approach can protect node identities and sensitive links. we quantify both identity disclosure and link disclosure when adversaries have one specific type of background knowledge (i.e., knowing the degrees of target individuals). we also conduct empirical comparisons with the recently proposed k-degree anonymization schemes in terms of both utility and risks of privacy disclosures.comparisons of randomization and k-degree anonymization schemes for privacy preserving social network publishing','Data anonymization and sanitization'
'user data is often unprotected on disk and tape drives or not erased when no longer needed, creating data security vulnerabilities that many computer users are unaware of. federal and state laws require data sanitization, which comprises a variety of data eradication methods. secure sanitization refers to methods meeting those federal and state laws. companies that fail to meet these laws can be subject to fines of $5 million, and individuals can be imprisoned for up to 10 years. physical destruction of storage devices offers the highest security. but executing the disk drive internal secure-erase command also offers a higher security level than external-block-overwrite software, according to federal guideline nist 800-88. recent disk drives with internal full disk encryption now implement an enhanced secure-erase command that takes only milliseconds to complete.disposal of disk and tape data by secure sanitization','Data anonymization and sanitization'
'in this paper we evaluate the effect of a document sanitization process on a set of information retrieval metrics, in order to measure information loss and risk of disclosure. as an example document set, we use a subset of the wikileaks cables, made up of documents relating to five key news items which were revealed by the cables. in order to sanitize the documents we have developed a semi-automatic anonymization process following the guidelines of executive order 13526 (2009) of the us administration, by (i) identifying and anonymizing specific person names and data, and (ii) concept generalization based on wordnet categories, in order to identify words categorized as classified. finally, we manually revise the text from a contextual point of view to eliminate complete sentences, paragraphs and sections, where necessary. we show that a significant sanitization can be applied, while maintaining the relevance of the documents to the queries corresponding to the five key news items.document sanitization','Data anonymization and sanitization'
'passive network measurement and packet header trace collection are vital tools for network operation and research. to protect a user\'s privacy, it is necessary to anonymize header fields, particularly ip addresses. to preserve the correlation between ip addresses, prefix-preserving anonymization has been proposed. the limitations of this approach for a high-performance measurement system are the need for complex cryptographic computations and potentially large amounts of memory. we propose a new prefix-preserving anonymization algorithm, top-hash subtree-replicated anonymization (tsa), that features three novel improvements: precomputation, replicated subtrees, and top hashing. tsa makes anonymization practical to be implemented on network processors or dedicated logic at gigabit rates. the performance of tsa is compared with a conventional cryptography based prefix-preserving anonymization scheme which utilizes caching. tsa performs better as it requires no online cryptographic computation and a small number of memory lookups per packet. our analytic comparison of the susceptibility to attacks between conventional anonymization and our approach shows that tsa performs better for small scale attacks and comparably for medium scale attacks. the processing cost for tsa is reduced by two orders of magnitude and the memory requirements are a few megabytes. the ability to tune the memory requirements and security level makes tsa ideal for a broad range of network systems with different capabilities.high-speed prefix-preserving ip address anonymization for passive measurement systems','Data anonymization and sanitization'
'the technique of k-anonymization allows the releasing of databases that contain personal information while ensuring some degree of individual privacy. anonymization is usually performed by generalizing database entries. we formally study the concept of generalization, and propose three information-theoretic measures for capturing the amount of information that is lost during the anonymization process. the proposed measures are more general and more accurate than those that were proposed by meyerson and williams [23] and aggarwal et al. [1]. we study the problem of achieving k-anonymity with minimal loss of information. we prove that it is np-hard and study polynomial approximations for the optimal solution. our first algorithm gives an approximation guarantee of o(\\ln k) for two of our measures as well as for the previously studied measures. this improves the best-known o(k)-approximation in [1]. while the previous approximation algorithms relied on the graph representation framework, our algorithm relies on a novel hypergraph representation that enables the improvement in the approximation ratio from o(k) to o(\\ln k). as the running time of the algorithm is o(n^{2k}), we also show how to adapt the algorithm in [1] in order to obtain an o(k)-approximation algorithm that is polynomial in both n and k.k-anonymization with minimal loss of information','Data anonymization and sanitization'
'over the last several years, there has been an emerging interest in the development of wide-area data collection and analysis centers to help identify, track, and formulate responses to the ever-growing number of coordinated attacks and malware infections that plague computer networks worldwide. as large-scale network threats continue to evolve in sophistication and extend to widely deployed applications, we expect that interest in collaborative security monitoring infrastructures will continue to grow, because such attacks may not be easily diagnosed from a single point in the network. the intent of this position paper is not to argue the necessity of internet-scale security data sharing infrastructures, as there is ample research [13, 48, 51, 54, 41, 47, 42] and operational examples [43, 17, 32, 53] that already make this case. instead, we observe that these well-intended activities raise a unique set of risks and challenges. we outline some of the most salient issues faced by global network security centers, survey proposed defense mechanisms, and pose several research challenges to the computer security community. we hope that this position paper will serve as a stimulus to spur groundbreaking new research in protection and analysis technologies that can facilitate the collaborative sharing of network security data while keeping data contributors safe and secure.large-scale collection and sanitization of network security data','Data anonymization and sanitization'
'movement data, that is, trajectories of mobile objects, are automatically collected in huge quantities by technologies such as gps, gsm or rfid, among others. publishing and exploiting such data is essential to improve transportation, to understand the dynamics of the economy in a region, etc. however, there are obvious threats to the privacy of individuals if their trajectories are published in a way which allows re-identification of the individual behind a trajectory. we contribute to the literature on privacy-preserving publication of trajectories by presenting a distance measure for trajectories which naturally considers both spatial and temporal aspects of trajectories, is computable in polynomial time, and can cluster trajectories not defined over the same time span. our distance measure can be naturally instantiated using other existing similarity measures for trajectories that are appropriate for anonymization purposes. then, we propose two heuristics for trajectory anonymization which yield anonymized trajectories formed by fully accurate true original locations. the first heuristic is based on trajectory microaggregation using the above distance and on location permutation; it effectively achieves trajectory k-anonymity. the second heuristic is based only on location permutation; it gives up trajectory k-anonymity and aims at location k-diversity. the strong point of the second heuristic is that it takes into account reachability constraints when computing anonymized trajectories. experimental results on a synthetic data set and a real-life data set are presented; for similar privacy protection levels and most reasonable parameter choices, our two methods offer better utility than comparable previous proposals in the literature.microaggregation- and permutation-based anonymization of movement data','Data anonymization and sanitization'
' static code attributes such as lines of code and cyclomatic complexity have been shown to be useful indicators of defects in software modules. as web applications adopt input sanitization routines to prevent web security risks, static code attributes that represent the characteristics of these routines may be useful for predicting web application vulnerabilities. in this paper, we classify various input sanitization methods into different types and propose a set of static code attributes that represent these types. then we use data mining methods to predict sql injection and cross site scripting vulnerabilities in web applications. preliminary experiments show that our proposed attributes are important indicators of such vulnerabilities. mining input sanitization patterns for predicting sql injection and cross site scripting vulnerabilities','Data anonymization and sanitization'
'recent work has shown the importance of considering the adversary\'s background knowledge when reasoning about privacy in data publishing. however, it is very difficult for the data publisher to know exactly the adversary\'s background knowledge. existing work cannot satisfactorily model background knowledge and reason about privacy in the presence of such knowledge.this paper presents a general framework for modeling the adversary\'s background knowledge using kernel estimation methods. this framework subsumes different types of knowledge (e.g., negative association rules) that can be mined from the data. under this framework, we reason about privacy using bayesian inference techniques and propose the skyline (b, t)-privacy model, which allows the data publisher to enforce privacy requirements to protect the data against adversaries with different levels of background knowledge. through an extensive set of experiments, we show the effects of probabilistic background knowledge in data anonymization and the effectiveness of our approach in both privacy protection and utility preservation.modeling and integrating background knowledge in data anonymization','Data anonymization and sanitization'
'a matrix m over a fixed alphabet is k-anonymous if every row in m has at least k - 1 identical copies in m. making a matrix k- anonymous by replacing a minimum number of entries with an additional *-symbol (called \"suppressing entries\") is known to be np-hard. this task arises in the context of privacy-preserving publishing. we propose and analyze the computational complexity of an enhanced anonymization model where the user of the k-anonymized data may additionally \"guide\" the selection of the candidate matrix entries to be suppressed. the basic idea is to express this by means of \"pattern vectors\" which are part of the input. this can also be interpreted as a sort of clustering process. it is motivated by the observation that the \"value\" of matrix entries may significantly differ, and losing one (by suppression) may be more harmful than losing the other, which again may very much depend on the intended use of the anonymized data. we show that already very basic special cases of our new model lead to np-hard problems while others allow for (fixed-parameter) tractability results.pattern-guided data anonymization and clustering','Data anonymization and sanitization'
'nowadays, more and more applications use sensitive and personal information. subsequently, hiding identities and respecting citizens\' privacy are becoming extremely important. dedicated to this issue, this paper is organized as follows: after defining the topic through an example of collaborative complex and heterogeneous system, this paper analyzes the most typical anonymization procedures. afterwards it proposes a rigorous approach to define anonymization requirements, as well as how to characterize, select and build solutions. finally, a new generic procedure to anonymize and link identities is proposed. we suggest that a critical part of this procedure is carried out in a smart card. according to needs, anonymized data are processed through cryptographic transformations in several organizations. our solution is suitable to collaborative environments; guarantees the user\'s consent; resists dictionary attacks; respects the least privilege principle and thus fulfills the legislation requirements. moreover, it remains flexible, adaptable to different fields, and supports some organizational changes like the merging of several systems.personal data anonymization for security and privacy in collaborative environments','Data anonymization and sanitization'
' software defect prediction studies have shown that defect predictors built from static code attributes are useful and effective. on the other hand, to mitigate the threats posed by common web application vulnerabilities, many vulnerability detection approaches have been proposed. however, finding alternative solutions to address these risks remains an important research problem. as web applications generally adopt input validation and sanitization routines to prevent web security risks, in this paper, we propose a set of static code attributes that represent the characteristics of these routines for predicting the two most common web application vulnerabilities&#8212;sql injection and cross site scripting. in our experiments, vulnerability predictors built from the proposed attributes detected more than 80\% of the vulnerabilities in the test subjects at low false alarm rates. predicting common web application vulnerabilities from input validation and sanitization code patterns','Data anonymization and sanitization'
'context: sql injection (sqli) and cross site scripting (xss) are the two most common and serious web application vulnerabilities for the past decade. to mitigate these two security threats, many vulnerability detection approaches based on static and dynamic taint analysis techniques have been proposed. alternatively, there are also vulnerability prediction approaches based on machine learning techniques, which showed that static code attributes such as code complexity measures are cheap and useful predictors. however, current prediction approaches target general vulnerabilities. and most of these approaches locate vulnerable code only at software component or file levels. some approaches also involve process attributes that are often difficult to measure. objective: this paper aims to provide an alternative or complementary solution to existing taint analyzers by proposing static code attributes that can be used to predict specific program statements, rather than software components, which are likely to be vulnerable to sqli or xss. method: from the observations of input sanitization code that are commonly implemented in web applications to avoid sqli and xss vulnerabilities, in this paper, we propose a set of static code attributes that characterize such code patterns. we then build vulnerability prediction models from the historical information that reflect proposed static attributes and known vulnerability data to predict sqli and xss vulnerabilities. results: we developed a prototype tool called phpmineri for data collection and used it to evaluate our models on eight open source web applications. our best model achieved an averaged result of 93\% recall and 11\% false alarm rate in predicting sqli vulnerabilities, and 78\% recall and 6\% false alarm rate in predicting xss vulnerabilities. conclusion: the experiment results show that our proposed vulnerability predictors are useful and effective at predicting sqli and xss vulnerabilities.predicting sql injection and cross site scripting vulnerabilities through mining input sanitization patterns','Data anonymization and sanitization'
'the internet and the world wide web democratized the means to publish and share corporate and personal data. many anecdotes occurred over the last decades that well illustrate the danger for privacy and confidentiality. the advent of cloud computing infrastructures is likely, if successful, to further encourage this trend. the analysis, diagnosis and prevention of privacy risk within a cloud computing infrastructure are therefore important services to provide to users. in recent years, several algorithms such as k-anonymity, l-diversity and anatomy, have been proposed to address the issue of data anonymization and diversification. they transform original data sets into modified data sets ensuring some privacy while minimizing the information loss incurred during the transformation. shared and published data can remain meaningful without jeopardizing privacy. we propose an integrated collection of privacy management services together with an interface to orchestrate their execution and assess their evaluation. the system consists of web services and cloud architecture. cloud users can explore and apply privacy management services as cloud services. this proposal is a first but significant step towards the general concept of a cloud of data services and data transformation processes for data privacy, anonymity, security, quality, mining, management, publishing and sharing of data.privacy and anonymization as a service','Data anonymization and sanitization'
'this article presents an overview of privacy problems and solutions to data in location based services, with the emphasis on understanding location privacy issues, alternative models, and architectures. it concludes with an outlook in location privacy research and its impact on mobile internet, pervasive computing, and geo-spatial data management.privacy and location anonymization in location-based services','Data anonymization and sanitization'
'in this paper, we address the problem of protecting somesensitive knowledge in transactional databases. the challengeis on protecting actionable knowledge for strategicdecisions, but at the same time not losing the great benefitof association rule mining. to accomplish that, we introducea new, efficient one-scan algorithm that meets privacyprotection and accuracy in association rule mining, withoutputting at risk the effectiveness of the data mining per se.protecting sensitive knowledge by data sanitization','Data anonymization and sanitization'
'research in data sanitization (including anonymization) emphasizes ways to prevent an adversary from desanitizing data. most work focuses on using mathematical mappings to sanitize data. a few papers examine incorporation of privacy requirements, either in the guise of templates or prioritization. essentially these approaches reduce the information that can be gleaned from a data set. in contrast, this paper considers both the need to ``desanitize\'\' and the need to support privacy. we consider conflicts between privacy requirements and the needs of analysts examining the redacted data. our goal is to enable an informed decision about the effects of redacting, and failing to redact data. we begin with relationships among the data being examined, including relationships with a known data set and other, additional, external data. by capturing these relationships, desanitization techniques that exploit them can be identified, and the information that must be concealed in order to thwart them can be determined. knowing that, a realistic assessment of whether the information and relationships are already widely known or available will enable the sanitizers to assess whether irreversible sanitization is possible, and if so, what to conceal to prevent desanitization.relationships and data sanitization','Data anonymization and sanitization'
'xml mediation for data validation and privacy anonymization of very large complicated xml messages defined in some industry-specific specifications such as hl7 is becoming increasingly important in soa because a lot of applications depend on various kinds of hard-coded data validation and privacy anonymization functions, which makes it difficult to keep consistency of the functions among the applications in soa when the schema of the xml messages in the specifications is updated. this paper proposes a uniform rule-based approach to realize the functions as an xml mediation separated from the applications, which makes it easy to maintain consistency of the functions even when they are updated in accord with the changes to the specifications. therefore, application developers can readily utilize the mediation with many applications in soa without additional modification to the applications. our approach allows the developers to define a set of rules that consist of two components: 1) constraint conditions in a conceptual data notation in the xml message, and 2) actions performed only when the conditions are satisfied. in order to make the rules independent from both the implementation-specific data representation and the industry-specific knowledge, we automatically transform the rules into the implementation-specific data representation using two more factors; one is the data mappings from the data notation in the rules to the concrete data representation in the implementation, and the other is the implied data relationships hidden in the rules. it is very important to take into consideration a general way to import the implied knowledge because it often depends on the industry-specific data structure and it is usually given outside of the mediation system.rule-based xml mediation for data validation and privacy anonymization','Data anonymization and sanitization'
'this work explores issues of computational disclosure control. we examine assumptions in the foundations of traditional problem statements and abstract models. we offer a comprehensive framework, based on the notion of an inference game, that unifies various inference problems by parameterizing their problem spaces. this work raises questions regarding the significance of intractability results. we analyze common structural aspects of inference problems via case studies; these emphasize why explicit policies are needed to specify all social context and ethical values relevant to a problem instance.sanitization models and their limitations','Data anonymization and sanitization'
'the revenue of search-engine providers strongly depends on targeted advertisement. targeted advertisement is becoming more reliant on personal data. this puts user privacy at risk. one way to improve privacy is to anonymize search logs, but this reduces usefulness for ad placement. further, the usefulness depends on the target function used for the anonymization. this paper is the first to study this tradeoff systematically. we quantify the usefulness of an anonymized search log for advertisement purposes, by estimating outcomes such as the number of clicks on ads or the number of ad impressions possible after anonymization. a main result is that anonymized search logs are still useful for advertisement purposes, but the extent strongly depends on the target function.search-log anonymization and advertisement','Data anonymization and sanitization'
'the line between personal and anonymous information is often unclear. increasingly it falls to lawyers to understand and manage the risks associated with the sharing of \"anonymized\" data sets.something bad might happen: lawyers, anonymization and risk','Data anonymization and sanitization'
'k-anonymity is a method for providing privacy protection by ensuring that data cannot be traced to an individual. in a k-anonymous dataset, any identifying information occurs in at least k tuples. to achieve optimal and practical k-anonymity, recently, many different kinds of algorithms with various assumptions and restrictions have been proposed with different metrics to measure quality. this paper presents the family of clustering based algorithms that are more flexible and even attempts to improve precision by ignoring the restrictions of user defined domain generalization hierarchies. the main finding of the paper will be that metrics may behave differently through different algorithms and may not show correlations with some applications\' accuracy on output data.thoughts on k-anonymization','Data anonymization and sanitization'
'numerous generalization techniques have been proposed for privacy-preserving data publishing. most existing techniques, however, implicitly assume that the adversary knows little about the anonymization algorithm adopted by the data publisher. consequently, they cannot guard against privacy attacks that exploit various characteristics of the anonymization mechanism. this article provides a practical solution tothis problem. first, we propose an analytical model for evaluating disclosure risks, when an adversary knows everything in the anonymization process, except the sensitive values. based on this model, we develop a privacy principle, transparent l-diversity, which ensures privacy protection against such powerful adversaries. we identify three algorithms that achieve transparent l-diversity, and verify their effectiveness and efficiency through extensive experiments with real data.transparent anonymization','Data anonymization and sanitization'
'when considering the publishing of web search query logs, there is an important trade-off between the privacy and the usefulness of the data. this paper introduces a novel approach to anonymize search query logs by means of microaggregation. it guaranties k-anonymity on the data without having to completely eliminate any record.tree-based microaggregation for the anonymization of search logs','Data anonymization and sanitization'
'privacy becomes a more and more serious concern in applications involving microdata. recently, efficient anonymization has attracted much research work. most of the previous methods use global recoding, which maps the domains of the quasi-identifier attributes to generalized or changed values. however, global recoding may not always achieve effective anonymization in terms of discernability and query answering accuracy using the anonymized data. moreover, anonymized data is often for analysis. as well accepted in many analytical applications, different attributes in a data set may have different utility in the analysis. the utility of attributes has not been considered in the previous methods.in this paper, we study the problem of utility-based anonymization. first, we propose a simple framework to specify utility of attributes. the framework covers both numeric and categorical data. second, we develop two simple yet efficient heuristic local recoding methods for utility-based anonymization. our extensive performance study using both real data sets and synthetic data sets shows that our methods outperform the state-of-the-art multidimensional global recoding methods in both discernability and query answering accuracy. furthermore, our utility-based method can boost the quality of analysis using the anonymized data.utility-based anonymization using local recoding','Data anonymization and sanitization'
'in this work, we study how continuous video monitoring and intelligent video processing can be used in eldercare to assist the independent living of elders and to improve the efficiency of eldercare practice. more specifically, we develop an automated activity analysis and summarization for eldercare video monitoring. at the object level, we construct an advanced silhouette extraction, human detection and tracking algorithm for indoor environments. at the feature level, we develop an adaptive learning method to estimate the physical location and moving speed of a person from a single camera view without calibration. at the action level, we explore hierarchical decision tree and dimension reduction methods for human action recognition. we extract important adl (activities of daily living) statistics for automated functional assessment. to test and evaluate the proposed algorithms and methods, we deploy the camera system in a real living environment for about a month and have collected more than 200 hours (in excess of 600 g bytes) of activity monitoring videos. our extensive tests over these massive video datasets demonstrate that the proposed automated activity analysis system is very efficient.activity analysis, summarization, and visualization for indoor human activity monitoring','Database activity monitoring'
'activity monitoring','Database activity monitoring'
'a new method for designing single rail asynchronous circuits is studied. it utilizes additional circuitry to monitor the activity of nodes within combinational logic blocks. when all transitions have halted a completion signal is generated. details of the circuit and design methodology are given and the influence of glitches on the proposed circuit is discussed. three different levels of granularity are investigated. experimental physical layout of the circuit with extracted and back-annotated simulation results is provided. the proposed approach results in faster operation than synchronous circuits with minimum circuit overhead incurred.activity-monitoring completion-detection (amcd)','Database activity monitoring'
'multi-agent system (mas) is a system composed of several agents, collectively capable of achieving goals that are difficult to achieve by an individual agent or monolithic system. mas is ideal for a network-like application for its flexibility, distributed nature, and modifiability, without the need for detailed rewriting of the application. in this paper, we have proposed agent based activity monitoring system (abams) for the monitoring of resources over a network, suitable for network of networks; commonly known as can (campus area network). the system is fully autonomous and once initialized with the given rules and domain knowledge abams manages resources on its own with the help of mobile agents.agent based activity monitoring system - abams','Database activity monitoring'
'this paper examines an increasingly relevant topic in the multimedia community of wearable devices that record the physical activity of a user throughout a day. while activity and other accelerometry-based data has been shown effective in various multimedia applications -- from context-aware music retrieval to approximating carbon footprint -- the most promising role of these target application for healthcare and personal fitness. recently, several low-cost devices have become available to consumers. in this paper, we perform an evaluation on the most popular devices available on the market (in particular fitbit and nike+) and report our findings in terms of accuracy, type of data provided, available apis, and user experience. this information is useful for researchers considering incorporating these activity-based data streams into their research and for getting a better idea of the reliability and accuracy for use in life-logging and other multimedia applications.an evaluation of wearable activity monitoring devices','Database activity monitoring'
'appliance load monitoring systems aim to achieve per appliance energy decomposition. such systems however lack automated set-up, which precludes widespread roll-outs. this work illustrates how human supervision can be reduced to a strict minimum; appliance activity states are captured with cheap wireless sensors, which enables accurate automated energy data annotation. this paper demonstrates the accuracy of this technique via an intuitive graphical user interface.appliance activity monitoring using wireless sensors','Database activity monitoring'
'automating basic system activity monitoring','Database activity monitoring'
'in this paper an automated bathroom activity monitoring system based on acoustics is described. the system is designed to recognize and classify major activities occurring within a bathroom based on sound. carefully designed hmm parameters using mfcc features are used for accurate and robust bathroom sound event classification. experiments to validate the utility of the system were performed firstly in a constrained setting as a proof-of-concept and later in an actual trial involving real people using their bathroom in the normal course of their daily lives. preliminary results are encouraging with the accuracy rate for most sound categories being above 84\%. we sincerely believe that the system contributes towards increased understanding of personal hygiene behavioral problems that significantly affect both informal care-giving and clinical care of dementia patients.bathroom activity monitoring based on sound','Database activity monitoring'
'this paper presents a novel approach for real-time egocentric activity recognition in which component atomic events are characterised in terms of binary relationships between parts of the body and manipulated objects. the key contribution is to summarise, within a histogram, the relationships that hold over a fixed time interval. this histogram is then classified into one of a number of atomic events. the relationships encode both the types of body parts and objects involved (e.g. wrist, hammer) together with a quantised representation of their distance apart and the normalised rate of change in this distance. the quantisation and classifier are both configured in a prior learning phase from training data. an activity is represented by a markov model over atomic events. we show the application of the method in the prediction of the next atomic event within a manual procedure (e.g. assembling a simple device) and the detection of deviations from an expected procedure. this could be used for example in training operators in the use or servicing of a piece of equipment, or the assembly of a device from components. we evaluate our approach (\'bag-of-relations\') on two datasets: \'labelling and packaging bottles\' and \'hammering nails and driving screws\', and show superior performance to existing bag-of-features methods that work with histograms derived from image features [1]. finally, we show that the combination of data from vision and inertial (imu) sensors outperforms either modality alone.egocentric activity monitoring and recovery','Database activity monitoring'
'real-time access to key performance indicators is necessary to ensure timeliness and effectiveness of operational business processes. the concept of business activity monitoring (bam) refers to the observation, analysis, and presentation of real-time information about business activities across systems\' and companies\' borders. designing and maintaining bam applications is challenging, as the involved concepts (e.g., business processes, audit logs, performance measures) --though being strongly interrelated-- are developed by different communities of practice. also, they reside on different levels of abstraction, and are handled by different it systems. hence, we developed a conceptual modeling language which extends the widely accepted business process modeling notation (bpmn) by bam-relevant concepts. the main results presented in this paper are: (1) a meta-model which formally describes the conceptual aspects of the developed bpmn extension (abstract syntax); (2) graphical symbols as an exemplary representation of this abstract syntax (concrete syntax); (3) a demo scenario that illustrates the application of the language in a fictitious scenario.extending bpmn for business activity monitoring','Database activity monitoring'
'monitoring login activity','Database activity monitoring'
'remote home care systems, which takes care about elderly people are going to be very necessary and useful in a few years. the population gets older and we need to solve problem how to take care about our related elderly. this article is mainly focused on movement activity monitoring of elderly people in their flats, which are equipped by our homecare system. this system consists of pir sensors, tilocation engine solution and own optoelectronic bars placed in doorframes. the data interpretation algorithm is proposed too.movement activity monitoring of elderly people','Database activity monitoring'
'oral session 4 activity monitoring','Database activity monitoring'
'the rich sensing ability of smart mobile phones brings an unique opportunity to detect and long-term monitor people\'s physical activities. however, with mobile phone the application has to comply with people\'s usage habit of it and thus capture the right moment to recognize activities, which will potentially cause great in-class variances. as a result, the model potentially becomes complex and costs much computing resources in mobile phone. this paper recognize people\'s physical activities when they place the mobile phone in the pockets near the pelvic region. experiment results show that the accuracy could reach 97.7\%. to reduce the model size, evaluation of each feature attribution contribution for the accuracy is performed. and the result shows that we can cut the feature dimension from 22 to 8 while obtaining the smallest model.physical activity monitoring with mobile phones','Database activity monitoring'
'business activity monitoring (bam) systems aggregate and analyze data from operational systems in order to provide real-time information about activities and events in organizations. events and business data used in bam systems frequently originate from enterprise resource planning (erp) systems. such erp systems generally store and process information only according to predefined process/event models that are specifically tailored to individual organizations. moreover, erp systems may not model inter-organizational business processes accurately. consequently, bam based on such systems is limited in its scope to information that has been anticipated at design time of these systems. \"unexpected\" information, such as incoming electronic data interchange (edi) messages that are not \"understood\" by the employed erp implementation, is generally ignored. in other words, potential business intelligence is lost unless the erp system in place gets adapted to handle the until-then ignored information. however, such adaptations are costly and potentially time-consuming. in this paper, we introduce our vision of using observed edi messages directly as a data source for events and key performance indicators (kpis) to be used for bam, thereby \"bypassing\" the typically intercalated erp systems. this may provide organizations with additional flexibility for monitoring activities and events which are not (yet) modeled in their erp systems. additionally, it may help integrating inter-organizational business processes into bam.towards edi-based business activity monitoring','Database activity monitoring'
'this paper presents a visual surveillance system for the automatic scene interpretation of airport aprons. the system comprises two modules scene tracking and scene understanding. the scene tracking module, comprising a bottom-up methodology, and the scene understanding module, comprising a video event representation and recognition scheme, have been demonstated to be a valid approach for apron monitoring.visual surveillance for aircraft activity monitoring','Database activity monitoring'
'launching a denial of service (dos) attack is trivial, but detection and response is a painfully slow and often a manual process. automatic classification of attacks as single- or multi-source can help focus a response, but current packet-header-based approaches are susceptible to spoofing. this paper introduces a framework for classifying dos attacks based on header content, and novel techniques such as transient ramp-up behavior and spectral analysis. although headers are easily forged, we show that characteristics of attack ramp-up and attack spectrum are more difficult to spoof. to evaluate our framework we monitored access links of a regional isp detecting 80 live attacks. header analysis identified the number of attackers in 67 attacks, while the remaining 13 attacks were classified based on ramp-up and spectral analysis. we validate our results through monitoring at a second site, controlled experiments, and simulation. we use experiments and simulation to understand the underlying reasons for the characteristics observed. in addition to helping understand attack dynamics, classification mechanisms such as ours are important for the development of realistic models of dos traffic, can be packaged as an automated tool to aid in rapid response to attacks, and can also be used to estimate the level of dos activity on the internet.a framework for classifying denial of service attacks','Denial-of-service attacks'
'phishing is form of identity theft that combines social engineering techniques and sophisticated attack vectors to harvest financial information from unsuspecting consumers. often a phisher tries to lure her victim into clicking a url pointing to a rogue page. in this paper, we focus on studying the structure of urls employed in various phishing attacks. we find that it is often possible to tell whether or not a url belongs to a phishing attack without requiring any knowledge of the corresponding page data. we describe several features that can be used to distinguish a phishing url from a benign one. these features are used to model a logistic regression filter that is efficient and has a high accuracy. we use this filter to perform thorough measurements on several million urls and quantify the prevalence of phishing on the internet todaya framework for detection and measurement of phishing attacks','Denial-of-service attacks'
'a new attack (called \"gradient statistical\") on block ciphers is suggested and experimentally investigated. we demonstrate the possibility of applying it to ciphers for which no attacks are known except for the exhaustive key search.a new type of attacks on block ciphers','Denial-of-service attacks'
'during the last ten years, power analysis attacks have been widely developed under many forms. they analyze the relation between the power consumption or electromagnetic radiation of a cryptographic device and the handled data during cryptographic operations. the goal of this paper is to give a global view of statistical attacks based on side channel analysis. these techniques are classified into two classes: attacks without reference device (e.g. differential power analysis, correlation power analysis) and attacks using a reference device (e.g. template attack, stochastic model attack). in this paper, we present the attacks with an easy comprehensible way and focus on their implementation aspect. the pros and cons of each attack is highlighted in details with concrete electromagnetic signals. at least, our paper proposes also some solutions to enhance the existing attacks.an overview of side channel analysis attacks','Denial-of-service attacks'
'in this paper we show how to simulate attacks on authentication protocols in a realistic environment. the attack on the needham-schroeder public key protocol found by gavin lowe is run inside a malicious agent communicating with normal agents. the simulator is also able to detect flaws in attacks: a previously unreported error on an attack on the shamir rivest adelman is detected by simulation.attacks are protocols too','Denial-of-service attacks'
'cryptographic algorithms, which withstand cryptanalysis after years of rigorous theoretical study and detailed scrutiny have been shown to succumb to attacks that exploit the vulnerabilities in their implementations. therefore, there has been a vast amount of research effort to find potential vulnerabilities in the implementation of cryptographic algorithms, and efficient and effective countermeasures if such vulnerabilities exist. in this paper, we survey side-channel and fault attacks, which are two powerful methods that have been demonstrated to render many implementations effectively broken. while we categorically analyze the attack techniques, possible countermeasures will also be discussed.attacks on implementations of cryptographic algorithms','Denial-of-service attacks'
'attacks on the birational permutation signature schemes','Denial-of-service attacks'
'attacks on threshold signature schemes with traceable signers','Denial-of-service attacks'
'as the privacy concerns related to information release in social networks become a more mainstream concern, data owners will need to utilize a variety of privacy-preserving methods of examining this data. here, we propose a method of data generalization that applies to social networks and present some initial findings for the utility/privacy tradeoff required for its use.automatic sanitization of social network data to prevent inference attacks','Denial-of-service attacks'
'network intrusion detection systems (nids) have become crucial to securing modern networks. to be effective, a nids must be able to counter evasion attempts and operate at or near wire-speed. failure to do so allows malicious packets to slip through a nids undetected. in this paper, we explore nids evasion through algorithmic complexity attacks. we present a highly effective attack against the snort nids, and we provide a practical algorithmic solution that successfully thwarts the attack. this attack exploits the behavior of rule matching, yielding inspection times that are up to 1.5 million times slower than that of benign packets. our analysis shows that this attack is applicable to many rules in snort\'s ruleset, rendering vulnerable the thousands of networks protected by it. our countermeasure confines the inspection time to within one order of magnitude of benign packets. experimental results using a live system show that an attacker needs only 4.0 kbps of bandwidth to perpetually disable an unmodified nids, whereas all intrusions are detected when our countermeasure is used.backtracking algorithmic complexity attacks against a nids','Denial-of-service attacks'
'published attacks against smartphones have concentrated on software running on the application processor. with numerous countermeasures like aslr, dep and code signing being deployed by operating system vendors, practical exploitation of memory corruptions on this processor has become a time-consuming endeavor. at the same time, the cellular baseband stack of most smart-phones runs on a separate processor and is significantly less hardened, if at all. in this paper we demonstrate the risk of remotely exploitable memory corruptions in cellular baseband stacks. we analyze two widely deployed baseband stacks and give exemplary cases of memory corruptions that can be leveraged to inject and execute arbitrary code on the baseband processor. the vulnerabilities can be triggered over the air interface using a rogue gsm base station, for instance using openbts together with a usrp software defined radio.baseband attacks','Denial-of-service attacks'
'in this paper we present a new kind of cryptanalytic attack which utilizes bugs in the hardware implementation of computer instructions. the best known example of such a bug is the intel division bug, which resulted in slightly inaccurate results for extremely rare inputs. whereas in most applications such bugs can be viewed as a minor nuisance, we show that in the case of rsa (even when protected by oaep), pohlig-hellman, elliptic curve cryptography, and several other schemes, such bugs can be a security disaster: decrypting ciphertexts on &lt;em&gt;any&lt;/em&gt;computer which multiplies &lt;em&gt;even one pair of numbers&lt;/em&gt;incorrectly can lead to full leakage of the secret key, sometimes with a single well-chosen ciphertext.bug attacks','Denial-of-service attacks'
'chosen-key attacks on a block cipher','Denial-of-service attacks'
'this article is the first in a 2-part series on the vulnerabilities of networks to client-side attacks, and the risk to networks from \'inside-out\' compromises. internet and computer usage is exponentially increasing; more people are beginning to use the internet as part of their everyday lives, these activities ranging from instant messaging to ordering shopping online. even most large corporate businesses provide their employees with some kind of access to the internet.client attacks','Denial-of-service attacks'
'how do competitors react to each other\'s price-promotion and advertising attacks? what are the reasons for the observed reaction behavior? we answer these questions by performing a large-scale empirical study on the short-run and long-run reactions to promotion and advertising shocks in over 400 consumer product categories over a four-year time span.our results clearly show that the most predominant form of competitive response is passive in nature. when a reaction does occur, it is usually retaliatory in the same instrument, i.e., promotion attacks are countered with promotions, and advertising attacks are countered with advertising. there are very few long-run consequences of any type of reaction behavior. by linking reaction behavior to both cross- and own-effectiveness, we further demonstrate that passive behavior is often a sound strategy, while firms that do opt to retaliate often use ineffective instruments, resulting in \"spoiled arms.\" accommodating behavior is observed in only a minority of cases, and often results in a missed sales opportunity when promotional support is reduced. the ultimate impact of most promotion and advertising campaigns depends primarily on the nature of consumer response, not the vigilance of competitors.competitive reactions to advertising and promotion attacks','Denial-of-service attacks'
'just as errors in sequential programs can lead to security exploits, errors in concurrent programs can lead to concurrency attacks. questions such as whether these attacks are feasible and what characteristics they have remain largely unknown. in this paper, we present a preliminary study of concurrency attacks and the security implications of real world concurrency errors. our study yields several interesting findings. for instance, we observe that the exploitability of a concurrency error depends on the duration of the timing window within which the error may occur. we further observe that attackers can increase this window through carefully crafted inputs. we also find that four out of five commonly used sequential defenses become unsafe when applied to concurrent programs. based on our findings, we propose new defense directions and fixes to existing defenses.concurrency attacks','Denial-of-service attacks'
'design of combiners to prevent divide and conquer attacks','Denial-of-service attacks'
'surveys of businesses and other organizations that rely on the internet for their communications show that around 83\% of inbound email traffic is either spam, or other types of illegitimate messages (1). together these are known as \'\'dark traffic\'\'.detecting attacks','Denial-of-service attacks'
'this paper addresses the issue of detecting and isolating topology attacks in power networks. a topology attack, unlike a data attack and power injection attack, alters the physical dynamics of the power network by removing bus interconnections. these attacks can manifest as both cyber and physical attacks. a physical topology attack occurs when a bus interconnection is physically broken, while a cyber topology attack occurs when incorrect information about the network topology is transmitted to the system estimator and incorporated as the truth. to detect topology attacks, a stochastic hypothesis testing problem is considered assuming noisy measurements are obtained by periodically sampling a dynamic process described by the networked swing equation dynamics, modified to assume stochastic power injections. a centralized approach to network topology detection and isolation is introduced as a two-part scheme consisting of topology detection followed by topology isolation, assuming a topology attack exists. to address the complexity issues arising with performing centralized detection in large-scale power networks, a decentralized approach is presented that uses only local measurements to detect the presence of a topology attack. simulation results illustrate that both the centralized and decentralized approaches accurately detect and isolate topology attacks.distributed detection and isolation of topology attacks in power networks','Denial-of-service attacks'
'in audio watermarking, the robustness against pitch-scaling attack, is one of the most challenging problems. in this paper, we propose an algorithm, based on traditional time-spread(ts) echo hiding based audio watermarking to solve this problem. in ts echo hiding based watermarking, pitch-scaling attack shifts the location of pseudonoise (pn) sequence which appears in the cepstrum domain. thus, position of the peak, which occurs after correlating with pn-sequence changes by an un-known amount and that causes the error. in the proposed scheme, we replace pn-sequence with unit-sample sequence and modify the decoding algorithm in such a way it will not depend on a particular point in cepstrum domain for extraction of watermark. moreover proposed algorithm is applied to stereo audio signals to further improve the robustness. experimental results illustrate the effectiveness of the proposed algorithm against pitch-scaling attacks compared to existing methods. in addition to that proposed algorithm also gives better robustness against other conventional signal processing attacks.echo hiding based stereo audio watermarking against pitch-scaling attacks','Denial-of-service attacks'
'sidejacking occurs when an attacker intercepts a session cookie and uses it to impersonate a user and gain unauthorized access to a web-based service. to prevent sidejacking, a server should enable https and configure all session cookies to only be transmitted over a secure link. many websites do not do this, however, and the user may be unaware. in this work we present a firefox extension that will allow users to quickly and easily determine whether the server they are visiting is susceptible to sidejacking attacks.empowering users against sidejacking attacks','Denial-of-service attacks'
'fast correlation attacks on certain stream ciphers','Denial-of-service attacks'
'in a fault attack, errors are induced during the computation of a cryptographic algorithm, and the faulty results are exploited to extract information about the secret key in embedded systems. fault attacks can break an unprotected system more quickly than any other kind of side-channel attack. furthermore, protecting against fault attacks is more costly in terms of chip area. this sidebar surveys fault injection methods, types of faults, and fault attack models.faults, injection methods, and fault attacks','Denial-of-service attacks'
'a dummy traffic strategy is described that can be implemented by mix nodes in an anonymous communication network to detect and counter active (n - 1) attacks and their variants. heartbeat messages are sent anonymously from the mix node back to itself in order to establish its state of connectivity with the rest of the network. in case the mix is under attack, the flow of heartbeat messages is interrupted and the mix takes measures to preserve the quality of the anonymity it provides by introducing decoy messages.heartbeat traffic to counter (n-1) attacks','Denial-of-service attacks'
'in recent years, researchers have proposed systems for running trusted code on an untrusted operating system. protection mechanisms deployed by such systems keep a malicious kernel from directly manipulating a trusted application\'s state. under such systems, the application and kernel are, conceptually, peers, and the system call api defines an rpc interface between them. we introduce iago attacks, attacks that a malicious kernel can mount in this model. we show how a carefully chosen sequence of integer return values to linux system calls can lead a supposedly protected process to act against its interests, and even to undertake arbitrary computation at the malicious kernel\'s behest. iago attacks are evidence that protecting applications from malicious kernels is more difficult than previously realized.iago attacks','Denial-of-service attacks'
'hirose and yoshida proposed an authenticated key agreement protocol based on the intractability of the computational diffie-hellman problem. recently, hirose and matsuura pointed out that hirose and yoshida\'s protocol is vulnerable to denial-of-service (dos) attacks. and they proposed two key agreement protocols which are resistant to the dos attacks. their protocols are the first authenticated key agreement protocols resistant to both the storage exhaustion attack and the cpu exhaustion attack. in this paper we show that hirose and matsuura\'s dos-resistant key agreement protocols and hirose and yoshida\'s key agreement protocol are vulnerable to impersonation attacks. we make suggestions for improvements.impersonation attacks on key agreement protocols resistant to denial of service attacks','Denial-of-service attacks'
'at eurocrypt\'05, wang et al. presented efficient collision attacks on md5 and md4 hash functions. they found a collision of md5 with a complexity of less than 237 md5 hash operations, and a collision of md4 with complexity less than 28 md4 hash operations. in their attack, the procedure to generate a collision is divided into 4 steps. first, they determine the message differential and output differentials of chaining variables in each step, which generates a collision with small complexity. second, they construct sufficient conditions that guarantee that the desired differential is always calculated. third, they find a message modification that can satisfy the sufficient conditions with high probability. finally, they search for a message that satisfies all sufficient conditions. in this paper, we focus on the message modification of md5 and md4, and propose a new message modification. using our message modification, a collision of md5 can be found with complexity less than 229 md5 hash operations, and a collision of md4 can be found with complexity less than 3 md4 hash operations. to improve the complexity from previous attacks, we mainly use two ideas. the first idea is to use message modification that can satisfy more sufficient conditions in the second round than in previous attacks. the second idea is to use message modification that can enable us to search for a collision starting from an intermediate step.improved collision attacks on md4 and md5','Denial-of-service attacks'
'a common misconception concerning network security is that the infrastructure is at considerable risk from external attackers. although there is always the opportunity that an enterprise environment may be targeted by a skilled, and above all, patient external attacker, enterprises face a significant challenge in identifying and reacting to, insider attacks. this article focuses on the range of insider based attacks that the enterprise environment may be vulnerable to, as well as methods of responding to an attack and mitigating the risks in the first instance.insider attacks','Denial-of-service attacks'
'we describe a lattice attack on the digital signature algorithm (dsa) when used to sign many messages, m_i, under the assumption that a proportion of the bits of each of the associated ephemeral keys, y_i, can be recovered by alternative techniques.lattice attacks on digital signature schemes','Denial-of-service attacks'
'jaques erasmus from anti-malware company prevx looks at the way malware authors and distributors get their end product to the victim\'s machine. he explores the routes these coders take and how they achieve the most effective distribution of their newly built malware. in doing so, he touches on a myriad of elements, from malware sdks, to exploitation packs and loaders. pulling examples of real world attacks into the article he looks at some trends that are becoming prevalent, and outlines what we can expect for 2009. since the first pc virus called brain appeared in january 1986, technologies available to aid the creation and monetisation of malware have undergone tremendous development. we have seen the rise and fall of a variety of different underground groups and are currently fighting a new breed of malware authors and distributors that have honed their tools to monetise as many victims as efficiently as possible. in this article we will focus on the tools that are keeping this criminal ecosystem alive.malware attacks','Denial-of-service attacks'
'mars attacks!','Denial-of-service attacks'
'information security is vital to many multiagent system applications. in this paper we formalise the notion of detectability of attacks in a mas setting and analyse its applicability. we introduce a taxonomy of detectability specifications expressed in temporal-epistemic logic. we illustrate the practical relevance of attack detectability in a case study applied to a variant of kerberos protocol. we model-check attack detectability in automatically generated mas models for security protocols.model checking detectability of attacks in multiagent systems','Denial-of-service attacks'
'software security is becoming a key quality concern as software applications are increasingly being used in untrustworthy computing environments such as the internet. software is designed with the mindset of its functionalities and cost, where the focus is on the operational behavior while security concerns are neglected or marginally considered. as a result, software engineers build the software while lacking the knowledge about security and its effect on the system. this paper presents an approach for modeling the behavior of security threats using statecharts. the proposed approach introduces modular design for representing threats through the use of components and reusability. through the focus on the behavior of an attack, software engineers can clearly define and understand security concerns as the application is being designed and developed. in addition, modeling security threats with statecharts makes it convenient to build a consistent semantic link between functional behaviors and security concerns.modeling security attacks with statecharts','Denial-of-service attacks'
'mozilla attacks with open-source','Denial-of-service attacks'
'back in 2007, hasegawa discovered a novel cross-site scripting (xss) vector based on the mistreatment of the backtick character in a single browser implementation. this initially looked like an implementation error that could easily be fixed. instead, as this paper shows, it was the first example of a new class of xss vectors, the class of mutation-based xss (mxss) vectors, which may occur in innerhtml and related properties. mxss affects all three major browser families: ie, firefox, and chrome. we were able to place stored mxss vectors in high-profile applications like yahoo! mail, rediff mail, openexchange, zimbra, roundcube, and several commercial products. mxss vectors bypassed widely deployed server-side xss protection techniques (like html purifier, kses, htmllawed, blueprint and google caja), client-side filters (xss auditor, ie xss filter), web application firewall (waf) systems, as well as intrusion detection and intrusion prevention systems (ids/ips). we describe a scenario in which seemingly immune entities are being rendered prone to an attack based on the behavior of an involved party, in our case the browser. moreover, it proves very difficult to mitigate these attacks: in browser implementations, mxss is closely related to performance enhancements applied to the html code before rendering; in server side filters, strict filter rules would break many web applications since the mxss vectors presented in this paper are harmless when sent to the browser. this paper introduces and discusses a set of seven different subclasses of mxss attacks, among which only one was previously known. the work evaluates the attack surface, showcases examples of vulnerable high-profile applications, and provides a set of practicable and low-overhead solutions to defend against these kinds of attacks.mxss attacks','Denial-of-service attacks'
'in this paper we study the influence of key scheduling algorithms on the strength of blockciphers. we show that the key scheduling algorithms of many blockciphers inherit obvious relationships between keys, and use these key relations to attack the blockciphers. two new types of attacks are described: new chosen plaintext reductions of the complexity of exhaustive search attacks (and the faster variants based on complementation properties), and new low-complexity chosen key attacks. these attacks are independent of the number of rounds of the cryptosystems and of the details of the f-function and may have very small complexities. these attacks show that the key scheduling algorithm should be carefully designed and that its structure should not be too simple. these attacks are applicable to both variants of loki and to lucifer. des is not vulnerable to the related keys attacks since the shift pattern in the key scheduling algorithm is not the same in all the rounds.new types of cryptanalytic attacks using related keys','Denial-of-service attacks'
'in this paper, we introduce a new class of side--channel attackscalled partitioning attacks. we have successfully launched a versionof the attack on several implementations of comp128, the popular gsmauthentication algorithm that has been deployed by different serviceproviders in several types of sim cards, to retrieve the 128 bit keyusing as few as 8 chosen plaintexts. we show how partitioning attackscan be used effectively to attack implementations that have beenequipped with ad hoc and inadequate countermeasures againstside--channel attacks. such ad hoc countermeasures are systemic inimplementations of cryptographic algorithms, such as comp128, whichrequire the use of large tables since there has been a mistaken beliefthat sound countermeasures require more resources than areavailable. to address this problem, we describe a newresource--efficient countermeasure for protecting table lookups incryptographic implementations and justify its correctness rigorously.partitioning attacks','Denial-of-service attacks'
'if an email that we receive appears actually to have been sent by our bank, we are less likely to question its authenticity, says dario forte, but we may still be the target of a phishing attack, whose objective is to trick us into revealing sensitive information. in the second part of a two-part article, he analyses the mechanics of a phishing attack as used in everyday cybercrime, and shows just how devious the proponents of this mature but still-effective method can be in their tactics. in the first of this two-part series, published last month, we explained the initial process of how phishers prepare the ground for their attacks. as stated in that article, phishing attacks can be subdivided into three phases: *creation of a bogus web site that mimics the web site of the bank that is the target of the attack. *uploading of the page onto one\'s own site or else the compromising of an existing site. *mass emailing to lure the unwary to the bogus site.  the combination of these three elements allows an attacker to carry out an attack. the success of the attack depends on many factors, such as the credibility of the site, the contents of the email message, and the final user\'s critical analysis capacity and it proficiency. this article will go into more depth on these issues.phishing attacks','Denial-of-service attacks'
'because of their shorter key sizes, cryptosystems based on elliptic curves are being increasingly used in practical applications. a special class of elliptic curves, namely, koblitz curves, offers an additional, but crucial, advantage of considerably reduced processing time. in this article, power analysis attacks are applied to cryptosystems that use scalar multiplication on koblitz curves. both the simple and the differential power analysis attacks are considered and a number of countermeasures are suggested. while the proposed countermeasures against the simple power analysis attacks rely on making the power consumption for the elliptic curve scalar multiplication independent of the secret key, those for the differential power analysis attacks depend on randomizing the secret key prior to each execution of the scalar multiplication. these countermeasures are computationally efficient and suitable for hardware implementation.power analysis attacks and algorithmic approaches to their countermeasures for koblitz curve cryptosystems','Denial-of-service attacks'
'one of the biggest challenges of designers of cryptographic devices is to protect the devices against implementation attacks. power analysis attacks are among the strongest of these attacks. this article provides an overview of power analysis attacks and discusses countermeasures against them. in particular, this article summarizes recent results with countermeasures that can be implemented at the cell level. many countermeasures of this kind have been proposed, but several limitations of these countermeasures have been identified.power analysis attacks and countermeasures','Denial-of-service attacks'
'at isc 2001 a method for securing elliptic curve point multiplication against side-channel attacks has been proposed by m&#246;ller [lecture notes in comput. sci., vol. 2200, springer-verlag, berlin, 2001, pp. 324-334]. we show that this method does not offer acceptable security. namely, differential and simple power analysis techniques may reveal the complete secret key.power attacks on a side-channel resistant elliptic curve implementation','Denial-of-service attacks'
'cryptography has become an indispensable mechanism for securing systems, communications and applications. while offering strong protection, cryptography makes the assumption that cryptographic keys are kept absolutely secret. in general this assumption is very difficult to guarantee in real life because computers may be compromised relatively easily. in this paper we investigate a class of attacks, which exploit memory disclosure vulnerabilities to expose cryptographic keys. we demonstrate that the threat is real by formulating an attack that exposed the private key of an openssh server within 1 minute, and exposed the private key of an apache http server within 5 minutes. we propose a set of techniques to address such attacks. experimental results show that our techniques are efficient (i.e., imposing no performance penalty) and effective -- unless a large portion of allocated memory is disclosed.protecting cryptographic keys from memory disclosure attacks','Denial-of-service attacks'
'in this paper, we propose a formal model of coordinated attacks in which several attackers cooperate towards a common malicious goal. the model investigates both attack planning and vulnerability analysis, thereby providing a uniform approach to system and adversary modelling. in addition, the model is general enough to explain both coordinated and single attacks. in the paper, we define the notion of coordinated-attack graph, propose an algorithm for efficient generation of coordinated-attack graphs, demonstrate how coordinated-attack can be used for vulnerability analysis, and discuss an implementation of a coordinated-attack graph. coordinated-attack graphs can facilitate a wide range of tasks, such as model checking, opponent modelling, intrusion response, sensor configuration, and so forth. in addition, they can be used in robotic warfare, where several intelligent software agents automatically produce and launch coordinated attacks.representation and analysis of coordinated attacks','Denial-of-service attacks'
'in 1996, jakobsson, sako, and impagliazzo and, on the other hand, chaum introduced the notion of designated verifier signatures to solve some of the intrinsic problems of undeniable signatures. the generalization of this concept was formally investigated by laguillaumie and vergnaud as multi-designated verifiers signatures. recently, laguillaumie and vergnaud proposed the first multi-designated verifiers signature scheme which protects the anonymity of signers without encryption. in this paper, we show that their scheme is insecure against rogue-key attacks.rogue-key attacks on the multi-designated verifiers signature scheme','Denial-of-service attacks'
'due to their high practical impact, cross-site scripting (xss) attacks have attracted a lot of attention from the security community members. in the same way, a plethora of more or less effective defense techniques have been proposed, addressing the causes and effects of xss vulnerabilities. noscript, and disabling scripting code in non-browser applications such as e-mail clients or instant messengers. as a result, an adversary often can no longer inject or even execute arbitrary scripting code in several real-life scenarios. in this paper, we examine the attack surface that remains after xss and similar scripting attacks are supposedly mitigated by preventing an attacker from executing javascript code. we address the question of whether an attacker really needs javascript or similar functionality to perform attacks aiming for information theft. the surprising result is that an attacker can also abuse cascading style sheets (css) in combination with other web techniques like plain html, inactive svg images or font files. through several case studies, we introduce the so called scriptless attacks and demonstrate that an adversary might not need to execute code to preserve his ability to extract sensitive information from well protected websites. more precisely, we show that an attacker can use seemingly benign features to build side channel attacks that measure and exfiltrate almost arbitrary data displayed on a given website. we conclude this paper with a discussion of potential mitigation techniques against this class of attacks. in addition, we have implemented a browser patch that enables a website to make a vital determination as to being loaded in a detached view or pop-up window. this approach proves useful for prevention of certain types of attacks we here discuss.scriptless attacks','Denial-of-service attacks'
'the automobile industry has grown to become an integral part of our everyday life. as vehicles evolve, the primarily mechanical solutions for vehicle control are gradually replaced by electronics and software solutions forming in-vehicle computer networks. an emerging trend is to introduce wireless technology in the vehicle domain by attaching a wireless gateway to the in-vehicle network. by allowing wireless communication, real-time information exchange between vehicles and between infrastructure and vehicles become reality. this communication allows for road condition reporting, decision making, and remote diagnostics and firmware updates over-the-air. however, allowing external parties wireless access to the in-vehicle network creates a potential entry-point for cyber attacks. in this paper, we investigate the security issues of allowing external wireless communication. we use a defense-in-depth perspective and discuss security challenges for each of the prevention, detection, deflection, countermeasures, and recovery layers.securing vehicles against cyber attacks','Denial-of-service attacks'
'contemporary malware makes extensive use of different techniques such as packing, code obfuscation, polymorphism, and metamorphism, to evade signature-based detection. traditional signature-based detection technique is hard to catch up with latest malware or unknown malware. behavior-based detection models are being investigated as a new methodology to defeat malware. this kind of approaches typically relies on system call sequences/graphs to model a malicious specification/pattern. in this paper, we present a new class of attacks, namely \"shadow attacks\", to evade current behavior-based malware detectors by partitioning one piece of malware into multiple \"shadow processes\". none of the shadow processes contains a recognizable malicious behavior specification known to single-process-based malware detectors, yet those shadow processes as an ensemble can still fulfill the original malicious functionality. to demonstrate the feasibility of this attack, we have developed a compiler-level prototype tool, autoshadow, to automatically generate shadow-process version of malware given the source code of original malware. our preliminary result has demonstrated the effectiveness of shadow attacks in evading several behavior-based malware analysis/detection solutions in real world. with the increasing adoption of multi-core computers and multi-process programs, malware writers may exploit more such shadow attacks in the future. we hope our preliminary study can foster more discussion and research to improve current generation of behavior-based malware detectors to address this great potential threat before it becomes a security problem of the epidemic proportions.shadow attacks','Denial-of-service attacks'
'sir: a hierarchy of heart attacks','Denial-of-service attacks'
'it is a general belief among the designers of block-ciphers that even a relatively weak cipher may become very strong if its number of rounds is made very large. in this paper we describe a new generic known- (or sometimes chosen-) plaintext attack on product ciphers, which we call the slide attack and which in many cases is independent of the number of rounds of a cipher. we illustrate the power of this new tool by giving practical attacks on several recently designed ciphers: treyfer, wake-rofb, and variants of des and blowfish.slide attacks','Denial-of-service attacks'
'in this paper, we divide steganographic attacking process into two phases: reasoning phase and attacking phase. the attacks of each phase are divided into several kinds. using the random oracle mechanism, we propose the definition of symmetric steganography secure against chosen message and original cover attacks. this is a kind of provable security. at last, we give a theoretical steganographic scheme and prove that it is secure against chosen message and original cover attacks.symmetric steganography secure against chosen message and original cover attacks','Denial-of-service attacks'
'confining a program during its execution so that it can\'t leak information to other programs is an old concern. recently, several researchers succeeded in fingerprinting distant machines by measuring temperature side effects on clocks. but can temperature also leak secrets in a computer or a chip? we started by implementing a covert channel between two processes (a sender and a receiver) running on the same machine. producing heat is simple: all the sender must do is launch massive calculations. to sense temperature in the machine, we considered three options: fan-based solutions, built-in sensors; and faults as heat detectors.temperature attacks','Denial-of-service attacks'
'we present template attacks, the strongest form of side channel attack possible in an information theoretic sense. these attacks can break implementations and countermeasures whose security is dependent on the assumption that an adversary cannot obtain more than one or a limited number of side channel samples. they require that an adversary has access to an identical experimental device that he can program to his choosing. the success of these attacks in such constraining situations is due manner in which noise within each sample is handled. in contrast to previous approaches which viewed noise as a hindrance that had to be reduced or eliminated, our approach focuses on precisely modeling noise, and using this to fully extract information present in a single sample. we describe in detail how an implementation of rc4, not amenable to techniques such as spa and dpa, can easily be broken using template attacks with a single sample. other applications include attacks on certain des implementations which use dpa-resistant hardware and certain ssl accelerators which can be attacked by monitoring electromagnetic emanations from an rsa operation even from distances of fifteen feet.template attacks','Denial-of-service attacks'
'many electoral bribery, control, and manipulation problems (which we will refer to in general as \"manipulative actions\" problems) are np-hard in the general case. it has recently been noted that many of these problems fall into polynomial time if the electorate is single-peaked (i.e., is polarized along some axis/issue). however, real-world electorates are not truly single-peaked. there are usually some mavericks, and so real-world electorates tend to merely be nearly single-peaked. this paper studies the complexity of manipulative-action algorithms for elections over nearly single-peaked electorates, for various notions of nearness and various election systems. we provide instances where even one maverick jumps the manipulative-action complexity up to np-hardness, but we also provide many instances where a reasonable number of mavericks can be tolerated without increasing the manipulative-action complexity.the complexity of manipulative attacks in nearly single-peaked electorates','Denial-of-service attacks'
'security protocols are indispensable in secure communication. we give an operational semantics of security protocols in terms of a prolog-like language. with this semantics, we can uncover attacks on a security protocol that are possible with no more than a given number of rounds. though our approach is exhaustive testing, the majority of fruitless search is cut off by selecting a small number of representative values that could be sent by an attacker. hence, the number of scenarios is relatively small and our method is quite practical. furthermore, our method not only reports possible attacks but also describes the attacks in great detail. this description would be very helpful to protocol designers and analyzers.uncovering attacks on security protocols','Denial-of-service attacks'
'the vulnerability and importance of computers, robots, internet etc, demand the employment of exceedingly reliable methods in the design of secure systems. security protocols are one of the most important design parameters. history has proven security protocols to be vulnerable even after they enjoyed circumspect design and meticulous review by experts. we posit that understanding the subtle issues in security protocols is important when designing a protocol. in particular, understanding a penetrator and the knowledge of different attack strategies that a penetrator can apply are among the most important issues that affect the design of security protocols. we describe the notion of a penetrator and specify his characteristics. our purpose is to emphasize the design criteria of an authentication protocol through the use of some nice and subtle attacks that existed in the literature in the field of the design of security protocols.understanding the intruder through attacks on cryptographic protocols','Denial-of-service attacks'
'ad hoc low-power wireless networks are an exciting research direction in sensing and pervasive computing. prior security work in this area has focused primarily on denial of communication at the routing or medium access control levels. this paper explores resource depletion attacks at the routing protocol layer, which permanently disable networks by quickly draining nodes\' battery power. these \"vampire&#8221; attacks are not specific to any specific protocol, but rather rely on the properties of many popular classes of routing protocols. we find that all examined protocols are susceptible to vampire attacks, which are devastating, difficult to detect, and are easy to carry out using as few as one malicious insider sending only protocol-compliant messages. in the worst case, a single vampire can increase network-wide energy usage by a factor of o(n), where n in the number of network nodes. we discuss methods to mitigate these types of attacks, including a new proof-of-concept protocol that provably bounds the damage caused by vampires during the packet forwarding phase.vampire attacks','Denial-of-service attacks'
'there are many factors that can dictate the success of a piece of malware. these include how and to whom it is delivered, how it is executed, how rapidly it propagates and how successfully it evades detection. the first two of these describe the process of threat delivery and execution, which are perhaps the most influential factors in the success of a threat. traditionally cybercriminals have used email as their preferred vector of attack, employing various social engineering tactics in order to entice the recipient into executing the malicious attachment. as companies have become more aggressive in blocking email content, criminals have shifted their attentions, and are now firmly focused on the web.web attacks','Denial-of-service attacks'
'this paper proves that several interactive proof systems are zero-knowledge against general quantum attacks. this includes the well-known goldreich-micali-wigderson classical zero-knowledge protocols for graph isomorphism and graph 3-coloring (assuming the existence of quantum computationally concealing commitment schemes in the second case). also included is a quantum interactive protocol for a complete problem for the complexity class of problems having \"honest verifier\" quantum statistical zero-knowledge proofs, which therefore establishes that honest verifier and general quantum statistical zero-knowledge are equal: qszk = qszkhv. previously no non-trivial proof systems were known to be zero-knowledge against quantum attacks, except in restricted settings such as the honest-verifier and common reference string models. this paper therefore establishes for the first time that true zero-knowledge is indeed possible in the presence of quantum information and computation.zero-knowledge against quantum attacks','Denial-of-service attacks'
'we report preliminary results of prestoprime, an eu fp7 integrated project, including audiovisual (av) archives, academics and industrial partners, focused on long-term digital preservation of av media objects and on improving access by integrating media archives with european on-line digital libraries, specifically europeana. project outcomes will result in tools and services to ensure the permanence of digital av content in archives, libraries, museums and other collections, enabling long-term future access in dynamically changing contexts. prestoprime has a special focus on digital preservation in broadcast environments, where very large files of digital video must be preserved at high quality (suitable for future re-use in an av production environment) in affordable distributed and federated archives. the adoption of standard solutions for digital preservation processes (metadata representation, content storage, digital rights governance, search and access) enables the interoperability of the proposed preservation framework and guidelines. the oais model was chosen for the reference architecture, mets is adopted as wrapper for metadata representation, while relevant standards (e.g. from w3c, iso/iec and others) are used for content and rights description. project outcomes will be delivered through a european networked competence centre, to gather knowledge and deliver advanced digital preservation advice and services in conjunction with europeana and other initiatives.100 million hours of audiovisual content','Digital rights management'
'contract and rights management - and thusproperty rights protection - has gained increasingimportance as a quality standard in brokerage andelectronic commerce environments. contract andrights management provides information on thelegal relationships associated with digital assets, aswell as intellectual property rights protection andthe enforcement of rights. however, many brokerageand e-commerce platforms currently in operationwere not originally designed to support contract andrights management. in this context, we identify openissues in digital contract and rights managementand present a framework design to resolve theseissues. this framework uses standardized xml-basedrights expression languages, reuses anexisting role-based access control component forrights enforcement, and is extensible with value-addedservice components for rights management.the reference project for our work is a(heterogeneous) p2p network of interactingbrokerage platforms for learning resources.a contract and rights management framework design for interacting brokers','Digital rights management'
'a database management system for interlibrary loan','Digital rights management'
'this paper presents a digital rights management approach for gray-level images. the ownership of the original image is identified with an ownership statement, which is a gray-level image as well. the proposed scheme utilizes block truncation coding (btc) to create a master share, which is then used to produce an ownership share against the ownership statement. when in doubt about the property of an image, the author should address his/her ownership share to reveal the ownership statement to claim the ownership. since our method does not embed the ownership statement into the host image, we can register more than one ownership statements for a single image without destroying the former ownership statements. besides, the original image does not need to involve in the process of identifying the ownership. finally, experimental results will show the robustness of our scheme against several common attacks.a digital rights management approach for gray-level images','Digital rights management'
'p2p content sharing is often blamed for copyright infringement, making the establishment of drm technologies an urgent need. a pdrm (p2p- based digital rights management) system is proposed with the support of next generation internet project. the system is based on a trust model that focuses on content security, rights management and access control. encryption, digital watermarking, and packaging technologies are adopted to protect the confidentiality and integrity of contents, and support copyrights verifying and piracy tracing. the structure of rights management integrates the distributed and centralized modes, which not only reduces the burdens of networks and rights server, but also provides controllability. the contents downloaded on the p2p networks can be played only with rights control. to realize access control, the password and identity authentication are used. the pdrm system is implemented to prove that it can provide a more robust intellectual property protection solution for p2p content delivery.a digital rights management architecture for multimedia in p2p','Digital rights management'
'electronic healthcare records promise to increase the efficiency and effectiveness of healthcare systems, but also introduce new risks to the security and privacy of healthcare information. in this paper, we outline how digital rights management can be used to protect health information transmitted throughout a distributed healthcare system. our proposal allows for information to be disclosed on a need-to-know basis as defined by workflows, and in line with the wishes of patients.a digital rights management model for healthcare','Digital rights management'
'there is a need to protect digital information content and the associated usage rights from unauthorized access, use, and dissemination. the protection mechanisms should meet the requirements for the correct management of fine-grained access and usage controls and the protection of user privacy. digital rights management (drm) solutions have significant relevance in this context. this paper describes a distributed drm model for a secure information-distribution system consisting of six trust-building blocks. these are (i) the user application, (ii) the authentication and authorization module, (iii) rights-carrying and self-enforcing objects (seos), (iv) the privacy enforcement module, (v) theusage tracking and monitoring proxy (utmp), and (vi) thesecurity infrastructure. seos are information objects that carry access and usage rights and are responsible for the fine-grained enforcement of these rights. the security infrastructure plays a pivotal role in the creation, distribution, storage, manipulation, and communication of information objects across organizational boundaries with the required level of security. our model was originally developed for an internet-based learning project in norwegian schools and meets most of the aforementioned requirements.a distributed digital rights management model for secure information-distribution systems','Digital rights management'
'the ever-growing volumes of textual information from various sources have fostered the development of digital libraries, making digital content readily accessible but also easy for malicious users to plagiarize, thus giving rise to &#60;i>security&#60;/i> problems. in this paper, we introduce a &#60;i>duplicate detection&#60;/i> scheme that is able to determine, with a particularly high accuracy, the degree to which one document is similar to another. our pairwise document comparison scheme detects the resemblance between the content of documents by considering document chunks, representing contexts of words selected from the text. the resulting duplicate detection technique presents a good level of security in the protection of intellectual property while improving the availability of the data stored in the digital library and the correctness of the search results. finally, the paper addresses efficiency and scalability issues by introducing new data reduction techniques. a document comparison scheme for secure duplicate detection','Digital rights management'
'drm(digital rights management) has no well-known standards about encryption algorithms. this paper proposes a novel fcsr (feedback with carry shift registers) based keystrem cipher generator for drm. the novel generator has passed the nist statistical test, which shows the pseudo-random properties of the novel keystream generator are ideal. we simulate the generator using vhdl under quartus ii. simulation shows that the proposed generator has better performance under the same target device.a hardware-oriented fast encryption keystream generator for digital rights management','Digital rights management'
'digital rights management (drm) technology aims to distribute digital contents to consumers in a controlled manner that can protect the copyright of digital contents. in order to meet consumer&#8217;s expectations, drm approaches and related services and applications upon which they are based should coexist in an environment that supports interoperability. in this paper, we propose a digital rights management system architecture based on chinadrm metadata schema; the approach supports import and export different types of rights license, and also provides the rights transfer capability, so that it can achieve a certain level of interoperability which is proved by a practical distance education project.a metadata-based interoperable digital rights management system architecture','Digital rights management'
'while digital rights and policy management (drm / dpm) technologies have matured to become mainstream they are now poised to thrive in all aspects of our daily electronic lives and digital assets. a key issue remains however unsolved with respect to managing exceptions in the context of drm enabled systems and information assets. this paper addresses this issue defining the problem and proposing a model based on credentials allowing to dynamically account for lawful unanticipated usage situations while still maintaining a given level of persistent protection, governed usage and audit trails.a model for credential based exception management in digital rights management systems','Digital rights management'
'a patent search and classification system','Digital rights management'
'as a general protection measure for copyright violations through digital technologies including peer to peer (p2p), copyright owners often uses digital rights management (drm) techniques to encrypt and watermark content or otherwise restrict access, totally blocking digital content to be accessed through the internet and the p2p infrastructure. this paper claims that drm and p2p can be quite complementary. specifically, a p2p infrastructure is presented which allows broad digital content exchange while on the same time supports copyright protection and management through drm technologies.a peer to peer network environment for optimized digital rights management of digital cultural heritage','Digital rights management'
'in this paper we first propose an i-frame encryption scheme for encryption of moving image video data. second, we propose a licensing agent which provides automatic user authentication and data decoding when multimedia data encrypted in the system server is executed in the client system by the user. the licensing agent performs user authentication based on public key infrastructure (pki) using a shared key pool and encryption/decryption of multimedia data. after designing and implementing the proposed system, performance tests were then performed using video data files of various sizes for performance evaluation. we verified that the proposed system significantly reduces delay time, including decryption time, when playing back video data files in the client system compared with existing systems.a pki based digital rights management system for safe playback','Digital rights management'
'this article presents a platform designed for context-aware and digital rights management-enabled content adaptation based on mpeg-21\'s digital item adaptation scheme and a unique profiling approach.a platform for context-aware and digital rights management-enabled content adaptation','Digital rights management'
'this paper proposes a new drm system for 3d graphics that makes use of biometric watermarking technology. the presented solution utilizes an image of a biometric trait e.g. face or fingerprint, and embeds it into the 3d graphics as a watermark. this biometric watermark is then used to authenticate a legitimate user. details for the components of the drm framework are presented. adoption of biometric watermarking allows the drm system to provide consumers unrestricted access to the graphics along with limiting graphics content access to only legitimate users, thereby protecting artists from large scale online piracy. a detailed survey of existing drm solutions for 3d graphics is provided to identify the limitations of each implementation which offers either restrictive content usage scenarios or is ineffective in preventing unauthorized usage.a proposed digital rights management system for 3d graphics using biometric watermarks','Digital rights management'
'a patient-centric drm approach is proposed for protecting privacy of health records stored in a cloud storage based on the patient\'s preferences and without the need to trust the service provider. contrary to the current server-side access control solutions, this approach protects the privacy of records from the service provider, and also controls the usage of data after it is released to an authorized user.a rights management approach to protection of privacy in a cloud of electronic health records','Digital rights management'
'as network capacity has increased over the past decade, individuals and organisations have found it increasingly appealling to make use of remote services in the form of service-oriented architectures and cloud computing services. data processed by remote services, however, is no longer under the direct control of the individual or organisation that provided the data, leaving data owners at risk of data theft or misuse. this paper describes a model by which data owners can control the distribution and use of their data throughout a dynamic coalition of service providers using digital rights management technology. our model allows a data owner to establish the trustworthiness of every member of a coalition employed to process data, and to communicate a machine-enforceable usage policy to every such member.a rights management approach to securing data distribution in coalitions','Digital rights management'
'a robust and flexible digital rights management system for home networks is presented. in the proposed system, the central authority delegates its authorization right to the local manager in a home network by issuing a proxy certificate, and the local manager flexibly controls the access rights of home devices on digital contents with its proxy certificate. furthermore, the proposed system provides a temporary accessing facility for external devices and achieves strong privacy for home devices. for the validation of delegated rights and the revocation of compromised local managers, a hybrid mechanism combining ocsp validation and periodic renewal of proxy certificates is also presented.a robust and flexible digital rights management system for home networks','Digital rights management'
'this paper presents a novel concept of a secure digital camera (sdc) with a built-in watermarking and encryption facility. the motivation is to facilitate real-time digital rights management (drm) by using the sdc right at the source end of multimedia content. the emerging field of drm systems addresses the issues related to the intellectual-property rights of digital content. the use of digital watermarking along with encryption for effective drm is proposed. in this context, a novel discrete cosine transform domain invisible-robust watermarking method that uses cryptography and watermarking methods simultaneously to provide a double-layer of protection to digital media is presented. the proposed method securely hides binary images in color image media and securely extracts and authenticates it by using a secret key. experimental results prove that the proposed technique is resilient to stringent watermarking attacks. hence, it is an effective method for providing protection of ownership rights. the corresponding application-specific architectures for invisible-robust watermarking and rijndael advanced encryption standard (aes) towards the prototyping of the sdc are presented. the proposed architectures are modeled and synthesized for field programmable gate array (fpga). the soft cores in the form of hardware description language resulting from this research can serve as intellectual-property core and can be integrated with any multimedia-producing electronic appliances which are built as embedded systems using system-on-a-chip (soc) technology.a secure digital camera architecture for integrated real-time digital rights management','Digital rights management'
'a shelf-management model implemented on multiplan','Digital rights management'
'the term digital rights management (drm) generally refers to a set of policies and techniques that guide the proper use of digital content. due to the nature of digital content, it is easily modified and distributed. drm is already progressive in many industries and has received a fair amount of attention in recent years. to prevent multimedia content from being usurped, there is much research which focuses on how to build a reliable drm system. in this paper, we propose a drm model to provide improvements for several existing research issues. we use a \"ticket\" to achieve anonymous consumption and design a protocol to protect against malicious servers. a solution is proposed which addresses these issues and brings more functionality to users.a ticket based digital rights management model','Digital rights management'
'current studies on digital rights management (drm) have focused on security and encryption as a means of solving the issue of illegal copying by purchasers. in this paper, we propose an end-to-end key management scheme that can cover a content protection on the overall value-chains of content distribution. the proposed scheme can protect digital content from attacks since an encrypted content is sent by a first package server and only drm client can decrypt the encrypted digital content. it makes it possible to protect content from content creator to purchaser.a trustworthy end-to-end key management scheme for digital rights management','Digital rights management'
'in order to improve the management of copyright in the internet, known as digital rights management, there is the need for a shared language for copyright representation. current approaches are based on purely syntactic solutions, i.e. a grammar that defines a rights expression language. these languages are difficult to put into practise due to the lack of explicit semantics that facilitate its implementation. moreover, they are simple from the legal point of view because they are intended just to model the usage licenses granted by content providers to end-users. thus, they ignore the copyright framework that lies behind and the whole value chain from creators to end-users. our proposal is to use a semantic approach based on semantic web ontologies. we detail the development of a copyright ontology in order to put this approach into practice. it models the copyright core concepts for creation, rights and the basic kinds of actions that operate on content. altogether, it allows building a copyright framework for the complete value chain. the set of actions operating on content are our smaller building blocks in order to cope with the complexity of copyright value chains and statements and, at the same time, guarantee a high level of interoperability and evolvability. the resulting copyright modelling framework is flexible and complete enough to model many copyright scenarios, not just those related to the economic exploitation of content. the ontology also includes moral rights, so it is possible to model this kind of situations as it is shown in the included example model for a withdrawal scenario. finally, the ontology design and the selection of tools result in a straightforward implementation. description logic reasoners are used for license checking and retrieval. rights are modelled as classes of actions, action patterns are modelled also as classes and the same is done for concrete actions. then, to check if some right or license grants an action is reduced to check for class subsumption, which is a direct functionality of these reasoners.a web ontologies framework for digital rights management','Digital rights management'
'the problem addressed in this paper is that of drm interoperability. the term drm interoperability, as used here, refers to approaches that provide for the transfer, from one \"upstream\" drm system to another \"downstream\" drm system, of drm protected content and an associated license. we introduce the concept of a rights issuer module (rim) that is functionally situated in the home network between the upstream device (which includes an upstream-drm agent) and downstream devices (which each include a downstream-drm agent). the novelty of our approach lies in the way the rim handles significant aspects of the translation operations, potentially leaving the upstream and downstream drm agents intact. security and implementation advantages of the rim are discussed. the tradeoffs involved with different rim implementations are examined.achieving media portability through local content translation and end-to-end rights management','Digital rights management'
'in most current digital rights management solutions, digital license is bound to the content rendering device using its hardware configuration. however, this strategy introduces an adaptive problem: protected contents can&#39;t be used any longer once the user changes the hardware configuration. this paper focuses on the problem and presents a feasible adaptive approach. with our approach, digital content buyers can still smoothly render the bought digital content without any extra operation when partial hardware replacement arises. our approach balances the needs of customers&#39; hardware alteration and providers&#39; copyright protection on a reasonable level and increases the flexibility of drm system.an adaptive approach to hardware alteration for digital rights management','Digital rights management'
'intellectual property (ip) reuse is essential for meeting the challenges of system-on-a-chip (soc) design productivity improvement, design quality and meeting time-to-market goals. recent trend in the design of complex soc is doing a joint development with the customer, where it is required to integrate some of their ips. in such a scenario, the usual paradigm followed for reuse has to be enhanced beyond the state-of-the-art to meet the design goals. this paper describes the reuse framework that has been successfully applied during such a joint development program. the methodology consists of imposing a specified degree of compliance for internal checklists comprising of code quality, design quality, verification quality, and testability checks, and aligning on the goals for design verification and test coverage. customized enhancements to the ips to meet the soc design goals are presented.an effective framework for enabling the reuse of external soft ip','Digital rights management'
'wireless multimedia streaming applications have been widely propagated with the rapid evolution of mobile handsets and wireless internet technologies, especially mpeg-4 video streams over wireless communication networks. we propose an efficient methodology for multimedia drm which is applicable to the mobile terminal software. the methodology is comprised of the encryption methods on the media data that are encoded with mpeg-4 video format. we constructed 3 methods, one is the way we can encrypt macroblocks (mbs) of i-vop (video object plane), another is p-vop\'s mbs encryption and the other is the combination of these two methods. they are designed to drm services for a wireless video streaming system without performance degradation that could be a burden to a mobile handset, where there is inherently preferred system that costs for low-power, low-network bandwidth. we report competitiveness of these methods on the perspective of performance evaluation with respect to the plain mpeg-4 files.an efficient methodology for multimedia digital rights management on mobile handset','Digital rights management'
'software engineering projects result in experiences that are valuable for continuous improvement. experience and knowledge management (ekm) deals with the proper presentation, engineering, and reuse of experiences, e.g. training new project members or supporting future projects. in globally distributed projects proper ekm is even more important: communication between project partners is more difficult than in co-located projects and may impair the awareness of knowledge residing at a project partner\'s location. project members might hesitate to share experience because of security considerations. we propose a hierarchical experience base with rights management aiming to positively influence their willingness to share. our concept includes special support for experience engineers to refine local experiences into best practices in globally distributed software projects. in this paper we show how our approach can rise awareness of existing experiences by presenting relevant experiences according to roles. we also argue, how this improves the willingness to share experiences in a distributed environment.an experience base with rights management for global software engineering','Digital rights management'
'rddonto provides an ontological approach to the rights data dictionary (rdd) part of mpeg-21one of the main intellectual property rights (ipr) management standardisation efforts. in order to build the ontology, the terms defined in the rdd specification have been modelled using owl, trying to capture the greatest part of its semantics. the ontology allows formalising a great part of the standard and simplifying its verification, consistency checking and implementation. during the rddonto construction, some integrity problems were detected, which even have led to standard corrigenda. additional checks were possible using description logic reasoning in order to test the standard consistency. moreover, rddonto is now helping on how new terms can be added to the rdd and to integrate the rdd with other parts of mpeg-21 also mapped to owl. finally, there are the implementation facilities provided by the ontology. they have been used to develop mpeg-21 licenses searching, validation and checking. existing ontology-enabled tools as semantic query engines or logic reasoners facilitate this.an ontological approach for the management of rights data dictionaries','Digital rights management'
'digitalisation and the internet have caused a content reproduction and distribution revolution with clear implications for copyright management there are many digital rights management (drm) efforts that facilitate copyright management in closed domains but they find great difficulties when they are forced to interoperate in an open domain like the world wide web in order to facilitate interoperation and automation, drm systems can be enriched with domain formalisations like the copyright ontology this ontology is implemented using the description logic variant of the web ontology language (owl-dl) this approach facilitates the implementation of efficient usages against licenses checking, which is reduced to description logics classification.an owl copyright ontology for semantic digital rights management','Digital rights management'
'applications that incorporate digital rights management (drm) capabilities are enabled to specify, implement and manage the rights and permissions associated with the use of intangible goods. many different inter-related technologies can be incorporated into drm enabled applications including technology that incorporates public key cryptography. the success of drm enabled applications will depend on how well the solutions satisfy requirements of the different stakeholders involved with the production, distribution and use of intangible goods. a key success factor is the ability of an application to provide superior ease of use from the end user\'s perspective. the success also depends on how well the application can adapt to new technology and emerging distribution and business models. this paper describes drm applications, the requirements of the different stakeholders in this environment and critical attributes of public key cryptosystems that must be considered to ensure effective solutions.applicability of public key cryptosystems to digital rights management applications','Digital rights management'
'the communication between accounting information (ai) producers (namely, company or enterprise) and consumers (stakeholder, revenue, auditor, and etc.) is more convenient than before, due to the benefits of xbrl implementation; however it is difficult to apply xbrl into more in-depth and daily accounting applications, since xbrl doesn\'t have a concrete access control mechanism to describe different ai access permissions for different types of ai consumers. concretely speaking, accounting information processing is different from general information processing, in that, ai producers have different obligations to different types of ai consumers; and vise versa, different ai consumers have different access rights to information from producers. therefore, in this paper, we introduce digital rights management (drm) idea and mechanism into xbrl implementation. so we can use the benefits of xbrl to publish ai for ai producers and ai consumers more conveniently. moreover, by introducing the drm mechanism, ai consumers can obtain abundant ai from daily accounting records (namely, the daily transaction actions recodes from ai data base) according to accounting principles.application of digital rights management to accounting information processing in the semantic web environment','Digital rights management'
'content rights are considered one of the most imperative elements of complexity in content processes. these processes include the construction of the content, its selection for commercial publication, its production, diffusion, and marketing, as well as itseventual use. in addition, the processes of tracking digital rights and managing access to content based rights information are gradually more necessary. the most powerful technique to control and track such access is to combine forms and meanings to achieve persistent protection. in this paper, after a quick explanation of content management systems and digital rights management, we address some legal and market issues that have led to complex content processes and systems. we also discuss some ways in which retailers of content management systems may incorporate digital rights into their products by means of rights expression languages.applying digital rights management to complex content management systems','Digital rights management'
'the digital content industry is facing significant challenges. one of the most significant challenges is the intellectual property protection. this challenge has been addressed technologically by using digital rights management (drm) systems that on a first stage ensured the appropriate management over digital content. however, rights management systems, as they are today, are completely non-interoperable, creating immense problems to the digital content final users. perhaps one of the biggest problems is the fact that the same digital content that is governed by different rights management systems cannot be exchanged between different rights governance systems. this paper presents some of the work that has been performed under the framework of the visnet-ii network of excellence (noe) to address some of the rights management systems interoperability problems. the approach followed by the proposed work consists on the usage of service description and service-oriented architectures as a mean to create a common and interoperable environment between the different systems.approaching the rights management interoperability problem using intelligent brokerage mechanisms','Digital rights management'
'this work describes a novel strategy for designing an xpath processor that acts over an rdf mapping of xml. we use a model-mapping approach to represent instances of xml and xml schema in rdf. this representation retains the node order, in contrast with the usual structure-mapping approach. the processor can be fed with an unlimited set of xml schemas and/or rdfs/owl ontologies. the queries are resolved taking into consideration the structural and semantic connections described in the schemas and ontologies. such behavior, schema-awareness and semantic integration, can be useful for exploiting schema and ontology hierarchies in xpath queries. we test our approach in the digital rights management (drm) domain. we explore how the processor can be used in the two main rights expression languages (rel),: mpeg-21 rel and odrl.architecture of a semantic xpath processor. application to digital rights management','Digital rights management'
'aspects of digital rights management and the use of hardware security devices','Digital rights management'
'since information is available in digital format, the protection of intellectual property and copyright fraud has become an important issue. this is, because the digital content can be copied without quality loss and with a reasonable effort of time, equipment and money. after copying, it can be distributed using the internet, again with little effort of time and money. in such an environment, the loss of revenue for the music and film industry -- not only due to sites like napster -- is becoming so tremendous, that mechanisms as described under the digital rights management become important. in the geospatial domain, spatial data infrastructures emerge that have the potential to provide high quality and up-to-date geographic information. this enables the endeavor of new market potentials and the creation of new business cases. however, the establishment of digital rights management for geographic information is important in the first place. this paper introduces requirements for geospatial digital rights management and illustrates the difference to known requirements for the music industry. the major contribution of this paper is the description of geospatial access control -- named geoxacml -- as it can possibly be a solution to the authorization requirement for digital rights management in the geospatial domain.authorization for digital rights management in the geospatial domain','Digital rights management'
'authorization management for digital libraries','Digital rights management'
'in today\'s digital age, e-commerce is emerging as an alternative and inexpensive solution to traditional selling of products through shops. sale of electronic data such as software, audio/video data, books etc. poses a problem as digital data can be easily replicated, modified and distributed. watermarks were introduced as a means to establish ownership of digital data and check unauthorized reuse and/or violation of copyrighted material. a good e-distribution system should be able to protect rights of both buyer and seller; ensure secure and confidential transactions; determine correct source of unauthorized copies. in this paper, we propose a framework for e-distribution of digital rights. this system is based on a trusted third party ca (certification authority) and uses pki (public key infrastructure). proposed system address problems of customer\'s check, copy detection, customers right,unbinding, non-repudiation and man in the middle attack.buyer seller watermarking protocol for digital rights management','Digital rights management'
'manufacturing variability is inherent to many silicon and nano-scale technologies and can be manifested in many different ways and modalities (e.g. power and delay). we propose a flow that starts with gate-level integrated circuit (ic) characterization which results in unique identification (id). the id\'s are an integrated part of the design functionality and software and provide a basis for conceptually new cad-based security protocols. as an examples, we present a new ic metering schemes that ensure very low overhead and digital right management in horizontally integrated ic market. therefore, after many years of cad importing and benefiting from many other areas such as numerical analysis, theoretical cs, vlsi design, computer architectures, and compilers, cad has its historical chance to impact many fields of computer science and engineering through manufacturing variability-based security and right management.cad-based security, cryptography, and digital rights management','Digital rights management'
'the business of content providers is being threatened by technology advances in hardware, software and ip-networks such as the internet or peer-to-peer file sharing systems. the result is an increasing amount of illegal copies available on-line as well as off-line.with the emergence of digital rights management systems (drms), the media and entertainment industry seems to have found the appropriate tool to simultaneously fight piracy and to monetize their assets. although these systems are very powerful and include multiple protection technologies, it is currently unknown to what extent such systems are used by content providers.this paper provides empirical results, analyses and conclusions related to digital rights management systems and the protection of digital content in the music, film, and print industries. it outlines the similarities and the differences of usages among the above mentioned industries. the paper concludes that each industry uses different protection technologies and that password and encryption are the most frequently used. the majority of the respondents are satisfied with their current protection but want to enforce it in the future due to fear of increasing piracy. the requirements for drms are perceived differently from industry to industry as is the average amount of investment. furthermore, approximately half of the respondents from the music industry do not believe in the ability of drms to reduce piracy, whereas respondents from the film and print industry believe that drms will be able to do so.comparing the usage of digital rights management systems in the music, film, and print industry','Digital rights management'
'conference on copyright and licensing of electronic content explores questions of rights','Digital rights management'
'in relation to the german trade ebook market, this paper presents results from an exploratory survey into ebooks consumers&#8217; attitudes to digital rights management (drm). thereby, this paper illuminates the effects of drm implementation on usability and the resulting marketability of ebooks. findings suggest that ebooks are perceived as being generally useful, although there is a considerable difference in the evaluation of free (unprotected) and purchasable (protected) ebooks.consumers\' attitudes to digital rights management (drm) in the german trade ebook market','Digital rights management'
'with the spreading of internet and wide use of computer, electronic publishing is becoming an indispensable measure to gain knowledge and skills. meanwhile, copyright is facing much more infringement than ever in this electronic environment. so, it is a key factor to effectively protect copyright of electronic publishing to foster the new publication fields. the paper analyzes the importance of copyright, main causes for copyright infringement in electronic publishing, gives out our viewpoints on the definition and application of fair use of a copyrighted work and thinking of some means to combat breach of copyright.copyright of electronic publishing','Digital rights management'
'although digital rights management (drm) has been proven effective and successful in protecting the confidentiality of sensitive documents by providing access control, drm products have not been widely adopted and used to their potential. one reason for this could be that cost and benefit of these products have not been analyzed in a systematic and quantitative manner to date. as a result, companies do not have an established procedure to evaluate the cost and benefit of implementing these products. in this document, the benefits of implementing drm products in enterprises are quantified using stochastic petri-net models and are compared with the security needs of a corporation and potential costs incurred by the implementation process. an evaluating procedure for implementing drm products is established. this procedure has the potential to be used to improve the ability of a corporation to make sensible security investment decisions. the implementation of ms irm (microsoft information rights management), one of the drm products, was studied as a type case. in this case study, the ms irm system was analyzed; a group of security metrics were developed for measuring and evaluating the effectiveness of the ms irm system, in terms of increased security provided. stochastic models are a core part of the process. it was found that the business process is a critical factor in determining document security. although drm products improve security, they typically increase the cost to the company and potentially reduce the productivity of staff. therefore, for a successful deployment of the drm system, it is recommended that a company evaluate the benefit and cost of drm systems quantitatively using the procedures described in this document.cost-benefit analysis of digital rights management products using stochastic models','Digital rights management'
'digital music consumers have to choose between illegal file swapping services and online music stores. the latter impose various restrictions to the established music consumption behaviour, such as limitations on the number of devices and proprietary music formats. we describe a business model that is based on a liberal management of music rights, instead of the dominant restrictions of access. the proposed business model facilitates the free flow of music content between different client devices (pc, mobile phone, portable player) and between heterogeneous networks (web, p2p, wireless, broadcast), but it controls the flow of rights for added value music bundles. the business model is presented over two stages of the customer activity cycle and along the revenue, process and technology elements.cross media digital rights management for online music stores','Digital rights management'
'as the capacity of fpga\'s increases to millions of equivalent gates the use of intellectual property (ip) cores becomes increasingly important to control design complexity. fpga\'s are becoming platforms for integrating a system solution from components supplied by independent vendors in the same way as printed circuit boards provided a platform for earlier generations of designers. however, the current commercial model for ip cores involves large up-front license fees reminiscent of asic nre charges. in order to match the ip core business model to the low to medium volume applications addressed by fpga customers it is important to develop cryptographic techniques which allow ip core vendors to sell their product on a pay-per-use basis rather than through up-front license fees.cryptographic rights management of fpga intellectual property cores','Digital rights management'
'digital rights management (drm) is an issue of controlling and managing digital rights over intellectual property. current research on the drm domain focuses on specifying requirements for management systems from variety of perspectives, however, mainly concentrating on protecting rights in business to consumer trade with certain mechanisms. this article takes a novel viewpoint in evaluating drm issues from the perspective of networked business operations. in contrast to recent studies on drm, the article evaluates the creation and delivery of content in along an asset creation-delivery continuum in terms of role specifications, content processes and management of digital rights over content, i.e. assets, in formations of firms. our interest in this conceptual study is on the issues of networked businessmodels and, consequently, we contribute in defining and analyzing the complexities of such scenarios and from case scenario deriving emerging requirements for digital rights management in the introduced context. while simultaneously providing certain advantages toenterprises, the inter-organizational interdependencies in networked business model are prone to cause conflicts, thus, creating an increased need for control, contracting and coordination activities. we conclude that in order to build trust between various actors, multiple agreementsdescribing rights and obligations of operation and over assets needs to be negotiated and signed, thus, establishing a requirement for utilization of standardized digital rights expressions and efficient agreement creation and management.current and emerging requirements for digital rights management systems through examination of business networks','Digital rights management'
'database creation and management at glasgow college','Digital rights management'
'database management software for library lists','Digital rights management'
'detroit area library network','Digital rights management'
'digital document delivery and digital rights management','Digital rights management'
'digital rights management - dealmaker for e-business?','Digital rights management'
'digital rights management makes it easier for vendors to practice price discrimination online. public resistance is perhaps the main reason it is not prevalent in online information sales.digital rights management and individualized pricing','Digital rights management'
'e-commerce has become a huge business and a driving factor in the development of the internet. online shopping services are well established and will, with the advent of evolved 2g and 3g mobile networks, soon be complemented by their wireless counterparts. furthermore, online delivery of digital media, such as mp3 audio or video, is very popular today and will become an increasingly important part of e-commerce and mobile e-commerce (m-commerce). however, a major obstacle for digital media distribution and associated business is the possibility of unlimited consecutive copying in the digital domain, which threatens intellectual property rights (e.g., copyrights). digital rights management systems are required to protect rights and business. drm systems typically incorporate encryption, conditional access, copy control mechanisms, and media identification and tracing mechanisms. watermarking is the technology used for copy control and media identification and tracing. most proposed watermarking methods use a so-called spread spectrum approach: a pseudo-noise signal with small amplitude is added to the host signal, and later on detected using correlation methods. a secret key is used to ensure that the watermark can only be detected and removed by authorized parties. thus, watermarking is an essential component of modern drm systems. several standardization bodies are involved in drm standardization. some examples, (mpeg-4, sdmi, and dvd), are discussed in this article. watermarking as an enabling technology is especially highlighted. furthermore, the relation between drm and m-commerce, and the impact on business models for m-commerce are discussed. a common experience today is that internet e-commerce applications cannot always easily be adapted for mobile telecommunications systems. we emphasize, however, that drm and watermarking can benefit from the additional information available in mobile telecommunications systems, and can thus help to improve rights management for digital media delivery.digital rights management and watermarking of multimedia content for m-commerce applications','Digital rights management'
'digital rights management (drm) is increasingly becoming a necessity for content management and distribution in highly networked environments such as the internet. however, very few drm models have been able to achieve commercial success and acceptance among users. this paper analyzes the problems with current drm environments and proposes an open layered framework for development of drm systems, where different technologies can interoperate within the framework. furthermore, interoperability is studied in terms of the proposed layered framework, and problems posed by the current rights expression languages (rels) are identified. we conclude that a refactoring of current rels based on a set of design principles is necessary to achieve a reasonable level of drm interoperability. we emphasize the need for middleware services for drm, along with their responsibilities and places of operation within the proposed framework. finally, a specific prototype architecture is introduced that makes use of existing infrastructures in order to implement a drm environment consistent with the design principles described in this paper.digital rights management architectures','Digital rights management'
'transferring the traditional business model for selling digital goods linked to physical media to the online world leads to the need for a system to protect digital intellectual property. digital rights management(drm) is a system to protect high-value digital assets and control the distribution and usage of those digital assets. this paper presents a review of the current state of drm, focusing on security technologies, underlying legal implications and main obstacles to drm deployment with the aim of providing a better understanding of what is currently happening to content management on a legal and technological basis and well prepared for grasping future prospects.digital rights management for content distribution','Digital rights management'
'in this paper, the business model and digital rights management (drm) solution are proposed for the home tv based on scalable video coding and convergent networks. in this environment, the tv program is broadcasted via digital video broadcasting for handheld terminals (dvb-h) to mobile phones, the access right is transmitted by global system for mobile communication (gsm/gprs) channel, and the tv program can also be transmitted from mobile phones to home tv through wifi. the typical application scenario is that the tv program with low quality (base layer) in the mobile phone can be enriched by enhancement information and shown on home tv. the business model is presented to pay for the enhancement information, and the drm solution is proposed for solving the copyright issues in the convergence between the dvb-h broadcasting, gsm/gprs and home network. since few work has been done to solve this problem, the work proposed in this paper is expected to attract more researchers.digital rights management for the home tv based on scalable video coding','Digital rights management'
'digital rights management for the mobile internet','Digital rights management'
'digital rights management has become a pressing concern for the online music business. existing digital rights management systems are backed by two license management models, the tethered model and the untethered model. these two license management models focus on the management of payments and usage rights. the problems with these models are that the tethered model forces consumers to be online, while the untethered model provides relatively less security to the license residing locally. this paper proposes an enhanced license management model for the online music business, which integrates both models together and thus enables online and offline purchasing.digital rights management for the online music business','Digital rights management'
'video sensor network is evolving from an isolated system to an integral component of the global information infrastructure. in this paper, we argue that when video sensor network becomes a public information source on the internet, drm (digital rights management) must be enforced, due to the sensitivity and the privacy natures of sensor content. moreover, existing drm solutions do not suffice because the explicit one-to-one mapping between content producer and consumer does not apply in the sensor network domain. we propose a drm-enabled content service architecture for video sensor network. within this architecture, we propose a binary-tree-based hierarchical key generation scheme for data encryption, and adopt a label-guided watermarking strategy to address the unique challenges of video sensor content. we present the evaluation results of our solution based on a preliminary video sensor testbed system.digital rights management for video sensor network','Digital rights management'
'the internet has revolutionized multimedia content distribution, shifting the way content producers and users approach digital rights. however, ubiquitous computing will alter the digital rights management environment even more, and current techniques are ill equipped to deal with the changes. distributed trust can help to overcome the challenge of maintaining digital rights for ubiquitous computing, using cellular automata to measure trust levels across a system.digital rights management in ubiquitous computing','Digital rights management'
'digital rights management on open and semi-open networks','Digital rights management'
'this paper proposes a drm system based on pkcs#12 to meet the requirement of security and flexibility in digital media application. it designs the system architecture and the security protocol of user registration, certificate issuing, encrypted digital content distribution, authorized license delivery, authentication and decryption, etc. with the security feature of pkcs#12 and the designed protocol, the proposed system can ensure the security of certificate and private key during the storage and transfer. and this system supports participation through different devices, can prevent digital rights from illegal sharing.digital rights management system based on pkcs#12','Digital rights management'
'this paper focuses on the problem of preventing the illegal copying of digital content whilst allowing content mobility within a single user domain. this paper proposes a novel solution for binding a domain to a single owner. domain owners are authenticated using two-factor authentication, which involves \"something the domain owner has\", i.e. a master control device that controls and manages consumers domains, and binds devices joining a domain to itself, and \"something the domain owner is or knows\", i.e. a biometric or password/pin authentication mechanism that is implemented by the master control device. these measures establish a one-to-many relationship between the master control device and domain devices, and a one-to-one relationship between domain owners and their master control devices, ensuring that a single consumer owns each domain. this stops illicit content proliferation. finally, the pros and cons of two possible approaches to user authentication, i.e. the use of a password/pin and biometric authentication mechanisms, and possible countermeasures to the identified vulnerabilities are discussed.digital rights management using a master control device','Digital rights management'
'this paper focuses on the problem of preventing illegal copying of digital assets without jeopardising the right of legitimate licence holders to transfer content between their own devices, which make up a domain. our novel idea involves the use of a domain-specific mobile phone and the mobile phone network operator to authenticate the domain owner before devices can join a domain. this binds devices in a domain to a single owner, that, in turn, enables the binding of domain licences to the domain owner. in addition, the way in which we control domain membership, and the use of the domain-specific mobile phone that enables a domain owner to add devices wherever he/she is physically present, ensures that devices joining the domain are in physical proximity to the mobile phone, preventing illicit content proliferation.digital rights management using a mobile phone','Digital rights management'
'the digital watermark technology and the mobile agent technology play significant roles in the industrial applications in tandem with each other. in recent years, the union of the mobile agent technology and the watermark technology has been intensively investigated. a new architecture is proposed using this integration technology to protect copyright by detecting the watermark on the internet. this architecture comprises four integral blocks: author block (ab), user block (ub), watermark agent block (wab) and mobile agent block (mab). in ab, author embeds his ownership watermark in his work and then registers the same in copyright management server (cms). then cms embeds the second watermark in the digital work produced by watermark agent. this is actually a hidden agent. in ub, user submits detail information about personality and digital work to cms. cms gives user the digital work and user key according to the input information. wab creates the custom-built watermark agents and designs the route strategy. in mab, the watermark agents are dispatched to the suspicious hosts by wab and are examined for their copyright.digital rights management using mobile agents','Digital rights management'
'the purpose of digital rights management (drm) is to protect the copyrights of content providers and to enable only designated user to access digital contents. for a user to share the contents among all his devices in the home network, several domain-based approaches that group multiple devices into a domain have been proposed. in these approaches, however, each device in a domain has equivalent rights on all contents although certain contents require an access control between the devices. in this paper, a new drm system for home networks is presented. this system enables access control on the contents by a right delegation strategy with proxy certificates. moreover, it also provides additional functionalities, including restricted sharing and temporal sharing of contents, which are necessary for ordinary scenarios in home networks.digital rights management with right delegation for home networks','Digital rights management'
'digital rights management technologies in the field of copyright protection should meet four objectives: &#8226; give consumers new freedom to enjoy music and other forms of content; &#8226; give copyright owners and other value chain participants the means to manage and protect their rights in published works; &#8226; implement elements of law, such as copyright exceptions, that ensure that rights are managed in accordance with the public interest; &#8226; provide users with the means to manage their legitimate personal rights and interests.we sketch how these goals can be achieved with current technology for peer-to-peer digital rights management (drm). this technology can ensure the neutrality, security, commercial reliability, and trusted interoperability of applications and services used to protect and manage rights in all forms of information, including creative works protected by copyright. the rapidly evolving area of digital commerce in information requires a framework of commercial trust comparable in scope, and at least as reliable, as the systems of trust that underpin commerce in the physical world.digital rights management, copyright, and napster','Digital rights management'
'in october 2005, sysinternals\' mark russinovich discovered a rootkit on his computer, which he later determined stemmed from a sony-bmg compact disc. in this article, the authors examine the copy-protection software found on those discs and the implications for digital rights management.digital rights management, spyware, and security','Digital rights management'
'digital rights management','Digital rights management'
'this paper introduces a suitable way for indexing multimedia metadata on a structured peer-to-peer overlay network, with special care to the management of rights metadata expressed by mpeg-21. we have selected a suitable subset of mpeg-21 rights expression language elements to be indexed, in order to map governed contents into a &#64258;at space and allow insertion and retrieval of digital contents. furthermore, we present a distributed application built on a structured overlay network enabling the search of multimedia items using rights related information. our solution is completely decentralized and can be exploited in any mpeg-21 compliant metadata representation.digital rights metadata management and retrieval on structured overlay networks','Digital rights management'
'distributed digital-ticket management for rights trading system','Digital rights management'
'fingerprinting embeds a secret message into a cover message. in media fingerprinting, the secret is usually a copyright notice and the cover a digital image. fingerprinting an object discourages intellectual property theft, or when such theft has occurred, allows us to prove ownership. the software fingerprinting problem can be described as follows. embed a structure w into a program p such that: w can be reliably located and extracted from p even after p has been subjected to code transformations such as translation, optimization and obfuscation; w is stealthy; w has a high data rate; embedding w into p does not adversely affect the performance of p; and w has a mathematical property that allows us to argue that its presence in p is the result of deliberate actions. in this article, we describe a software fingerprinting technique in which a dynamic graph fingerprint is stored in the execution state of a program. because of the hardness of pointer alias analysis such fingerprints are difficult to attack automatically.dynamic graph-based software fingerprinting','Digital rights management'
'dynamic rights','Digital rights management'
'a usual way for content protection of digital libraries is to use digital watermarks and a drm-based access-control environment. these methods, however, have limitations. digital watermarks embedded in digital content could be removed by malicious users via post-processing, whereas drm-based access-control solutions could be hacked. in this paper, we introduce a content tracking mechanism that we have built for multimedia-content near-replica detection as the second line of defense. the integrated framework aims to detect unlawful copyright infringements on the internet, and combines the strengths of static rights enforcement and dynamic illegal content tracking. the issues of accuracy and huge computation cost in copy detection have been addressed by the introduced content-based techniques. our experiments demonstrate the efficacy of proposed copy detector.effective content tracking for digital rights management in digital libraries','Digital rights management'
'the paper examines the idea of copyright and how it functions for both digital and non-digital publications. various different interpretations of copyright and its application are discussed. ideas such as databases, fair use and exceptions are explored in their relationship to technological measures used to control the use of copyright material. examples from the cited, copysmart, imprimatur, and copicat projects of the european union are described briefly. the impact of the latest eu directive on copyright and the information society is explained and the need for co-operative planning and implementation of technical measures throughout the information industry is emphasised.electronic information management and intellectual property rights','Digital rights management'
'today, digital content is routinely distributed over the internet, and consumed in devices based on open platforms. however, on open platforms users can run exploits, reconfigure the underlying operating system or simply mount replay attacks since the state of any (persistent) storage can easily be reset to some prior state. faced with this difficulty, existing approaches to digital rights management (drm) are mainly based on preventing the copying of protected content thus protecting the needs of content providers. these inflexible mechanisms are not tenable in the long term since their restrictiveness prevents reasonable usage scenarios, and even honest users may be tempted to circumvent drm systems. in this paper we present a security architecture and the corresponding reference implementation that enables the secure usage and transfer of stateful licenses (and content) on a virtualized open platform. our architecture allows for openness while protecting security objectives of both users (flexibility, fairer usage, and privacy) and content providers (license enforcement). in particular, it prevents replay attacks that is fundamental for secure management and distribution of stateful licenses. our main objective is to show the feasibility of secure and fairer distribution and sharing of content and rights among different devices. our implementation combines virtualization technology, a small security kernel, trusted computing functionality, and a legacy operating system (currently linux).enabling fairer digital rights management with trusted computing','Digital rights management'
'encoding the law into digital libraries','Digital rights management'
'group formation and access rights management become crucial issues when shared workspaces are used to support flexible, emerging group work. end-users should be able to form groups and adapt access rights for changing groups and workspaces. current shared workspace systems do not support this sufficiently. our approach combines a room metaphor-based shared workspace with the key-metaphor for facilitating both, end-user controlled flexible group formation and access rights management. an evaluation of this approach during four month of use has indicated that end-users can form groups and manage the access rights of their shared spaces.end-user controlled group formation and access rights management in a shared workspace system','Digital rights management'
'the protection of privacy has gained considerable attention recently. in response to this, new privacy protection systems are being introduced. sitdrmis one such system that protects private data through the enforcement of licenses provided by consumers. prior to supplying data, data owners are expected to construct a detailed license for the potential data users. a license specifies whom, under what conditions, may have what type of access to the protected data. the specification of a license by a data owner binds the enterprise data handling to the consumer\'s privacy preferences. however, licenses are very detailed, may reveal the internal structure of the enterprise and need to be kept synchronous with the enterprise privacy policy. to deal with this, we employ the platform for privacy preferences language (p3p) to communicate enterprise privacy policies to consumers and enable them to easily construct data licenses. a p3p policy is more abstract than a license, allows data owners to specify the purposes for which data are being collected and directly reflects the privacy policy of an enterprise.enforcing p3p policies using a digital rights management system','Digital rights management'
'in this paper, we describe an innovative approach for aligning the business layer and the application layer of archimate to ensure that applications manage access rights consistently with enterprise goals and risk tolerances. the alignment is realized by using the responsibility of the employees, which we model using remola. the main focus of the alignment targets the definition and the assignment of the access rights needed by the employees according to business specification. the approach is illustrated and validated with a case study in a municipal hospital in luxembourg.enhancing the archimate&#174; standard with a responsibility modeling language for access rights management','Digital rights management'
'considering pervasive computing environments and a global digital market having complex, often contradictory national and international regulations, it is impossible for rights holders to define universal digital rights management (drm) policies governing the usage of their content while still considering user rights. exceptions are unanticipated usage situations where some rights should be waived while still maintaining a given level of persistent protection and governed usage. the industry and traditional drm approaches haven&#8217;t considered such alternatives. to tackle this issue and demonstrate the feasibility of such an approach, this paper reports and discusses a proof of concept prototype implementation based on a model supporting exception management in drm environments using credentials.exception-aware digital rights management architecture experimentation','Digital rights management'
'content consumption in mobile devices is no longer limited to ringtones, games or images. the increasing capabilities of new mobile devices include now music and video clips. the content sent to the mobile device and even content created from devices has an increasing value. the application of security mechanisms and the introduction of digital rights management (drm) can give companies and final users a new way of interchanging content. nevertheless, it is not easy to provide applications that work on different mobile devices. the specific features of these devices (manufacturers, operating systems, programming environments, capabilities, etc.) make difficult to implement innovative applications for them, especially those related to drm, which usually involve several security aspects. different approaches have been taken to overcome these problems, which are presented here.experiencing digital rights management in mobile environments','Digital rights management'
'a wave of change is coming: the new computer.org will offer content that\'s compelling, relevant, interactive, and user-friendly to a new demographic of it professionals. but we need to ensure that it\'s still peer reviewed to maintain quality.fast-tracking content','Digital rights management'
'because of complexities of the structured finance, the securitization of the assets, distribution of the securities and the protection of the private information among transactions of secirities on it enable networks became serious problems. in this paper, we show that the information capsule for the copyright management could be used to the securitization and its concurrency. we propose the method of the securitization with the information capsule which includes mobile agents. by applying this method to the system that a investor could check the uncertainness of the own securities. for example which obligation is contained or how to estimate the value of the securities.financial securitization with digital rights management system','Digital rights management'
'the requirements for secure document workflows in enterprises become increasingly sophisticated, with employees performing different tasks under different roles using the same proprietary platform. particularly, fine-grained access control to document information is necessary in certain scenarios where the integrity and confidentiality of parts of documents is of high priority. in this paper, we present a secure and flexible enterprise rights management (erm) system based on a refined version of the trusted virtual domains (tvds) security model that allows to establish isolated execution environments spanning over virtual entities across separate physical resources. our security concept achieves a two-layered policy enforcement on documents: a tvd policy ensuring isolation of the workflow from other tasks on the user platforms, and a role-based document-policy ensuring both confidentiality and integrity of document parts. moreover, in contrast to existing solutions, our architecture offers advanced features for secure document workflows such as offline access to documents and transparent encryption of documents exchanged via usb, external storage or vpn communication between peer platforms. we also shed the light on key management, document structure and document policy enforcement mechanisms to support the erm infrastructure. finally, we prove our concept based on an implementation.flexible and secure enterprise rights management based on trusted virtual domains','Digital rights management'
'the development and wider use of wireless networks and mobile devices has led to novel pervasive computing environments which pose new problems for software rights management and enforcement on resource-constrained and occasionally connected devices. software vendors are, however, still applying old usage rights models to a platform where application rights will be specified, managed and distributed in new and different ways. the characteristics of pervasive environments, such as occasional connectivity, require the introduction of more flexible usage rights models, such as audit-based model, that do not assume the availability of network connections. in this paper we describe a pervasive application rights management architecture for both desktop and mobile applications that provides an integrated platform for the specification, generation, delivery, and management of application usage rights based on web services standards. we also introduce flexible usage rights models required by pervasive environments that can be embedded in target applications using aspect-oriented technology.flexible application rights management in a pervasive environment','Digital rights management'
'digital rights management (drm) promises to enable a secure electronic marketplace where content providers can be remunerated for the use of their digital content. in the last few years, countless research efforts have been devoted to drm technologies. however, drm systems are not only technological phenomena: they pose complex legal, business, organizational and economic problems. this article tries to show that from a lawyer\'s perspective some of the innovativeness and potential of drm can only be understood when one looks at it from a multidisciplinary viewpoint. the article gives an overview of the various ways by which digital content is protected in a drm system. the intertwining protection by technology, contracts, technology licenses and anti-circumvention regulations could lead to a new \"property right\" making copyright protection obsolete. however, there is a danger of over-protection: questions of fair use and other limitations to traditional copyright law have to be addressed. if competition is not able to solve this tension between the interests of content providers and the interests of users or the society at large -- which seems to be doubtful at least -- it is the law that has to provide a solution. the legislators in the u.s. and europe use different approaches to address this problem. by looking at drm in this way, several patterns can be observed which are characteristic of many areas of internet law.from copyright to information law - implications of digital rights management','Digital rights management'
'generally, drm(digital rights management) system is achieved with individual function modules of cryptography, watermarking and so on. in this typical system flow, all digital contents are temporarily disclosed with perfect condition via decryption process. this paper describes the fundamental idea of a novel drm method which is composed of an incomplete cryptography and user identification mechanism to control the quality of digital contents. our proposed prototype jpeg codec based on a simple incomplete cryptography can easily control the open level of contents, besides at decoding process, an identification code embedded with an authorized key is used to prevent unauthorized duplication or business of digital contents. experimental results with simulation confirmed that the modified codes keep compatibility with standard jpeg format, and revealed that the proposed method is suitable for drm in the network distribution system.fundamental incomplete cryptography method to digital rights management based on jpeg lossy compression','Digital rights management'
'music, books and video can be distributed very cost effectively over the internet to end consumers. as bandwidth capacity is growing and getting cheaper, the economics is so clearly on the side of digital distribution that distribution of digital goods on the internet will surely happen. digital rights management (drm) technology makes it possible to manage all the intellectual property aspects of electronic distribution and also the exchange of value for receiving digital goods. thus it is a key component of any electronic marketplace for information goods. in this paper i will point to some of the reasons that digital distribution (and thereby drm) will be successful in a mass market; point to some common misconceptions about drm; argue that we have most of the core technology for an attractive, yet still reasonably secure, drm system in place; and discuss how security and privacy features can and should be implemented.golden times for digital rights management?','Digital rights management'
'the paper starts with a description of the fundamental principles of modern digital rights management systems. this is the basis for the discussion of their most important security aspects from the provider&#39;s view on the one hand and the customer&#39;s view on the other hand. the second half of the paper focuses the new drm standard from the open mobile alliance (oma) and its implementation on &#8220;open&#8221; systems like windows. the security anchor of the oma drm is the device private key. as long as no trusted storage facilities for open systems work effectively, techniques for software obfuscation could be a solution. therefore the obfuscation of the device private key and its secure download is described. currently on windows pcs there is no chance for a full tamper-proof solution, but the authors try to make the job of an attacker as hard as possible, without affecting the consumer&#39;s security.how to increase the security of digital rights management systems without affecting consumer\'s security','Digital rights management'
'technologies that aim to protect digital content have fallen short of their mission. the computing community must find ways to make protection schemes interoperable and adopt a use model that lifts restrictions on paid-for protected content.how viable is digital rights management?','Digital rights management'
'with privacy enhancing identity management, end users are given better ways for managing their identities for specific contexts. one could easily argue that the need to implement identity management systems that are privacy enhancing follows from the eu data protection regulation. one of the challenges while developing privacy enhancing identity management is getting governments to become genuinely interested, both in their capacity of data processing organisation and legislator or policy maker. another challenge, this time for the private sector, is to find the right balance between data protection perfection and simplicity or users\' convenience, while developing privacy enhancing identity management systems. after a brief discussion of these challenges we discuss the growing human rights recognition of the value of digital identity and its management. in particular, the german constitutional court seems to pave the way for a basic right to have digital identity protected and secured.identity management of e-id, privacy and security in europe. a human rights view','Digital rights management'
'with the development of internet and smart phone, the access to multimedia content (ebook, music, movie, video etc.) has become easier and easier on the mobile phone. thanks to its good performance, android mobile phone has attracted more and more people since the introduction of android os by google. it\'s necessary to pay much attention to the security of digital content on the android mobile phone. in this paper, a enhanced drm system based on oma 2.1 was proposed, which can efficiently protect the digital content on the mobile phone.implementation of digital rights management on the android mobile terminal','Digital rights management'
'this paper will examine and categorize potential business model scenarios for online music. the virtualization of music leads to market uncertainties. on the supply side, the offering party might not be able to sufficiently privatize online music by using digital rights management technologies. on the demand side, with a changing cost structure for digital goods, consumers might not be willing to pay directly for digital goods so that revenues would have to be collected indirectly by public or private entities. as a result, business models for online music can be categorized into four scenarios. in the first scenario, online music is used to promote the traditional offline business while in the second scenario, consumers are willing to pay for additional services to access online music. the third scenario is significantly different from the first two scenarios as music providers are expected to be able to protect their content by using digital rights management technology. in the fourth scenario peer-to-peer technologies allow consumers to use a mechanism called super distribution with which they can share and recommend songs. the paper concludes with a recommendation to music companies regarding privacy and strategic positioning.implications of digital rights management for online music - a business perspective','Digital rights management'
'the security of content distribution is a critical issue that all the content providers have to treat seriously in order to guarantee their business. emmanuel and kankanhalli proposed an efficient video-broadcast scheme in [1] which is able to distribute video to many subscribers by multicasting one video copy only. the scheme is also designed to achieve the security such that the illegal distributors can be identified based on the client-side watermarking. in this paper, we present an attack which enables any eavesdropper to create a pirate video of good quality from the protected video. concretely, an adversary can estimate the secret interference watermark which is used to protect the distributed video by exploiting the statistics of randomly selected video frames/images. to fix the flaw, we propose two alternative countermeasures. one is to encrypt the protected video in transmission, and the other is to modify the interference signal frame by frame. both countermeasures can disable an eavesdropper to obtain a video of acceptable quality, but maintain the nice features that the original scheme is designed for, i.e., efficiently multicasting the protected video with only one channel; managing the broadcaster copyright; and preventing from framing innocent subscribers.improving a digital rights management scheme for video broadcast','Digital rights management'
'this paper describes the basic idea of a novel digital rights management(drm) method which is composed of an incomplete cryptography using invariant huffman code length feature and the user identification mechanism to control the quality of the digital contents. we adopt the huffman code length feature of the dct coefficient in the jpeg codec to the implement incomplete cryptography. the encoding process and the decoding process are presented by randomly selected the coefficients that belong to same category in the huffman table. in the our scheme, the copyright information is embedded into the decoded content while decoding process, and the size of digital contents are invariance during process. experimental results with simulation confirmed that the modified codes keep compatibility with standard jpeg format, and revealed the proposed method is suitable for drm in the network distribution system.incomplete cryptography method using invariant huffman code length to digital rights management','Digital rights management'
'independent revocation of access rights in database management systems','Digital rights management'
'information access company confirms writers\' rights issue in database marketing','Digital rights management'
'digital rights management (drm) of integrated circuits (ics) is a crucially important task both economically and strategically. several ic metering techniques have been proposed, but until now their effectiveness for royalty management has not been quantified. ic auditing is an important drm step that goes beyond metering; it not only detects that a pirated ic has been produced but also determines the quantity of pirated ics. our strategic objective is to create a new intrinsic passive metering technique as well as the first ic auditing technique, and to maximize and quantify their effectiveness using statistical analysis and ic characterization techniques. our main technical innovations include physical level gate characterization, a bayesian approach for coincidence analysis, and an adaptation of animal counting techniques for ic production estimation. we evaluate the accuracy of the ic metering and auditing approach using simulations on a set of iscas benchmarks.integrated circuit digital rights management techniques using physical level characterization','Digital rights management'
'this paper presents an overview of hardware and integrated circuits (ic) metering methods. ic metering or hardware metering refers to tools, methodologies, and protocols that enable post-fabrication tracking of the ics. metering enables prevention and detection of overbuilt and counterfeit ics in the dominant semiconductor contract-foundry model. post-silicon identification and tagging of the individual ics fabricated by the same mask is a precursor for metering: in passive metering, the ics are specifically identified, either in terms of their functionality, or by other forms of unique identification. the identified ics may be matched against their record in a pre-formed database that could reveal unregistered ics or overbuilt ics (in case of collisions). in active metering, not only the ics are uniquely identified, but also parts of the chip\'s functionality can be only accessed, locked (disabled), or unlocked (enabled) by the designer and/or ip rights owners using a high level knowledge of the design not transferred to the foundry. we provide a systematic overview of the field, along with a taxonomy of available methods.integrated circuits metering for piracy protection and digital rights management','Digital rights management'
'with the rapid increase of learning resources, the contradiction between learning content intelligent properties and its share and reuse becomes an outstanding event. in order to solve the problem, we probe into a way that is to integrate the drm technologies and learning technology standards into the development of learning content management system. we also design a novel online management system for broadcasting environments of virtual classroom or courseware on demand based on a browser/server framework, which carries out learning content management for reuse, share, interoperation and security. we place particular emphasis on the strategies of integration of digital rights management and flexible commercial model by license services into applications, management, exchange and trade of web-based learning content and try to solve the problem between reuse and protection.integration of digital rights management into learning content management system','Digital rights management'
'although digital rights management (drm) has gained increasing importance in today\'s digital services and electronic commerce, it is not addressed explicitly in the internet open trading protocol (iotp) specification. in this paper, we propose a digital rights management system (rms) that is integrated into iotp for electronic commerce applications and services. we introduce a rights insertion phase and a rights verification phase for iotp. in the proposed framework, digital watermarking plays a very important role in facilitating digital rights management. the proposed digital rights management system was implemented on an online music web site and the proposed concepts and approaches proved to be successful.integration of digital rights management into the internet open trading protocol','Digital rights management'
'in this paper, we propose a new encryption method for intellectual property rights management of mpeg-4 coder. dct coefficients and motion vectors (mvs) are used for the combination encryption of the mpeg-4 video. experimental results indicate that the proposed joint and partial encryption technique provides levels of security and achieves a simple and coding-efficient architecture with no adverse impact on error resilience.intellectual property rights management using combination encryption in mpeg-4','Digital rights management'
'this paper describes an interoperable digital rights management architecture promoted by the mpeg standardization group in its new standard known as mpeg-m or mpeg extensible middleware (mxm). the goal of this standard is to promote the packaging and reusability of mpeg technologies, and for this it specifies a software middleware platform and a complete set of apis and protocols. these apis allow uniformly handling digital content and developing generic multimedia applications, through a set of modules communicated with standard protocols. the mxm standard provides the necessary mechanisms to digitally manage the rights over intellectual property, the tools to protect the media and the means to grant the rights enforcement, besides a rich set of libraries to reproduce the content.interoperable digital rights management based on the mpeg extensible middleware','Digital rights management'
'introduction to human rights','Digital rights management'
'the quest for modular concurrency reasoning has led to recent proposals that extend program assertions to include not just knowledge about the state, but rights to access the state. we argue that these rights are really just sugar for knowledge that certain updates preserve certain invariants.invariants, modularity, and rights','Digital rights management'
'library applications of database management systems','Digital rights management'
'digital rights management allows information owners to control the use and dissemination of electronic documents via a machine-readable licence. this paper describes the design and implementation of a system for creating and enforcing licences containing location constraints that can be used to restrict access to sensitive documents to a defined area. documents can be loaded onto a portable device and used in the approved areas, but cannot be used if the device moves to another area. our contribution includes a taxonomy for access control in the presence of requests to perform non-instantaneous controlled actions.location constraints in digital rights management','Digital rights management'
'in this paper we present a concept and an architecture for a location dependent digital rights management system. the solution is based on a trusted hardware which incorporates the decryption of digital data, a precise secure clock, and a gps position receiver. a prototype is currently being developed in cooperation with a hardware manufacturer.location dependent digital rights management','Digital rights management'
'this paper examines the question whether, and to what extent, john locke\'s classic theory of property can be applied to the current debate involving intellectual property rights (iprs) and the information commons. organized into four main sections, section 1 includes a brief exposition of locke\'s arguments for the just appropriation of physical objects and tangible property. in section 2, i consider some challenges involved in extending locke\'s labor theory of property to the debate about iprs and digital information. in section 3, it is argued that even if the labor analogy breaks down, we should not necessarily infer that locke\'s theory has no relevance for the contemporary debate involving iprs and the information commons. alternatively, i argue that much of what locke has to say about the kinds of considerations that ought to be accorded to the physical commons when appropriating objects from it --- especially his proviso requiring that \"enough and as good\" be left for others --- can also be applied to appropriations involving the information commons. based on my reading of locke\'s proviso, i further argue that locke would presume in favor of the information commons when competing interests (involving the rights of individual appropriators and the preservation of the commons) are at stake. in this sense, i believe that locke offers us an adjudicative principle for evaluating the claims advanced by rival interests in the contemporary debate about iprs and the information commons. in section 4, i apply locke\'s proviso in my analysis of two recent copyright laws: the copyright term extension act (ctea), and the digital millennium copyright act (dmca). i then argue that both laws violate the spirit of locke\'s proviso because they unfairly restrict the access that ordinary individuals have previously had to resources that comprise the information commons. noting that locke would not altogether reject copyright protection for iprs, i conclude that locke\'s classic property theory provides a useful mechanism for adjudicating between claims about how best to ensure that individuals will be able to continue to access information in digitized form, while at the same time also allowing for that information to enjoy some form of legal protection.locke, intellectual property rights, and the information commons','Digital rights management'
'management information for the cip program','Digital rights management'
'this poster presents the work matching system implemented in the european library for identifying different publications with the same underlying intellectual work. this work is contextualized in the rights management framework of project arrow, where the european library is the main source of bibliographic metadata as an aggregator of europe\'s national library catalogues.matching intellectual works for rights management in the european library','Digital rights management'
'in the paper, we introduce a mobile software agent-based scheme for digital rights management targeted for personal mobile devices . the goal of the proposed scheme is to provide a solution secure yet simple to implement and maintain for controlled consumption of digital media objects. in order to protect the digital content in transport and to prevent non-authorized use, we apply a combination of a threshold scheme, one-time password system, key agreement protocol, and clueless mobile agents. here, every participating software agent receives a share from a dealer and only the end user is able to decrypt the drm content after gathering all the shares. clueless mobile agents traveling between the participants\' sites do the distribution and the gathering of the shares. the proposed drm scheme is compared to the industry-leading open mobile alliance\'s digital rights management specification and their applicability and the differences are thoroughly discussed. finally, we evaluate the weaknesses of the scheme and propose few variants satisfying different security requirements.mobile agent-based digital rights management scheme','Digital rights management'
'the phenomenon of dominant majority shareholders is widespread among china&#8217;s listed companies, despite it could reduce the agent cost between shareholders and managers, but major interest conflict often occur between majority shareholder and exterior minority shareholders. due to &#8220;the information asymmetry&#8221; between them, majority shareholder could obtain private interest through &#8220;managing&#8221; the earning information, which is a low cost manner, so they have the motivation on earning management. this paper explains through the motivation model of earning management that under the governance structure of chinese listed companies&#8217; large shareholder, majority shareholders tend to manage the reported earnings one year before transfer to maximize their own interest. while empirical research results in this paper indicate that under the estimation of controlling rights transfer, majority shareholders in positive earning companies have the motivation to uplift earnings so as to increase transfer revenue; however, companies with negative earnings may substantially decrease earnings so as to create a &#8220;high quality shell company&#8221;.motivation research of earning management in chinese listed companies with controlling rights transferred','Digital rights management'
'this paper describes multimedia rights management enforcement for the multiple devices of end-user. today, contents sharing and superdistribution of the multiple devices has become easy due to advancement in computer technology. however, most drm vendors require onlydrm client system of their own then end-user must have installed various clients and may worry about limited computing capacity. and the drm vendors may worry that end-users could give or lend the multimedia rights of protected contents to device of another user what the drm vendors don\'t allow. to resolve these problems, we propose a multimedia rights management scheme in which multiple devices of end-user are managed by only proxy manager controlled with interoperable client of each device. this scheme supports rights governance by public key based group communication that ensures that only legitimate operations can apply to the multimedia content.multimedia rights management for the multiple devices of end-user','Digital rights management'
'digital rights management (drm) is an important yet controversial issue in the information goods markets. although drm is supposed to help copyright owners by protecting digital content from illegal copying or distribution, it is controversial because drm imposes restrictions on even legal users, and there are many industry practitioners who believe that the industry would be better off without drm. in this paper, we model consumers\' utilities and their incentives to purchase legal products versus pirate illegal ones. this allows us to endogenize the level of piracy and understand how it is influenced by the presence or absence of drm. our analysis suggests that, counterintuitively, download piracy might decrease when the firm allows legal drm-free downloads. furthermore, we find that a decrease in piracy does not guarantee an increase in firm profits and that that copyright owners do not always benefit from making it harder to copy music illegally. by analyzing the competition among the traditional retailer, the digital retailer, and pirated sources of information goods, we get a better understanding of the competitive forces in the market and provide insights into the role of digital rights management.music downloads and the flip side of digital rights management','Digital rights management'
'network management with snmp','Digital rights management'
'many firms that sell digital copies of copyrighted materials online face a common dilemma: the use of digital rights management to impede pirates often also has negative implications for legitimate customers. we introduce a two-period model in which the use of drm in the first period affects the probability of consumers encountering pirated copies in the second period, the threat of legal action affects the probability of consumers obtaining pirated copies, and firms choose whether to sell, and at what prices, either strongly or weakly drm protected copies, or both. we are able to explain a range of observable firm behaviors with this model, including the use of price discrimination to offer both strongly and weakly protected files simultaneously, with weaker protection commanding a higher price, and the eventual abandonment of drm protections, both of which have been observed at various times, for example, with apple\'s itunes service.optimal digital rights management with uncertain piracy','Digital rights management'
'driven by an unparalleled advance in network infrastructure support as well as a boom in the number of interconnected personal communication, computation and storage devices, the modern mobile customer experience has become increasingly compelling. traditional barriers between the roles of information consumer and producer have disappeared. users increasingly produce and distribute valuable and often personal content such as pictures and free or purchased copyrighted media. it becomes essential to enable user-level drm controls for content access, data integrity and rights management. in this presentation we will overview the design and implementation of a a personal digital rights management system for mobile devices. the personal drm manager enables user-defined orcon-type controls for personal content originating in a cell phone or other mobile device. users can transparently define, generate, package and migrate content licenses between mobile devices on-demand. networked cellular devices cooperate in the enforcement mechanisms.personal digital rights management for mobile cellular devices','Digital rights management'
'with ubiquitous use of digital camera devices, especially in mobile phones, privacy is no longer threatened by governments and companies only. the new technology creates a new threat by ordinary people, who could take and distribute pictures of an individual with no risk and little cost in any situation in public or private spaces. fast distribution via web based photo albums, online communities and web pages expose an individual&#39;s private life to the public. social and legal measures are increasingly taken to deal with this problem, but they are hard to enforce in practice. in this paper, we proposed a model for privacy infrastructures aiming for the distribution channel such that as soon as the picture is publicly available, the exposed individual has a chance to find it and take proper action in the first place. the implementation issues of the proposed protocol are discussed. digital rights management techniques are applied in our proposed infrastructure, and data identification techniques such as digital watermarking and robust perceptual hashing are proposed to enhance the distributed content identification.personal rights management &#8211; taming camera-phones for individual privacy enforcement','Digital rights management'
'with ubiquitous use of digital cameras, e.g. in mobile phones, privacy is no longer threatened by governments and companies only. a new threat exists by people, who take photos of unaware people with no risk and little cost anywhere in public and private spaces. fast distribution via online communities and web pages expose an individual&#39;s private life to the public. social and legal measures are taken to deal with this, but they are hardly enforcable. we propose a supportive infrastructure aiming for the distribution channel such that if the picture gets publicly available, the exposed individual has a chance to detect it and take action.personal rights management&#8211; enabling privacy rights in digital online content','Digital rights management'
'privacy engineering for digital rights management systems','Digital rights management'
'the worldwide growth of e-services has brought to the forefront the importance of citizen privacy management. korba and kenny proposed a privacy rights management system in order to support the privacy principles derived from eu data directive 95/46/ec of the european parliament and the council of 24 october 1995. in this paper, we extend their system by proposing a privacy rights management framework for entity modeling and expression. the proposed framework manages and monitors the use of personal information. in addition, it provides interoperable mechanisms to support privacy compliance systems.privacy rights management for privacy compliance systems','Digital rights management'
'traditional digital rights management (drm) systems are one level distributor system which involve single distributor. however, for a flexible and scalable content distribution mechanism, it is necessary to accommodate multiple distributors in drm model so that different strategies can be implemented in diverse geographical areas. we develop a multiparty multilevel drm model using facility location and design a prototype drm system that provides transparent and flexible content distribution mechanism while maintaining the users\' privacy along with accountability in the system.privacy rights management in multiparty multilevel drm system','Digital rights management'
'privacy in business processes with proxies is not possible. users need to share attributes with their proxies which leads to &#8220;big brothers&#8221;. this is the reason why identity management systems such as liberty alliance and microsoft .net passport are not successful. we propose a generic privacy-preserving protocol for sharing identifying attributes as credentials with others. this delegation protocol extends current identity management systems.privacy with delegation of rights by identity management','Digital rights management'
'we present a privacy-preserving drm scheme for a (future) cloud computing software market. in such a market, applications are packed into virtual machines (vms) by software providers and the vms can be executed at any computing center within the cloud. we propose the introduction of a software tpm as a container for vm-specific keys within the vm that moves around with the vm within the cloud. the software tpm is coupled to a virtual tpm at a computing center to constitute the root of trust for a local drm enforcement system within the vm that checks the license before each application execution. this allows flexible price models, e.g. execute at most n times-like models. users have proof that their personally identifiable information, stored and processed within the vm at a computing center, cannot be obtained by the computing center. a feature of our solution is that neither software provider nor computing center are able to build usage profiles of the software executions.privacy-preserving digital rights management in a trusted cloud environment','Digital rights management'
'the internet has made it possible to distribute and exchange various forms of digital media (e.g., music, video, etc.) cheaply. endusers now have an effective, low cost means to distribute, search and obtain digital content. some end-users now even expect the content to be free, causing the intellectual property to holders find new approaches to obtain revenues from the use of the content. as new technology is being developed to make it easier to obtain digital content, a fast growing industry is forming to control the use of digital content. here we discuss the financial cryptography 2001 panel on protecting digital rights.protecting digital rights','Digital rights management'
'the author suggests that us patent and copyright law systems can be described in terms of models, which facilitates understanding of their operation and permits some amount of simulation. this may permit the circumstances that impose stresses on the systems and may strain them beyond their limits to be ascertained. the two models, those of patent and copyright law, on which the conventional approaches to intellectual property protection rely in the us are discussed. it is shown that these two models of legal protection have limitations, particularly in regard to late 20th century computer software technology. systems based on these models are particularly unsuited to protecting noncode aspects of computer software or to protecting against nonverbatim, nonliteral copying of computer programs. other legal models for protecting at least the nonliteral, noncode aspects of software, are consideredprotecting industrial property rights','Digital rights management'
'this paper reports our experimental work in using commercial secure coprocessors to control access to private data. in our initial project, we look at archived network traffic. we seek to protect the privacy rights of a large population of data producers by restricting computation on a central authority\'s machine. the coprocessor approach provides more flexibility and assurance in specifying and enforcing access policy than purely cryptographic schemes. this work extends to other application domains, such as distributing and sharing academic research data.prototyping an armored data vault rights management on big brother\'s computer','Digital rights management'
'in the horizontal semiconductor business model where the designer\'s intellectual property (ip) is transparent to foundry and to other entities on the production chain, integrated circuits (ics) overbuilding and ip piracy are prevalent problems. active metering is a suite of methods enabling the designers to control their chips postfabrication. we provide a comprehensive description of the first known active hardware metering method and introduce new formal security proofs. the active metering method uniquely and automatically locks each ic upon manufacturing, such that the ip rights owner is the only entity that can provide the specific key to unlock or otherwise control each chip. the ic control mechanism exploits: 1) the functional description of the design, and 2) unique and unclonable ic identifiers. the locks are embedded by modifying the structure of the hardware computation model, in the form of a finite state machine (fsm). we show that for each ic hiding the locking states within the modified fsm structure can be constructed as an instance of a general output multipoint function that can be provably efficiently obfuscated. the hidden locks within the fsm may also be used for remote enabling and disabling of chips by the ip rights owner during the ic\'s normal operation. an automatic synthesis method for low overhead hardware implementation is devised. attacks and countermeasures are addressed. experimental evaluations demonstrate the low overhead of the method. proof-of-concept implementation on the h.264 mpeg decoder automatically synthesized on a xilinix virtex-5 field-programmable gate array (fpga) further shows the practicality, security, and the low overhead of the new method.provably secure active ic metering techniques for piracy avoidance and digital rights management','Digital rights management'
'reader\'s rights','Digital rights management'
'first page of the articlereading your rights','Digital rights management'
'digital watermarking is a data hiding technology that can be used for digital rights management. it embeds an invisible signal including owner identification and copy control information into multimedia data for copyright protection against illegal copying and distribution. in this paper, we embed the watermark in i-pictures of the mpeg video sequence by modifying vlcs (variable length codes) directly to avoid inverse dct (discrete cosine transform) and inverse quantization. the modification of vlcs is intended to minimize the perceptual degradation of video quality caused by the embedded watermark. because we select vlcs with the same run value and code word length to be modified, the compressed video stream does not increase in size. furthermore, our proposed watermarking scheme protects the embedded information by means of a secret key and a prng (pseudo random number generator) to generate a random binary sequence for watermarking. we apply three mpeg-1 video sequences to test our proposed algorithm. since the watermark can be extracted by simple table lookup without using original video sequences, the computational overhead is very small. therefore, the decoding of the watermarked video sequence can be real time for viewing. the proposed algorithm can be easily applied to mpeg-2 and mpeg-4 videos also for digital rights management.real time digital video watermarking for digital rights management via modification of vlcs','Digital rights management'
'the increasingly intensive coexistence of diverse radio systems and the inability of existing institutions to resolve conflicts in a timely manner require a change in the way operating rights are defined, assigned, and enforced. this paper proposes a regulatory approach that increases delegation to operators and reduces ambiguity by (1) more clearly defining operating rights and harmful interference using the three p approach of probabilistic reception protections and transmission permissions; (2) facilitating transactions by limiting the number of parties to a negotiation, only altering the rights in a license at renewal, and implementing a registry; and (3) making rights enforcement more efficient by enabling direct enforcement of rights, separating rulemaking from adjudication, and defining remedies up-front.reception-oriented radio rights','Digital rights management'
'this paper presents a hardware implementation of a decoder for digital cinema images. this decoder enables us to deal with image size of 2k with 24 frames per second and 36 bits per pixels. it is the first implementation known nowadays that perfectly fits in one single virtex-ii&#174; fpga and includes aes decryption, jpeg 2000 decompression and fingerprinting blocks. this hardware offers therefore high-quality image processing as well as robust security.reconfigurable hardware solutions for the digital rights management of digital cinema','Digital rights management'
'digital rights management &#40;drm&#41; covers the description, identification, trading, protection, monitoring and tracking of all forms of rights over both tangible and intangible assets. the digital object identifier &#40;doi&#41; system provides a framework for the persistent identification of entities involved in this domain. although the system has been very well designed to manage object identifiers, some important questions relating to the creation and assignment of identifiers are left open. the paradigm of a referent tracking system &#40;rts&#41; recently advanced in the healthcare and life sciences environment is able to fill these gaps. this is demonstrated by pointing out inconsistencies in the existing doi models and by showing how they can be corrected using an rts.referent tracking for digital rights management','Digital rights management'
'multimedia information management, which implies all steps from creation and production to distribution and consumption, is a complex and challenging research area. to have a secure and trusted system we need to take into account aspects such as digital rights management (drm), certification, control and security. as current solutions rely on proprietary architectures and tools, we propose an open architecture, as general as possible and not restricted to a specific standard, which provides trust and rights management in multimedia information systems. we analyse how the elements of the architecture provide trust to the whole value chain by managing multimedia content and digital rights represented using current standards, such as mpeg-21 and oma drm, and we compare it with an alternative approach. then, we illustrate the system operation through a content composition use case, and finally, we present the software tools that we have already developed and the future work.rights and trust in multimedia information management','Digital rights management'
'the central aspect of digital rights management is rights expression languages (rels) which express and transfer rights from one party to another interoperable format. this paper provides an analysis on rels and defines the main entities model to express rights for database based on xrml.rights expression in database rights management based on xrml','Digital rights management'
'building digital rights management (drm) systems for electronic commerce is still a complex task because such systems are usually required to manage a large set of different media contents, rights information, and enabling technologies. the secure digital music initiative (sdmi) was proposed to provide a secure environment in which rights management is performed for music distribution through the internet. rights management in sdmi is implemented by digital watermarking, but it is not the responsibility of sdmi to provide guidelines for the design of digital watermarks and to explore other potential benefits of using digital watermarks for the music distribution business. this paper fills this gap by proposing an sdmi-based rights management system. the proposed rights management system can support basic sdmi functions and non-sdmi features such as video. a prototype of the sdmi-based rights management system has been developed and the interfaces of the major components are presented.sdmi-based rights management systems','Digital rights management'
'with the development of storage and computing technologies, digital content such as music, digital movies, games, cartoon and dv et al gets more and more popular for entertainment, how to control the rights of the digital content is now becoming a very important issue. in this paper, a secure and flexible content protection secure drm scheme(named cpsec drm) is proposed for online/ offline rights management, in which content objects(cos) and rights objects(ros) were separated respectively, the cos was encrypted by content encryption key(cek), while ros was encapsulated rights encryption key(rek) that is related to the device information of end user\'s, thus even if ros were illegally copied and spread, however it will not pass the authentication of license verification. as for domain and offline license management, a license transfer scheme was developed for n-total sub-licenses redistribution, once the sub- licenses was released to n copies, then the master license can not be transferred and redistributed again. the proposed cpsec drm scheme does not only support online rights management, but can still works in an offline mode for pervasive usage.secure and flexible digital rights management in a pervasive usage mode','Digital rights management'
'the new trend in mobile applications world is to securely ensure a variety of content for all the customers. since the providers invest large amounts of money in producing multimedia content such as m-applications, presentations, advertising, music clips and games, there should be considered some sensitive problems regarding the \"forward lock\" issue, in particular the content transfer from one mobile device to another. this paper presents models for handling digital rights management problems and a proposal for a practical distributed secure architecture used in a complex model for massive secure distribution of m-contents. also, the concepts related to m-application, xml signature, keys management, and secure architectures are combined in a practical manner that helps the researchers, designers and developers from it&c field to consider multifarious approaches of the digital rights management issues.secure architecture for the digital rights management of the m-content','Digital rights management'
'content protection is now becoming more and more important for digital rights management (drm), which involves rights embedding, identification, rights validation digital multimedia resource is popular in the real world, how to protect multimedia content from be violated without rights control is an important thing especially to resist copy-spread kind violation. in this paper, an novel approach for multimedia rights management is proposed based on partial encryption method, which can control multimedia resource played in a rights-constraint environment, which can protect multimedia resource from being copying and spreading, and in the authorization usage environment, the protected multimedia is properly played as normal, however once the resource is beyond the authorization environment, the protected resource will not be played correctly. experiments showed our proposed partial encryption approach was efficient with real-time quality of service, which was suitable for online multimedia streaming in content delivery network.secure multimedia streaming with trusted digital rights management','Digital rights management'
'we typically think of documents as carrying information. however, certain kinds of documents do more than that: they are not only informative but also performative in that they represent rights. when these documents are in paper form or some other physical medium, holding the document indicates holding the right. since the document represents a right, a hazard is that by duplicating the document, one may fraudulently claim a new right. for this reason, physical documents that represent rights are both tamper resistant and copy resistant. however, problems arise when such performative documents are converted to electronic form: duplicates are bit for bit perfect and undetectable. thus, the normal heuristic of uniqueness of the document token as representing the uniqueness of the right no longer holds for performative electronic documents. this is especially challenging when the rights are transferable, as with various financial instruments such as stocks and bonds. this paper presents an analysis, based on deontic logic, about the necessary requirements for electronic documents and their corresponding electronic procedures in order to guarantee the uniqueness of rights and prevention of fraud. a design is sketched, based on a notion we call digital parchment, which offers improved flexibility.securing uniqueness of rights e-documents','Digital rights management'
'this paper provides an overview of digital cinema including some of the architectural ideas that have been proposed and are being considered. there are a number of security tools that can be used with the goal of secure delivery of motion picture content to the projector. there are many technical and business constraints that guide these designs. independently, there have been a number of efforts in the watermarking community to attach forensic tracking information to the motion picture content. such information would provide persistent tracking beyond the projector and would be useful in identifying compromised equipment.security and rights management in digital cinema','Digital rights management'
'most real-life systems delegate responsibilities to different authorities. we apply this idea of delegation to a digital rights management system, to achieve high flexibility without jeopardizing the security. in our model, a hierarchy of authorities issues certificates that are linked by cryptographic means. this linkage establishes a chain of control, identity-attribute-rights, and allows flexible rights control over content. typical security objectives, such as identification, authentication, authorization and access control can be realized. content keys are personalized to detect illegal super distribution. we describe a working prototype, which we develop using standard techniques, such as standard certificates, xml and so forth. we present experimental results to evaluate the scalability of the system. a formal analysis demonstrates that our design is able to detect a form of illegal super distribution.security attributes based digital rights management','Digital rights management'
'secure distribution of digital goods is now a significantly important issue for protecting publishers\' copyrights. in this paper, we study a useful primitive for constructing a secure and efficient digital rights management system (drm) where a server which encrypts digital content and one which issues the corresponding decryption key works independently, and existing schemes lack this property. we first argue the desired property necessary of an encryption scheme for constructing an efficient drm, and formally define an encryption scheme as split encryption scheme containing such property. also, we show that an efficient split encryption scheme can be constructed from any identity-based scheme. however, since currently there is no identity-based encryption scheme which is based on well-known computational assumption and/or provable security without the random oracle, by reasonably tuning the system parameter, we show another construction of split encryption which is secure against chosen ciphertext attacks in the standard model assuming that the decision diffie-hellman problem is hard to solve.separating encryption and key issuance in digital rights management systems','Digital rights management'
'shareware database management software','Digital rights management'
'software publishers use digital rights management, specifically copy-protection techniques, to prevent unauthorized and illegal copying of their software products. common forms of prevention are copy-protection techniques based on physical tokens. while physical tokens provide better protection from unauthorized copying than intangible ones, the protected digital content becomes unsuitable for online distribution. this paper investigates the role of copy-protection techniques based on physical and intangible tokens in software piracy prevention. an internationally organized online survey among users of sequencer software, a particular kind of music software, provides the data for the subsequent descriptive analysis and logistic regression. based on our findings, we present the general implications of our results for a software publisher\'s anti-piracy and online distribution policy.software piracy prevention through digital rights management systems','Digital rights management'
'intellectual property rights (ipr) management is akey issue for the deployment of real e-commerce andmultimedia content distribution on the web. musicmarket knows very well this problem, and very differentapproaches are being considered to cope with the issue.one valid approach to solve the problem is todevelop standards allowing interoperability ofsolutions, a relevant problem nowadays. mpeg is oneof the international standardisation initiatives that istrying to go along this way. in particular, the newmpeg-21 standard is proposing that thestandardisation of a digital rights expression languageand a rights data dictionary would help to improvesystems and applications interoperability.the main objective of the paper is to present ourapproach to this issue, that goes one step forward thanmpeg, since we are developing a semantic approach torepresent and manage ipr.standardisation of the management of intellectual property rights in multimedia content','Digital rights management'
'why is drm important to the bbc? there are three simple reasons for this, each of which becomes even more important when we\'re talking about the digital world -- be it broadcasting or broadband. the first point is one of ownership. the rights structure of much of our content is incredibly complex. we quite often buy-in rights, for sports or for feature films. sometimes our production partners and talent retain certain rights when we commission a programme and quite often we only have rights to broadcast certain content a certain number of times. in addition to this, we might have co-production partners involved in a production which means that we share the rights of content, often based upon territories. the next point is legality. all of the above are usually legally enforced in the form of a contract.standards based digital rights management--potentially viable or just a pipe dream?','Digital rights management'
'stretching database management software','Digital rights management'
'technologies for repository interoperation and access control','Digital rights management'
'in order to evaluate the performance of information retrieval and extraction algorithms, we need test collections. a test collection consists of a set of documents, a clearly formed problem that an algorithm is supposed to provide solutions to, and the answers that the algorithm should produce when executed on the documents. defining the association between elements in the test collection and answers is known as labeling. for mainstream information retrieval problems, there are publicly available test collections which have been maintained for years. however, the scope of these problems, and thus the associated test collections, is limited. in other cases, researchers need to build, label, and manage their own test collections, which can be a tedious and error-prone task. we were building test collections of html documents, for problems in which the answers that the algorithm supplies is a sub-tree of the dom (document object model). to lighten the burden of this task, we developed a test collection management and labeling system (tcmls), to facilitate usability in the process of building test collections, applying them to validate algorithms, and potentially sharing them across the research community.test collection management and labeling system','Digital rights management'
'through further study on the characteristics of digital rights management system and the expansion of hierarchical role-based access control (rbac) model, we proposed a context and hierarchical role based access control model and applied it to the digital rights management system. this model dynamically changes the permissions of users by obtaining context information related to security under complex network environment, and also can keep the advantages of traditional hierarchical rbac model. this extended access control system is being implemented in practice, which makes the access control of digital rights management system become more flexible, safe and effective.the application of rbac in digital rights management system','Digital rights management'
'first of all clarify the research background, and introduces the necessary theory and techniques involved in the relevant. through the management information system access system resources study of various control methods, using rbac access control technology to design and implement a middleware based rights management that can run on a variety of hardware and software platform to facilitate the conduct of secondary development.the design and realization of middleware based on rights management','Digital rights management'
'this article aims to integrate the operation and management of mining rights market with e-government building, and exploring a better mode of mining rights of the secondary market ,which will help us to rationalize the allocation of mineral resources and provide some ideas to enhance the efficiency of public administration.the management about the secondary market of mineral rights based on e-government','Digital rights management'
'the management of intellectual property','Digital rights management'
'over the last few years, digital rights management (drm) systems have become increasingly successful in various areas of digital content production and distribution. while drm-related technology research has achieved a certain degree of maturity since the 1990\'s, the legal framework surrounding drm systems remains in flux and is fiercely debated. this keynote speech first provides an overview of the various ways by which digital content is protected in a drm system. the intertwining protection by technology, contracts, technology licenses and anti-circumvention regulations may lead to a new \"property right\" making copyright protection obsolete. however, there is a danger of over-protection: questions of fair use and other limitations to traditional copyright law have to be addressed. if competition is not able to solve this tension between the interests of content providers and the interests of users or the society at large -- which seems doubtful at least -- it is the law that has to provide a solution. the speech presents the different approaches legislatures in europe and in the united states have adopted. second, the speech discusses recent developments at the intersection of drm and the law, in particular the discussions about drm interoperability, patents on drm technology, drm standards, and the relationship between drm and levy systems as well as between drm and trusted computing. third, the speech analyzes recent scholarship that attempts to solve legal and policy questions surrounding drm on a technological level. the speech questions whether the tension between technological protection and public values such as fair use, privacy, competition and an innovation commons is really irreconcilable. it argues for a more nuanced approach that explores possibilities and limitations of a value-centered technology design of drm systems. only if technology research focuses on such questions, the disapproval of drm systems by many legal scholars and policy activists could possibly diminish in the long term.the present and future of digital rights management','Digital rights management'
'the role of subsidiary rights in scholarly communication','Digital rights management'
'digital rights management (drm) is a system to protect high-value digital assets and control the distribution and usage of those digital assets. digital licensing controls the contents to be accessed by the consumers. one of the major issues raised by drm systems concerns the integrity of this license. in this paper, we propose a threshold based group-oriented nominative proxy signature scheme (tb-go-npss) which supports an original provider to delegate his/her signing ability to the partial members of the proxy group having n members and to designate the partial members of the verifier group having l members to verify his/her digital licenses for the mobile users. the proposed scheme can guarantee that the digital products come from the authorized providers. a formal security analysis demonstrates that our scheme is secure enough to be used in drm systems.threshold based group-oriented nominative proxy signature scheme for digital rights management','Digital rights management'
'trusted computing has received criticism from those who fear it will be used by influential market forces to exert power over the software used on consumer platforms. this paper describes an open architecture for digital rights management (drm) enforcement on trusted computing platforms that empowers the consumer to select their operating-system and applications, including open-source options, without weakening the strength of the security functions. a key component in the architecture is a security manager that enforces mandatory access controls on shared devices, restricted information flows between virtual machines, and drm policy on protected objects. the paper describes two use-cases: a drm scenario with protected media content and remote home-working on sensitive medical data.towards an open, trusted digital rights management platform','Digital rights management'
'digital transactions are usually based on mutual trust. in case of drm (digital rights management) this initial trust is missing on both sides. neither do the content providers trust their clients &#8211; therefore drm was established. nor do the clients trust the content providers and react with not using these systems. the release of an open drm standard by the open mobile alliance (oma) was a first step to increase the trustworthiness of drm. but from the content providers&#8217; perspective a secure implementation for pc platforms was missing. especially the mechanisms to obfuscate and install the device private key which is the security anchor were not established there. this paper shows a software solution for that. a more riskless way to solve this problem is the involvement of trusted computing which is also shown by the authors. finally the authors claim the necessity not to leave the users&#8217; security behind.towards trust in digital rights management systems','Digital rights management'
'digital rights management (drm) is concerned with the controlled distribution and usage of digital works in order to prevent unauthorized usage of digital content. a significant component of a drm system is a well-defined language for expressing the rights which a user/device has been granted. such a language is known as a rights expression lanugage (rel).various technical communities have developed alternative rights expression languages. it is expected that in the marketplace different rights expression languages will be used as determined by the business needs and constraints of content and service providers. from such market fragmentation arises the need to move content between different drm schemes, and so there is a need to translate rights expressions between rels.in this paper, we examine the challenges that must be addressed when developing a rights expression translation and then present several approaches to rights expression translation.translation of rights expressions','Digital rights management'
'digital rights management allows content providers to control the distribution and usage of digital contents for their e-commerce applications. most of previously proposed digital rights management mechanisms focus on the protection mechanisms for digital contents and pay less attention to the discussion on business models and users&#8217; convenience. it will decrease effective distribution for digital contents and the intentions of users&#8217; usage. with the super distribution model, we first propose a digital rights management system that increases distribution channels to increase the efficiency of distribution for digital contents. then, we implement the proposed digital rights management system by using the api hook with microsoft foundation classes (mfc) as a wrapper. the implemented digital rights management system can achieve content protection without sacrificing user&#8217;s convenience. in addition, the proposed digital rights management system satisfies the following requirements: description, identification, transaction, protection, monitoring, tracing, super distribution, transparency, and portability.transparent digital rights management system with superdistribution','Digital rights management'
'society has grown increasingly reliant on technologies that allow the reproduction of both physical and abstract entities. the ease with which content can be copied has brought widespread benefits, however in many cases these benefits are also contingent on a suitable counter balance of content protection, allowing the process of copying to be restricted in certain circumstances. dissemination of knowledge, mass production, financial systems and privacy protection all rely on both kinds of technology. this is as true in the digital world as it has been traditionally. moreover, the success of future computing developments, such as the move towards ubiquitous computing, rely on increasingly fluid movements of data throughout a networked environment, a requirement that is often seen as being in conflict with the need to be able to restrict the movement of certain rights protected data. yet customers can be unwilling to accept stringent restrictions imposed through technological means, and a suitable balance needs to be met. in this paper we build on our previous work on community based digital rights management to show how it can be applied in an existing peer-to-peer file sharing network. we extend the gnutella peer-to-peer protocol to show how a realistic implementation of a community-based trust mechanism can be achieved. our mechanism models the peer-to-peer network as a cellular automaton using trust as a means to regulate the illegitimate flow of data. we discuss the implementation details and the synchronisation process that we have developed based on our initial results.trusted digital rights management in peer-to-peer communities','Digital rights management'
'internet research often assumes users may connect devices without consent by their service providers. however, in many networks the service provider only allows use of devices obtained directly from the provider. we review how united states communications law addresses the rights of users to connect devices of their choice. we explicate a set of user and service provider rights. we propose legal requirements for attachment and management of devices. we illustrate how these proposed regulations would affect the services currently offered on telephone, cable, satellite, video networks, and cellular networks, as well as on the internet.user and isp rights of device attachment and device management','Digital rights management'
'the rapid growth of internet and technologies make digital products easy to duplicate and deliver. piracy follows to be a serious problem and challenge. thus, how to effectively protect and manage the rights of digital contents becomes a very important issue. this paper aims to propose the user friendly digital rights management system based on smart cards. our system uses different keys to protect digital contents. besides, the consumers are allowed to play digital contents freely, no matter in the on-line or off-line environments.user friendly digital rights management system based on smart cards','Digital rights management'
'in the process of digitizing a book, a library needs to clear the rights associated with it. rights clearance is a time consuming process, and possibly, with higher costs than the actual digitization. to analyze the rights situation, a range of information is required, which is distributed across several national databases hosted in national libraries, publishers and collective rights organizations. national bibliographies are key data sources in these processes, as they are the only source to identify all the publications of a specific intellectual work per country. however, national bibliographies are not built for rights clearance purposes. the information in bibliographic records results from cataloguing practices with users and library management in mind, and links between different publications of a single intellectual work are not available. this paper presents a study on the implications of data quality problems of national bibliographies for the identification of all publications of a work. it also presents an approach for work data extraction and matching based on similarity of the most discriminatory attributes of works. evaluation has shown that the data quality problems are difficult to overcome, as our best approach achieved an f0,5-measure of 0,91. these results help to speed up the process of discovering all relevant publications per work significantly, with sufficient recall.using national bibliographies for rights clearance','Digital rights management'
'sitdrm is a privacy protection system that protects private data through the enforcement of mpeg rel licenses provided by consumers. direct issuing of licenses by consumers has several usability problems that will be mentioned in this paper. further, we will describe how sitdrm incorporates p3p language to provide a consumer-centered privacy protection system.using sitdrm for privacy rights management','Digital rights management'
'this paper describes how social factors can be incorporated into digital rights management. specifically, we outline a design for a social distribution network that is built by agents that have incentive to discourage piracy. we pose the social distribution network formation as a game theoretic problem and identify the games played by the two types of agents.using social factors in digital rights management','Digital rights management'
'vast quantities of video data are distributed around the world every day. video content owners would like to be able to automatically detect any use of their material, in any media or representation. we investigate techniques for identifying similar video content in large collections. current methods are based on related technology, such as image retrieval, but the effectiveness of these techniques has not been demonstrated for the task of locating video clips that are derived from the same original. we propose a new method for locating video clips, shot-length detection, and compare it to methods based on image retrieval. we test the methods in a variety of contexts and show that they have different strengths and weaknesses. our results show that the shot-based approach is promising, but is not yet sufficiently robust for practical application.video similarity detection for digital rights management','Digital rights management'
'the business of content providers is being threatened by technology advances in hardware, software and ip networks resulting in an increasing amount of illegal copies available online as well as offline. digital rights management standards are being developed for the digital content and digital watermarking, a popular technique for content authentication and forgery prevention is viewed as an enabling technology to protect distributed content from unauthorized reuse, or reuse with inadequate credit. this paper establishes the role of an administrative body, by the name of central watermarking body, in sharing the responsibilities of the content providers, in securing the genuine content and in protecting the rights of the customers by allowing them to judge if the content they have paid for, is genuine. the paper also highlights the objectives, technological details, the economic feasibility, the issues pertaining to the practical implementation and the limitations of the watermark based drm model.watermark based digital rights management','Digital rights management'
'the issue addressed in this paper is at first a brief presentation of the technical guidelines for ipr protection and management applied to greek cultural digitization projects. secondly, the work focuses on the analysis and implementation of a typical digital rights management system for organizations and projects aiming at the digitization and exploitation of cultural content. both technical guidelines and the drms are setting a solid framework for providing answers to a crucial and complex issue, the issue of the protection and management of intellectual property rights for analog and digital content.watermarking and digital rights management - a pilot drm system implementation and technical guidelines to cultural digitization projects','Digital rights management'
'the paper focuses on the implementation of an advanced digital rights management (drm) system which supported by web services offers copyright protection and management of digital media. the main components of the drm system are a digital image library, which offers specialized services for storing and searching and a copyright protection and digital rights management subsystem for the digitized media based on innovative watermarking and web technologies.web services for digital rights management and copyright protection in digital media','Digital rights management'
'what digital rights management means today','Digital rights management'
'the development of access rights as, perhaps, a replacement for copyright in digital rights management (drm) systems, draws our attention to the importance of \'the balance problem\' between information industries and the individual user. the nature of just what this \'balance\' is, is often mentioned in copyright writings and judgments, but is rarely discussed. in this paper i focus upon elucidating the idea of balance in intellectual property and propose that the balance concept is not only the most feasible way to examine whether past solutions to copyright problems are fair, but it also provides the ability to predict what will be the better solution for all affected parties. based upon an envy-free contribution towards predicting the efficient balance, game theory is applied in a novel manner to the drm problem to infer where and what might be the optimal balance in the debate over the nature of access right.who should own access rights? a game-theoretical approach to striking the optimal balance in the debate over digital rights management','Digital rights management'
'the primitive of proxy signatures allows the original signer to delegate proxy signers to sign on messages on behalf of the original signer. it has found numerous applications in distributed computing scenarios where delegation of signing rights is common. certificate less public key cryptography eliminates the complicated certificates in traditional public key cryptosystems without suffering from the key escrow problem in identity-based public key cryptography. in this paper, we reveal the relationship between the two important primitives of proxy signatures and certificate less signatures and present a generic conversion from the latter to the former. following the generic transformation, we propose an efficient proxy signature scheme with a recent certificate less signature scheme.a generic construction of proxy signatures from certificateless signatures','Digital signatures'
'content extraction signatures (ces) enable the selective disclosure of verifiable content from signed documents. we have previously demonstrated a ces extraction policy for fragment grouping to allow the document signer to designate which subsets of the original document are valid subdocuments. extending this ability, we introduce a new &#60;i>hierarchical grouping extraction policy&#60;/i> that is more powerful, and for which the encoding is dramatically smaller, than the existing grouping extraction policy. this new extraction policy maps naturally onto the hierarchically structured documents commonly found in digital libraries. after giving a motivating example involving digital libraries we then conjecture as to how to enrich their functionality through the use of cess. we also show how to implement the new extraction policy using xml signatures with a custom transform along with an improved design for the xml signature structure in order to achieve ces functionality. a hierarchical extraction policy for content extraction signatures: selectively handling verifiable digital content','Digital signatures'
'the main advantage of ring signatures is to ensure anonymity in ad hoc groups. however, since a group manager is not present in ad hoc groups, there is no existing way to identify the signer who is responsible for or benefit from a disputed ring signature. in this paper, we address this issue by formalizing the notion of ad hoc group signature. this new notion bridges the gap between the ring signature and group signature schemes. it enjoys the same advantage of ring signatures to provide anonymity whilst not requiring any group manager. furthermore, it allows a member in an ad hoc group to provably claim that it has (not) issued the anonymous signature on behalf of the group. we propose the first construction of ad hoc group signatures that is provably secure in the random oracle model under the strong rsa assumption. our proposal is very simple and additionally, it produces a constant size signature length and requires constant modular exponentiations. this is to ensure that our scheme is very practical for ad hoc applications where a centralized group manager is not present.ad hoc group signatures','Digital signatures'
'since the blind signature scheme has the unlinkability property, it can be used for untraceable electronic cash systems. the unlinkability property protects the privacy of users; however, it may be abused by criminals, e.g., to launder money or to safely get a ransom. the techniques of fair blind signature are developed to withstand the misuse of unlinkability property. in this paper, we propose not only a user-efficient but also a signer-efficient fair blind signature scheme for untraceable electronic cash. only with the help from a judge or government, the signer or the banker can derive the link between a signature and the instance of the signing protocol which produces the signature when the unlinkability property is abused. comparing with existing fair blind signature schemes, our method greatly reduces the computational load of an on-line judge. hence, the proposed method with an efficient judge can let the signer provide more efficient services to the signature requesters. it is also suitable for the limited computation capacities of users such as smart cards or mobile units. so, the proposed scheme is very useful for e-commerce transactions.an untraceable electronic cash system using fair blind signatures','Digital signatures'
'we define a general model for consecutive delegations of signing rights with the following properties: the delegatee actually signing and all intermediate delegators remain anonymous. as for group signatures, in case of misuse, a special authority can &lt;em&gt;open&lt;/em&gt;signatures to reveal the chain of delegations and the signer\'s identity. the scheme satisfies a strong notion of non-frameability generalizing the one for dynamic group signatures. we give formal definitions of security and show them to be satisfiable by constructing an instantiation proven secure under general assumptions in the standard model. our primitive is a proper generalization of both group signatures and proxy signatures and can be regarded as non-frameable dynamic hierarchical group signatures.anonymous proxy signatures','Digital signatures'
'at pkc 2006, yang, wong, deng and wang proposed the notion of anonymous signature schemes where signatures do not reveal the signer\'s identity, as long as some parts of the message are unknown. they also show how to modify the rsa scheme and the schnorr scheme to derive anonymous signatures in the random oracle model. here we present a general and yet very efficient approach to build such anonymous schemes from ordinary signature schemes. when instantiated in the random oracle model, our solution is essentially as efficient as the original scheme, whereas our construction also supports an almost as efficient instantiation in the standard model.anonymous signatures made easy','Digital signatures'
'we revisit the notion of the anonymous signature, first formalized by yang, wong, deng and wang [10], and then further developed by fischlin [4] and zhang and imai [11]. we present a new formalism of anonymous signature, where instead of the message, a part of the signature is withheld to maintain anonymity. we introduce the notion &lt;em&gt;unpretendability&lt;/em&gt; to guarantee infeasibility for someone other than the correct signer to pretend authorship of the message and signature. our definition retains applicability for all previous applications of the anonymous signature, provides stronger security, and is conceptually simpler. we give a generic construction from any ordinary signature scheme, and also show that the short signature scheme by boneh and boyen [2] can be naturally regarded as such a secure anonymous signature scheme according to our formalism. anonymous signatures revisited','Digital signatures'
'we present a new primitive &#8211; append-only signatures (aos) &#8211; with the property that any party given an aos signature sig[m1] on message m1 can compute sig[m1 || m2] for any message m2, where m1 || m2 is the concatenation of m1 and m2. we define the security of aos, present concrete aos schemes, and prove their security under standard assumptions. in addition, we find that despite its simple definition, aos is equivalent to hierarchical identity-based signatures (hibs) through efficient and security-preserving reductions. finally, we show direct applications of aos to problems in network security. our investigations indicate that aos is both useful in practical applications and worthy of further study as a cryptographic primitive.append-only signatures','Digital signatures'
'the concept of concurrent signatures allows two entities to produce two signatures in such a way that, the signer of each signature is ambiguous from a third party&#8217;s point of view until the release of a secret, known as the keystone. once the keystone is released, both signatures become binding to their respective signers concurrently. previous concurrent signature schemes use the concept of ring signatures in their construction. ring signatures identify the ring and thus concurrent signatures constructed from ring signature are related and linkable. we propose a new concurrent signature scheme which is independent of the ring signature concept. our concurrent signatures are anonymous. the ordinary signatures obtained from our concurrent signature protocol are unlinkable and do not reveal which concurrent signature transaction has occurred. the price we pay is our concurrent signatures are asymmetric in the sense that the initial signature and subsequent signatures are not of the same construction.asymmetric concurrent signatures','Digital signatures'
'we introduce attribute-based signatures (abs), a versatile primitive that allows a party to sign a message with fine-grained control over identifying information. in abs, a signer, who possesses a set of attributes from the authority, can sign a message with a predicate that is satisfied by his attributes. the signature reveals no more than the fact that a single user with some set of attributes satisfying the predicate has attested to the message. in particular, the signature hides the attributes used to satisfy the predicate and any identifying information about the signer (that could link multiple signatures as being from the same signer). furthermore, users cannot collude to pool their attributes together. we give a general framework for constructing abs schemes, and then show several practical instantiations based on groups with bilinear pairing operations, under standard assumptions. further, we give a construction which is secure even against a malicious attribute authority, but the security for this scheme is proven in the generic group model. we describe several practical problems that motivated this work, and how abs can be used to solve them. also, we show how our techniques allow us to extend groth-sahai nizk proofs to be simulationextractable and identity-based with low overhead.attribute-based signatures','Digital signatures'
'in this paper we present a novel type of digital signatures, which we call blank digital signatures. the basic idea behind this scheme is that an originator can define and sign a message template, describing fixed parts of a message as well as multiple choices for exchangeable parts of a message. one may think of a form with blank fields, where for such fields the originator specifies all the allowed strings to choose from. then, a proxy is given the power to sign an instantiation of the template signed by the originator by using some secret information. by an instantiation, the proxy commits to one allowed choice per blank field in the template. the resulting message signature can be publicly verified under the originator\'s and the proxy\'s signature verification keys. thereby, no verifying party except the originator and the proxy learn anything about the \"unused\" choices from the message template given a message signature. consequently, the template is hidden from verifiers. we discuss several applications, provide a formal definition of blank digital signature schemes and introduce a security model. furthermore, we provide an efficient construction of such a blank digital signature scheme from any secure digital signature scheme, pairing-friendly elliptic curves and polynomial commitments, which we prove secure in our model. we also provide a detailed efficiency analysis of our proposed construction supporting its practicality. finally, we outline several open issues and extensions for future work.blank digital signatures','Digital signatures'
'previously known blind signature systems require an amount of computation at least proportional to the number of signature types, and also that the number of such types be fixed in advance. these requirements are not practical in some applications. here, a new blind signature technique is introduced that allows an unlimited number of signature types with only a (modest) constant amount of computation.blinding for unanticipated signatures','Digital signatures'
'concurrent signature was introduced as an alternative approach to solving the problem of fair exchange of signatures. it allows two entities to produce two signatures in such a way that, the signer of each signature is ambiguous from a third party\'s point of view until the release of a secret, known as the keystone. once the keystone is released, both signatures become binding to their respective signers concurrently. certificate-based public key cryptography was introduced to remove the use of certificate to ensure the authentication of the user\'s public key in the traditional public key cryptography and to overcome the key escrow problem in the identity-based public key cryptography. combining the concept of concurrent signature with the concept of certificate-based cryptography, in this paper, we propose a certificate-based perfect concurrent signature scheme assuming the hardness of computational diffie-hellman problem.certificate-based perfect concurrent signatures','Digital signatures'
'partially blind signature is a variant of blind signature. it allows a signer to explicitly include common information in a blind signature under some agreement with a receiver. it provides anonymity of users in applications such as electronic voting and electronic payment systems. while certificateless public key cryptography eliminates the key escrow problem in identity public key cryptography. in this paper, we introduce the concept of partially blind signature into certificateless public key cryptography. the security model of certificateless partially blind signature scheme is defined. a certificateless partially blind signature scheme is presented as well. the scheme is proven existentially unforgeable against adaptive chosen message attacks under the computational diffie-hellman assumption.certificateless partially blind signatures','Digital signatures'
'we present a study of security in certificateless signatures. we divide potential adversaries according to their attack power, and for the first time, three new kinds of adversaries are introduced into certificateless signatures. they are normal adversary, strong adversary and super adversary (ordered by their attack power). combined with the known type i adversary and type ii adversary in certificateless cryptography, we then define the security of certificateless signatures in different attack scenarios. our new security models, together with others in the literature, provide a clear definition of the security in certificateless signatures. two concrete schemes with different security levels are also proposed in this paper. the first scheme, which is proven secure (in the random oracle model) against normal type i and super type ii adversaries, has the shortest signature length among all known certificateless signature schemes. the second scheme is secure (in the random oracle model) against super type i and type ii adversaries. compared with another scheme that has a similar security level, our second scheme requires less operational cost but a little longer signature length. two server-aided verification protocols are also proposed to reduce the verification cost on the verifier.certificateless signatures','Digital signatures'
'comments on two group signatures','Digital signatures'
'in this paper, a new notion which we call compact sequential aggregate signatures is introduced and formalized. informally, a compact sequential aggregate signature states the following thing: for a given message vector m=(m1, &#183; &#183; &#183; m&iota;), a public key vector pk=(pk1, &#183; &#183; &#183; , pk&iota;) and a path p=(v1, &#183; &#183; &#183; , v&iota;), where vi=(idi, pki), the size of the third component &sigma; in a sequential aggregate signature (m, p, &sigma;) is independent of the path length &iota; we propose a novel implementation of rsa-based regular signature scheme that works in an extended domain, and then transform it into a compact sequential aggregate signature scheme that works in a common domain such that the size of overflow bits is independent of the path length &iota; finally, we show that our implementation is provably secure in the random oracle model assuming that the rsa problem is hard.compact sequential aggregate signatures','Digital signatures'
'we consider conditional digital signatures (cds for short). according to this scheme a creator of a cds signature, say alice, signs a message m1 conditioned by a bob&#39;s signature of m2. the string created by alice can be transformed into an alice&#39;s digital signature of m1, once we are given a signature of m2 generated by bob. until the moment of creating a bob&#39;s signature of m2, alice&#39;s signature of m1 does not exist in a technical sense. this differs from the previous solutions where merely a condition about m2 has been included into a message signed by alice. the key feature of our scheme is that alice prepares the cds signature before bob actually signs m2. we propose two cds schemes &#8211; the first one prohibits checking that a signature of m1 has been prepared by alice until bob signs m2. in the second case, alice can prove interactively that the string created hides a cds signature of some form, but the proof is useless for a third party. we present applications of cds signatures in business and european legal frameworks. in particular, cds schemes can be used to build a system in which a signature can be retrieved at a given future date. this feature requires only an institution signing periodically the current time. the scheme is also quite useful for wireless mobile networks, where unreliability of communication may cause many problems. cds scheme may be used there for signing in advance even if a protocol requires a fixed sequential schedule.conditional digital signatures','Digital signatures'
'motivated by emerging needs in online interactions, we define a new type of digital signature called a \'content extraction signature\' (ces). a ces allows the owner, bob, of a document signed by alice, to produce an \'extracted signature\' on selected extracted portions of the original document, which can be verified (to originate from alice) by any third party cathy, without knowledge of the unextracted (removed) document portions. the new signature therefore achieves verifiable content extraction with minimal multi-party interaction. we specify desirable functional and security requirements from a ces (including an efficiency requirement: a ces should be more efficient in either computation or communication than the simple multiple signature solution). we propose and analyse four provably secure ces constructions which satisfy our requirements, and evaluate their performance characteristics.content extraction signatures','Digital signatures'
'this paper introduces a new concept called controllable ring signature which is ring signature with additional properties as follow. (1) anonymous identification: by an anonymous identification protocol, the real signer can anonymously prove his authorship of the ring signature to the verifier. and this proof is non-transferable. (2) linkable signature: the real signer can generate an anonymous signature such that every one can verify whether both this anonymous signature and the ring signature are generated by the same anonymous signer. (3) convertibility: the real signer can convert a ring signature into an ordinary signature by revealing the secret information about the ring signature. these additional properties can fully ensure the interests of the real signer. especially, compared with a standard ring signature, a controllable ring signature is more suitable for the classic application of leaking secrets. we construct a controllable ring signature scheme which is provably secure according to the formal definition.controllable ring signatures','Digital signatures'
'convertible group signatures','Digital signatures'
'group undeniable signatures are like ordinary group signatures except that verifying signatures needs the help of the group manager. in this paper, we propose a convertible group undeniable signature scheme in which the group manager can turn all or selective signatures, which are originally group undeniable signatures, into ordinary group signatures without compromising security of the secret key needed to generate signatures. the proposed scheme also allows the group manager to delegate the ability to confirm and deny to a limited set of parties without providing them the capability of generating signatures. for business applications, convertible group undeniable signatures can be widely used to validate price lists, press release or digital contracts when the signatures are commercially sensitive or valuable to a competitor. our scheme is unforgeable, signature-simulatable and coalition-resistant. the confirmation and denial protocols are also zero-knowledge. furthermore, the time, space and communication complexity are independent of the group size.convertible group undeniable signatures','Digital signatures'
'in the undeniable signatures, the validity or invalidity can only be verified via the confirmation/disavowal protocol with the help of the signer. convertible undeniable signatures provide the flexibility that a signer can convert an undeniable signature into publicly verifiable one. a proxy signature scheme allows an entity to delegate his/her signing capability to another entity in such a way that the latter can sign messages on behalf of the former when the former is not available. proxy signatures have found numerous practical applications in ubiquitous computing, distributed systems, mobile agent applications, etc. in this paper, we propose the first convertible undeniable proxy signature scheme with rigorously proven security. the properties of unforgeability, invisibility and soundness in the context of convertible undeniable proxy signatures are also clearly defined. the security of our construction is formally proven in the random oracle models, based on some natural complexity assumptions.convertible undeniable proxy signatures','Digital signatures'
'we introduce a new concept called convertible undeniable signature schemes. in these schemes, release of a single bit string by the signer turns all of his signatures, which were originally undeniable signatures, into ordinary digital signatures. we prove that the existence of such schemes is implied by the existence of digital signature schemes. then, looking at the problem more practically, we present a very efficient convertible undeniable signature scheme. this scheme has the added benefit that signatures can also be selectively converted.convertible undeniable signatures','Digital signatures'
'current signatures','Digital signatures'
'this paper describes a proxy signature scheme where a signer can delegate a partial signing right to a party who can then sign on behalf of the original signer to generate a partial proxy signature. a partial proxy signature can be converted into a full signature with the aid of the original signer. our proxy signature scheme has the feature of deniability, i.e., only the designated receiver can verify the partial proxy signature and the full signature associated to him, while they are not transferable. this paper also describes an application of our scheme in a deniable optimistic fair exchange.deniable partial proxy signatures','Digital signatures'
'in this paper, we describe a proxy signature scheme where a signer can delegate his signing right to a party who can then sign on behave of the original signer to generate a proxy signature. our proxy signature scheme possesses the features of deniability and anonymity. it allows the signer to send messages to another while the latter can not prove to a third party the fact of communication and insures no one can determine the identity of the proxy signer without the help of original signer. through the analysis, it shows our scheme meets correctness, deniability and other security requirements such as, anonymity, unforgeability and traceability.deniable proxy-anonymous signatures','Digital signatures'
'previous definitions of designated confirmer signatures in the literature are incomplete, and the proposed security definitions fail to capture key security properties, such as unforgeability against malicious confirmers and non-transferability. we propose new definitions. previous schemes rely on the random oracle model or set-up assumptions, or are secure with respect to relaxed security definitions. we construct a practical scheme that is provably secure with respect to our security definition under the strong rsa-assumption, the decision composite residuosity assumption, and the decision diffie-hellman assumption. to achieve our results we introduce several new relaxations of standard notions. we expect these techniques to be useful in the construction and analysis of other efficient cryptographic schemes.designated confirmer signatures revisited','Digital signatures'
'the concept of designated verifier signatures (dvs) was introduced by jakobsson, sako and impagliazzo at eurocrypt&#39;96. these signatures are intended to a specific verifier, who is the only one able to check their validity. in this context, we formalize the notion of privacy of signer&#39;s identity which captures the strong designated verifier property investigated in their paper. we propose a variant of the pairing-based dvs scheme introduced at asiacrypt&#39;03 by steinfeld, bull, wang and pieprzyk. contrary to their proposal, our new scheme can be used with any admissible bilinear map, especially with the low cost pairings and achieves the new anonymity property (in the random oracle model). moreover, the unforgeability is tightly related to the gap-bilinear diffie-hellman assumption, in the random oracle model and the signature length is around 75\% smaller than the original proposal.designated verifier signatures','Digital signatures'
'digital signatures to secure documents','Digital signatures'
'digital signatures today','Digital signatures'
'digital signatures vs. electronic signatures','Digital signatures'
'digital signatures','Digital signatures'
'distance signatures','Digital signatures'
'the physical analog of \"blind signatures\" of chaum is a document and a carbon paper put into an envelope, allowing the signer to transfer his signature onto the document by signing on the envelope, and without opening it. only the receiver can present the signed document while the signer cannot \"unblind\" its signature and get the document signed. when an authority signs \"access tokens\", \"electronic coins\", \"credentials\" or \"passports\", it makes sense to assume that whereas the users can typically enjoy the disassociation of the blindly signed token and the token itself (i.e. anonymity and privacy), there may be cases which require \"unblinding\" of a signature by the signing authority itself (to establish what is known as \"audit trail\" and to \"revoke anonymity\" in case of criminal activity). this leads us to consider a new notion of signature with the following physical parallel: the signer places a piece of paper with a carbon paper on top in an envelope as before (but the document on the paper is not yet written). the receiver then writes the document on the envelope using magic ink, e.g., ink that is only visible after being \"developed\". due to the carbon copy, this results in the document being written in visible ink on the internal paper. then, the signer signs the envelope (so its signature on the document is made available). the receiver gets the internal paper and the signer retains the envelope with the magic ink copy. should the signer need to unblind the document, he can develop the magic ink and get the document copy on the envelope. note that the signing is not blinded forever to the signer. we call this new type of signature a magic ink signature. we present an efficient method for distributively generating magic ink signatures, requiring a quorum of servers to produce a signature and a (possibly different) quorum to unblind a signature. the scheme is robust, and the unblinding is guaranteed to work even if a set of up to a threshold of signers refuses to cooperate, or actively cheats during either the signing or the unblinding protocol. we base our specific implementation on the dss algorithm. our construction demonstrates the extended power of distributed signing.distributed \"magic ink\" signatures','Digital signatures'
'in this paper, we introduce the abstraction of dual form signatures as a useful framework for proving security (existential unforgeability) from static assumptions for schemes with special structure that are used as a basis of other cryptographic protocols and applications. we demonstrate the power of this framework by proving security under static assumptions for close variants of pre-existing schemes: the lrsw-based camenisch-lysyanskaya signature scheme, and the identity-based sequential aggregate signatures of boldyreva, gentry, o\'neill, and yum. the camenisch-lysyanskaya signature scheme was previously proven only under the interactive lrsw assumption, and our result can be viewed as a static replacement for the lrsw assumption. the scheme of boldyreva, gentry, o\'neill, and yum was also previously proven only under an interactive assumption that was shown to hold in the generic group model. the structure of the public key signature scheme underlying the bgoy aggregate signatures is quite distinctive, and our work presents the first security analysis of this kind of structure under static assumptions.dual form signatures','Digital signatures'
'the problem of building privacy-preserving accountability systems is long-standing and has been extensively studied by the network research community. we observe that blind signatures have potential to form critical building blocks of network security protocols, where an authority needs to vouch for the legitimacy of a message but there is also a need to keep the ownership of the message secret from the authority. different forms of blind signature constructions exist in the literature and have found valuable use in areas such e-cash technology and e-voting schemes. however, conventional blind signatures are quite heavyweight and thus, a direct application of these traditional signatures face scalability and performance challenges. in this paper, we present a novel third-order linear feedback shift register (lfsr) sequence-based, 2-party signature scheme, egclfsr, following a well-known variant of the generalized elgamal signature scheme. using egclfsr, and following fundamentals of a well known blind signature, originally used for e-cash systems, we present an efficient blind signature bclfsr (also the first blind signature based on lfsr sequences), which can serve as a protocol building block for privacy-preserving accountability systems. we perform a theoretical analysis including correctness and security of bclfsr and also present a performance (computation and communication costs, storage overhead) comparison of the proposed scheme with well-known traditional constructions.efficient blind signatures for accountability','Digital signatures'
'this paper describes an efficient and strong authentication mechanism for ad hoc sensor networks. our protocol focuses on providing strong authentication and privacy for requests from query nodes to the network and for the corresponding responses. our scheme uses the asymmetrical energy consumption of the well known public key cryptosystems rsa and rabin. as the sensor nodes are assumed to be power-restrained, we only employ efficient public key operations at their side of the protocol, this leaves us only with the public operations encryption and signature verification. we have extended this set with a novel building block that allows nodes to sign messages cooperatively. we show that our protocol is robust against attacks from both outsiders and insiders.efficient cooperative signatures','Digital signatures'
'we propose a generic method to construct forward-secure signature schemes from standard signature schemes. the proposed construction is more computationally efficient than previously proposed schemes. in particular, the key updating operation in the proposed scheme is orders of magnitude more computationally efficient than previous schemes, making it attractive for a variety of applications, such as electronic checkbooks. another advantage of our proposed scheme is the ability to be easily extended to proxy signature schemes. we define two notions of forward-security in the proxy signature setup, namely, strong forward-secure proxy signatures and weak forward-secure proxy signatures. we then describe a construction of a scheme that satisfies the strong forward-secure proxy signature property.efficient generic forward-secure signatures and proxy signatures','Digital signatures'
'in this paper, we first propose an efficient provably secure identity-based signature (ibs) scheme based on bilinear pairings, then propose an efficient identity-based blind signature (ibbs) scheme based on our ibs scheme. assuming the intractability of the computational diffie-hellman problem, our ibs scheme is unforgeable under adaptive chosen-message and id attack. efficiency analyses show that our schemes can offer advantages in runtime over the schemes available. furthermore, we show that, contrary to the authors claimed, zhang and kim&#8217;s scheme in acisp 2003 is one-more forgeable, if the ros-problem is solvable.efficient identity-based signatures and blind signatures','Digital signatures'
'we consider the problem of revocation of identity in group signatures. group signatures are a very useful primitive in cryptography, allowing a member of a group to sign messages anonymously on behalf of the group. such signatures must be anonymous and unlinkable, but a group authority must be able to open them in case of dispute. many constructions have been proposed, some of them are quite efficient. however, a recurrent problem remains concerning revocation of group members. when misusing anonymity, a cheating member must be revoked by the authority, making him unable to sign in the future, but without sacrifying the security of past group signatures. no satisfactory solution has been given to completely solve this problem. in this paper, we provide the first solution to achieve such action for the camenish-stadler [6] scheme. our solution is efficient provided the number of revoked members remains small.efficient revocation in group signatures','Digital signatures'
'electronic commerce has changed the way we buy books, sell pez dispensers, and pay bills. now, with the advent of electronic and digital signatures, e-commerce is changing the way we sign and store documents. eventually, any business that wants to succeed in e-commerce must deal with electronic signatures. as an it professional, you need to understand: what a signature is, legally, and when you need one; the various electronic signature standards; current federal and state electronic signature legislation; and some basics about electronic signature technologyelectronic and digital signatures','Digital signatures'
'the advent of the internet saw technological innovations such as electronic signatures, in particular digital signatures, as an electronic equivalent to manuscript signatures in the online environment. however, the use of this technology is still insignificant. the aim of this paper is to review the various studies that have explored the technical and legal issues associated with electronic signatures and digital signatures with an objective to provide insights on their lack of acceptance.electronic signatures','Digital signatures'
'in this work, we propose a new generalization of the notion of group signatures, that allows signers to cover the entire spectrum from complete disclosure to complete anonymity. previous group signature constructions did not provide any disclosure capability, or at best a very limited one (such as subset membership). our scheme offers a very powerful language for disclosing exactly in what capacity a subgroup of signers is making a signature on behalf of the group.expressive subgroup signatures','Digital signatures'
'sanitizable signatures introduced by ateniese et al. is a powerful and fairly practical tool that enables an authorised party called the censor to modify designated parts of a signed message in an arbitrary way without interacting with the signer. in our paper we present several extensions of this paradigm that make sanitizable signatures even more useful. first of all we show how to limit the censor&#39;s abilities to modify mutable parts of a signed message to a predetermined set of strings. in our next proposal we show how to construct a scheme wherein the censor can insert an arbitrary string into a document, but this must be the same string in all designated places. we also present a construction based on a sanitizable signature that allows the censor to present only a constant number of versions of the sanitized message. another extension provides so-called strong transparency. in this case the verifier does not know which parts of the message could have been modified. finally, we point out new applications of sanitizable signatures based on combining them with time released cryptography techniques.extended sanitizable signatures','Digital signatures'
'fail-stop confirmer signatures','Digital signatures'
'this paper presents a formal model for fair blind signature schemes and a provably secure scheme based on bilinear maps. a blind signature scheme is a protocol for obtaining a signature on a message which is unknown from the signer. furthermore, the signer cannot link his transcript of a protocol to the resulting message-signature pair. fair blind signatures were introduced by stadler et al. at eurocrypt\'95 in [37]. a fair blind signature scheme is a blind signature scheme allowing two types of blindness revocation: link a signature to the session which conducted this signature (session tracing) or, conversely, identify a signature knowing a signing session (signature tracing). various fair blind signature schemes have been proposed in the past years, but none of them presents a secure fair blind signature scheme that allows polynomially many signatures to be securely issued, even if abe et al.\'s claimed it in [3]. in this paper, we first show a flaw in the blindness of most (fair) blind signature schemes where the signer is able to link signatures if he chooses his keys in an appropriate way. then, we show a flaw in the proof of unforgeability of abe et al.\' scheme and propose a stronger security model than theirs. it possesses all the needed properties for fair blind signature schemes: blindness, traceability and non frameability for both revocations (the one-more unforgeability is implied by these properties). finally, we describe a new fair blind signature scheme based on bilinear maps. this scheme thwarts the flaw against previous blind signatures and is proved secure in the random oracle model with respect to our model.fair blind signatures revisited','Digital signatures'
'a blind signature scheme is a protocol for obtaining a signature from a signer such that the signer\'s view of the protocol cannot be linked to the resulting message-signature pair. blind signature schemes are used in anonymous digital payment systems. since the existing proposals of blind signature schemes provide perfect unlinkability, such payment systems could be misused by criminals, e.g. to safely obtain a ransom or to launder money. in this paper, a new type of blind signature schemes called fair blind signature schemes is proposed. such schemes have the additional property that a trusted entity can deliver information allowing the signer to link his view of the protocol and the message-signature pair. two types of fair blind signature schemes are distinguished and several realizations are presented.fair blind signatures','Digital signatures'
'in this paper, we propose efficient fair blind (t,n) threshold signature schemes in wallet with observers. by these schemes, any t out of n signers in a group can represent the group to sign fair blind threshold signatures, which can be used in anonymous e-cash systems. since blind signature schemes provide perfect unlinkability, such e-cash systems can be misused by criminals, e.g. to safely obtain a ransom or to launder money. our schemes allow the judge (or the judges) to deliver information allowing anyone of the t signers to link his view of the protocol and the message signature pair.fair blind threshold signatures in wallet with observers','Digital signatures'
'in eurocrypt 2003, boneh et al. proposed verifiably encrypted signatures from the concept of aggregate signatures that support aggregation. such signatures enable verifiers to test that a given ciphertext is the encryption of a signature on a given message. verifiably encrypted signatures are used in fair exchange protocols of signatures. in this paper, we first show that boneh et al.\'s verifiably encrypted signature is not secure against rogue-key attacks. moreover, the fairness of fair exchange protocols of signatures with the adjudicator relies on the neutrality of the adjudicator, which has become a major practical hindrance to fair exchange protocols of signatures getting widely deployed. then we propose a fair exchange protocol of signatures from pairings by using aggregate signatures. we not only enhance the fair exchange protocol of signatures against three types of inside attackers but also relax the need of the trust in the adjudicator so that it only needs to be trusted by the signer.fair exchange protocol of signatures based on aggregate signatures','Digital signatures'
'it is well-known that blind signature schemes provide full anonymity for the receiving user. for many real-world applications, however, this leaves too much room for fraud. there are two generalizations of blind signature schemes that compensate this weakness: fair blind signatures and partially blind signatures. fair blind signature schemes allow a trusted third party to revoke blindness in case of a dispute. in partially blind signature schemes, the signer retains a certain control over the signed message because signer and user have to agree on a specific part of the signed message. in this work, we unify the previous well-studied models into a generalization, called fair partially blind signatures. we propose an instantiation that is secure in the standard model without any setup assumptions. with this construction, we also give a positive answer to the open question of whether fair blind signature schemes in the standard model exist.fair partially blind signatures','Digital signatures'
'in many applications a document needs to be signed by more than one signer. when a signature depends on more than one signer we call it a multi-signature. further, ordinary digital signatures have an inherent weakness: if the secret key is leaked, then all signatures, even the ones generated before the leak, are no longer trustworthy. forward-secure digital signatures were proposed to address this weakness, they ensure that the past signatures remain secure even if the current secret key is leaked. we propose to apply the concept of forward-security to multi-signatures. the basic signature scheme that we have considered is elgamal signature scheme which is based on discrete log problem. we initially make this signature scheme forward-secure and then apply it to multi-signatures. by this all signers of the document can guarantee the security of document signed in the past even if their secret key is exposed today. an adversary will not be able to forge a multi-signature unless the secret key of all the signers are compromised in the same time period, which is practically not possible. further, we propose two types of forward-secure multi-signatures : forward-secure parallel multi-signatures and forward-secure serial multi-signatures. once a user switches to use forward-secure signatures in place of ordinary signatures, he can easily extend it to use it as a multi-signature. in all applications where parallel or serial multi-signatures are used, the corresponding forward-secure multi-signatures can be used. forward-secure multi-signatures','Digital signatures'
'recently, a first step toward establishing foundations for group signatures was taken [5], with a treatment of the case where the group is static. however the bulk of existing practical schemes and applications are for dynamic groups, and these involve important new elements and security issues. this paper treats this case, providing foundations for dynamic group signatures, in the form of a model, strong formal definitions of security, and a construction proven secure under general assumptions. we believe this is an important and useful step because it helps bridge the gap between [5] and the previous practical work, and delivers a basis on which existing practical schemes may in future be evaluated or proven secure.foundations of group signatures','Digital signatures'
'a signature scheme is fully leakage resilient (katz and vaikuntanathan, asiacrypt \'09) if it is existentially unforgeable under an adaptive chosen-message attack even in a setting where an adversary may obtain bounded (yet arbitrary) leakage information on all intermediate values that are used throughout the lifetime of the system. this is a strong and meaningful notion of security that captures a wide range of side-channel attacks. one of the main challenges in constructing fully leakage-resilient signature schemes is dealing with leakage that may depend on the random bits used by the signing algorithm, and constructions of such schemes are known only in the random-oracle model. moreover, even in the random-oracle model, known schemes are only resilient to leakage of less than half the length of their signing key. in this paper we construct fully leakage-resilient signature schemes without random oracles. we present a scheme that is resilient to any leakage of length (1 - o(1))l bits, where l is the length of the signing key. our approach relies on generic cryptographic primitives, and at the same time admits rather efficient instantiations based on specific number-theoretic assumptions. in addition, we show that our approach extends to the continual-leakage model, recently introduced by dodis, haralambiev, lopez-alt andwichs (focs \'10), and by brakerski, tauman kalai, katz and vaikuntanathan (focs \'10). in this model the signing key is allowed to be refreshed, while its corresponding verification key remains fixed, and the amount of leakage is assumed to be bounded only in between any two successive key refreshes.fully leakage-resilient signatures','Digital signatures'
'we extend the idea of fuzzy signature to fuzzy rough signature (frs). the proposed fuzzy rough signature is capable of handling most kind of uncertainty: epistemic and random uncertainty, vagueness due to indiscernibility, and linguistic vagueness that exists in both large as well as small sample data sets. additionally, this system is capable of hierarchical organization of inputs and use of flexible aggregation selection will simplify the combinations of inputs from different sources.fuzzy rough signatures','Digital signatures'
'in 1990, boyar, chaum, damg&#229;rd and pedersen introduced &lt;em&gt;convertible undeniable signatures&lt;/em&gt;which limit the self-authenticating property of digital signatures but can be converted by the signer to ordinary signatures. michels, petersen and horster presented, in 1996, an attack on the elgamal-based seminal scheme of boyar &lt;em&gt;et al.&lt;/em&gt;and proposed a repaired version without formal security analysis. in this paper, we modify their protocol so that it becomes a generic one and it provides an advanced feature which permits the signer to universally convert &lt;em&gt;achronously&lt;/em&gt;all signatures pertaining to a specific time period. we supply a formal security treatment of the modified scheme: we prove, in the generic group model, that the protocol is existentially unforgeable and anonymous under chosen message attacks, assuming new assumptions (though reasonable) on the underlying hash function.gradually convertible undeniable signatures','Digital signatures'
'group blind digital signatures','Digital signatures'
'group signatures for hierarchical multigroups','Digital signatures'
'in this paper we present a new type of signature for a group of persons, called a group signature, which has the following properties: (i) only members of the group can sign messages; (ii) the receiver can verify that it is a valid group signature, but cannot discover which group member made it; (iii) if necessary, the signature can be \"opened\", so that the person who signed the message is revealed. the group signatures are a \"generalization\" of the credential/ membership authentication schemes, in which one person proves that he belongs to a certain group. we present four schemes that satisfy the properties above. not all these schemes arc based on the same cryptographic assumption. in some of the schemes a trusted centre is only needed during the setup; and in other schemes, each pason can create the group he belongs to.group signatures','Digital signatures'
'this paper introduces hidden identity-based signatures (hidden-ibs), a type of digital signatures that provide mediated signer-anonymity on top of shamir\'s identity-based signatures. the motivation of our new signature primitive is to resolve an important issue with the kind of anonymity offered by \"group signatures\" where it is required that either the group membership list is public or that the opening authority is dependent on the group manager for its operation. contrary to this, hidden-ibs do not require the maintenance of a group membership list and they enable an opening authority that is totally independent of the group manager. as we argue this makes hidden-ibs much more attractive than group signatures for a number of applications. in this paper, we provide a formal model of hidden-ibs as well as two efficient constructions that realize the new primitive. our elliptic curve construction that is based on the sdh/dldh assumptions produces signatures that are merely 4605 bits long and can be implemented very efficiently. to demonstrate the power of the new primitive, we apply it to solve a problem of current onion-routing systems focusing on the tor system in particular. posting through tor is currently blocked by sites such as wikipedia due to the real concern that anonymous channels can be used to vandalize online content. by injecting a hidden-ibs inside the header of an http post request and requiring the exit-policy of tor to forward only properly signed post requests, we demonstrate how sites like wikipedia may allow anonymous posting while being ensured that the recovery of (say) the ip address of a vandal would be still possible through a dispute resolution system. using our new hidden-ibs primitive in this scenario allows to keep the listing of identities (e.g., ip addresses) of tor users computationally hidden while maintaining an independent opening authority which would not have been possible with previous approaches.hidden identity-based signatures','Digital signatures'
'certificateless cryptography eliminates the key escrow problem in identity-based cryptography. hierarchical cryptography exploits a practical security model to mirror the organizational hierarchy in the real world. in this paper, to incorporate the advantages of both types of cryptosystems, we instantiate hierarchical certificate less cryptography by formalizing the notion of hierarchical certificate less signatures. furthermore, we propose an hcls scheme which, under the hardness of the computational diffie-hellman (cdh) problem, is proven to be existentially unforgeable against adaptive chosen-message attacks in the random oracle model. as to efficiency, our scheme has constant complexity, regardless of the depth of the hierarchy. hence, the proposal is secure and scalable for practical applications.hierarchical certificateless signatures','Digital signatures'
'we introduce the notion of hierarchical group signatures. this is a proper generalization of group signatures, which allows multiple group managers organized in a tree with the signers as leaves. when opening a signature a group manager only learns to which of its subtrees, if any, the signer belongs. we provide definitions for the new notion and construct a scheme that is provably secure given the existence of a family of trapdoor permutations. we also present a construction which is relatively practical, and prove its security in the random oracle model under the strong rsa assumption and the ddh assumption.hierarchical group signatures','Digital signatures'
'human motion is the composite consequence of multiple elements, including the action performed and a motion signature that captures the distinctive pattern of movement of a particular individual. we develop a new algorithm that is capable of extracting these motion elements and recombining them in novel ways. the algorithm analyzes motion data spanning multiple subjects performing different actions. the analysis yields a generative motion model that can synthesize new motions in the distinctive styles of these individuals. our algorithms can also recognize people and actions from new motions by comparing motion signaturesand action parameters.human motion signatures','Digital signatures'
'this paper describes new methods in pairing-based signature schemes for identifying the invalid digital signatures in a batch, after batch verification has failed. these methods efficiently identify non-trivial numbers of invalid signatures in batches of (potentially large) numbers of signatures. our methods use \"divide-and-conquer\" search to identify the invalid signatures within a batch, but prune the search tree to substantially reduce the number of pairing computations required. the methods presented in this paper require computing on average &lt;em&gt;o&lt;/em&gt; (&lt;em&gt;w&lt;/em&gt; ) products of pairings to identify &lt;em&gt;w&lt;/em&gt; invalid signatures within a batch of size &lt;em&gt;n&lt;/em&gt; , compared with the &lt;em&gt;o&lt;/em&gt; (&lt;em&gt;w&lt;/em&gt; (log2 (&lt;em&gt;n&lt;/em&gt; /&lt;em&gt;w&lt;/em&gt; ) + 1)) [for &lt;em&gt;w&lt;/em&gt; &lt; &lt;em&gt;n&lt;/em&gt; /2] that traditional divide-and-conquer methods require. our methods avoid the problem of &lt;em&gt;exponential growth&lt;/em&gt; in expected computational cost that affect earlier proposals which, on average, require computing &lt;em&gt;o&lt;/em&gt; (&lt;em&gt;w&lt;/em&gt; ) products of pairings. we compare the expected performance of our batch verification methods with previously published divide-and-conquer and exponential cost methods for cha-cheon identity-based signatures [6]. however, our methods also apply to a number of short signature schemes and as well as to other identity-based signature schemes. identification of multiple invalid signatures in pairing-based batched signatures','Digital signatures'
'an aggregate signature is a single short string that convinces any verifier that, for all 1 &#8804; i &#8804; n, signer si signed message mi, where the n signers and n messages may all be distinct. the main motivation of aggregate signatures is compactness. however, while the aggregate signature itself may be compact, aggregate signature verification might require potentially lengthy additional information &#8211; namely, the (at most) n distinct signer public keys and the (at most) n distinct messages being signed. if the verifier must obtain and/or store this additional information, the primary benefit of aggregate signatures is largely negated. this paper initiates a line of research whose ultimate objective is to find a signature scheme in which the total information needed to verify is minimized. in particular, the verification information should preferably be as close as possible to the theoretical minimum: the complexity of describing which signer(s) signed what message(s). we move toward this objective by developing identity-based aggregate signature schemes. in our schemes, the verifier does not need to obtain and/or store various signer public keys to verify; instead, the verifier only needs a description of who signed what, along with two constant-length &#8220;tags&#8221;: the short aggregate signature and the single public key of a private key generator. our scheme is secure in the random oracle model under the computational diffie-hellman assumption over pairing-friendly groups against an adversary that chooses its messages and its target identities adaptively.identity-based aggregate signatures','Digital signatures'
'image signatures are generated from the comparison of segments contained within an image to a database of segments collected over a large variety of images. it is impossible to retain all of the segments from all of the images so the segments are clustered becomes an image primitive as each cluster contains a unique set of similar segments. the size of the image signature is nk where n is the number of segments and k is the number of clusters. these numbers are significantly smaller than the dimensions of the image and so a signature is a condensed representation of the contents of the image.image primitive signatures','Digital signatures'
'signature files provide an efficient access method for text in documents, but retrieval is usually limited to finding documents that contain a specified boolean pattern of words. effective retrieval requires that documents with similar meanings be found through a process of plausible inference. the simplest way of implementing this retrieval process is to rank documents in order of their probability of relevance. in this paper techniques are described for implementing probabilistic ranking strategies with sequential and bit-sliced signature tiles and the limitations of these implementations with regard to their effectiveness are pointed out. a detailed comparison is made between signature-based ranking techniques and ranking using term-based document representatives and inverted files. the comparison shows that term-based representations are at least competitive (in terms of efficiency) with signature files and, in some situations, superior.implementing ranking strategies using text signatures','Digital signatures'
'it is well known that excessive computational demands of public key cryptography have made its use limited especially when constrained devices are of concern. to reduce the costs of generating public key signatures one viable method is to employ a third party; the server. in open networks, getting help from a verifiable-server has an advantage over proxy-based solutions since as opposed to proxy-server, verifiable-server\'s cheating can be proven. verifiable-server assisted signatures were proposed in the past but they could not totally eliminate public key operations for the signer. in this paper, we propose a new alternative called saots (server assisted one-time signatures) where just like proxy signatures generating a public key signature is possible without performing any public key operations at all. this feature results in both computational efficiency and implementation simplicity (e.g. a reduction in the code size) of the proposed protocol. in addition, saots is a more promising approach since the signature is indistinguishable from a standard signature, no storage is necessary for the signer to prove the server\'s cheating and the protocol works in less number of rounds (two instead of three). on the other hand, the drawback of saots is the increased bandwidth requirement between the sender and server.improved server assisted signatures','Digital signatures'
'this paper describes a new approach to indexing time series. indexing time series is more problematic than indexing text since in extreme we need to find all possible subsequences in the time series sequence. we propose to use signature files to index the time series and we also propose a new method to index the signature files to speed up the search of large time series. we propose a novel index structure, the signature tree, for time series indexing. we implemented the signature tree and we discuss its performance.indexing time series using signatures','Digital signatures'
'signer-base intrusion-resilient (sibir) signature schemes were defined in [ir02]. in this model, as in the case of forward security, time is divided into predefined time periods (e.g., days); each signature includes the number of the time period in which it was generated; while the public key remains the same, the secret keys evolve with time. in addition, in sibir model, the user has two modules, signer and home base: the former generates all signatures on its own, and the latter is needed only to help update the signer\'s key from one time period to the next. the main strength of the intrusion-resilient schemes, is that they remain secure even after arbitrarily many compromises of both modules, as long as the compromises are not simultaneous. moreover, even if the intruder does compromise both modules simultaneously, she will still be unable to generate any signatures for the previous time periods (i.e., the forward security is guaranteed even in the case of simultaneous exposures). this paper provides the first generic implementation, called gsibir, of the intrusion-resilient signature schemes: it can be based on any ordinary signature scheme used as a black-box. gsibir is also the first sibir scheme secure against fully-adaptive adversary and does not require random oracle. our construction does require one-way (and cryptographic hash) functions. another contribution of this paper is a new mechanism extending tree-based constructions such as gsibir and that of [bm99] to avoid the limit on the total number of periods (required by [ir02] and many forward-secure ones). this mechanism is based on explicit use of prefixless (or self-delimiting) encodings. applied to the generic forward-secure singature constructions of [bm99, mmm02], it extends the first and yields modest but noticable improvements to the second. with this mechanism, gsibir becomes the first generic intrusion-resilient signature scheme with no limit on the number of periods.intrusion-resilient signatures','Digital signatures'
'we provide an alternative method for constructing lattice-based digital signatures which does not use the \"hash-and-sign\" methodology of gentry, peikert, and vaikuntanathan (stoc 2008). our resulting signature scheme is secure, in the random oracle model, based on the worst-case hardness of the &#213;(n1.5)-sivp problem in general lattices. the secret key, public key, and the signature size of our scheme are smaller than in all previous instantiations of the hash-and-sign signature, and our signing algorithm is also quite simple, requiring just a few matrix-vector multiplications and rejection samplings. we then also show that by slightly changing the parameters, one can get even more efficient signatures that are based on the hardness of the learning with errors problem. our construction naturally transfers to the ring setting, where the size of the public and secret keys can be significantly shrunk, which results in the most practical to-date provably secure signature scheme based on lattices.lattice signatures without trapdoors','Digital signatures'
'group signatures are a useful cryptographic construct for privacy-preserving non-repudiable authentication, and there have been many group signature schemes. in this paper, we introduce a variant of group signatures that offers two new security properties called leak-freedom and immediate-revocation. intuitively, the former ensures that an insider (i.e., an authorized but malicious signer) be unable to convince an outsider (e.g., a signature receiver) that she indeed signed a certain message; whereas the latter ensures that the authorization for a user to issue group signatures can be immediately revoked whenever the need arises (temporarily or permanently). these properties are not offered in existing group signature schemes, nor captured by their security definitions. however, these properties might be crucial to a large class of enterprise-centric applications because they are desirable from the perspective of the enterprises who adopt group signatures or are the group signatures liability-holders (i.e., will be held accountable for the consequences of group signatures). in addition to introducing these new security properties, we present a scheme that possesses both traditional and these newly introduced properties. our scheme is constructed using an architectural approach where a mediation server is exploited to trade on-line communications for the extra security properties, which explains why the resulting scheme is called &#8220;leak-free mediated group signatures&#8221;.leak-free mediated group signatures','Digital signatures'
'redactable signatures for linear-structured data such as strings have already been studied in the literature. in this paper, we propose a formal security model for leakage-free redactable signatures (lfrs) that is general enough to address authentication of not only trees but also graphs and forests. lfrs schemes have several applications, especially in enabling secure data management in the emerging cloud computing paradigm as well as in healthcare, finance and biological applications. we have also formally defined the notion of secure names. such secure names facilitate leakage-free verification of ordering between siblings/nodes. the paper also proposes a construction for secure names, and a construction for leakagefree redactable signatures based on the secure naming scheme. the proposed construction computes a linear number of signatures with respect to the size of the data object, and outputs only one signature that is stored, transmitted and used for authentication of any tree, graph and forest.leakage-free redactable signatures','Digital signatures'
'we design a proxy signature in the setting of leakage-resilient cryptography for the first time. our motivation is that the proxy signatures are often used in scenarios where the signing is done in an insecure environment, e.g., the proxy key is stored in a device which is not secure enough example of ic card. in such setting, an adversary who can launch side-channel attacks may detect some leakage information about the proxy key. the traditional security models of the proxy signatures cannot provide such security. hence we extend them to the leakage-resilient setting, and also give a concrete construction which conforms to the extended security model.leakage-resilient proxy signatures','Digital signatures'
'the strongest standard security notion for digital signature schemes is unforgeability under chosen message attacks. in practice, however, this notion can be insufficient due to &#8220;side-channel attacks&#8221; which exploit leakage of information about the secret internal state. in this work we put forward the notion of &#8220;leakage-resilient signatures,&#8221; which strengthens the standard security notion by giving the adversary the additional power to learn a bounded amount of arbitrary information about the secret state that was accessed during every signature generation. this notion naturally implies security against all side-channel attacks as long as the amount of information leaked on each invocation is bounded and &#8220;only computation leaks information.&#8221; the main result of this paper is a construction which gives a (tree-based, stateful) leakage-resilient signature scheme based on any 3-time signature scheme. the amount of information that our scheme can safely leak per signature generation is 1/3 of the information the underlying 3-time signature scheme can leak in total. signature schemes that remain secure even if a bounded total amount of information is leaked were recently constructed, hence instantiating our construction with these schemes gives the first constructions of provably secure leakage-resilient signature schemes. the above construction assumes that the signing algorithm can sample truly random bits, and thus an implementation would need some special hardware (randomness gates). simply generating this randomness using a leakage-resilient stream-cipher will in general not work. our second contribution is a sound general principle to replace uniform random bits in any leakage-resilient construction with pseudorandom ones: run two leakage-resilient stream-ciphers (with independent keys) in parallel and then apply a two-source extractor to their outputs.leakage-resilient signatures','Digital signatures'
'we propose signature linear discriminant analysis (signature-lda) as an extension of lda that can be applied to signatures, which are known to be more informative representations of local image features than vector representations, such as visual word histograms. based on earth mover\'s distances between signatures, signature-lda does not require vectorization of local image features in contrast to lda, which is one of the main limitations of classical lda. therefore, signature-lda minimizes the loss of intrinsic information of local image features while selecting more discriminating features using label information. empirical evidence on texture databases shows that signature-lda improves upon state-of-the-art approaches for texture image classification and outperforms other feature selection methods for local image features.linear discriminant analysis for signatures','Digital signatures'
'in a variety of group-oriented applications cryptographic primitives like group signatures or ring signatures are valuable methods to achieve anonymity of group members. however, in their classical form, these schemes cannot be deployed for applications that simultaneously require (i) to avoid centralized management authority like group manager and (ii) the signer to be anonymous only against non-members while group members have rights to trace and identify the signer. the idea of recently introduced democratic group signatures is to provide these properties. based on this idea we introduce a group-oriented signature scheme that allows the group members to trace the identity of any other group member who issued a signature while non-members are only able to link the signatures issued by the same signer without tracing. for this purpose the signature scheme assigns to every group member a unique pseudonym that can be used by any non-member verifier to communicate with the anonymous signer from the group. we present several group-oriented application scenarios where this kind of linkability is essential. we propose a concrete linkable democratic group signature scheme for two-parties, prove its security in the random oracle model, and describe how to modularly extend it to the multi-party case.linkable democratic group signatures','Digital signatures'
'a ring signature scheme is a group signature scheme but with no group manager to setup a group or revoke a signer&#8217;s identity. the formation of a group is spontaneous in the way that diversion group members can be totally unaware of being conscripted to the group. it allows members of a group to sign messages on the group&#8217;s behalf such that the resulting signature does not reveal their identity (anonymity). the notion of linkable ring signature, introduced by liu, et al. [10], also provides signer anonymity, but at the same time, allows anyone to determine whether two signatures have been issued by the same group member (linkability). in this paper, we enhance the security model of [10] for capturing new and practical attacking scenarios. we also propose two polynomial-structured linkable ring signature schemes. both schemes are given strong security evidence by providing proofs under the random oracle model.linkable ring signatures','Digital signatures'
'we present a new technique of universe reduction. primary applications are the dictionary problem and the predecessor problem. we give several new results on static dictionaries in different computational models: the word ram, the practical ram, and for strings - the cache-oblivious model. all algorithms and data structures are deterministic and use linear space. representative results are: a dictionary with a lookup time of o(log log n) and construction time of o(n log log n) on a word ram, and a static predecessor structure for variable-length binary strings with a query performance of o(|s|/b + log |s| + log log n) i/os, for query argument s, in the cache-oblivious model.making deterministic signatures quickly','Digital signatures'
'we present a general implementation for providing the properties of digital signatures using macs in a system consisting of any number of untrusted clients and &lt;em&gt;n&lt;/em&gt;servers, up to &lt;em&gt;f&lt;/em&gt;of which are byzantine. at the heart of the implementation is a novel &lt;em&gt;matrix signature&lt;/em&gt;that captures the collective knowledge of the servers about the authenticity of a message. matrix signatures can be generated or verified by the servers in response to client requests and they can be transmitted and exchanged between clients independently of the servers. the implementation requires that no more than one third of the servers be faulty, which we show to be optimal. the implementation places no synchrony requirements on the communication and only require fair channels between clients and servers.matrix signatures','Digital signatures'
'legal information certification and secured storage combined with documents electronic signature are of great interest when digital documents security and conservation are in concern. therefore, these new and evolving technologies offer powerful abilities, such as identification, authentication and certification. the latter contribute to increase the global security of legal digital archives conservation and access. however, currently used cryptographic and hash coding concepts cannot intrinsically enclose cognitive information about both the signer and the signed content. indeed, an evolution of these technologies may be necessary to achieve full text researches within hundreds or thousands of electronically signed documents. this article aims at describing a possible model along with associated processes to create and make use of these new electronic signatures called \"meaningful electronic signatures\" as opposed to traditional electronic signatures based on bit per bit computation.meaningful electronic signatures based on an automatic indexing method','Digital signatures'
'we define the mesh signature primitive as an anonymous signature similar in spirit to ring signatures, but with a much richer language for expressing signer ambiguity. the language can represent complex access structures, and in particular allows individual signature components to be replaced with complete certificate chains. because withholding one\'s public key from view is no longer a shield against being named as a possible cosignatory, mesh signatures may be used as a ring signature with compulsory enrollment.we give an efficient construction based on bilinear maps in the common random string model. our signatures have linear size, achieve everlasting perfect anonymity, and reduce to very efficient ring signatures without random oracles as a special case. we prove non-repudiation from a mild extension of the sdh assumption, which we introduce and justify meticulously.mesh signatures','Digital signatures'
'in many real-life situations, massive quantities of signatures have to be issued on cheap passive supports (e.g. paper-based) such as bank-notes, badges, id cards, driving licenses or passports (hereafter ids); while large-scale id replacements are costly and prohibitive, one may reasonably assume that the updating of verification equipment (e.g. off-line border checkpoints or mobile patrol units) is exceptionally acceptable.in such a context, an attacker using coercive means (e.g. kidnapping) can force the system authorities to reveal the infrastructure\'s secret signature keys and start issuing signatures that are indistinguishable from those issued by the authority.the solution presented in this paper withstands such attacks up to a certain point: after the theft, the authority restricts the verification criteria (by an exceptional verification equipment update) in such a way that the genuine signatures issued before the attack become easily distinguishable from the fresher signatures issued by the attacker.needless to say, we assume that at any point in time the verification algorithm is entirely known to the attacker.monotone signatures','Digital signatures'
'the concept of concurrent signatures was introduced by chen, kudla and paterson at eurocrypt 2004. in a concurrent signature scheme, users sign their messages in an ambiguous way so that the signatures are only verifiable by the users themselves but not by any other outsiders. at a later stage, one of the users releases an extra bit of information called the keystone, then all the signatures become binding to their signers concurrently. at this stage, any outsider can verify the signatures. chen, kudla and paterson proposed a concurrent signature scheme for two users. recently, susilo and mu constructed a scheme for three users. it is an open problem to construct concurrent signature schemes for multi users. in this paper, we answer this open problem affirmatively. using techniques of ring signatures and bilinear pairings, for the first time we construct a concurrent signature scheme for multi-users.multi-party concurrent signatures','Digital signatures'
'multi-verifier signatures generalize public-key signatures to a secret-key setting. just like public-key signatures, these signatures are both transferable and secure under arbitrary (unbounded) adaptive chosen-message attacks. in contrast to public-key signature schemes, however, we exhibit practical constructions of multi-verifier signature schemes that are provably secure and are based only on pseudorandom functions in the plain model without any random oracles.multi-verifier signatures','Digital signatures'
'the study of non-transferability of digital signatures, such as confirmer signatures, has enjoyed much interest over the last twenty years. in pkc \'08, liskov and micali noted that all previous constructions of confirmer signatures consider only offline untransferability -- nontransferability is not preserved if the recipient interacts concurrently with the signer/confirmer and an unexpected verifier. we view this as a result of all these schemes being interactive in the confirmation step. in this paper, we introduce the concept of non-interactive confirmer signatures (which can also be interpreted as extractable universal design-atedverifier signatures). non-interactive confirmer signatures give a neat way to ensure the online untransferability of signatures. we realize our notion under the \"encryption of a signature\" paradigm using pairings and provide a security proof for our construction without random oracles.non-interactive confirmer signatures','Digital signatures'
'antipatterns are poor object-oriented solutions to recurring design problems. the identification of occurrences of antipatterns in systems has received recently some attention but current approaches have two main limitations: either (1) they classify classes strictly as being or not antipatterns, and thus cannot report accurate information for borderline classes, or (2) they return the probabilities of classes to be antipatterns but they require an expensive tuning by experts to have acceptable accuracy. to mitigate such limitations, we introduce a new identification approach, abs (antipattern identification using b-splines), based on a numerical analysis technique. the results of a preliminary study show that abs generally outperforms previous approaches in terms of accuracy when used to identify blobs.numerical signatures of antipatterns','Digital signatures'
'oblivious signatures','Digital signatures'
'ancient graphical documents are invaluable heritages which have been handed down since generations. they possess both intellectual and spiritual worth for humanity. in this context, many digitization processes have been started, producing very large warehouse of images. these huge amount of data raise the problem of indexing the information in order to make easier navigation in the databases. in the context of a french research program, called madonne, this paper proposes a set of complementary contributions concerning ancient graphic images indexing.on defining signatures for the retrieval and the classification of graphical drop caps','Digital signatures'
'on digital signatures','Digital signatures'
'this paper specializes the signature forgery by coron, naccache and stern (1999) to rabin-type systems. we present a variation in which the adversary may derive the private keys and thereby forge the signature on any chosen message. further, we demonstrate that, contrary to the rsa, the use of larger (even) public exponents does not reduce the complexity of the forgery. finally, we show that our technique is very general and applies to any rabin-type system designed in a unique factorization domain, including the williams\' m3 scheme (1986), the cubic schemes of loxton et al. (1992) and of scheidler (1998), and the cyclotomic schemes (1995).on rabin-type signatures','Digital signatures'
'in a traditional signature scheme, a signature &#963; on a message m is issued under a public key pk, and can be interpreted as follows: &#8220;the owner of the public key pk and its corresponding secret key has signed message m.&#8221; in this paper we consider schemes that allow one to issue signatures on behalf of any np statement, that can be interpreted as follows: &#8220;a person in possession of a witness w to the statement that x &#8712;l has signed message m.&#8221; we refer to such schemes as signatures of knowledge. we formally define the notion of a signature of knowledge. we begin by extending the traditional definition of digital signature schemes, captured by canetti&#39;s ideal signing functionality, to the case of signatures of knowledge. we then give an alternative definition in terms of games that also seems to capture the necessary properties one may expect from a signature of knowledge. we then gain additional confidence in our two definitions by proving them equivalent. we construct signatures of knowledge under standard complexity assumptions in the common-random-string model. we then extend our definition to allow signatures of knowledge to be nested i.e., a signature of knowledge (or another accepting input to a uc-realizable ideal functionality) can itself serve as a witness for another signature of knowledge. thus, as a corollary, we obtain the first delegatable anonymous credential system, i.e., a system in which one can use one&#39;s anonymous credentials as a secret key for issuing anonymous credentials to others.on signatures of knowledge','Digital signatures'
'non-transferability of digital signatures is an important security concern, traditionally achieved via interactive verification protocols. such protocols, however, are vulnerable to \"online transfer attacks\" -- i.e., attacks mounted during the protocols\' executions. in this paper, we show how to guarantee online untransferability of signatures, via a reasonable public-key infrastructure and general assumptions, without random oracles. our untransferable signatures are as efficient as prior ones that provably provide weaker types of untransferability.online-untransferable signatures','Digital signatures'
'structure preservation captures the notion of pairing-based schemes that rely on generic group operations and where the components are group elements. their structural properties make it easy to compose them with other pairing-based schemes. in this talk, we will take a closer look at structure-preserving signatures. the structure preserving property allows us to analyze the efficiency of signature schemes in the generic group model. using the generic group model to analyze the efficiency of a cryptographic scheme stands in contrast to the more common usage of the generic group model to rule out certain types of attack. we will show that structure-preserving signatures need to consist of at least 3 group elements. we also discuss recent constructions of structure-preserving signatures that consist of 3 group elements. these constructions match our lower bounds, thus giving us provably optimal structure-preserving signatures.optimal structure-preserving signatures','Digital signatures'
'this work provides the essential foundations for modular construction of (typed) unification grammars for natural languages. much of the information in such grammars is encoded in the signature, and hence the key is facilitating a modularized development of type signatures. we introduce a definition of signature modules and show how two modules combine. our definitions are motivated by the actual needs of grammar developers obtained through a careful examination of large scale grammars. we show that our definitions meet these needs by conforming to a detailed set of desiderata.partially specified signatures','Digital signatures'
'we present a digital signature scheme where users sign by using a password instead of a long secret key. our approach uses a signing server to prevent dictionary attacks. we present two efficient and secure schemes, both based on blind signatures. our schemes are resistant against dictionary attacks from anyone except the signing server.password-based signatures','Digital signatures'
'in this paper, we explore the use of the diffusion geometry framework for the fusion of geometric and photometric information in local heat kernel signature shape descriptors. our construction is based on the definition of a diffusion process on the shape manifold embedded into a high-dimensional space where the embedding coordinates represent the photometric information. experimental results show that such data fusion is useful in coping with different challenges of shape analysis where pure geometric and pure photometric methods fail.photometric heat kernel signatures','Digital signatures'
'in this paper, we present a new cryptographic primitive called &#8220;policy-controlled signature&#8221;. in this primitive, a signer can sign a message and attach some policies to it. only a verifier who satisfies the policies attached can verify the authenticity of the message. this type of signature schemes has many applications, in particular to deal with sensitive data, where the signer does not want to allow anyone who is not authorized to verify its authenticity. nonetheless, there is no existing cryptographic primitives that can offer this feature in the literature. policy-controlled signatures can be seen to be similar to the notion of designated verifier signatures, as it can also be used to designate a signature to multiple recipients. when there is only a single attribute involved in a policy presented by a verifier, then we will achieve a designated verifier signature (with some trivial modifications). therefore, policy-controlled signatures can be viewed as the generalization of the notion of the designated verifier signatures. we present a formal model to capture this notion. furthermore, we also present a concrete scheme that is secure in our model. finally, we briefly mention about an implementation that incorporates p3p to realize policy-controlled signatures.policy-controlled signatures','Digital signatures'
'there are important details that give legal validity to handwritten signatures: first, the document to be signed is under control of the signatory and it is not possible to substitute or alter it, and second, the tools to produce the signature (the pen and the signatory itself) are also under control of the signatory. these details make possible that handwritten signatures are used in a law court to prove the willingness of the signatory to be bound by the content of the document. digital signatures require complex calculations that can not be done using mental arithmetic by the signatory. in this case neither document nor tools are under direct control of the signatory but under control of a computer. consequently, the willingness of the signatory can not be sufficiently demonstrated. furthermore, to be able to perform digital signatures, we must assume that the user trusts the computer to perform exactly what is intended. this yields digital signatures unusable in scenarios that require mobility. in this paper we present a system to perform digital signatures in environments that require mobility. the system is based on the use of personal digital assistants and smart cards and fulfils the common requirements established in different national laws regarding digital signatures.practical mobile digital signatures','Digital signatures'
'we present an rsa threshold signature scheme. the scheme enjoys the following properties: 1. it is unforgeable and robust in the random oracle model, assuming the rsa problem is hard; 2. signature share generation and verification is completely non-interactive; 3. the size of an individual signature share is bounded by a constant times the size of the rsa modulus.practical threshold signatures','Digital signatures'
'very strong definitions of security for signature schemes have been proposed in the literature. constructions for such schemes have been proposed, but so far they have only been of theoretical interest and have been considered far too inefficient for practical use.here we present a new scheme that satisfies these strongest definitions and uses essentially the same amount of computation and memory as the widely applied rsa scheme. the scheme is based on the well known rsa assumption.our signatures can be thought of as products resulting from a two-dimensional lamport scheme, where one dimension consists of a list of public constants, and the other is the sequence of odd primes.provably unforgeable signatures','Digital signatures'
'the undeniable signature, introduced by chaum et al. in 1989, provides a nice property that the signer has an additional control over who will benefit from being convinced by the signature. however, a conspicuous drawback of undeniable signature is that the signer may be unavailable or refuse to cooperate. chaum in 1994 proposed a designated confirmer signature scheme to protect the recipient\'s right. there exists a confirmer, who can always help the recipient prove the validity of the signature to others. unfortunately, chaum\'s paper did not consider that a malicious confirmer proves the validity of the signature to any persons as his will or even leaks the sensitive information to the signer\'s enemies. this paper proposes a new signature scheme called proxy confirmation signature where the proxy confirmer can only acquire a temporary proxy confirmation capability instead of a perpetual one from the signer. that is, the signer not only can delegate the confirmation capability to the proxy confirmer, but also can revoke the proxy confirmer\'s capability for avoiding the abuse. moreover, our scheme also provides a technique to properly restrict the proxy confirmer to convincing only some specified verifiers that the signature is valid.proxy confirmation signatures','Digital signatures'
'in 1998, blaze, bleumer, and strauss (bbs) proposed proxy re-signatures, in which a semi-trusted proxy acts as a translator between alice and bob. to translate, the proxy converts a signature from alice into a signature from bob on the same message. the proxy, however, does not learn any signing key and cannot sign arbitrary messages on behalf of either alice or bob. since the bbs proposal, the proxy re-signature primitive has been largely ignored, but we show that it is a very useful tool for sharing web certificates, forming weak group signatures, and authenticating a network path.we begin our results by formalizing the definition of security for a proxy re-signature. we next substantiate the need for improved schemes by pointing out certain weaknesses of the original bbs proxy re-signature scheme which make it unfit for most practical applications. we then present two secure proxy re-signature schemes based on bilinear maps. our first scheme relies on the computational diffie-hellman (cdh) assumption; here the proxy can translate from alice to bob and vice-versa. our second scheme relies on the cdh and 2-discrete logarithm (2-dl) assumptions and achieves a stronger security guarantee -- the proxy is only able to translate in one direction. constructing such a scheme has been an open problem since proposed by bbs in 1998. furthermore in this second scheme, even if the delegator and the proxy collude, they cannot sign on behalf of the delegatee. both schemes are efficient and secure in the random oracle model.proxy re-signatures','Digital signatures'
'proxy signatures, revisited','Digital signatures'
'we develop a three-level hierarchy of privacy notions for (unforgeable) digital signature schemes. we first prove mutual independence of existing notions of anonymity and confidentiality, and then show that these are implied by higher privacy goals. the top notion in our hierarchy is pseudorandomness: signatures with this property hide the entire information about the signing process and cannot be recognized as signatures when transmitted over a public network. this implies very strong unlinkability guarantees across different signers and even different signing algorithms, and gives rise to new forms of private public-key authentication. we show that one way towards pseudorandom signatures leads over our mid-level notion, called indistinguishability: such signatures can be simulated using only the public parameters of the scheme. as we reveal, indistinguishable signatures exist in different cryptographic settings (e.g. based on rsa, discrete logarithms, pairings) and can be efficiently lifted to pseudorandomness deploying general transformations using appropriate encoding techniques. we also examine a more direct way for obtaining pseudorandomness for any unforgeable signature scheme. all our transformations work in the standard model. we keep public verifiability of signatures in the setting of system-wide known public keys. some results even hold if signing keys are disclosed to the adversary --- given that signed messages have high entropy.pseudorandom signatures','Digital signatures'
'courtois-finiasz-sendrier (cfs) digital signatures critically depend on the ability to efficiently find a decodable syndrome by random sampling the syndrome space, previously restricting the class of codes upon which they could be instantiated to generic binary goppa codes. in this paper we show how to construct t-error correcting quasi-dyadic codes where the density of decodable syndromes is high, while also allowing for a reduction by a factor up to t in the key size.quasi-dyadic cfs signatures','Digital signatures'
'the common n-gram (cng) classifier is a text classification algorithm based on the comparison of frequencies of character n-grams (strings of characters of length n) that are the most common in the considered documents and classes of documents. we present a text analytic visualization system that employs the cng approach for text classification and uses the differences in frequency values of common n-grams in order to visually compare documents at the sub-word level. the visualization method provides both an insight into n-gram characteristics of documents or classes of documents and a visual interpretation of the workings of the cng classifier.relative n-gram signatures','Digital signatures'
'in many applications of group signatures, not only a signer\'s identity but also which group the signer belongs to is sensitive information regarding signer privacy. in this paper, we study these applications and combine a group signature with a ring signature to create a ring group signature, which specifies a set of possible groups without revealing which member of which group produced the signature. the main contributions of this paper are a formal definition of a ring group signature scheme and its security model, a generic construction and a concrete example of such a scheme. both the construction and concrete scheme are provably secure if the underlying group signature and ring signature schemes are secure.ring group signatures','Digital signatures'
'though anonymity of ring signature schemes has been studied in many literatures for a long time, these papers showed different definitions and there is no consensus. recently, bender et al. proposed two new anonymity definitions of ring signature which is stronger than the traditional definition, that are called anonymity against attribution attacks/full key exposure. also, ring signature schemes have two levels of unforgeability definitions, i.e., existential un-forgeability (euf) and strong existential unforgeability (suf). in this paper, we will redefine anonymity and unforgeability definitions from the standpoint of universally composable (uc) security framework. first, we will formulate new ideal functionalities of ring signature schemes for each security levels separately. next, we will show relations between cryptographic security definitions and our uc definitions. finally, we will give another proof of the bender et al.\'s ring signature scheme following the uc secure definition by constructing a simulator to an adversary of suf, which can be adaptable to the case of suf under the assumption of a standard single suf signature scheme.ring signatures','Digital signatures'
'we present threshold dss (digital signature standard) signatures where the power to sign is shared by n players such that for a given parameter t &lt; n/2 any subset of 2t + 1 signers can collaborate to produce a valid dss signature on any given message, but no subset of t corrupted players can forge a signature (in particular, cannot learn the signature key). in addition, we present a robust threshold dss scheme that can also tolerate n/3 players who refuse to participate in the signature protocol. we can also endure n/4 maliciously faulty players that generate incorrect partial signatures at the time of signature computation. this results in a highly secure and resilient dss signature system applicable to the protection of the secret signature key, the prevention of forgery, and increased system availability. our results significantly improve over a recent result by langford from crypto\'95 that presents threshold dss signatures which can stand much smaller subsets of corrupted players, namely, t &#8773; &#8730;n, and do not enjoy the robustness property. as in thc case of langford\'s result, our schemes require no trusted party. our techniques apply to other threshold elgamal-like signatures as well. we prove the security of our schemes solely based on the hardness of forging a regular dss signature.robust threshold dss signatures','Digital signatures'
'constructing round-optimal blind signatures in the standard model has been a long standing open problem. in particular, fischlin and schr&#246;der recently ruled out a large class of three-move blind signatures in the standard model (eurocrypt\'10). in particular, their result shows that finding security proofs for the well-known blind signature schemes by chaum, and by pointcheval and stern in the standard model via black-box reductions is hard. in this work we propose the first roundoptimal, i.e., two-move, blind signature scheme in the standard model (i.e., without assuming random oracles or the existence of a common reference string). our scheme relies on the decisional diffie hellman assumption and the existence of sub-exponentially hard 1-to-1 one way functions. this scheme is also secure in the concurrent setting.round optimal blind signatures','Digital signatures'
'we present a round-optimal blind signature scheme based on waters\' signature scheme. our construction resembles that of fischlin [10], but does not rely on generic non-interactive zero-knowledge proofs. in addition to a common reference string, our scheme requires a registered public key for the signer. round-optimal blind signatures from waters signatures','Digital signatures'
'rsa-based undeniable signatures','Digital signatures'
'a sanitizable signature scheme is a signature scheme which allows a sanitizer to hide parts of the original message after the message is signed, without interacting with the signer. there exists many security models, properties and constructions for sanitizable signatures, which are useful in different scenarios. the aim of this paper is twofold. firstly, we summarize different properties in the literature and gives some generic conversions between them. we propose a security model to capture most of these properties. secondly, we present the &lt;em&gt;first&lt;/em&gt; concrete construction of sanitizable signatures which is proven secure in the standard model. sanitizable signatures revisited','Digital signatures'
'we introduce the notion of sanitizable signatures that offer many attractive security features for certain current and emerging applications. a sanitizable signature allows authorized semi-trusted censors to modify &#8211; in a limited and controlled fashion &#8211; parts of a signed message without interacting with the original signer. we present constructions for this new primitive, based on standard signature schemes and secure under common cryptographic assumptions. we also provide experimental measurements for the implementation of a sanitizable signature scheme and demonstrate its practicality.sanitizable signatures','Digital signatures'
'digital signatures provide authentication and nonrepudiation in a public way in the sense that anyone can verify the validity of a digital signature using the corresponding public key. in this paper, we consider the issues of (1) signature privacy and (2) the corresponding public provability of signature. we propose a new digital signature variant, secret signature, which provides authentication and nonrepudiation to the designated receiver only. if required, the correctness of the secret signature can be proven to the public either by the signer or the receiver. we conclude with a discussion to demonstrate the usefulness of the proposed cryptographic primitive (e.g., achieving signature privacy in an efficient manner).secret signatures','Digital signatures'
'in pkc 2006, chow, boyd and gonz&#225;lez neito introduced the notion of security mediated certificateless (smc) cryptography. smc cryptography equips certificateless cryptography with instantaneous revocation. they presented a formal security model with two constructions for smc encryption. this paper studies smc signatures. we first present a security analysis of a previous attempt by ju &lt;em&gt;et al.&lt;/em&gt;in constructing a smc signature scheme. we then formalize the notion of smc signatures and propose the first concrete provable scheme without bilinear pairing. our scheme is existential unforgeable in the random oracle model based on the intractability of the discrete logarithm problem, has a short public key size, and achieves a trust level which is the same as that of a traditional public key signature.security mediated certificateless signatures','Digital signatures'
'a digital signature provides the authenticity of a signed message with respect to a public key, while the authenticity of the public key with respect to a signer lies on a certificate provided by a certificate authority. whenever a verifier wants to verify a signature, he has first to verify the corresponding certificate. to reduce this burden, lee and kim proposed a new digital signature scheme called self-certified signature (scs). in this scheme, verifiers can validate both the signature on the message and the related certificate information simultaneously. then they extended the proposed self-certified signature scheme to a multi-certificate signature scheme (mcs), in which multiple certificate information need to be verified. in this paper, we show that their multi-certificate signature scheme does not satisfy certification as lee and kim claimed.security of self-certified signatures','Digital signatures'
'a digital signature provides the authenticity of a signed message with respect to a public key and a certificate provides the authorization of a signer for a public key. digital signature and certificate are generated independently by different parties, but they are verified by the same verifier who wants to verify the signature. in the point of a verifier, verifying two independent digital signatures (a digital signature and the corresponding certificate) is a burden.in this paper we propose a new digital signature scheme called self-certified signature. in this scheme a signer computes a temporary signing key with his long-term signing key and its certification information together, and generates a signature on a message and certification information using the temporary signing key in a highly combined and unforgeable manner. then, a verifier verifies both signer\'s signature on the message and related certification information together. this approach is very advantageous in efficiency. we extend the proposed self-certified signature scheme to multi-certification signature in which multiple certification information are verified. we apply it to public key infrastructure (pki) and privilege management infrastructure (pmi) environments.self-certified signatures','Digital signatures'
'a server-aided verification signature scheme consists of a digital signature scheme and a server-aided verification protocol. by executing the server-aided verification protocol with the server, one can perform the verification of signatures with less computational cost compared to the original verification algorithm. this mechanism is therefore indispensable for low-power devices such as smart cards. the contributions of this paper are manyfold. firstly, we introduce and define the existential unforgeability of server-aided verification signatures. we prove that the new notion includes the existing security requirements in server-aided verification signatures. then, we analyze the girault-lefranc scheme in asiacrypt 2005 and show that their scheme can be made secure in our model, but the computational cost is more than the claimed in the original scheme. after that, we propose the first server-aided verification bls, which is existentially unforgeable in the random oracle model under the bilinear diffie-hellman assumption. finally, we consider the collusion and adaptive chosen message attack in server-aided verification signatures. for the first time in the literature, the security of server-aided verification signatures against such attacks is defined. we provide a concrete construction of a server-aided verification bls secure against the collusion and chosen message attack. server-aided verification signatures','Digital signatures'
'server-supported signatures','Digital signatures'
'this paper presents a practical digital signature scheme to be used in conjunction with network coding. furthermore, the authors&#39; idea simultaneously provides authentication and detects malicious nodes that intentionally corrupt content on the network.signatures for network coding','Digital signatures'
'in this paper, we address the problem of computing a canonical representation of an n-dimensional combinatorial map. to do so, we define two combinatorial map signatures: the first one has a quadratic space complexity and may be used to decide of isomorphism with a new map in linear time whereas the second one has a linear space complexity and may be used to decide of isomorphism in quadratic time. experimental results show that these signatures can be used to recognize images very efficiently. signatures of combinatorial maps','Digital signatures'
'we introduce signatures of correct computation (scc), a new model for verifying dynamic computations in cloud settings. in the scc model, a trusted source outsources a function f to an untrusted server, along with a public key for that function (to be used during verification). the server can then produce a succinct signature &#963; vouching for the correctness of the computation of f, i.e., that some result v is indeed the correct outcome of the function f evaluated on some point a. there are two crucial performance properties that we want to guarantee in an scc construction: (1) verifying the signature should take asymptotically less time than evaluating the function f; and (2) the public key should be efficiently updated whenever the function changes. we construct scc schemes (satisfying the above two properties) supporting expressive manipulations over multivariate polynomials, such as polynomial evaluation and differentiation. our constructions are adaptively secure in the random oracle model and achieve optimal updates, i.e., the function\'s public key can be updated in time proportional to the number of updated coefficients, without performing a linear-time computation (in the size of the polynomial). we also show that signatures of correct computation imply publicly verifiable computation (pvc), a model recently introduced in several concurrent and independent works. roughly speaking, in the scc model, any client can verify the signature &#963; and be convinced of some computation result, whereas in the pvc model only the client that issued a query (or anyone who trusts this client) can verify that the server returned a valid signature (proof) for the answer to the query. our techniques can be readily adapted to construct pvc schemes with adaptive security, efficient updates and without the random oracle model.signatures of correct computation','Digital signatures'
'reputation systems have become an increasingly important tool for highlighting quality information and filtering spam within online forums. however, the dependence of a user&#8217;s reputation on their history of activities seems to preclude any possibility of anonymity. we show that useful reputation information can, in fact, coexist with strong privacy guarantees. we introduce and formalize a novel cryptographic primitive we call signatures of reputation which supports monotonic measures of reputation in a completely anonymous setting. in our system, a user can express trust in others by voting for them, collect votes to build up her own reputation, and attach a proof of her reputation to any data she publishes, all while maintaining the unlinkability of her actions.signatures of reputation','Digital signatures'
'randomizable encryption allows anyone to transform a ciphertext into a fresh ciphertext of the same message. analogously, a randomizable signature can be transformed into a new signature on the same message. we combine randomizable encryption and signatures to a new primitive as follows: given a signature on a ciphertext, anyone, knowing neither the signing key nor the encrypted message, can randomize the ciphertext and adapt the signature to the fresh encryption, thus maintaining public verifiability. moreover, given the decryption key and a signature on a ciphertext, one can compute (\"extract\") a signature on the encrypted plaintext. as adapting a signature to a randomized encryption contradicts the standard notion of unforgeability, we introduce a weaker notion stating that no adversary can, after querying signatures on ciphertexts of its choice, output a signature on an encryption of a new message. this is reasonable since, due to extractability, a signature on an encrypted message can be interpreted as an encrypted signature on the message. using groth-sahai proofs and waters signatures, we give several instantiations of our primitive and prove them secure under classical assumptions in the standard model and the crs setting. as an application, we show how to construct an efficient non-interactive receipt-free universally verifiable e-voting scheme. in such a scheme a voter cannot prove what his vote was, which precludes vote selling. besides, our primitive also yields an efficient round-optimal blind signature scheme based on standard assumptions, and namely for the classical waters signature.signatures on randomizable ciphertexts','Digital signatures'
'the aim of this paper is to present a new method to compare histograms. the main advantage is that there is an important time-complexity reduction with respect to the methods presented before. this reduction is statistically and analytically demonstrated in the paper. the distances between histograms that we presented are defined on a structure called signature, which is a lossless representation of histograms. moreover, the type of the elements of the sets that the histograms represent are ordinal, nominal and modulo. we show that the computational cost of these distances is o(z^\') for the ordinal and nominal types and o(z^\'^2) for the modulo one, z^\' being the number of non-empty bins of the histograms. the computational cost of the algorithms presented in the literature depends on the number of bins of the histograms. in most of the applications, the obtained histograms are sparse; then considering only the non-empty bins drastically decreases the time consumption of the comparison. the distances and algorithms presented in this paper are experimentally validated on the comparison of images obtained from public databases and positioning of mobile robots through the recognition of indoor scenes (captured in a learning stage).signatures versus histograms','Digital signatures'
'this paper presents a new framework for the symbolic representation of data which is referred to as signatures. the definitions of signatures and of signature trees are first given. original operators on signatures are next presented, i.e., contraction, extension, pruning, addition, multiplication, and grafting. attractive applications of signatures related to the modelling of fuzzy inference systems are suggested and discussed. an example is included to accompany the theoretical results.signatures','Digital signatures'
'data on large dynamic social networks, such as telecommunications networks and the internet, are pervasive. however, few methods conducive to efficient large-scale analysis exist. in this paper, we focus on the task of re-identification. re-identification in the context of dynamic networks is a matching problem that involves comparing the behavior of networked entities across two time periods. prior research has reported success in the domains of e-mail alias detection, author attribution, and identifying fraudulent consumers in the telecommunications industry. in this work, we address the question of \"why are we able to re-identify entities on real world dynamic networks?\" our contribution is two-fold. first, we address the challenge of scale with a framework for matching that does not require pairwise comparisons to ascertain the similarity scores between networked entities. second, we show our method is robust against missing links but less tolerant to noise. using our framework, we provide a performance estimate for re-identification on networks based solely on their degree distribution and dynamics. this work has significant implications for re-identification problems where scale is a challenge as well as for problems where false negatives (e.g.,when fraudulent consumers are not labeled as fraudulent) cannot be observed.social network signatures','Digital signatures'
'group signature schemes enable to create digital signatures such that the signers are hidden in a group of potential signers. however, in a case of need it is possible to reveal the actual signer either by a group administrator or collectively by the group members. we design a new kind of signatures that we call step-out group signature where the situation is reversed: any member of the group except the signer may prove that he or she was not the signer. this is a dual solution that is useful in certain scenarios: in many cases it is unnecessary to find the signer, it suffices to eliminate some potential signers (e.g. during prosecutions and court procedures). our solution is more convenient for implementing personal data protection rules: since the signer is not revealed, there is no need to protect this information. on the other hand, the traditional scheme may lead to serious legal problems: if the legal case is to find out whether bob has created group signature s, it might be illegal to reveal that alice has created s.step-out group signatures','Digital signatures'
'we propose a version of ring signatures for which the set of potential signers may be reduced: the real signer can prove that he or she has created the signature, while every other member of the ring can prove not to be the signer. possibility to run these protocols is triggered by publishing certain secret information. the proposed scheme is an intermediate solution between the classical ring and group signatures, and can be used for instance for e-auction schemes.step-out ring signatures','Digital signatures'
'an aggregate signature scheme is a digital signature scheme where anyone given n signatures on n messages from n users can aggregate all these signatures into a single short signature. unfortunately, no \"fully non-interactive\" aggregate signature schemes are known outside of the random oracle heuristic; that is, signers must pass messages between themselves, sequentially or otherwise, to generate the signature. interaction is too costly for some interesting applications. in this work, we consider the task of realizing aggregate signatures in the model of gentry and ramzan (pkc 2006) when all signers share a synchronized clock, but do not need to be aware of or interactive with one another. each signer may issue at most one signature per time period and signatures aggregate only if they were created during the same time period. we call this synchronized aggregation. we present a surprisingly efficient synchronized aggregate signature scheme secure under the computational diffie-hellman assumption in the standard model. our construction is based on the stateful signatures of hohenberger and waters (eurocrypt 2009). those signatures do not aggregate since each signature includes unique randomness for a chameleon hash and those random values do not compress. to overcome this challenge, we remove the chameleon hash from their scheme and find an alternative method for moving from weak to full security that enables aggregation. we conclude by discussing applications of this construction to sensor networks and software authentication.synchronized aggregate signatures','Digital signatures'
'signatures are the most widely used form of legally binding identification and authentication. the repeatability of a person&#253;s signature underpins its recognition and hence usefulness in everyday authentication situations. this study aims to assess the stability of a set of common features used for analysing signatures both within a single capture session and over time (multiple sessions). secondly, the physical characteristics of signatures which result in the most repeatable performance for each feature are also analyzed. these results have implications for biometric signature verification systems and the document forensic field in that it gives an indication as to the stability of features leading potentially to improved performance and the types of features that should be analyzed given particular characteristics of the signature under investigation.the repeatability of signatures','Digital signatures'
'titan: an information management system for faster retrieval from massive databases using signatures','Digital signatures'
'our hypothesis is that data applications leave a signature when used over networks. data that is sent from the original source and travels to the destination utilises the network in a very similar manner, if no strong congestion is experienced. should the network experience congestion, the unavoidable frame losses will alter the signature of the application and change the statistical property of the data. however, certain properties remain the same, even after a network disruption and congestion. these properties can together be used to create an \'\'application signature\'\', which is unique to the specific application. this is due to the protocols used and the parameters set in the environment of the traffic source. we think that these findings could help to understand the traffic characterisation and the processes involved. the traffic profiles could then be used for the dynamic resource allocation (e.g. resource reservation protocol) in future networks as well as with the simulation of such networks.traffic profiles and application signatures','Digital signatures'
'we present novel realizations of the transitive signature primitive introduced by micali and rivest, enlarging the set of assumptions on which this primitive can be based, and also providing performance improvements over existing schemes. more specifically, we propose new schemes based on factoring, the hardness of the one-more discrete logarithm problem, and gap diffie-hellman (dh) groups. all these schemes are proven transitively unforgeable under adaptive chosen-message attack in the standard (not random-oracle) model. we also provide an answer to an open question raised by micali and rivest regarding the security of their rivest-shamir-adleman (rsa)-based scheme, showing that it is transitively unforgeable under adaptive chosen-message attack assuming the security of rsa under one-more inversion. we then present hash-based modifications of the rsa, factoring, and gap diffie-hellman based schemes that eliminate the need for \"node certificates\" and thereby yield shorter signatures. these modifications remain provably secure under the same assumptions as the starting scheme, in the random oracle model.transitive signatures','Digital signatures'
'all known digital signature schemes can be forged by anyone having enough computing power. for a finite set of participants, we can overcome this weakness.we present a polynomial time protocol in which a participant can convince (with an exponentially small error probability) any other participant that his signature is valid. moreover, such a convinced participant can convince any other participant of the signature\'s validity, without interaction with the original signer.an extension allows, in most cases, a participant who receives a signature from any source to convince each other participant of its validity. if a participant cannot use the signature to convince others, he knows so when he receives it.unconditionally secure digital signatures','Digital signatures'
'undeniable signatures','Digital signatures'
'a major problem of mobile agents is their inability to authenticate transactions in a hostile environment. users will not wish to equip agents with their private signature keys when the agents may execute on untrusted platforms. undetachable signatures were introduced to solve this problem by allowing users to equip agents with the means to sign signatures for tightly constrained transactions, using information especially derived from the user private signature key. however, the problem remains that a platform can force an agent to commit to a sub-optimal transaction. in parallel with the work on undetachable signatures, much work has been performedon threshold signature schemes, which allow signing power to be distributed across multiple agents, thereby reducing the trust in a single entity. we combine these notions and introduce the concept of an undetachable threshold signature scheme, which enables constrained signing power to be distributed across multiple agents, thus reducing the necessary trust in single agent platforms. we also provide an rsa-based example of such a scheme basedon a combination of shoup\'s threshold signature scheme, [1] and kotzanikolaou et al\'s undetachable signature scheme, [2].undetachable threshold signatures','Digital signatures'
'uniform complexity and digital signatures','Digital signatures'
'many variants of chaum and van antwerpen&#8217;s undeniable signatures have been proposed to achieve specific properties desired in real-world applications of cryptography. among them, directed signatures were introduced by lim and lee in 1993. directed signatures differ from the well-known confirmer signatures in that the signer has the simultaneous abilities to confirm, deny and individually convert a signature. the universal conversion of these signatures has remained an open problem since their introduction in 1993. this paper provides a positive answer to this quest by showing a very efficient design for universally convertible directed signatures (ucds) both in terms of computational complexity and signature size. our construction relies on the so-called xyz-trick applicable to bilinear map groups. we define proper security notions for ucds schemes and show that our construction is secure in the random oracle model, under computational assumptions close to the cdh and ddh assumptions. finally, we introduce and realize traceable universally convertible directed signatures where a master tracing key allows to link signatures to their direction.universally convertible directed signatures','Digital signatures'
'sanitizable signatures allow a designated party, called the sanitizer, to modify parts of signed data such that the immutable parts can still be verified with respect to the original signer. ateniese et al. (esorics 2005) discuss five security properties for such signature schemes: unforgeability, immutability, privacy, transparency and accountability. these notions have been formalized in a recent work by brzuska et al. (pkc 2009), discussing also the relationships among the security notions. in addition, they prove a modification of the scheme of ateniese et al. to be secure according to these notions. here we discuss that a sixth property of sanitizable signature schemes may be desirable: unlinkability. basically, this property prevents that one can link sanitized message-signature pairs of the same document, thus allowing to deduce combined information about the original document. we show that this notion implies privacy, the inability to recover the original data of sanitized parts, but is not implied by any of the other five notions. we also discuss a scheme based on group signatures meeting all six security properties.unlinkability of sanitizable signatures','Digital signatures'
'secure use of the bgls [1] aggregate signature schemes is restricted to the aggregation of distinct messages (for the basic scheme) or per-signer distinct messages (for the enhanced, prepend-public-key version of the scheme). we argue that these restrictions preclude interesting applications, make usage of the schemes error-prone and are generally undesirable in practice. via a new analysis and proof, we show how the restrictions can be lifted, yielding the first truly unrestricted aggregate signature scheme. via another new analysis and proof, we show that the distinct signer restriction on the sequential aggregate signature schemes of [2] can also be dropped, yielding an unrestricted sequential aggregate signature scheme.unrestricted aggregate signatures','Digital signatures'
'this note continues a sequence of attempts to define efficient digital signature schemes based on low-degree polynomials, or to break such schemes. we consider a scheme proposed by satoh and araki (1997), which generalizes the ong-schnorr-shamir scheme to the noncommutative ring of quaternions. we give two different ways to break the scheme.weakness in quaternion signatures','Digital signatures'
'extensible markup language (xml) has become an important universal language for the internet-based business world. an xml document can be generated from various resources with varying security requirements. in order to ensure the integrity of the contents in the transactions, to maintain privacy and confidentiality, security is increasingly important. xml signatures are designed for the security of xml document transactions. they are involved with the authentication, data integrity and support for non-repudiation of the data they sign. they ensure that signatures cannot be verified without interaction with the signer. in this paper, we propose a new xml undeniable signature method that builds a bridge between the existing xml technologies and data security theories after discussing the existing xml digital signatures and rsa based undeniable signature technologies. we aim to provide a framework for the improvement of security technologies to improve xml applications.xml undeniable signatures','Digital signatures'
'most large scale traffic information systems rely on fixed sensors (e.g. loop detectors, cameras) and user generated data, this latter in the form of gps traces sent by smartphones or gps devices onboard vehicles. while this type of data is relatively inexpensive to gather, it can pose multiple security and privacy risks, even if the location tracks are anonymous. in particular, creating bogus location tracks and sending them to the system is relatively easy. this bogus data could perturb traffic flow estimates, and disrupt the transportation system whenever these estimates are used for actuation. in this article, we propose a new framework for solving a variety of privacy and cybersecurity problems arising in transportation systems. the state of traffic is modeled by the lighthill-whitham-richards traffic flow model, which is a first order scalar conservation law with concave flux function. given a set of traffic flow data, we show that the constraints resulting from this partial differential equation are mixed integer linear inequalities for some decision variable. the resulting framework is very flexible, and can in particular be used to detect spoofing attacks in real time, or carry out attacks on location tracks. numerical implementations are performed on experimental data from the~\\emph{mobile century} experiment to validate this framework.a framework for privacy and security analysis of probe-based traffic information systems','Distributed systems security'
'this paper adopts a model-based security (mbs) approach to identify security requirements during the early stages of multi-agent system development. our adopted mbs approach is underpinned by a metamodel independent of any specific methodology. it allows for security considerations to be embedded within any situated agent methodology which then prescribes security considerations within its work products. using a standard model-driven engineering (mde) approach, these work products are initially constructed as high abstraction models and then transformed into more precise models until code-specific models can be produced. a multi-agent system case study is used to illustrate the applicability of the proposed security-aware metamodel.a security-aware metamodel for multi-agent systems (mas)','Distributed systems security'
'using our alchemy research system as an exemplarsystem, we discuss different approaches to overcomingport-controlled security boundary limitations togeographically distributed computing environments whilemaintaining the inherent security provided by theseboundaries. this discussion is relevant to a number ofparallel/distributed strategies for grid and clusterarchitectures. it also provides insight into generalsecurity concerns in future distributed computingnetworks and how these may best be overcome.addressing security issues in geographically distributed systems','Distributed systems security'
'we present nessi2, the network security simulator, a simulation environment that is based on the service-centric agent platform jiac. it focuses on network security-related scenarios such as attack analysis and evaluation of counter-measures. we introduce the main nessi2 concepts and discuss the motivation for realizing them with agent technology. then, we present the individual components and examples where nessi2 has been successfully applied.agent-based network security simulation','Distributed systems security'
'the tremendous growth of the network-centred (internet and intranet) computing environments requires new architecture for security services. computer crimes are growing rapidly in these environments. in addition, these computing environments are open, and users may be connected or disconnected at any time. this makes computer security a necessity to all computer users. this paper presents a multi-agent system architecture for security services. the main objective of this system is to address some of the shortcomings that are present in contemporary security service systems that focused on providing solutions for specific security issues, such as authentication and authorization. another objective is to provide a relatively complete security service solution to protect hosts and users. the proposed system architecture includes four types of agents: interface, authentication, authorization, and service provider agents (spas). the interface agents interact with the users to fulfill their interests. the authentication agents investigate the validity of using keystroke dynamics to strengthen security. the authorization agents make all decisions regarding who can access which resources and for what purposes. the spas offer different encryption services to different users. this paper provides the agents\' architecture, design and implementations that enable them to cooperate, coordinate, and communicate with each other to provide a secure environment. a prototype of the system is implemented using the java agent development framework.an agent-based approach to security service','Distributed systems security'
'scavenged storage systems harness unused disk space from individual workstations the same way idle cpu cycles are harnessed by desktop grid applications like seti@home. these systems provide a promising low cost, high-performance storage solution in certain high-end computing scenarios. however, selecting the security level and designing the security mechanisms for such systems is challenging as scavenging idle storage opens the door for security threats absent in traditional storage systems that use dedicated nodes under a single administrative domain. moreover, increased security often comes at the price of performance and scalability. this paper develops a general threat model for systems that use scavenged storage, presents the design of a protocol that addresses these threats and is optimized for throughput, and evaluates the overheads brought by the new security protocol when configured to provide a number of different security properties.configurable security for scavenged storage systems','Distributed systems security'
'in open multi-agent systems, trust plays the central role in facilitating interactions. most of trust models need to collect the witness reports, and they may suffer due to existence of malicious witness. to detect the malicious witness reports, this paper presents an approach, osm (opinion similarity measure). evaluator calculates the opinion similarity state (oss) between evaluator and witness according to their reports, and evaluates the witness credibility according to the oss between them. experiment and analysis show that osm is more robust than existing approaches.detecting malicious witness reports in multi-agent systems','Distributed systems security'
'distributed systems security','Distributed systems security'
'our work presents a mechanism designed for the selection of the optimal information provider in a multi-agent, heterogeneous and unsupervised monitoring system. the self-adaptation mechanism is based on the insertion of a small set of prepared challenges that are processed together with the real events observed by the system. the evaluation of the system response to these challenges is used to select the optimal information source. our algorithm uses the concept of trust to identify the best source and to optimize the number of challenges inserted into the system. the mechanism is designed for intrusion/fraud detection systems, which are frequently deployed as part of online transaction processing (banking, telecommunications or process monitoring systems). our approach features unsupervised adjustment of its configuration and dynamic adaptation to the changing environment, which are both vital for these domains.dynamic information source selection for intrusion detection systems','Distributed systems security'
'one of the more mature instances of a service-oriented architecture is the model known as grid computing. computational grids and data grids are becoming commonplace in certain sectors, yet the style of security they implement is suitable only for a fairly small subset of possible user communities. using some case studies and experience, we describe the existing grid security models, explain why they represent shortcomings for some applications, and describe some emerging architectures, trusted computing and virtualisation, which help address the problems.grid security','Distributed systems security'
'handling signature purposes in workflow systems','Distributed systems security'
'as part of a major redesign of our computer science curriculum we are developing an information security option. our approach highlights the many topics in information security that build upon concepts the students will already have seen in their computer systems courses, especially courses on internet protocols and operating systems. in this paper, we describe this integrated approach to information security and computer systems.information security and computer systems','Distributed systems security'
'introduction to special issue on stabilization, safety, and security of distributed systems','Distributed systems security'
'motivated by recent deployments of stackelberg security games (ssgs), two competing approaches have emerged which either integrate models of human decision making into game-theoretic algorithms or apply robust optimization techniques that avoid adversary modeling. recently, a robust technique (match) has been shown to significantly outperform the leading modeling-based algorithms (e.g., quantal response (qr)) even in the presence of significant amounts of subject data. as a result, the effectiveness of using human behaviors in solving ssgs remains in question. we study this question in this paper.modeling human adversary decision making in security games','Distributed systems security'
'detecting security bugs during the development cycle of a software is extremely difficult as effective testing approaches for such bugs do not exist. applications are often deployed without being tested for security vulnerabilities even though the application domain demands highly secure software. hence there is a need to develop systems which can monitor such applications for security violations and take immediate actions if any violation occurs. in this paper we describe an approach for monitoring the security health of a software system. our methodology involves an agent based approach which communicates with the health monitoring system running as an independent process. we make this agent a part of the application(binary) and modify the binary at appropriate locations to transfer the control to the agent attached. the agent sends critical information regarding the execution to the monitoring system. the monitoring system analyzes the data and takes suitable actions. currently our system monitors the following security bugs - buffer overflow, race conditions( time of check to time to use vulnerability), random number vulnerability and can be extended for other vulnerabilities also.monitoring the security health of software systems','Distributed systems security'
'in many exciting multiagent applications--including future battlefields, law enforcement, and commerce--agents must communicate in inherently or potentially hostile environments in which an adversaries disrupt or intercept the communication between agents for malicious purposes, but the wireless ad hoc networks often proposed for these applications are particularly susceptible to attack. intelligent agents must balance network performance with possible harm suffered from an adversary\'s attack, while accounting for the broadcast nature of their communication and heterogenous vulnerabilities of communication links. furthermore, they must do so when the adversary is also actively and rationally attempting to counter their efforts. we address this challenge in this paper by representing the problem as a game between a sender agent choosing communication paths through a network and an adversary choosing nodes and links to attack. we introduce a network-flow-based approach for compactly representing the competing objectives of network performance and security from adversary attack, and provide a polynomial-time algorithm for finding the equilibrium strategy for the sender. through empirical evaluation we show how this technique improves upon existing approaches.multiagent communication security in adversarial settings','Distributed systems security'
'recent work has applied game-theoretic models to real-world security problems at the los angeles international airport (lax) and federal air marshals service (fams). the analysis of these domains is based on input from domain experts intended to capture the best available intelligence information about potential terrorist activities and possible security countermeasures. nevertheless, these models are subject to significant uncertainty---especially in security domains where intelligence about adversary capabilities and preferences is very difficult to gather. this uncertainty presents significant challenges for applying game-theoretic analysis in these domains. our experimental results show that standard solution methods based on perfect information assumptions are very sensitive to payoff uncertainty, resulting in low payoffs for the defender. we describe a model of bayesian stackelberg games that allows for general distributional uncertainty over the attacker\'s payoffs. we conduct an experimental analysis of two algorithms for approximating equilibria of these games, and show that the resulting solutions give much better results than the standard approach when there is payoff uncertainty.robust bayesian methods for stackelberg security games','Distributed systems security'
'scaling security for big, parallel file systems','Distributed systems security'
'security for distributed systems','Distributed systems security'
'this paper provides an extensive survey of the different methods of addressing security issues in the grid computing environment, and specifically contributes to the research environment by developing a comprehensive framework for classification of these research endeavors. the framework presented classifies security literature into system solutions, behavioral solutions, hybrid solutions and related technologies. each one of these categories is explained in detail in the paper to provide insight as to their unique methods of accomplishing grid security, the types of grid and security situations they apply best to, and the pros and cons for each type of solution. further, several areas of research were identified in the course of the literature survey where more study is warranted. these avenues for future research are also discussed in this paper. several types of grid systems exist currently, and the security needs and solutions to address those needs for each type vary as much as the types of systems themselves. this research framework will aid in future research efforts to define, analyze, and address grid security problems for the many varied types of grid setups, as well as the many security situations that each grid may face.security in grid computing','Distributed systems security'
'security of systems and information has always been a challenge to organisations and industries. many technical solutions including firewalls, encryption and anti-virus software have been used, yet security still remains a problem. these security solutions failures are largely due to the fact that as systems become more complex, a lot of interaction is involved between various actors. some of these interactions usually leave room for security vulnerabilities which are simply not accounted for by the technical security solutions: there are just too many possibilities. my research is focused on this aspect of organisational security. the proposed approach to this involves the monitoring of events for traces of behaviours that may eventually circumvent the security regulations of the organisation. the methodology includes organisational modeling and simulation of self monitoring agents using a normative framework.security in the context of multi-agent systems','Distributed systems security'
'in the past, multi-agent systems were used in proprietary environments. nowadays, these systems have been used broadly in open distributed networks, such as e-commerce applications for internet. an environment such as the internet cannot be considered a safe place. thus, multi-agent systems should have security mechanisms, e.g. confidentiality and integrity. the xml security specifications are standards that are based on xml and provide security mechanisms. they include: xml digital signature for digital signature; xml encryption for cryptography; xml key management specification for public key infrastructure. agents may use a fipa standard called rdf, which is a message content standard in xml language. using this standard, agents can communicate exchanging xml messages, but these messages are not secure. in this article, we propose a secure communication model for agents based on rdf and the xml security specifications.security on mass with xml security specifications','Distributed systems security'
'the paper reviews different approaches applied to enable security in grid systems. it analyzes security mechanisms compatible with grid platforms. it focuses on the following aspects of grid security: anomaly detection and security policy verification. petri-net-based model is proposed for access control security analysis in grid systems. that model enhances grid security with trusted \'jobs\' submission and security verification.security policy verification in grid systems','Distributed systems security'
'seeking security','Distributed systems security'
'cloud computing is a model for enabling convenient, on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or service provider interaction. with this pay-as-you-go model of computing, cloud solutions are seen as having the potential to both dramatically reduce costs and increase the rapidity of development of applications.state of security readiness','Distributed systems security'
'authentication and authorisation infrastructures (aais) are gaining momentum throughout the internet. solutions have been proposed for various scenarios among them academia, grid computing, company networks, and above all ecommerce applications. products and concepts vary in architecture, security features, target group, and usability containing different strengths and weaknesses. in addition security needs have changed in communication and business processes. security on the internet is no longer defined as only security measures for an ecommerce provider against an untrustworthy customer but also vice versa. consequently, privacy, data canniness, and security are demands in this area. the authors define criteria for an ecommerce provider federation using an aai with a maximum of privacy and flexibility. the criteria is derived concentrating on b2c ecommerce applications fulfilling the demands. in addition to best practices found, xacml policies and an attribute infrastructure are deployed. among the evaluated aais are shibboleth, microsoft passport, the liberty alliance framework, and permis.a reference model for authentication and authorisation infrastructures respecting privacy and flexibility in b2c ecommerce','Domain-specific security and privacy architectures'
'data mining is becoming a pervasive technology in several activities as using historical data to predict the success of a marketing campaign, looking for patterns in financial transactions to discover illegal activities or analyzing genome sequences. in this paper we adopt a reference flexible mining architecture able to discover knowledge in a distributed and heterogeneous environment. in the context of security, the information we are seeking is the knowledge of whether a security breach has been experienced, and if the answer is yes, who is the perpetrator. to this purpose, the guide lines of the service oriented architecture, soa, and the orchestration model have been considered as a way to realize a our proposal of knowledge discovery process to intrusion detection.applying a flexible mining architecture to intrusion detection','Domain-specific security and privacy architectures'
'we propose an approach using elliptic curve-based zero-knowledge proofs in e-commerce applications. we demonstrate that using elliptic curved-based zero-knowledge proofs provide privacy and more security than other existing techniques. the improvement of security is due to the complexity of solving the discrete logarithm problem over elliptic curves.better privacy and security in e-commerce','Domain-specific security and privacy architectures'
'in this paper we introduce a system called crowds for protecting users\' anonymity on the world-wide-web. crowds, named for the notion of &#8220;blending into a crowd,&#8221; operates by grouping users into a large and geographically diverse group (crowd) that collectively issues requests on behalf of its members. web servers are unable to learn the true source of a request because it is equally likely to have originated from any member of the crowd, and even collaborating crowd members cannot distinguish the originator of a request from a member who is merely forwarding the request on behalf of another. we describe the design, implementation, security, performance, and scalability of our system. our security analysis introduces degrees of anonymity as an important  tool for describing and proving anonymity properties.crowds','Domain-specific security and privacy architectures'
'raising awareness and providing guidance to on-line data protection is by all means a crucial issue worldwide. equally important is the issue of applying privacy-related legislation in a coherent and coordinated way. both these topics become even more critical when referring to medical environments and thus to the protection of patients\' privacy and medical data. electronic medical transactions require the transmission of personal and medical information over insecure communication channels like the internet. it is therefore a rather straightforward task to construct &#8220;patient profiles&#8221; that capture the electronic medical behavior of a patient, or even reveal sensitive information in regard with her/his medical history. clearly, the consequence from maintaining such profiles is the violation of the patient\'s privacy. this paper studies medical environments that can support electronic medical transactions or/and the provision of medical information through the web. specifically it focuses on the countermeasures that the various actor categories can employ for protecting the privacy of personal and medical data transmitted during electronic medical transactions.enhancing privacy and data protection in electronic medical environments','Domain-specific security and privacy architectures'
'to realize the idea of web services as a scalable technology, enabling access to a provider\'s resources for a wide range of clients, requires a similar scalable security solution. management of user accounts for all possible clients in each provider is simply unfeasible. the alternative approach to having federated identity management is currently being developed by main software vendors. in this paper we present the design and implementation of a lightweight security infrastructure, for the federated security, that enable the establishment of a trust federation between several organizations. the infrastructure consists of an augmented security layer placed on top of the web service protocol. the solution utilizes the latest ws-security specifications and, at the infrastructure level, is compatible with shibboleth &#8212; a federated security solution for web resources. in order to illustrate the potential of the infrastructure, we describe it in the context of two case studies: an object repository with complex access policies and the connection with the authenticated p2p network for learning resources.federated security','Domain-specific security and privacy architectures'
'following the events of september 11th, the leaders of developednations have moved quickly to establish new agreements forinternational security cooperation. many of these agreements arebeing forged secretively, and with little democratic oversight.this session discusses the new era of control and surveillance thathas arisen since that tragic day, and what it will mean for ourprivacy and for national security and law enforcement.international security cooperation and privacy','Domain-specific security and privacy architectures'
'safeweb, a venture-backed startup founded by a team of physics phds, created an internet privacy service which was used by millions of people worldwide. despite widespread adoption, its advertising-based business model proved unsupportable, and the service was discontinued in late 2001. safeweb\'s main product now is a hardware device based on its core technology (the secure extranet appliance), which is used by enterprise security customers such as corporations and government agencies. i discuss my experiences as co-founder of safeweb, and topics such as the internet in china and the future of privacy in the information age.internet privacy and security','Domain-specific security and privacy architectures'
'despite the use of state of the art methods to protect against malicious programs, they continue to threaten and damage computer systems around the world. in this paper we present met, the malicious email tracking system, designed to automatically report statistics on the flow behavior of malicious software delivered via email attachments both at a local and global level. met can help reduce the spread of malicious software worldwide, especially self-replicating viruses, as well as provide further insight toward minimizing damage caused by malicious programs in the future. in addition, the system can help system administrators detect all of the points of entry of a malicious email into a network. the core of met\'s operation is a database of statistics about the trajectory of email attachments in and out of a network system, and the culling together of these statistics across networks to present a global view of the spread of the malicious software. from a statistical perspective sampling only a small amount of traffic (for example, .1 \%) of a very large email stream is sufficient to detect suspicious or otherwise new email viruses that may be undetected by standard signature-based scanners. therefore, relatively few met installations would be necessary to gather sufficient data in order to provide broad protection services. small scale simulations are presented to demonstrate met in operation and suggests how detection of new virus propagations via flow statistics can be automated.met','Domain-specific security and privacy architectures'
'at the \"computers, freedom, and privacy (cfp) conference held in berkeley, california, the spotlight was on the twin weights of national security and personal liberty - with technology the fulcrum on which all turns. it highlights included sessions devoted to the new international cybercrime treaty, a global crusade to spread technology to underdeveloped nations, laws meant to block illegal sites at the ip-address level, and wiretapping voice-over-ip (voip) communications.preserving security and privacy','Domain-specific security and privacy architectures'
'the third in a series of articles providing basic information on legal issues facing people and businesses that operate in computing-related markets focuses on the responsibility to ensure privacy and data security. the featured web extra is an audio podcast by brian m. gaff and thomas j. smedinghoff, two of the article\'s coauthors.privacy and data security','Domain-specific security and privacy architectures'
'i will argue that one class of issues in computer ethics oftenassociated with privacy and a putative right to privacy isbest-analyzed in terms that make no substantive reference toprivacy at all. these issues concern the way that networkedinformation technology creates new ways in which conventionalrights to personal security can be threatened. however onechooses to analyze rights, rights to secure person and propertywill be among the most basic, the least controversial, and themost universally recognized. a risk-based approach to theseissues provides a clearer statement of what is ethicallyimportant, as well as what is ethically problematic. once theissues of security have been articulated clearly, it becomespossible to make out genuine issues of privacy in contrast tothem.privacy, secrecy and security','Domain-specific security and privacy architectures'
'security and privacy after september 11','Domain-specific security and privacy architectures'
'security challenges are still among the biggest obstacles when considering the adoption of cloud services. this triggered a lot of research activities, resulting in a quantity of proposals targeting the various cloud security threats. alongside with these security issues, the cloud paradigm comes with a new set of unique features, which open the path toward novel security approaches, techniques, and architectures. this paper provides a survey on the achievable security merits by making use of multiple distinct clouds simultaneously. various distinct architectures are introduced and discussed according to their security and privacy capabilities and prospects.security and privacy-enhancing multicloud architectures','Domain-specific security and privacy architectures'
'security and privacy','Domain-specific security and privacy architectures'
'software security and privacy risks in mobile e-commerce','Domain-specific security and privacy architectures'
'the distributed nature of the environment in which privacy and security policies operate requires tools that help enforce consistency of policy rules across different domains. furthermore, because changes to policy rules are required as policies evolve over time, such tools can be used by policy administrators to ensure the consistency of policy changes. in this paper, we describe a number of different policy analysis tools and techniques that we have developed over the years and present them in a unified framework in which both privacy and security policies are discussed. we cover dominance analyses of general policies, conflicts among authorizations and prohibitions, and other analyses of obligations, as well as policy similarity analysis and policy distribution.analysis of privacy and security policies','Economics of security and privacy'
'commerce is a rapidly emerging application area of ubiquitous computing. in this paper, we discuss the market forces that make the deployment of ubiquitous commerce infrastructures a priority for grocery retailing. we then proceed to report on a study on consumer perceptions of mygrocer, a recently developed ubiquitous commerce system. the emphasis of the discussion is on aspects of security, privacy protection and the development of trust; we report on the findings of this study. we adopt the enacted view of technology adoption to interpret some of our findings based on three principles for the development of trust. we expect that this interpretation can help to guide the development of appropriate strategies for the successful deployment of ubiquitous commerce systems.consumer perceptions of privacy, security and trust in ubiquitous commerce','Economics of security and privacy'
'expectations of security and privacy','Economics of security and privacy'
'managing security and privacy of information','Economics of security and privacy'
'at the \"computers, freedom, and privacy (cfp) conference held in berkeley, california, the spotlight was on the twin weights of national security and personal liberty - with technology the fulcrum on which all turns. it highlights included sessions devoted to the new international cybercrime treaty, a global crusade to spread technology to underdeveloped nations, laws meant to block illegal sites at the ip-address level, and wiretapping voice-over-ip (voip) communications.preserving security and privacy','Economics of security and privacy'
'the third in a series of articles providing basic information on legal issues facing people and businesses that operate in computing-related markets focuses on the responsibility to ensure privacy and data security. the featured web extra is an audio podcast by brian m. gaff and thomas j. smedinghoff, two of the article\'s coauthors.privacy and data security','Economics of security and privacy'
'the concept of a smart grid -- an intelligent and active power distribution network that uses advanced communication technology to collect and use real time operational information for efficient control of the grid -- has become one of the hottest research topics in the areas of information technology and electrical engineering. governments and the private sector have started to invest billions of dollars into this new technology that will not only allow more efficient management of current grids, better load distribution, demand control, up-to-date status monitoring and faster failure recovery, but also promises better integration of new services and applications such as smart homes and intelligent appliances, new energy sources, and ev grids. these features will be made possible by a wide deployment of data collection devices such as embedded sensors, smart meters and communication networks to bring this data into control centres for analysis, as well as automated controls and algorithms for decision making based on the current status information. as more intelligence is built into the electrical grid, the issue of grid security becomes extremely important and must be considered within the broader field of cyber-security. the automated control functions of the smart grid could be manipulated by intruders to gain control of the power distribution networks, steal customer information, or inflict other damages. network-based attacks may be used to disrupt the network, overload part of the grid or disconnect regions. the security of the access devices that provide real-time information must be examined carefully. a major challenge at the moment lies in the variety of options for smart grid communication at both access and core networks. smart meter communication and messaging technologies could be based on dlms/cosem standard or ip-based, using a wired or wireless sensor platform e.g. 6lowpan or ieee802.15.4/zigbee, in a mesh or cluster-based topology, connected using fibers or based on cellular technologies such as lte or wimax. the variety of technologies available poses an enormous challenge in incorporating security and survivability features into the smart grid design. interoperability issues between devices pose further security challenges that must be addressed based on the undergoing standardization works, most notably the ieee p2030 project. overcoming security challenges in smart grid communication will require careful evaluation of the proposed communication technologies and their interoperation. this includes the creation of security test beds, which would allow examination of the survivability of the grid system against a variety of security attacks and further system hardening and the development of integrated simulation environments for the communication network and the power grid. also required is the development of various components of a situational awareness system, such as intrusion detection and prevention systems and network management agents, to monitor and present the common operational picture (cop) of the grid from individual sensors and meters to the main controllers at the network control centers. in addition to the security of grid communication systems, the privacy of communication and consumer data are also extremely important, in particular in the case of smart home/smart grid. this is because detailed consumer data may be collected and analyzed without active participation from the homeowner. the canadian law greatly emphasizes the importance of protecting the privacy of users, and puts that responsibility squarely on the shoulder of data collectors, i.e. grid operators and utility companies in this case. any design for smart grid communication must strike a delicate balance between functionality and privacy. while from an operational point of view it might be beneficial to collect and maintain detailed individual usage information in order to optimize the network operations, this would also cause significant privacy concerns if such data ever fell into the wrong hands or was used for individual identification in cases where consumer protection laws were applied. many questions must be answered, for example: how do we aggregate data and minimize individual user identification without compromising the usefulness of data? how do we to maintain privacy of data along the entire path of the sensors and to the main databases? what is the most secure model for interactions between users and the grid, to monitor and control the trust levels of devices that are connected to the grid? the aim of this workshop was to explore the latest progress and research in the field of smart grid communication security and privacy, and to provide a forum for researchers, students and business experts from both academia and industry, to discuss the latest innovations and future works in the field. the new initiatives by federal and provincial government institutions in north america, including ontario smart grid initiative and british columbia smart metering program, as well as the comprehensive us policy framework for the 21st century grid (released in june 2011) further highlight the urgency and importance of such discussion in academia and industry. considering the broad challenges involved with the design and operation of smart grids, this workshop focused on issues specifically dealing with security and privacy issues. the topics of interest for this workshop included the following: &#8226; smart grid architecture security &#8226; smart grid security risk assessment &#8226; smart grid physical and device security &#8226; mitigating cyber attacks against the smart grid &#8226; intrusion detection for smart grid &#8226; privacy issues in smart metering &#8226; smart grid resilience &#8226; smart grid restoration and failure recovery &#8226; grid access and sensor network security and privacy &#8226; evaluation of smart grid communication protocols &#8226; smart home security and privacy issues &#8226; trust models for smart grid/smart home experts from the business and industry were invited to present an overview of the current trends and challenges in smart grid security and privacy. presentations from the academia provided a glance into the latest academic research for this field. the workshop was concluded with a panel discussion on the topic of information security and privacy that also set the agenda for future workshops of this kind.privacy and security of smart grid communication','Economics of security and privacy'
'i will argue that one class of issues in computer ethics oftenassociated with privacy and a putative right to privacy isbest-analyzed in terms that make no substantive reference toprivacy at all. these issues concern the way that networkedinformation technology creates new ways in which conventionalrights to personal security can be threatened. however onechooses to analyze rights, rights to secure person and propertywill be among the most basic, the least controversial, and themost universally recognized. a risk-based approach to theseissues provides a clearer statement of what is ethicallyimportant, as well as what is ethically problematic. once theissues of security have been articulated clearly, it becomespossible to make out genuine issues of privacy in contrast tothem.privacy, secrecy and security','Economics of security and privacy'
'death is an uncomfortable subject for many people, and digital systems are rarely designed to deal with this event. in particular, the wide array of existing digital authentication infrastructure rarely deals with gracefully retiring credentials in a uniform fashion. this research paper highlights an emerging paradigm: gracefully dealing with expired digital identities in a secure, privacy-preserving fashion. it examines the confluence of modern browser technology, cloud services, and human factors involved in managing a person\'s digital footprint while they live and retiring it when they die. we contemplate a potential approach to dealing with credentials after death by using cloud computing. we consider the reasons that such an approach may actually provide an opportunity for enhancing authentication security by frustrating identity stealing attacks. we note that this paper is not aimed at trivializing the real grief and loss that people feel, but rather an attempt to understand how security and privacy concerns are shaped by the end of life, with the ultimate goal of easing this transition for friends and family.security and privacy considerations in digital death','Economics of security and privacy'
'cloud computing - the new paradigm, the future for it consumer utility, the economy of scale approach, the illusion of un infinite resources availability, yet the debate over security and privacy issues is still undergoing and a common policy framework is missing. research confirms that users are concern when presented with scenarios in which companies may put their data to uses of which they may not be aware. therefore, privacy and security should be considered at every stage of a system design whereas advantages and disadvantages should be rated and compared to internal and external factors once a company or a person decides to go into the business of cloud computing or become just an user.security and privacy implications of cloud computing','Economics of security and privacy'
'the development of transnational computer-communication systems and the associated flows of computer data across international borders have created a number of issues and problems: privacy and security of personal data, non-tariff restrictions, concerns over potential erosion of national sovereignty, protectionism, and so forth. these developments are important to the data processing community in the united states since restrictions may be placed on the systems it develops and the data processing services it offers internationally. this session will address the issues involved in general, then concentrate on privacy protection problems, and finally explore a specific, new problem area---privacy rights of business, industry and other organizations regarding data about themselves.security and privacy of data flows','Economics of security and privacy'
'security and privacy','Economics of security and privacy'
'us president barack obama promised a \"new comprehensive approach\" to cybersecurity and guaranteed to preserve \"personal privacy and civil liberties,\" but the administration has stopped short of committing to the legal changes necessary to protect either information infrastructure or privacy. this tendency to undervalue law as a tool for enhancing both security and individual privacy is shared with other governments. sound cybersecurity policy requires better incentives to secure data and systems, and those incentives will emerge, at least in part, from legal requirements. similarly, serious efforts to protect against cyberthreats will compromise privacy and other civil rights unless those rights are protected by law.security, privacy, and the role of law','Economics of security and privacy'
'this paper presents the recent technical research on the problems of privacy and security for radio frequency identification (rfid). rfid technology is already used widely and is increasingly becoming a part of daily life. however, issues regarding security and privacy with respect to rfid technology have not been resolved satisfactorily. there are huge number of challenges, which must be overcome to resolve rfid security and privacy issues. it is because of the many constraints attached to the provision of security and privacy in rfid systems. these challenges are chiefly technical and economic in nature but also include ethical and social issues. along with meeting the security and privacy needs of rfid technology, solutions must be inexpensive, practical, reliable, scalable, flexible, inter-organizational, and long lasting. this paper reviews the approaches which had been proposed by scientists for privacy protection and integrity assurance in rfid systems, and treats the social and technical context of their work. this paper can be useful as a reference for non specialist, as well as for specialist readers.the evolution of rfid security and privacy','Economics of security and privacy'
'road safety, traffic management, and driver convenience continue to improve, in large part thanks to appropriate usage of information technology. but this evolution has deep implications for security and privacy, which the research community has overlooked so far.the security and privacy of smart vehicles','Economics of security and privacy'
'a low-cost cryptographic processor for security embedded system is presented in this paper. the processor, without any assistance of dedicated cryptographic coprocessors, is scalable and very efficient for popular cryptographic algorithms such as rsa/ecc, aes, hash, etc. based on smic 0.18um standard cmos technology, the core circuit of the test chip has only about 32k gates, and a max frequency of 200mhz, under which the 1024-bit rsa algorithm takes only 150ms and the throughout of aes reaches 256mbits/s.a low-cost cryptographic processor for security embedded system','Embedded systems security'
'past efforts at designing and implementing ultra high assurance systems for government security and safety have centered on the concept of a monolithic security kernel responsible for a system-wide security policy. this approach leads to inflexible, overly complex operating systems that are too large to evaluate at the highest assurance levels (e.g., common criteria eal 5 and above). we describe a new multi-layered approach to the design and verification of embedded trustworthy systems that is currently being used in the implementation of real time, embedded applications. the framework supports multiple levels of safety and multiple levels of security, based on the principle of creating separate layers of responsibility and control, with each layer responsible for enforcing its own security policy.a multi-layered approach to security in high assurance systems','Embedded systems security'
'this paper describes a complete off-chip memory security solution for embedded systems. our security core is based on a one-time pad (otp) encryption circuit and a crc-based integrity checking module. these modules safeguard external memory used by embedded processors against a series of well-known attacks, including replay attacks, spoofing attacks and relocation attacks. our implementation limits on-chip memory space overhead to less than 33\% versus memory used by a standard microprocessor and reduces memory latency achieved by previous approaches by at least half. the performance loss for software execution with our solution is only 10\% compared with a non-protected implementation. an fpga prototype of our security core has been completed to validate our findings.a security approach for off-chip memory in embedded microprocessor systems','Embedded systems security'
'real-time applications are indispensable for conducting research and business in government, industry, and academic organizations. recently, real-time applications with security requirements increasingly emerged in large-scale distributed systems such as grids. however, the complexities and specialties of diverse security mechanisms dissuade users from employing existing security services for their applications. to effectively tackle this problem, in this paper we propose a security middleware (smw) model from which security-sensitive real-time applications are enabled to exploit a variety of security services to enhance the trustworthy executions of the applications. a quality of security control manager (qscm), a centerpiece of the smw model, has been designed and implemented to achieve a flexible trade-off between overheads caused by security services and system performance, especially under situations where available resources are dynamically changing and insufficient. a security-aware scheduling mechanism, which plays an important role in qscm, is capable of maximizing quality of security for real-time applications running in distributed systems as large-scale as grids. our empirical studies based on real world traces from a supercomputing center demonstratively show that the proposed model can significantly improve the performance of grids in terms of both security and schedulability.a security middleware model for real-time applications on grids','Embedded systems security'
'we introduce a mobile e-mail communication platform which employs smart card to provide authentication and confidentiality for e-mail exchange. the security of the system is incorporated by pgp based asymmetric encryption. smart card is used to store the mail account information, as well as pgp private key of each user to offer further security, authentication and mobility to the system. the real contribution of our scheme is the absorption of all the complexity of pgp to a pin based smart card by causing no compromise on the security of pgp. we use javacard technology in real life implementation of the proposed platform.a smart card mediated mobile platform for secure e-mail communication','Embedded systems security'
'a virus detection framework based on spmos','Embedded systems security'
'to be generally useful a theory must be both intellectually sound and practically applicable. we consider the noninterference approach to security specification, focusing in particular on roscoe\'s work on nondeterminism. this provides a starting point for reflecting on what features are desirable in a development method for secure systems. in an attempt to meet at least some of these requirements we use action systems which combine both event and state-based specification approaches. using butler\'s correspondence between action systems and csp we define determinism and security properties directly in action systems. we give examples of the action system approach and discuss its advantages and disadvantages.action systems for security specification','Embedded systems security'
'active security &#8212; a proactive approach for computer security systems','Embedded systems security'
'a key-store is a facility for storing sensitive information, most typically the keys of a cryptographic application which provides a security service. in this paper, we present a hardware implemented key-store, which allows secure storage and high performance retrieval of rsa keys. since rsa is the most widely adopted standard for cryptographic keys, our key-store can be effectively used to improve the dependability of a wide class of security services. the device is implemented on top of a commercial off the shelf (cots) programmable hardware board, namely a celoxica rc1000 mounting a xilinx virtex-e 2000 fpga part. we describe the architecture of the hardware device, illustrate the organization of the associated device driver, and evaluate the security and performance gain which can be achieved by integrating our device in real-world applications. 1an fpga-based key-store for improving the dependability of security services','Embedded systems security'
'analogue speech security systems','Embedded systems security'
'standard approaches to functional safety as described in the automotive functional safety standard iso 26262 are focused on reducing the risk of hazards due to random hardware faults or systematic failures during design (e.g. software bugs). however, as vehicle systems become increasingly complex and ever more connected to the internet of things, a third source of hazard must be considered, that of intentional manipulation of the electrical/electronic control systems either via direct physical contact or via the systems\' open interfaces. this article describes how the process prescribed by the iso 26262 can be extended with methods from the domain of embedded security to protect the systems against this third source of hazard.automotive functional safety = safety + security','Embedded systems security'
'deeply embedded systems present a number of new challenges and opportunities in security. in this essay, we introduce some of them and explore potential ideas for addressing them.challenges and opportunities in deeply embedded systems security','Embedded systems security'
'this is the first year for this minitrack that evolved from the restructuring the electric power industry minitrack of the emerging technologies track of previous years. this minitrack is now part of the new complex systems track. this minitrack focuses on topics related to the ability of complex systems such as power systems to survive disturbances with minimal impact on performance. specific topics include: steady-state and dynamic security assessment where the impacts of pre-specified contingencies are analyzed; available transfer capability (atc) which quantifies the ability of the interconnected system to accept increases in power transfers; and related technologies.this year\'s papers focus on how security and reliability of electric power systems are affected by changes that continue to emerge from the industry restructuring. the topics presented in these papers are: a. the impact of distributed generation on system voltage stability. b. the definition of available transfer capability as an interval based on alternative dispatch options. c. multi-area probabilistic reliability assessment. d. network control as a distributed, dynamic game. e. the impact of modeling errors on state estimation and system operation. f. evaluation of new on-line automatic generation control techniques. g. extended factor for linear contingency analysis.collectively, these papers offer new ideas for dealing with the challenges of complex power systems and the demands of a competitive environment.complex systems security and reliability','Embedded systems security'
'an automatic control system is a preset closed-loop control system that requires no operator action. this assumes the process remains in the normal range for the control system. elements of automatic control. finding a transfer function for block diagram of cnc machine-tool control system. the control system design will attempt to evaluate the system response by determining a mathematical model for the system. we have described the generalized schematic block-diagram for a closed loop, or feedback control system. feedback loop is the signal path from the output back to the input to correct for any variation between the output levels from the set level. programming serial and parallel port. the broad usage of java language and java cards in various different fields has made them quite applicable and implementable in different applications and systems.control systems, smart sensors, controller, elements in a control loop, java card and security','Embedded systems security'
'this paper reports a cryptosystem designed forsecurity of embedded systems. it is based on the theoryof cellular automata(ca). the cellular automata basedcryptosystem(cac) employs a series of transforms - simple,moderatel complex, complex - all generated with differentclasses of ca. the code size of cac can be found tobe lesser than that of an other scheme published in the literature.cryptanalysis of the proposed scheme is reportedalong with similar analysis for two popular systems - desand aes. experimental results confirm that the security ofthe system is significantly better than des and comparableto that of aes. hardware-soft are co-design of cacwith a simple ca hardware is also reported to establish theefficiency in terms of small code size and high speed of execution.cryptosystem designed for embedded system security','Embedded systems security'
'security challenges in cyber physical systems necessitate solutions that are robust to threats posed especially when used in applications to monitor and secure critical infrastructures. in this work, we provide a decent bibliographic review of the existing literature on security of cyber physical systems, identify key research challenges and discuss future directions on open research issues.cyber physical systems security','Embedded systems security'
'after a general introduction of the viking eu fp7 project two specific cyber-attack mechanisms, which have been analyzed in the viking project, will be discussed in more detail. firstly an attack and its consequences on the automatic generation control (agc) in a power system are investigated, and secondly the cyber security of state estimators in scada systems is scrutinized.cyber-security of scada systems','Embedded systems security'
'a unified approach to dependability and security assessment will let architects and designers face the challenges of shrinking feature size and increasing error rates.dependability and security will change embedded computing','Embedded systems security'
'this paper deals with embedded system networking support, which is aimed at applications based on distributed components interconnected by wired internet and wireless sensor networks. it presents a safety and security-driven approach to embedded system networking that offers reusable patterns for designs aiming at various application domains. after introducing terminology dealing with functionality and dependability, the article copes with industrial, sensor network and internet based architectures. it discusses an integrated networking framework stemming from the ieee 1451.1 smart transducer interface standard, which is an object-based networking model supporting client-server and publish-subscribe communication patterns in group messaging, and from the ip multicast communication, mediating safe and secure access to wireless sensor networks through internet. the case study demonstrates how clients can access groups of wireless smart pressure and temperature sensors and valves, which monitor and control gas pipelines effectively through internet using developed networking architecture that respects prescribed requirements for application dependent safety and security.dependability-driven embedded systems networking','Embedded systems security'
'distributed processing systems security','Embedded systems security'
'we propose a method for dynamic security-domain scaling on smps that offers both highly scalable performance and high security for future high-end embedded systems. its most important feature is its highly efficient use of processor resources, accomplished by dynamically changing the number of processors within a security-domain (i.e., dynamically yielding processors to other security-domains) in response to application load requirements. two new technologies make this scaling possible without any virtualization software: (1) self-transition management and (2) unified virtual address mapping. evaluations show that this domain control provides highly scalable performance and incurs almost no performance overhead in security-domains. the increase in oss in binary code size is less than 1.5&percnt;, and the time required for individual state transitions is on the order of a single millisecond. this scaling is the first in the world to make possible the dynamic changing of the number of processors within a security-domain on an arm smp.dynamic security domain scaling on embedded symmetric multiprocessors','Embedded systems security'
'an increasing number of real-time applications, such as aircraft control and medical electronics systems, require high quality of security to assure confidentiality, authenticity and integrity of information. however, most existing algorithms for scheduling independent tasks in real-time systems do not adequately consider security requirements of real-time tasks. in recognition of this problem we propose a novel dynamic scheduling algorithm with security awareness, which is capable of achieving high quality of security for real-time tasks while improving resource utilization. we have conducted extensive simulation experiments to quantitatively evaluate the performance of our approach. specifically, experimental results show that compared with three heuristic algorithms, the proposed algorithm can consistently improve overall system performance in terms of quality of security and system guarantee ratio under a wide range of workload characteristics.dynamic task scheduling with security awareness in real-time systems','Embedded systems security'
'embedded systems have become an integral part of our everyday life. devices like vehicles, household appliances, and cell phones are already equipped with embedded microcontrollers. the networking of the myriads of embedded devices gives rise to the brave new world of pervasive computing. pervasive computing offers enormous advantages and opportunities for users and businesses through new applications, increased comfort, and cost reduction. one often overlooked aspect of pervasive computing, however, are new security threats. this article describes security issues in current and future pervasive security scenarios, ranging from privacy threats and unreliable products to loss of revenue. we also highlight the opportunities, such as new business models, which are enabled through strong embedded security solutions. current research issues are also summarized. as case studies, we introduce security aspects in future automotive systems and in ad-hoc networks.embedded security in a pervasive world','Embedded systems security'
'there is an ever increasing concern about security threats as embedded systems are moving towards networked applications. model based approaches have proven to be effective techniques for embedded systems design. however, existing modeling tools were not designed to meet the current and future security challenges of networked embedded systems. in this paper, we propose a framework to incorporate security modeling into embedded system design. we\'ve developed a security analysis tool that can easily integrate with existing tool chains to create co-design environments that addresses security, functionality and system architecture aspects of embedded systems concurrently.embedded systems security co-design','Embedded systems security'
'we present an overview of how the need for security will impactthe design of embedded systems and the ics they contain. aftera brief introduction to security, we discuss the challenges securityprocessing poses to embedded system architects, integrators, hwdesigners, and sw engineers. these challenges, as well as currentand emerging solutions to address them, are discussed in the contextof a specific application domain, namely resource-constrainedwireless embedded systems.embedding security in wireless embedded systems','Embedded systems security'
'energy and security are hot topics for real-time embedded systems. however, little traditional research consider the two factors together, which leaves a great challenge to guarantee the security of embedded applications running under battery-powered real-time embedded systems. in this paper, we investigate the characteristics of energy consumption for the most popular security algorithms. to precisely measure the energy cost of various cryptographic algorithms, we build a real measurement platform based on arm9 and &#181;c/osii. we mainly test confidentiality and integrity algorithms included in a public cryptlib. based on the experimental results, we conduct the energy features of asymmetric/symmetric algorithms, hash algorithms, hmac algorithms and the security parameters. furthermore, some recommendations about the utilization of cryptographic algorithms are proposed, which may be very valuable for running security applications in energy constrained embedded systems.energy measurement and analysis of security algorithms for embedded systems','Embedded systems security'
'pekka nikander: do you have any feeling for how much of this system you can model? reply: it&#39;s a moveable feast: you can choose the boundary. but if you don&#39;t have any boundary at all then i don&#39;t think you&#39;ve got enough context...you need to talk about conditions that are established through combinations of sub-protocols, and they have to be in context. i think it&#39;s not just a simple matter of fact, of saying &#8220;ok, we&#39;ll just work with the protocol and hope everything else will go away.&#8221; it won&#39;t. and i don&#39;t yet know where to stop. bruce christianson: when you say &#8220;the protocol&#8221; i assume you&#39;re using that in a narrow sense of just meaning the messages and the state of the protocol itself, is that right? it&#39;s clear that you need to model some of the other aspects of those systems on which the protocols run. reply: yes, that&#39;s where i was coming from. i originally started doing this work on very constrained message sequence charts. when people asked if i could prove completeness, i said that there may be a key to prove completeness for this very narrow class of protocols, but what was the point, because the real problems were all around the side. you need to broaden out the model and then completeness questions probably disappear.from security protocols to systems security','Embedded systems security'
'as computing and communications increasingly pervade our lives, security and protection of sensitive data and systems are emerging as extremely important issues. this is especially true for embedded systems, often operating in non-secure environments, and with limited amount of computational, storage, and communication resources available. in servers and desktop systems, security enhanced linux (selinux) is currently used as a method to enhance security by enforcing a security control based on policies that confine user programs, or processes, to the minimum amount of privileges that they require for their execution. while providing a powerful mean for enhancing security in unix-like systems, selinux still remains a feature that is too heavy to be fully supported by constrained devices. in this paper, we propose a hardware architecture for enhancing security and accelerating retrieval and applications of selinux policies in embedded processors. we describe the general ideas behind our work, discussing motivations, advantages, and limits of the solution proposed, while suggesting the main steps needed to implement the described architecture on common embedded processors.hardware-assisted security enhanced linux in embedded systems','Embedded systems security'
'security requirements for embedded systems such as consumer devices are becoming stronger. current designs need an isolated environment that stores and processes sensitive data. new hardware technologies are arriving that provide low-cost, high-performance, isolated environments. standard open apis are providing a route to interoperability, defragmentation. and reduced software development costs. securely, flexibly, and efficiently taking advantage of these standards is a complex software design problem. this article is an introduction to one such hardware technology, and a case study of the design of a programmable security software framework. the discussion will be of interest to all types of system designers, from soc to software, because security must be designed into the system from the outset.implementing embedded security on dual-virtual-cpu systems','Embedded systems security'
'implementing information security management systems','Embedded systems security'
'while many scheduling algorithms for periodic tasks ignore security requirements posed by sensitive applications and are, consequently, unable to perform properly in embedded systems with security constraints, in this paper, we present an approach to scheduling periodic tasks in embedded systems subject to security and timing constraints. we design a necessary and sufficient feasibility check for a set of periodic tasks with security requirements. with the feasibility test in place, we propose a scheduling algorithm, or sases (security-aware scheduling for embedded systems), which accounts for both security and timing requirements. sases judiciously distributes slack times among a variety of security services for a set of periodic tasks, thereby optimizing security for embedded systems without sacrificing schedulability. to demonstrate the effectiveness of sases, we apply the proposed sases to real-world embedded systems such as an automated flight control system. we show, through extensive simulations, that sases is able to maximize security for embedded systems while guaranteeing timeliness. in particular, sases significantly improves security over three baseline algorithms by up to 107&percnt;.improving security for periodic tasks in embedded systems through scheduling','Embedded systems security'
'information security in open systems','Embedded systems security'
'this study investigated employees\' information systems security policy (issp) compliance behavioural intentions in organisations from the theoretical lenses of social bonding, social influence, and cognitive processing. given that previous research on issp compliance has been based on deterrence theory, this study seeks to augment and diversify research on issp compliance through its theoretical perspective. relevant hypotheses were developed to test the research conceptualisation. data from a survey of business managers and is professionals confirmed that social bonds that are formed at work largely influence attitudes towards compliance and subjective norms, with both constructs positively affecting employees\' issp compliance. employees\' locus of control and capabilities and competence related to is security issues also affect issp compliance behavioural intentions. overall, the constructs in the research model enhance our understanding of the social-organisational and psychological factors that might encourage or accentuate employees\' issp compliance in the workplace.information systems security policy compliance','Embedded systems security'
'this paper emerges from research by [1],[11], [22] and [21], and it draws on real-world examples so as to underline some limits of quantitative risk assessment. the paper is a case study and emphasis that theoretical formulas used in information security risk assessments do not contain the time dimension of the analysis. the article further develops findings published in our article information security risk assessment: the qualitative versus quantitative dilemma [21] as we agree that the risk of information system security may only be assessed or estimated, but in practice, it cannot be measured accurately. a degree of trust should be associated with the assessment made by the security analyst. there are other elements that must be evaluate: average time for threat identification, average time for releasing technical procedures to reduce or accept threat and average time necessary until the system becomes operational and the threat is eliminated. the value of loss is different in any of the three moments and should be estimate for any of them.information systems security risk assessment','Embedded systems security'
'to achieve a certain degree of information systems security different techniques have been proposed and implemented. it is the aim of this paper to form a basis for their evaluation and comparison. for this purpose a general framework of security is established by defining its scope, most common threats against the security, and two kinds of different comparison and evaluation criteria. the first criteria is a set of requirements on the secrecy and confidentiality of information while the second consists of several structural requirements which we believe are essential for a successful and powerful security technique. in our evaluation we include the discretionary models, the mandatory models, the personal knowledge approach, the chinese wall policy and the clark and wilson model of security.information systems security','Embedded systems security'
'the corporate information systems users often engage in risky behavior that can threaten the security and integrity of an organization by exposing sensitive information or weakening the existing technological perimeter security. this risky user behavior can be intentional or unintentional, but in either case can cause severe damage to an organization\'s reputation as well as potentially extending harm to the organization\'s clients and customers. information systems users not following the corporate security policies, even though they know the policies, is known as user omissive behavior, also known as the knowing-doing gap. this research examines the information assurance understanding and security awareness at the user level by developing a structured model of the user knowing-doing gap. the model examines the role of organizational narcissism and its affect on user attitudes towards following the organization\'s information security policies and procedures. it also includes perceived threat as a factor affecting user attitudes towards following information security rules, as well as subjective norms and perceived behavior control consistent with the theory of planned behavior. this structured model provides a framework and description of user information security behavior and the knowing-doing gap.information systems user security','Embedded systems security'
'in the past five years, mandatory security requirements and fault tolerance have become critical criteria for most real-time systems. although many conventional fault-tolerant or security approaches were investigated and applied to real-time systems, most existing schemes only addressed either security demands ignoring the fault-tolerant requirements or vice versa. to bridge this technology gap in real-time systems, in this paper we propose a way of integrating fault recovery and confidentiality services. the novel integration of security and fault recovery makes it possible to implement next-generation real-time systems with high reliability and quality of security. experimental results from real-world applications show that our approach can significantly improve security over the conventional approaches by up to 661.56\% while providing an efficient means of fault tolerance.integrating fault recovery and quality of security in real-time systems','Embedded systems security'
'security is a crucial issue for information systems. traditionally, security is considered after the definition of the system. however, this approach often leads to problems, which translate into security vulnerabilities. from the viewpoint of the traditional security paradigm, it should be possible to eliminate such problems through better integration of security and systems engineering. this paper argues for the need to develop a methodology that considers security as an integral part of the whole system development process. the paper contributes to the current state of the art by proposing an approach that considers security concerns as an integral part of the entire system development process and by relating this approach with existing work. the different stages of the approach are described with the aid of a case study; a health and social care information system.integrating security and systems engineering','Embedded systems security'
'there is an ever increasing concern about security threats as embedded systems are moving towards networked applications. model based approaches have proven to be effective techniques for embedded systems design. however, existing modeling tools were not designed to meet the current and future security challenges of networked embedded systems. in this paper, we propose a framework to incorporate security modeling into embedded system design. we\'ve developed a security analysis tool that can easily integrate with existing tool chains to create co-design environments that addresses security, functionality and system architecture aspects of embedded systems concurrently.integrating security modeling into embedded system design','Embedded systems security'
'a variety of solutions have been proposed for ensuring data integrity in nonreal-time systems (i.e. batch or on-line systems). a brief review is made of some of the techniques employed in these solutions. it is indicated why the data integrity problem is different in a real-time system than in a nonreal-time system. two models of interprocess communication are presented and it is demonstrated that the models are sufficient to preserve data integrity in a real-time system.interprocess communication in real-time systems','Embedded systems security'
'issues and approaches to supporting timeliness and security in real-time database systems','Embedded systems security'
'we propose an interdisciplinary it security lab projectwhich combines topics of computer architecture, cryptographyand software engineering. in this lab undergraduatestudents of ee/cs are supposed to efficiently implement thecryptographic block cipher advanced encryption standard(aes) on a smart card with an embedded atmel atmega163reduced instruction set computer (risc) microcontrollerin assembly. moreover, the students learn about side channelattacks. side channel attacks are based on the fact thatdata processed by a microprocessor is generally correlatedwith its power consumption and electromagnetic radiation.finally, the students learn how to secure their implementationsagainst these attacks using software countermeasures,such as random operand blinding and dummy cylces.it security project','Embedded systems security'
'in [12], the authors present a new light-weight cryptographic primitive which supports an associated rfid-based authentication protocol. the primitive has some structural similarities to aes, but is presented as a keyed one-way function using a 128-bit key. although a security analysis is included, this is at a high-level only. to provide a more concrete idea as to the security of this primitive, we therefore make three contributions: first, a structural attack requiring o(25) plaintext/ciphertext pairs (and hence effort online) plus o(221) effort offline, second algebraic attacks on round reduced versions of the primitive which requires only a single plaintext/ciphertext pair, and, third debunk the claimed attack of [36] on the same primitive. our structural attack completely breaks the primitive and the algebraic attack highlights a crucial weakness of the primitive; we conclude that although one can consider countermeasures against these specific attacks, the design in general is questionable and should therefore be avoided.light-weight primitive, feather-weight security','Embedded systems security'
'we propose a lightweight rfid authentication protocol that supports forward and backward security. the only cryptographic mechanism that this protocol uses is a pseudorandom number generator (prng) that is shared with the backend server. authentication is achieved by exchanging a few numbers (3 or 5) drawn from the prng. the lookup time is constant, and the protocol can be easily adapted to prevent online man-in-the-middle relay attacks. security is proven in the uc security framework.lightweight rfid authentication with forward and backward security','Embedded systems security'
'the paper presents a spatial role-based access control (srbac) framework and its application to healthcare information systems that allow wireless access to information. the framework secures access to medical information and resources accessible through mobile computing devices by healthcare personal and patients. the framework utilizes location information in access control decisions, in order to determine the permissions a role encompass at a given location. the permissions of a role are changing dynamically depending of role owner movement.location-based security framework for use of handheld devices in medical information systems','Embedded systems security'
'the increasing rate of reported computer security problems suggests that the highly structured and technical traditional approaches to the management of is security do not appear to be successful. this paper presents the orion strategy, a participative approach to the planning and management of information security in organisations. the details of this approach are discussed along with an overview of its pilot implementation within an australian organisation. the findings from the study indicate that a high level of user participation in the planning and management of security results in raised awareness of security issues and an ownership of responsibility for the successful operations of chosen security measures.managing information systems security','Embedded systems security'
'fpgas combine the programmability of processors with the performance of custom hardware. as they become more common in critical embedded systems, new techniques are necessary to manage security in fpga designs. this article discusses fpga security problems and current research on reconfigurable devices and security, and presents security primitives and a component architecture for building highly secure systems on fpgas.managing security in fpga-based embedded systems','Embedded systems security'
'java 2 micro edition (j2me) is a runtime environment for resource-constrained environments. there are already millions of j2me enabled mobile devices that support the firstversion of the mobile information device profile (midp) in j2me. many security threats exist in midp 1.0 environment since the specification addresses only a limited number of security issues. it is supposed that the next version of the midp will have more secure environment for mobile business and personal applications and solve the important security problems in midp 1.0.in this paper, we give short introduction to the j2me and midp environment. based on existing literature, we explain the threats and security needs in mobile environment for midp 1.0 applications. we cover the new security mechanisms and features in midp 2.0 and analyze against presented threats, how these address the existing security problems in midp 1.0.in this paper we conclude that midp 2.0 improves several different issues in midp security. especially the secure network protocols and signed applications are major improvements. we also conclude that there still exist problems in midp 2.0 security, mainly related to the pki that is part of trusted applications and new secure protocols.midp 2.0 security enhancements','Embedded systems security'
'broadcast authentication is a critical security service in sensor networks; it allows a sender to broadcast messages to multiple nodes in an authenticated way. &#181;tesla and multi-level &#181;tesla have been proposed to provide such services for sensor networks. however, none of these techniques are scalable in terms of the number of senders. though multi-level &#181;tesla schemes can scale up to large sensor networks (in terms of receivers), they either use substantial bandwidth and storage at sensor nodes, or require significant resources at senders to deal with dos attacks. this paper presents ef?cient techniques to support a potentially large number of broadcast senders using &#181;tesla instances as building blocks. the proposed techniques are immune to the dos attacks. this paper also provides two approaches, a revocation tree based scheme and a proactive distribution based scheme, to revoke the broadcast authentication capability from compromised senders. the proposed techniques are implemented, and evaluated through simulation on tinyos. the analysis and experiment show that these techniques are ef?cient and practical, and can achieve better performance than the previous approaches.practical broadcast authentication in sensor networks','Embedded systems security'
'secure hardware is a useful tool for enhancing computer system security. traditionally, researchers have attempted to build secure operating systems by creating secure hardware and developing on top of it. our approach is to integrate commodity secure hardware, i.e., smartcards, into existing operating systems.this paper describes three projects aimed at practical secure operating systems based on smartcards: smartcard integration with kerberos v5, a unix filesystem for smartcards, and internet protocol on smartcards. the first two are implemented and indicate satisfactory performance, while the last is under development.practical security systems with smartcards','Embedded systems security'
'several research groups are working on designing new security architectures for 4g networks such as hokey and y-comm. since designing an efficient security module requires a clear identification of potential threats, this paper attempts to outline the security challenges in 4g networks. a good way to achieve this is by investigating the possibility of extending current security mechanisms to 4g networks. therefore, this paper uses the x.805 standard to investigate the possibility of implementing the 3g&#8217;s authentication and key agreement (aka) protocol in a 4g communication framework such as ycomm. the results show that due to the fact that 4g is an open, heterogeneous and ip-based environment, it will suffer from new security threats as well as inherent ones. in order to address these threats without affecting 4g dynamics, y-comm proposes an integrated security module to protect data and security models to target security on different entities and hence protecting not only the data but, also resources, servers and users.providing security in 4g systems','Embedded systems security'
'vehicle stealing is now a day\'s common problem. keeping this in consideration we are trying to build a project which can recover your vehicle after stolen. for this we need to developed and install some system inside vehicle which will tell you the vehicle location after stolen. we also are trying to develop gps and gsm enabled system which will help customer track his vehicle remotely using just a mobile phone and one internet application. if customer want to track his vehicle location he only need to make a sms to system installed in to vehicle then vehicle will detect it\'s position using gps system and just send reply customer the location of vehicle in the form of sms. after getting sms reply customer can get vehicle location by providing location as an input to google earth software. a wireless and intelligent vehicle alarm system that sends signals directly to your mobile phone through mobile gsm and gps signals to let you know your vehicle is safe. if your vehicle is moved or the door is opened you can send signals to your vehicle to cut off the gas or power supply for the ultimate in vehicle security. gps functions also allow you to know where your vehicle is at all times. the best in vehicle security is right at your fingertips.real time avl security system','Embedded systems security'
'automobile tracking in the private and defense sector has required a vast amount of research and development. this paper presents the two-way multiple vehicles tracking system using gsm network and satellite communication. the multi-vehicle tracking system uses an extensive combination of global positioning system (gps), gsm network and digital mapping with cost effective hardware solution. the tracking system works on the synchronization of the vehicle client unit and the base station. multi-layered digitized maps results real-time and precise location tracking and provides the various detail information of environment. the system is exploited for vehicle security providing opportunity to remote server to secure the vehicle in case of theft with indispensable anti theft device. moreover the system provides text guidance to the client through embedded lcd and keypad interfacing.real time fleet monitoring and security system using gsm network','Embedded systems security'
'passive network monitoring that observes user traffic has many advantages over active monitoring that uses test packets. it can provide characteristics of real user traffic, that cannot be detected actively. however, when processing user traffic, wemust guarantee user privacy. this is a task of packet header anonymization that removes sensitive information, while keeping as much as possible of the original traffic properties. in this paper we present design and implementation of an fpga-based packet header anonymization that unlike previous approaches operates in real time and prevents sensitive information from getting to the monitoring pc and beyond.real-time anonymization in passive network monitoring','Embedded systems security'
'embedded systems present significant security challenges due to their limited resources and power constraints. we propose a novel security architecture for embedded systems (sanes) that leverages the capabilities of reconfigurable hardware to provide efficient and flexible architectural support to both security standards and a range of attacks. this paper shows the efficiency of reconfigurable architecture to implement security primitives within embedded systems. we also propose the use of hardware monitors to detect and defend against attacks. the sanes architecture is based on three main ideas: 1) reconfigurable security primitives, 2) reconfigurable hardware monitors and 3) a hierarchy of security controllers at the primitive, system and executive level. results are presented for a reconfigurable aes security primitive within the ipsec standard and highlight the interest of such a solution.reconfigurable security support for embedded systems','Embedded systems security'
'research on information security has been based on a well-established definition of the subject. consequently, it has delivered a plethora of methods, techniques, mechanisms and tools to protect the so-called security attributes (i.e. availability, confidentiality and integrity) of information. however, a modern information system (is) appear rather vulnerable and people show mistrust on their ability to deliver the services expected. this phenomenon leads us to the conclusion that information security does not necessarily equal is security. in this paper, we argue that is security, contrary to information remains a confusing term and a neglected research area. we attempt to clarify the meaning and aims of is security and propose a framework for building secure information systems, or as we suggest them to be called, viable information systems.redefining information systems security','Embedded systems security'
'in this paper we attempt to answer two questions: (1) why should we be interested in the security of control systems? and (2) what are the new and fundamentally different requirements and problems for the security of control systems? we also propose a new mathematical framework to analyze attacks against control systems. within this framework we formulate specific research problems to (1) detect attacks, and (2) survive attacks.research challenges for the security of control systems','Embedded systems security'
'the real-time immersive network simulation environment (rinse) simulator is being developed to support large-scale network security preparedness and training exercises, involving hundreds of players and a modeled network composed of hundreds of local-area networks (lans). the simulator must be able to present a realistic rendering of network behavior as attacks are launched and players diagnose events and try counter measures to keep network services operating. the authors describe the architecture and function of rinse and outline how techniques such as multiresolution traffic modeling, multiresolution attack models, and new routing simulation methods are used to address the scalability challenges of this application. they also describe in more detail new work on cpu/memory models necessary for the exercise scenarios and a latency absorption technique that will help when extending the range of client tools usable by the players.rinse','Embedded systems security'
'government and industry increasingly rely on modern information systems (is) for mission successes. but their critical is must survive in hostile environments; thus, mission owners need systems security engineers to build systems that are secure against real-world attacks but not over-engineered against a particular one. by understanding which attacks are most likely and which risks are most serious, mission owners can make cost-effective countermeasures decisions. we describe a systems security-engineering methodology for enumerating system attacks, assessing risks, and choosing countermeasures that best mitigate the risks.risk-based systems security engineering','Embedded systems security'
'we present a view on the system security, which draws from the previous experiences in dealing with system safety. this survey paper focuses on exploring the commonalities between safety and security with both treated as mutually complementary view of the same problem: security as protecting a computer system against the threats of the external environment, and safety as protecting the environment from potential dangers of a computer system. mutual relationships of safety and security are discussed.safety and security in industrial control','Embedded systems security'
'the paper presents a safety and security-driven approach to embedded system design for an industrial class of internet-based applications. it discusses an integrated networking framework stemming from the ieee 1451.1 smart transducer interface standard, which is an object-based networking model supporting client-server and publish-subscribe communication patterns in group messaging, and from the ip multicast communication, all together mediating safe and secure access to smart sensors through internet. the case study demonstrates how clients can access groups of wireless smart pressure and temperature sensors and valves effectively through internet using developed system architecture while respecting prescribed requirements for application dependent safety and security.safety and security-driven design of networked embedded systems','Embedded systems security'
'radio frequency identification (rfid) raises privacy concerns such as malicious traceability and clandestine information collection. in this paper, we consider the need for scalable privacy protecting scheme as rfid technology will grow more prevalent. our contribution is twofold: first we propose a lightweight privacy protecting scheme that aims at reducing the calculation burden on the back-end database. our protocol achieves more scalable tag identification by taking advantage of the characteristics of some current rfid applications; second, we evaluate the performance of our protocol in terms of computational load for the database and we examine how this performance varies according to applications. moreover we analyze our protocol against various attacks and demonstrate the security of our proposed scheme.scalable privacy protecting scheme through distributed rfid tag identification','Embedded systems security'
'security-critical real-time applications such as military aircraft flight control systems have mandatory security requirements in addition to stringent timing constraints. conventional real-time scheduling algorithms, however, either disregard applications\' security needs and thus expose the applications to security threats or run applications at inferior security levels without optimizing security performance. in recognition that many applications running on clusters demand both real-time performance and security, we investigate the problem of scheduling a set of independent real-time tasks with various security requirements. we build a security overhead model that can be used to reasonably measure security overheads incurred by the security-critical tasks. next, we propose a security-aware real-time heuristic strategy for clusters (sarec), which integrates security requirements into the scheduling for real-time applications on clusters. further, to evaluate the performance of sarec, we incorporate the earliest deadline first (edf) scheduling policy into sarec to implement a novel security-aware real-time scheduling algorithm (saedf). experimental results from both real-world traces and a real application show that saedf significantly improves security over three existing scheduling algorithms (edf, least laxity first, and first come first serve) by up to 266.7 percent while achieving high schedulability.scheduling security-critical real-time applications on clusters','Embedded systems security'
'many real-time database applications arise in electronic financialservices, safety-critical installations and military systems whereenforcing security is crucial to the success of the enterprise. forreal-time database systems supporting applications with firm deadlines,we investigate here the performance implications, in terms of killedtransactions, of guaranteeing multilevel secrecy. in particular, wefocus on the concurrency control (cc) aspects of this issue.our main contributions are the following: first, we identify whichamong the previously proposed real-time cc protocols are capable ofproviding covert-channel-free security. second, using a detailedsimulation model, we profile the real-time performance of arepresentative set of these secure cc protocols for a variety ofsecurity-classified workloads and system configurations. ourexperiments show that a prioritized optimistic cc protocol, opt-wait,provides the best overall performance. third, we propose and evaluatea novel &#8220;dual-cc&#8221; approach that allows the real-time database systemto simultaneously use different cc mechanisms for guaranteeing securityand for improving real-time performance. by appropriately choosingthese different mechanisms, concurrency control protocols that provideeven better performance than opt-wait are designed. finally, wepropose and evaluate guard, an adaptive admission-control policydesigned to provide fairness with respect to the distribution of killedtransactions across security levels. our experiments show that guardefficiently provides close to ideal fairness for real-time applicationsthat can tolerate covert channel bandwidths of upto one bit persecond.secure concurrency control in firm real-time database systems','Embedded systems security'
'the paper presents the electronic wallet solution implemented within a gsm sim technology for accessing medical services. the medical e-wallet should achieve two major goals: accessing information on the patient\'s medical history and payment for private medical services. the security issue is a very important one as the patient\'s history is confidential and the payment has to be safe.secure mobile electronic card used in medical services','Embedded systems security'
'a top-down, multiabstraction layer approach for embedded security design reduces the risk of security flaws, letting designers maximize security while limiting area, energy, and computation costs.securing embedded systems','Embedded systems security'
'how do we protect systems? the answer is straightforward: each component must be evaluated independently and protected as necessary. beware the easy answers, such as deploying stronger encryption while ignoring vulnerable end points; that\'s too much like looking under the streetlamp for lost keys, not because they\'re likely to be there but because it\'s an easy place to search. remember, too, that people and processes are system components as well, and often the weakest ones&#8212;think about phishing, but also about legitimate emails that are structurally indistinguishable from phishing attacks. i\'m not saying you should ignore one weakness because you can\'t afford to address another serious one&#8212;but in general, your defenses should be balanced. after that, of course, you have to evaluate the security of the entire system. components interact, not always in benign ways, and there may be gaps you haven\'t filled.security as a systems property','Embedded systems security'
'embedded systems security is a significant requirement in emerging environments, considering the increasing deployment of embedded systems in several application domains. the large number of deployed embedded systems, their limited resources and their increasing complexity render systems vulnerable to an increasing number of threats. additionally, the involvement of sensitive, often private, information and the expectation for safe and dependable embedded platforms lead to strong security requirements, even legal ones, which require new technologies for their provision. in this article, we provide an overview of embedded security issues, used methods and technologies, identifying important challenges in this emerging field.security challenges in embedded systems','Embedded systems security'
'security is usually not in the main focus in the development of embedded systems. however, strongly interconnected embedded systems play vital roles in many everyday processes and also in industry and critical infrastructures. therefore, security engineering for embedded systems is a discipline that currently attracts more interest. this paper presents the vision of security engineering for embedded systems formulated by the fp7 project secfutur [1].security engineering for embedded systems','Embedded systems security'
'computers and internet have evolved into necessary tools for our professional, personal and social lives. as a result of this growing dependence, there is a concern that these systems remain protected and available. this concern increases exponentially when considering systems such as smart power grids. therefore, research should be conducted to develop effective ways of detecting system anomalies. to have realistic results, the studies should be tested on real systems. however, it is not possible to test these experiments on the live network. with the recent collaboration of universities and research labs, a new experiment test bed has been established. as a result, experiments can now be implemented on real networks. in our study, we design an experiment to analyze distributed denial of service attacks (ddos attack) on a real network with real internet traffic. the approach that we use in our study can easily be generalized to apply to smart power grids.security experimentation using operational systems','Embedded systems security'
'security for distributed systems','Embedded systems security'
'the recent trend towards dynamically extensible systems, such as java, spin or vino, promises more powerful and flexible systems. at the same time, the impact of extensibility on overall system security and, specifically, on access control is still ill understood, and protection mechanisms in these extensible systems are rudimentary at best. we identify the structure of extensible systems as it relates to system security and postulate an initial model for access control. this model extends the discretionary access control of traditional operating systems to encompass extensions and, by using ideas explored by the security community, introduces a notion of mandatory access control. while a new access control model does not address all aspects of system security, we believe that it can serve as a solid foundation for developing a fully featured and flexible security model for extensible systems.security for extensible systems','Embedded systems security'
'there are four possible security attacks: attacks from host to host, from agents to hosts, from agents to agents and finally from hosts to agents. our main concern in this paper is attacks from a malicious host on an agent. these attacks can take many forms including rerouting, spying out code, spying out data, spying out control flow, manipulation of code, manipulation of data, manipulation of control flow, incorrect execution of code, masquerading and denial of execution. this paper presents a three-tier solution of code mess up, encryption and time out.security in mobile agent systems','Embedded systems security'
'security in mobile systems','Embedded systems security'
'from the viewpoints of dependable computing and ubiquitous computing, a new type of reactive systems, named persistently reactive systems, was proposed. persistently reactive systems cause some new security issues because of their continuous and persistent running without stopping their services. based on the recognition that a persistently reactive systems can be constructed following the methodology of soft system buses, this paper defines security issues in persistently reactive systems with security requirements and security functions. to solve the issues, we propose a framework of ssb-connector, such that designers and developers can easily design and develop reliable and secure functional components of a persistently reactive system.security in persistently reactive systems','Embedded systems security'
'security in wireless systems','Embedded systems security'
'computing systems designed using reconfigurable hardware are increasingly composed using a number of different intellectual property (ip) cores, which are often provided by third-party vendors that may have different levels of trust. unlike traditional software where hardware resources are mediated using an operating system, ip cores have fine-grain control over the underlying reconfigurable hardware. to address this problem, the embedded systems community requires novel security primitives that address the realities of modern reconfigurable hardware. in this work, we propose security primitives using ideas centered around the notion of &#8220;moats and drawbridges.&#8221; the primitives encompass four design properties: logical isolation, interconnect traceability, secure reconfigurable broadcast, and configuration scrubbing. each of these is a fundamental operation with easily understood formal properties, yet they map cleanly and efficiently to a wide variety of reconfigurable devices. we carefully quantify the required overheads of the security techniques on modern fpga architectures across a number of different applications.security primitives for reconfigurable hardware-based systems','Embedded systems security'
'with more embedded systems networked, it becomes animportant research problem to effectively defend embeddedsystems against buffer overflow attacks and efficientlycheck if systems have been protected. in this paper, we proposethe hsdefender (hardware/software defender) techniquethat considers the protection and checking togetherto solve this problem. our basic idea is to design a secureinstruction set and require third-party software developersto use secure instructions to call functions. then the securitychecking can be easily performed by system integratorseven without the knowledge of the source code. wefirst classify buffer overflow attacks into two categories,stack smashing attacks and function pointer attacks, andthen provide two corresponding defending strategies. weanalyze the hsdefender technique in respect of hardwarecost, security, and performance, and experiment with it onthe simplescalar/arm simulator using benchmarks frommibench. the results show that hsdefender can defend asystem against more types of buffer overflow attacks withless overhead compared with the previous work.security protection and checking in embedded system integration against buffer overflow attacks','Embedded systems security'
'with the potential to dramatically decrease costs and improve efficiency in diverse application areas ranging from asset tracking and supply chain automation to healthcare patient administration and animal tracking, rfid has become one of the fastest-growing new it technologies. in order for businesses to recover costs, rfid tags must be inexpensive, making security infrastructure difficult to install. the public has reacted with alarm to the possibility that anyone with an rfid reader could obtain information on their purchases, finances, and travel habits, and possibly even more sensitive information such as medical records, and thus, in response, manufacturers are introducing more privacy and security features in new rfid standards.in this column we survey current and future developments in rfid security standards.security standards for the rfid market','Embedded systems security'
'researchers interested in security often wish to introduce new primitives into a language. extensible languages hold promise in such scenarios, but only if the extension mechanism is sufficiently safe and expressive. this paper describes several modifications to an extensible language motivated by end-to-end security concerns.security through extensible type systems','Embedded systems security'
'to have certainty about identities is crucial for secure communication in digital environments. the number of digital identities that people and organizations need to manage is rapidly increasing, and proper management of these identities is essential for maintaining security in online markets and communities. traditional identity management systems are designed to facilitate the management of identities from the perspective of the service provider, but provide little support on the user side. the difficulty of managing identities on the user side causes vulnerabilities that open up for serious attacks such as identity theft and phishing. petname systems have been proposed to provide more user friendly and secure identity management on the user side. this paper provides an analysis of the petname model by describing its history and background, properties, application domains and usability issues with emphasis on security usability. by covering a broad set of aspects, this paper is intended to provide a comprehensive reference for the petname system. security usability of petname systems','Embedded systems security'
'security and reliability are important attributes of complex software systems. it is now common to use quantitative methods for evaluating and managing reliability. in this work we examine the feasibility of quantitatively characterizing some aspects of security.in particular, we investigate if it is possible to predict the number of vulnerabilities that can potentially be identified in a future release of a software system. we use several major operating systems as representatives of complex software systems. the data on vulnerabilities discovered in some of the popular operating systems is analyzed. we examine this data to determine if the density of vulnerabilities in a program is a useful measure. we try to identify what fraction of software defects are security related, i.e., are vulnerabilities. we examine the dynamics of vulnerability discovery hypothesizing that it may lead us to an estimate of the magnitude of the undiscovered vulnerabilities still present in the system. we consider the vulnerability-discovery rate to see if models can be developed to project future trends. finally, we use the data for both commercial and open-source systems to determine whether the key observations are generally applicable. our results indicate that the values of vulnerability densities fall within a range of values, just like the commonly used measure of defect density for general defects. our examination also reveals that vulnerability discovery may be influenced by several factors including sharing of codes between successive versions of a software system.security vulnerabilities in software systems','Embedded systems security'
'this article presents a mixed hardware-software approach for balancing security and performance of cryptographic computations in embedded systems. the authors assume that the system employs elliptic-curve cryptography (ecc) for data protection. specifically, they consider ecc based on elliptic curves over prime fields recommended by the national institute of standards and technology (nist). the authors study a system prototype that uses a flexible hardware processor for accelerating expensive ecc computations. their approach supports multiple levels of security and performance, which allows the system to adapt to different application requirements.security-performance trade-offs in embedded systems using flexible ecc hardware','Embedded systems security'
security,'Embedded systems security'
'cscw systems provide computer support to facilitate cooperation between users. in this paper we propose an approach for the formal specification of functionality requirements and confidentiality security requirements of a cscw application. these requirements give rise to safety and confidentiality properties that a cscw system, supporting the application, should uphold. the specification technique is illustrated with a case study.specifying security for cscw systems','Embedded systems security'
'technological developments such as biometrics are providing new potential for next generation security systems. at the same time these developments can make the system more complex to manage. some classes of systems have a fundamental requirement to survive be that to ensure an organization does not loose tens of millions of dollars due to downtime or to ensure there is not a security breach. autonomic self-managing systems are motivated to hide the ever increasing complexity in today&#8217;s systems but through their selfware approach they also offer the potential to create survivable systems. this paper details one such approach, to create a survivable security system for correction centers.survivable security systems through autonomicity','Embedded systems security'
'real-time embedded systems are increasingly being networked. in distributed real-time embedded applications, e.g., electric grid management and command and control applications, it is required to not only meet real-time constraints but also support the data confidentiality,integrity, and authenticity. unfortunately, in general, cryptographic functions are computationally expensive, possibly causing deadline misses in real-time embedded systems with limited resources. as a basis for cost-effective security support in real-time embedded systems, we define a quantitative notion of strength of defense (sod). based on the sod concept, we propose a novel adaptive security policy in which the sod can be degraded by decreasing the cryptographic key length for certain tasks, if necessary, to improve the success ratio under overload conditions. our approach is lightweight. the time complexity of our approach is linear and its amortized version has the constant overhead per sod adaptation period. moreover, our approach supports desirable security features requiring an attacker to do extra work to find the cryptographic key. in the performance evaluation, we show that our approach can considerably improve the success ratio due to controlled sod degradation under overload.systematic security and timeliness tradeoffs in real-time embedded systems','Embedded systems security'
'today\'s security standards aren\'t based in empirical success at securing systems but in the combined experience of successful security engineers. traditional systems-engineering approaches haven\'t until recently been systematically applied to security problems. these methods now show promise in shedding light on increasingly hard security problems. a systems-engineering security roadmap recommends that systems engineers and security engineers converge on empirical methods.systems security engineering','Embedded systems security'
'existing measures to secure the internal data on mobile devices can generally fall into two categories: passwords and biometrics. neither is satisfactory, for different reasons. we propose a new scheme, analogous to using physical keys to unlock doors or a plug-in security dongle to unlock software. the user wears one or more security tokens whose presence in the close proximity of the user\'s cell phone allows the secrets on the phone to be unlocked. as long as the thief is unable to steal both the token and the cell phone, security is mostly guaranteed. we call this approach tangible security. this preliminary paper describes how such a system could be built and used.tangible security for mobile devices','Embedded systems security'
'in this paper we describe the construction of a taxonomy for port security systems that we performed as part of the eu fp-7 project support (security upgrade for ports). the purpose of the taxonomy is to enable port stakeholders to exchange information and to provide them with computer-based automatic decision support systems, assisting the human operator in the assessment of threat levels for a number of pre-defined threats. the decision support system uses text based automatic reasoning and high-level information fusion to identify threat indicators in the input data. thus, the existence of a taxonomy containing well-defined terms that can be used by the reasoning system is essential. in the paper we describe the method used to construct the taxonomy, viz. first constructing a draft taxonomy and then gathering feedback on this using questionnaires. the questionnaires were motivated by the necessity to embody experience and knowledge from different groups of people involved in the project, most of which are not used to formally defining their vocabulary. over-all, the method proved to work well and produced the expected result, namely a basic taxonomy that can be used by a decision support system, and that can be extended during the project according to need.taxonomy for port security systems','Embedded systems security'
'today, the embedded systems field is growing rapidly, ranging from low-end systems such as cellular phones, pdas, smart cards to high-end systems such as gateways, firewalls, storage servers, and web server. security of embedded system becomes a paramount issue in embedded system design. compared to an embedded system\'s functionality and other design metric (e.g., area, performance, power), security is currently specified by system architects in a vague and imprecise manner. this paper proposes a fuzzy integrated security evaluation method based on man-computer combined data collection and fuzzy expert evaluation in delphi method. the method could reduce the subjectivity of expert evaluation and alleviate the difficulty of data collection and makes possible a better combination of qualitative and quantitative evaluations. firstly, the hierarchy structure model of embedded system security is constructed. secondly, according to data collected and the evaluation comment of each expert, the subjection degree matrix is constructed. finally, a new concept of &#8220;degree of assurance&#8221; is presented for the quantificational evaluation of embedded system security. in this paper a study of a wireless biometric authentication device is also shown. the case illustrates that the method can be easily used and its results conform to the actual situation.the fuzzy integrated evaluation of embedded system security','Embedded systems security'
'smart messages (sms) are migratory execution units used to describe distributed computations over mobile ad hoc networks of embedded systems. the main benefits providedby sms are flexibility, scalability, and the ability to perform distributed computations over networks composed of heterogeneous, resource constrained, unattended embedded systems. a key challenge that confronts sms, however, is how to define a security architecture that protects both the sms and the hosts, while preserving the sm benefits.in this paper, we present a basic sm security architecture which sets up a framework for the security related issues of sms and provides solutions for authentication, authorization, and secure sm migration. since this paper is the first attempt to investigate the unique security challenges posed by a system based on mobile code executed over mobile ad hoc networks, we also discuss the main issues that remain to be solved for a more comprehensive sm security architecture.toward a security architecture for smart messages','Embedded systems security'
'a number of real-time embedded systems (rtess) are used to manage critical infrastructure such as electric grids or c4i systems. in these systems, it is essential to meet deadlines, for example, to avoid a power outage or loss of a life. the importance of security support is also increasing, because more rtess are being networked. to securely transmit sensitive data, e.g., a battle field status, across the network, rtess need to protect the data via cryptographic techniques. however, security support may cause deadline misses or unacceptable qos degradation. as an initial effort to address this problem, we formulate the security support in rtess as a qos optimization problem. also, we propose a novel adaptive approach for security support in which a rtes initially uses a relatively short cryptographic key to maximize the qos, while increasing the key length when the security risk level is raised. in this way, we can make a possible cryptanalysis several orders of magnitude harder by requiring the attacker to search a larger key space, while meeting all deadlines by degrading the qos in a controlled manner. to minimize the overhead, we derive the appropriate qos levels for several key lengths via an offine polynomial time algorithm. when the risk level is raised online, a real-time task can use a longer key and adapt to the corresponding qos level (derived offine) in o(1) time.towards security and qos optimization in real-time embedded systems','Embedded systems security'
'we propose and develop home security system based on sensor network (hssn) configured by sensor nodes including radio frequency (rf), ultrasonic, temperature, light and sound sensors. our system can acknowledge security alarm events that are acquired by sensor nodes and relayed in the hop-by-hop transmission way. there are sensor network, home security mobile robot (hsmr) and home server in this system. in the experimental results of this system, we presented that our system has more enhanced performance of response to emergency context and more speedy and accurate path planning to target position for arriving an alarm zone and acquiring the context-aware information.ubiquitous home security robot based on sensor network','Embedded systems security'
'one of the widely used security mechanism for sensor network is the software &#8211; hardware implementation of cryptographic algorithms. if the confidential data is lost as a result of adversary effect, then the whole sensor network is prone to get exposed to the intruder. what we need is a strong mechanism, to protect such sensitive data. in this paper we present a technique where we mask all the intermediate input and output data with some values in order to de-correlate information leaked, if any, so that the original/actual information is not exposed to the attacker. an architecture is proposed which is embedded on the sensor node to incorporate built in security feature at the chip level itself. this is done by the addition of a new hardware component called an lfsr, at the chip level itself, which is capable of generating random numbers to mask the output. as far as the clustering technique used is concerned, we&#8217;ve considered the hierarchical clustering to ease data aggregation.use of lfsr for sensor network security','Embedded systems security'
'high quality healthcare environment is an important aspect of modern society. we investigate security and networking architecture of clinical information system, with emphasis on the wireless hop which includes sensor networks and wireless local area or mesh networks. we review confidentiality and integrity polices for clinical information systems and discuss the feasible enforcement mechanisms over wireless hop.wireless sensor networks for clinical information systems','Embedded systems security'
'deeply embedded systems often have unique constraints because of their small size and vital roles in critical infrastructure. problems include limitations on code size, limited access to the actual hardware, etc. these problems become more critical in real-time systems where security policies must not only work within the above limitations but also ensure that task deadlines are not missed. a critical piece of information for security policies in real-time systems is the worst-case execution time (wcet) of the security code. this paper addresses some of the issues faced in the implementation of such security policies and also the process of determining wcets for them.worst-case execution time analysis of security policies for deeply embedded real-time systems','Embedded systems security'
'this paper describes a technique for maintaining data integrity that can be implemented using capabilities typically found in existing file systems. integrity is a property of a total collection of data. it cannot be maintained simply by using reliable primitives for reading and writing single units&#8212;the relations between the units are important also. the technique suggested in this paper ensures that data integrity will not be lost as a result of simultaneous access or as a result of crashes at inopportune times. the approach is attractive because of its relative simplicity and its modest demands on the underlying file system. the paper gives a detailed description of how consistent, atomic transactions can be implemented by client processes communicating with one or more file server computers. the discussion covers file structure, basic client operations, crash recovery, and includes an informal correctness proof.a client-based transaction system to maintain data integrity','File system security'
'the move toward publically available services that store private information has increased the importance of tracking information flow in applications. for example, network systems that store credit-card transactions and medical records must be assured to maintain the confidentiality and integrity of this information. one way to ensure this is to use a language that supports static reasoning about information flow in the type system. while useful in practice, current type systems for checking information flow are imprecise, unnecessarily rejecting safe programs. this annoys programmers and often results in increased code complexity in order to work around these artificial limitations. in this work, we present a new type system for statically checking information flow properties of imperative programs with exceptions. our key insight is to propagate a context of exception handlers and check exceptions at the throw point rather than propagating exceptions outward and checking them at the catch sites. we prove that our type system guarantees the standard non-interference condition and that it is strictly more permissive than the existing type system for jif, a language that extends the java type system to reason about information flow.a more precise security type system for dynamic security tests','File system security'
'auditing information systems security is difficult and becomes crucial to ensure the daily operational activities of organizations as well as to promote competition and to create new business opportunities. a conceptual security framework to manage and audit information system security is proposed and discussed. the proposed framework is based on a conceptual model approach, based on the iso/iec_jct1 standards, to assist organizations to better manage their in-formation systems security.a security framework for audit and manage information system security','File system security'
'most desktop search systems maintain per-user indices to keep track of file contents. in a multi-user environment, this is not a viable solution, because the same file has to be indexed many times, once for every user that may access the file, causing both space and performance problems. having a single system-wide index for all users, on the other hand, allows for efficient indexing but requires special security mechanisms to guarantee that the search results do not violate any file permissions. we present a security model for full-text file system search, based on the unix security model, and discuss two possible implementations of the model. we show that the first implementation, based on a postprocessing approach, allows an arbitrary user to obtain information about the content of files for which he does not have read permission. the second implementation does not share this problem. we give an experimental performance evaluation for both implementations and point out query optimization opportunities for the second one.a security model for full-text file system search in multi-user environments','File system security'
'passenger screening is a critical component of aviation security systems. this paper introduces the sequential stochastic security design problem (sssdp), which models passenger and carry-on baggage-screening operations in an aviation security system. sssdp is formulated as a two-stage model, where in the first stage security devices are purchased subject to budget and space constraints, and in the second stage a policy determines how passengers that arrive at a security station are screened. passengers are assumed to check in sequentially, with passenger risk levels determined by a prescreening system. the objective of sssdp is to maximize the total security of all passenger-screening decisions over a fixed time period, given passenger risk levels and security device parameters. sssdp is transformed into a deterministic integer program, and an optimal policy for screening passengers is obtained. examples are provided to illustrate these results, using data extracted from the official airline guide.a sequential stochastic security system design problem for aviation security','File system security'
'as exemplified in the 2010 stuxnet attack on an iranian nuclear facility, attackers have the capabilities to embed infections in equipment that is employed in nuclear power systems. in this paper, a new systems engineering focused approach for mitigating such risks is described. this approach involves the development of a security architectural formulation that integrates a set of reusable security services as an architectural solution that is an embedded component of the system to be protected. the system-aware architectural approach embeds security components into the system to be protected. the architecture includes services that (1) collect and assess real-time security relevant measurements from the system being protected, (2) perform security analysis on those measurements, and (3) execute system security control actions as required. this architectural formulation results in a defense that is referred to as system-aware cyber security. this includes (1) the integration of a diverse set of dynamically interchangeable redundant subsystems involving hardware and software components provided from multiple vendors to significantly increase the difficulty for adversaries by avoiding a monoculture environment, (2) the development of subsystems that are capable of rapidly changing their attack surface through hardware and software reconfiguration (configuration hopping) in response to perceived threats, (3) data consistency checking services (e.g., intelligent voting mechanisms) for isolating faults and permitting moving surface control actions to avoid operations in a compromised configuration, and (4) forensic analysis techniques for rapid post-attack categorization of whether a given fault is more likely the result of an infected embedded hardware or software component (i.e., cyber attack) or a natural failure. in this paper we present these key elements of the system-aware cyber security architecture and show, including an application example, how they can be integrated to mitigate the risks of insider and supply chain attacks. in addition, this paper outlines an initial vision for a security analysis framework to compare alternative system-aware security architectures. finally, we summarize future research that is necessary to facilitate implementation across additional domains critical to the nation\'s interest. &#x00a9; 2012 wiley periodicals, inc. syst eng &#169; 2012 wiley periodicals, inc.a system-aware cyber security architecture','File system security'
'in this paper, two independent definitions of system security are given through two distinct aspects of a system execution, i.e. state and transform. these two definitions are proven to be equivalent, which gives both confidence to the soundness of our explication and insight into the internal causality of information flow. using this definition of information flow security, a general security model for nondeterministic computer systems is presented. on one hand, our model is based on information flow which allows it to explicate security semantically as other information flow models. on the other hand, our model imposes concrete constraints on the internal system processes which facilitate implementation and verification in the fashion of access security models. our model is also more general than previous state-based information flow models, e.g. allowing for concurrency among system processes, which is more suitable for distributed systems.a theory for system security','File system security'
'we present a method for the security analysis of realistic models over off-the-shelf systems and their configuration by formal, machine-checked proofs. the presentation follows a large case study based on a formal security analysis of a cvs-server architecture.the analysis is based on an abstract architecture (enforcing a role-based access control), which is refined to an implementation architecture (based on the usual discretionary access control provided by the posix environment). both architectures serve as a skeleton to formulate access control and confidentiality properties.both the abstract and the implementation architecture are specified in the language z. based on a logical embedding of z into isabelle/hol, we provide formal, machine-checked proofs for consistency properties of the specification, for the correctness of the refinement, and for security properties.a verification approach to applied system security','File system security'
'distributed computing infrastructure\'s data storage subsystems are usually physically scattered among several nodes and logically shared among several users and (local) administrators. it is therefore necessary to provide users with adequate mechanisms and tools for information and data security management, especially in large scale systems since the complexity of the problem increases with the number of users and the amounts of data. in this paper we propose a solution based on a lightweight cryptography algorithm combining the strong and highly secure asymmetric cryptography technique (rsa) with the symmetric cryptography (aes). we describe a possible implementation of our solution going into details of all the algorithms and the mechanisms specified.achieving distributed system information security','File system security'
'we have been investigating a speech processing system for ensuring safety and security, namely, acoustic-based security system.focusing on indoor security, we have been studying for an advanced security system which can discriminate emergency shout from the other acoustic sound events based on automatic understanding of speech events.in this paper, we present our investigations, and describe fundamental results.acoustic-based security system','File system security'
'this paper presents the design of an information- sharing based or server-assisted anti-phishing system. the system follows a client-server architecture and makes decision based on not only client side heuristics but also collective information from multiple clients. when visiting a web site, a client side proxy, installed as a plug-in to a browser, decides on the legitimacy of the web site based on a combination of white list, black list and heuristics. in case the client side proxy does not have sufficient information to make a clear judgment, it reports the suspicious site to a central server which has access to more complete and up to date information and is in a much better position than individual clients to make informed decisions. our system is designed to counter against deceptive phishing as well as dns-hijack attack.an information-sharing based anti-phishing system','File system security'
'automated control systems (acss) lie at the heart of industrial and infrastructure systems and, as such, are one of the most critical parts of critical infrastructures. yet the information security world has largely ignored these systems, and most information security folks seem to think that the protective processes, measures, and mechanisms that apply to general-purpose enterprise computers also apply to acss. at the same time, most control systems engineers know almost nothing about information protection and don\'t recognize even the potential for the sorts of things that information security professionals consider standard. this mismatch must be addressed, or we\'ll be paying the price for it for at least one generation.automated control system security','File system security'
'an electric utility system is predicated to be in one of four possible states: normal, alert, emergency, or restorative. examination of these states and the structure of the system\'s operating problems has led to a multilevel, multicomputer approach involving computers at various action centers interconnected by data links. within the hierarchical computer arrangement, a four-step approach of monitor and display, contingency evaluation, corrective strategy, and automatic control is suggested. major attention is directed toward minimizing the probability of leaving the normal state as well as the time required to return to this state. the computer functions to be undertaken at each level are described.automation and utility system security','File system security'
'current protection strategies against insider adversaries are expensive, intrusive, not systematically implemented, and operate independently; too often, these strategies are defeated. the authors discuss the development of methods for a systems-based approach to insider security. to investigate insider evolution within an organization, they use system dynamics to develop a preliminary model of the employee life cycle that defines and analyzes the employee population\'s interactions with insider security protection strategies. the authors exercised the model for an example scenario that focused on human resources and personnel security activities&#8212;specifically, prehiring screening and security clearance processes. the model provides a framework for understanding important interactions, interdependencies, and gaps in insider protection strategies. this work provides the basis for developing an integrated systems-based process for building&#8212;that is, designing, evaluating, and operating&#8212;a system for effective insider security.building a system for insider security','File system security'
'the main purpose of this study is to give an idea to the readers about how big and important the computing and information problems that hospital managers as well as policy makers will face with after collecting the ministry of labor and social security (molss) and ministry of health (moh) hospitals under single structure in turkey by comparing the current level of computing capability of hospitals owned by two ministries. the data used in this study were obtained from 729 hospitals that belong to both ministries by using a data collection tool.the results indicate that there have been considerable differences among the hospitals owned by the two ministries in terms of human resources and information systems. the hospital managers and decision makers making their decisions based on the data produced by current hospital information system (his) would more likely face very important difficulties after merging moh and molss hospitals in turkey. it is also possible to claim that the level and adequacy of computing abilities and devices do not allow the managers of public hospitals to use computer technology effectively in their information management practices. lack of technical information, undeveloped information culture, inappropriate management styles, and being inexperienced are the main reasons of why his does not run properly and effectively in turkish hospitals.comparison of computing capability and information system abilities of state hospitals owned by ministry of labor and social security and ministry of health','File system security'
'traditional physical security policy considers an intentional use of budgets and a way of using physical spaces. it represents difficulties in responses to the environment, which is dynamically changed, because the plan is determined as static and stereotypical manners. thus, in this study a wireless ip camera is used as a method that expands monitoring ranges in existing physical spaces in order to propose a security service structure that can support instantaneous detections and responses to disasters. although the effort for protecting facilities and equipments using a video monitoring system using wireless ip cameras for temporal threats is a proper way for providing a physical security service according to needs of an adaptable security level. moreover, if a security level for a specific section is to be upgraded according to changes in external environments, the security level, which is guaranteed by periodic guard patrols, can be improved and supported using a wireless ip camera-based video monitoring system for 24 h during the period to be required. also, it is possible to establish an environment that enables instantaneous detections and responses for a moment of accidents through performing a continuous video monitoring work from the beginning of rainfall to a dangerous level of flooding.continuity in wireless video security system-based physical security services','File system security'
'in this short paper we outline the problems of developing a cyber security alert warning system. we review some of the current warning systems and examine how they are related to the security metrics area. we then propose a socio-technical coordinate system to scale and classify cyber security warnings along with security posture levels.cyber security alert warning system','File system security'
'developing ethernet enhanced-security system','File system security'
'security of e-voting systems does not only depend on the voting protocol or the software used but concerns the whole system with all its components. to guarantee security a holistic approach, which considers all parts of such a complex system, has to be chosen. the security of an election system cannot be ensured unless every single element and its security-related characteristics, interfaces to other elements, and their impact on the whole system are examined.this paper presents the e-voting system security optimization method, which is based on such an approach and was developed to evaluate the security of e-voting systems. this method points out security flaws, shows security optimization potential, and can be used to compare different election systems. the methodology differs from other approaches insofar as it uses a holistic approach, visualizes the security situation of an e-voting system in a clear way, and shows its potential for improvement.e-voting system security optimization','File system security'
'computer risk exposures and security in general are reviewed, and factors suggesting that expert system security is a unique problem are examined. security requirements associated with the unique characteristics of expert systems are investigated. they include technical aspects of knowledge (certainty factors, symbolic information and special fixes), structure (user interface, knowledge base, inference engine and database), design methodology (prototyping) and the current delivery environment (e.g., pcs and expert system shells). the security of working expert systems as discussed in the literature is examined. rationales for the current apparent lack of expert system security are analyzed and the impact of possible security controls on expert system users and developers is assessed.expert system security','File system security'
'clusters built from commodity computers are popular because of their cost efficiency. there are two basic types of such clusters. a prevalent type is a dedicated cluster where all machines participating in the cluster belong to a single administrative domain. then there are non-dedicated clus- ters, where the cluster itself forms its own administrative domain, but the participating computers retain their own administrative domains. non-dedicated clusters have a bet- ter potential for attracting computer owners to participate in a cluster, as they do not have to give up control over their computers. on the other hand, these clusters have much higher security demands than the dedicated ones. in this paper, we focus on the security issues of distributed file sys- tems in the environment of the non-dedicated clusters. we will identify the issues implied by the environment, propose our solution, and demonstrate the feasibility by the experi- mental implementation.file system security in the environment of non-dedicated computer clusters','File system security'
'sharing network data between unix and nt systems is becoming increasingly important as nt moves into areas previously serviced entirely by unix. one difficulty in sharing data between unix and nt is that their file system security models are quite different. nt file servers use access control lists (acls) that allow permissions to be specified for an arbitrary number of users and groups, while unix nfs servers use traditional unix permissions that provide control only for owner, group, and other. this paper describes a merged model in which a single file system can contain both files with nt-style acls and files with unix-style permissions. for native file service requests (nfs requests to unix-style files and nt requests to nt-style files) the security model exactly matches a unix or nt fileserver. for non-native requests, heuristics allow a reasonable level of access without compromising the security guarantees of the native model.file system security','File system security'
'computer-supported cooperative work (cscw) requires coordinated access to shared information over computer networks; such networks have tended to use wires, but wireless networks are now becoming common. there are a large number of tools aimed at helping users to work cooperatively but these tend to be application specific, leading to proliferation and requiring a large amount of development effort. a more general purpose mechanism would keep the number of tools manageable, and would obviate the need to develop a completely new tool for each problem area. data security is also a very important requirement in distributed systems. a solution to the problems of cooperative working must take this security requirement into account. this paper describes a mechanism aimed at both problems: a general purpose tool for cooperative working that is more secure than existing proposals. our approach is novel in that we do not require explicit locking, which can lead to a number of problems, particularly in distributed systems, as we shall explain. client routines act upon user requests to insert or delete blocks in a file, and request a file-server to modify a shared file according to those requests. the fileserver receives excrypted requests asynchronously and merges these requests into the current version of the document without decrypting the requests. indeed, an interesting feature of our proposal is that the server could not decrypt the content of these requests even if it wanted to. we call this mechanism \"privacy enhanced merging\". our current implementation includes a concurrent editing application that we call \"network bbs\"; the server is able to make use of a conventional file-system. this is an experimental tool of our proposed \"collaborative file system\".file-based network collaboration system','File system security'
'guest editorial integrated circuit and system security','File system security'
'improving operating system security','File system security'
'in the past, control networks were completely isolated from exterior attack with the use of proprietary control protocols running on specialized hardware and software. since then, tcp/ip based systems have made their way into the industrial control system. control networks based on tcp/ip provide better connectivity and remote access capabilities, making the industrial control system vulnerable to cyber threats such as computer virus, internet worm, manipulation of operational data which can result in the disruption of the entire control system. in this paper, security risks of the industrial control system based on control network are analyzed and the differences between the disruption of control performance and that of control function are presented. the countermeasures are put forward.industrial control system security','File system security'
'this study investigated how external influences motivate senior management to commit to information system security (iss) by examining the mediating role of senior management between external influences and organizational change. neo-institutional theory was used to examine normative, mimetic, and coercive mechanisms that affect iss assimilation in organizations. findings show senior management beliefs about iss and participation in iss mediate effects of external influences on iss assimilation. the findings from this pilot study give merit to a more comprehensive study, and provide a better understanding of how to motivate senior management to lead iss in their organizations.information system security commitment','File system security'
'in this paper, we survey current literature concerning information systems security training and education. this paper also describes current information system security training and education dynamics. finally, this paper presents a graduate level information system security specialization that was developed using this information.information system security curricula development','File system security'
'information system security engineering','File system security'
'integrating the spiralog file system into the openvms operating system','File system security'
'in 1982, security pacific automation company (spac), the information systems subsidiary of security pacific bank, undertook a number of changes to improve the data processing services they were providing to the bank. central to this effort was the development of the management by results 9mbr) system. part of the mbr was the introduction of a series of measures designed to track the success of the changes spac was undertaking. these measures were linked to each of the four major aspects of mbr: strategic planning, service level agreements, commitment planning, and performance appraisal and compensation. although the introduction of mbr and its accompanying measures were not without problems, they are now well established; and they allow senior management o measure spac\'s contribution to the bank at the operational, managerial, and strategic levels.measuring information systems performance: experience with the management by results system at security pacific bank','File system security'
'in information technology, biometrics refers to the technique that measure and analyse the human body characteristics either physiological and/or behavioural such as dna, fingerprints, eye retina, iris, voice patterns, facial patterns and hand measurements for authentication purposes. biometric authentication is usually permanent and is highly difficult to forge. biometrics is used to enhance the privacy and security issues which exist in the current techniques such as using password and remembering pin authentication. by using biometrics, a person could be identified based on \'who she/he is \'rather than \'what she/he has\' (card, token, key) or \'what she/he knows\' (pin, password). considering all these advantages of biometric system along with the need for such security in the mobile devices such as tablets and smart phones, this paper surveys some of the biometric traits for mobile device authentication.next generation biometric security system','File system security'
'a formal model for description of passive and active timing attacks is presented, studied and compared with other security concepts. it is based on a timed process algebra and on a concept of observations which make only a part of system behaviour visible. from this partial information which contains also timing of actions an intruder tries to deduce some private system activities.observation based system security','File system security'
'authorization plays an essential role to ensure the security of a wide variety of computing and it systems such as data management systems, e-trading systems, database transaction systems, etc. this paper aims to propose a high level formal language for specifying and evaluating distributed authorizations with delegation, develop a new method for credential chain discovery, and implement a system prototype for representing and reasoning about access control policies in distributed environments. by applying the new methodology and technology developed from this work, we will be able to design highly secure computing and it systems in many different complex problemdomains.on distributed system security','File system security'
'preparing system security policies','File system security'
'vehicle stealing is now a day\'s common problem. keeping this in consideration we are trying to build a project which can recover your vehicle after stolen. for this we need to developed and install some system inside vehicle which will tell you the vehicle location after stolen. we also are trying to develop gps and gsm enabled system which will help customer track his vehicle remotely using just a mobile phone and one internet application. if customer want to track his vehicle location he only need to make a sms to system installed in to vehicle then vehicle will detect it\'s position using gps system and just send reply customer the location of vehicle in the form of sms. after getting sms reply customer can get vehicle location by providing location as an input to google earth software. a wireless and intelligent vehicle alarm system that sends signals directly to your mobile phone through mobile gsm and gps signals to let you know your vehicle is safe. if your vehicle is moved or the door is opened you can send signals to your vehicle to cut off the gas or power supply for the ultimate in vehicle security. gps functions also allow you to know where your vehicle is at all times. the best in vehicle security is right at your fingertips.real time avl security system','File system security'
'remote system security','File system security'
'modern critical infrastructures have command and control systems. these command and control systems are commonly called supervisory control and data acquisition (scada). in the past, scada system has a closed operational environment, so these systems were designed without security functionality. nowadays, as a demand for connecting the scada system to the open network growths, the study of scada system security is an issue. a key-management scheme is critical for securing scada communications. numerous key-management structures for scada also have been suggested. 11770-2 mechanism 9 key establishment protocol has been used in scada communication however a security proof for the 11770-2 mechanism 9 protocol is needed. the purpose of this paper is to provide a general overview about scada system, and its related security issues. furthermore, we try to investigate the importance of key management protocol and the need of formal security poof.scada system security, complexity, and security proof','File system security'
'planetlab is a globally distributed network of hosts designed to support the deployment and evaluation of planetary scale applications. support for planetary applications development poses several security challenges to the team maintaining planetlab. the planetary nature of planetlab mandates nodes distributed across the globe, far from the physical control of the team. the application development requirements force every user to have access to the equivalent of root on each machine, and use of firewalls is discouraged. if an account is compromised, planetlab administrators needed a way to track the actions of users on the nodes. if an entire node is compromised, then the administrators need a way to regain control despite the lack of physical access. encryption was built into planetlab to ensure confidentiality and integrity of system downloads. a special reset packet, combined with keeping a boot cd in the machine, enables planetlab system administrators to remotely regain control of machines if they are compromised and return to the nodes into a safe known state. the linux vserver implementation is used to provide root access to planetlab users for development purposes while isolating users from each other. a network abstraction layer provides accounting of traffic and allows safe access to raw sockets. these mechanisms have proven very useful in managing planetlab. after a compromise of large numbers of planetlab hosts, control of the planetlab network was regained in 10 minutes. the compromise spawned a review of planetlab security, which pointed out a number of flaws. the need for a central site for maintaining planetlab was cited as a key weakness. future work includes distributing the functions of planetlab\'s central administrative database and improving integrity checks.securing the planetlab distributed testbed: how to manage security in an environment with no firewalls, with all users having root, and no direct physical control of any system','File system security'
'practitioners as well as researchers have repeatedly deplored that it security research has failed to produce practical solutions to growing security threats. this paper attributes this failure to the fact that it departments no longer invest in security as an ideal. rather, money is being spent on technologies that enable compliance with security requirements. academia has not embraced this shift in perspective and still tries to \"sell\" security when organizations seek to \"buy\" compliance. this disconnect has lead to research that fails to improve real-world security because it is not embraced in the market place. the conclusion drawn in this paper is that academia needs to complement current security research by additional research into security compliance. to encourage more work in this relatively new direction, the paper describes the major compliance research challenges that await solutions.security compliance','File system security'
'security is the very important principle of distributed systems. security in distributed systems can be divided into two parts. one part concerns the communication between users and processes, possibly residing on different machines. the principal mechanism for ensuring secure communication is that of a secure channel and via authorization. in this research paper, we focus on the study of several encryption algorithms for security in distributed systems. the rsa algorithm has some important parameters affecting its level of security. it is shown here that increasing the modules length plays an important role in increasing the complexity of decomposing it into its factors. this will increase the length of the public key and the length of the encrypted message making it more difficult to be decrypted without knowing the decryption key. however the public key length has no major affect on the private key length. when the length of the message is changed then the length of the encrypted message will proportionally change, hence larger chunks are selected to obtain larger encrypted message to increase the security of the data in use.security in distributed system','File system security'
'despite its many practical benefits, mobile agent technology results in significant new security threats from both malicious agents and hosts. this paper investigates the problems and approaches of mobile agent system, which shows that bi-directional and layered security model may be a good idea to resolve the security problems in mobile agent systems. other topics about mobile agent security, such as constrained execution and virus detection, are also discussed.security in mobile agent system','File system security'
'with the recent definition of the security policy system, ipsec has joined the area of policy-based networking. this paper discusses the general architectural and functional requirements for systems in charge of security policy provisioning, and presents a critical evaluation of sps. some extensions are also suggested to increase sps functionality in the network access control field.security policy system','File system security'
'this paper focuses on the security and privacy threats being faced by the low-cost rfid communication system, the most challenging of which relate to eavesdropping, impersonation, and tag cloning problems. the security issues can be improved and solved by utilizing both prevention and detection strategies. prevention technique is needed since it offers resistance capabilities toward eavesdroppers and impersonators. detection technique is vital to minimize the negative effects of tag cloning threats. this paper proposes the use of both prevention and detection techniques to make rfid communication more secure. lightweight cryptographic algorithm, which conforms to the epc class-1 generation-2 standard, is used in the proposed mutual authentication protocol for rfid system to raise security levels. in addition, electronic fingerprinting system is deployed in the proposed solution as a detection method to distinguish counterfeit and legitimate tags.security problems in an rfid system','File system security'
'owing to their inherent security problems, migrants have limited applications especially when they are more prone to misuse by some other applications, resulting in increase in the scale of threats. nevertheless migrants are potential contributors for implementation of migrating crawlers because of their capability to move to the information resource itself. in this paper, general security objectives for migrants are identified and corresponding mechanisms for facing the identified threats have been designed.security system for migrating crawlers','File system security'
'this paper shows a control system based in a microcontroller, personal computer and the internet service for security purposes. the security system alerts to the user that one strange person is in his confidential place, sending a message to his cellular phone and also it records the situation in video of the strange while he is in that place, the last mentioned is done using a moving detector sensor and a pc with a video camera and internet service to send a e-mail to the user.security system for mobile users','File system security'
'in recent years,government,enterprise and bank network have become highly dependent on internet.the drawback of this situation is that the consequences of disturbances of the underlying internet may be serious as cascading effects can occur.this raised a high demand for security assurance.in order to impoving security,every year,they spend a lot of money to buy security products,with a high importance assigned to security evaluation.this paper will introduce the security evaluations process and will forcus on the tests done on hardware products,specifically intrusion provention system.security testing for intrusion provention system - the security evaluation practice','File system security'
'separating key management from file system security','File system security'
'a security system for a company network is progressing as a esm (enterprise security management) in an existing security solution foundation. the establishment of the security policy is occupying a very important area in esm of the security system. we tried to analyze the existing esm system for this and designed a security solution structure for enhancing the internal security. we applied implementing directly ids system and tested it. this study examined the structure of security solutions in order to build an enterprise security management system. for this purpose, we analyzed existing enterprise security management systems and, based on the results, proposed a enterprise security management system with reinforced internal security and tested the system. for the test, we used a firewall through log analysis and designed an intra-network using virtual ip system.structure design and test of enterprise security management system with advanced internal security','File system security'
'computational e-mail systems, which allow mail messages to contain command scripts that automatically execute upon receipt, can be used as a basis for building a variety of collaborative applications. however, their use also presents a serious security problem because a command script from a sender may access/modify receiver\'s private files or execute applications on receiver\'s behalf. existing solutions to the problem either severely restrict i/o capability of scripts, limiting the range of applications that can be supported over computational e-mail, or permit all i/o to scripts, potentially compromising the security of the receiver\'s files. our model, called the intersection model of security, permits i/o for e-mail from trusted senders but without compromising the security of private files. we describe two implementations of our security model: an interpreter-level implementation and an operating systems-level implementation. we discuss the tradeoffs between the two implementations and suggest directions for future work.support for the file system security requirements of computational e-mail systems','File system security'
'organizations\' integrate different systems and software applications in order to provide a complete set of services to their customers. however, different types of organisations are facing a common problem today, namely problems with security in their systems. the reason is that focus is on functionality rather than security. besides that, security, if considered, comes too late in the system and software engineering processes; often during design or implementation phase. moreover, majority of system engineers do not have knowledge in security. however, security experts are rarely involved in development process. thus, systems are not developed with security in mind, which usually lead to problems and security breaches. we propose an approach of integration security throughout engineering process. to assure that necessary actions concerning security have been taken during development process, we propose semi-automated preventive controls. system engineering security','File system security'
'system security administration has, for the most part, been largely ignored as network administration has flourished. the result of this is that there are large installations of nt that need to be retrofitted with some form of security administration and management system. there are various third party tools available to assist in this endeavor, but they are somewhat general and not tailored to meet the needs of a particular user or organization. system administrators must therefore learn to mold their infrastructure to the tool, rather than the other way around. often times, the tools may also be quite expensive, and difficult to learn and maintain.system security administration for nt','File system security'
'in this paper, we will describe the system security research at the university of birmingham, uk, and briefly sketch directions of future work.system security research at birmingham','File system security'
'we describe current system security efforts and future research roadmap of lab of security engineering at newcastle university, england.system security research at newcastle','File system security'
'scalable trusted computing seeks to apply and extend the fundamental technologies of trusted computing to large-scale systems. to provide the functionality demanded by users, bootstrapping a trusted platform is but the first of many steps in a complex, evolving mesh of components. the bigger picture involves building up many additional layers to allow computing and communication across large-scale systems, while delivering a system retaining some hint of the original trust goal. not to be lost in the shuffle is the most important element: the system\'s human users. unlike 40 years ago, they cannot all be assumed to be computer experts, under the employ of government agencies which provide rigorous and regular training, always on tightly controlled hardware and software platforms. it seems obvious that the design of scalable trusted computing systems necessarily must involve, as an immutable design constraint, realistic expectations of the actions and capabilities of normal human users. experience shows otherwise. the security community does not have a strong track record of learning from user studies, nor of acknowledging that it is generally impossible to predict the actions of ordinary users other than by observing (e.g., through user experience studies) the actions such users actually take in the precise target conditions. we assert that because the design of scalable trusted computing systems spans the full spectrum from hardware to software to human users, experts in all these areas are essential to the end-goal of scalable trusted computing.system security, platform security and usability','File system security'
'in this paper we outline the need for a new systems engineering architecturally focused approach for addressing the growing threats of debilitating cyber attacks: system-aware security. this novel security architecture resides at the application layer and is based on smart reusable system security services. we layout an initial vision for this architectural formulation and show how it can potentially enhance the security of systems by complementing the traditional perimeter security model. in addition, we outline an ongoing research activity involving the development of an initial application for a specific system-aware security architecture embedded in a command and control system. the architecture includes three interactive situational adapting smart reusable security services: data continuity checking, configuration hopping, and honey pots. finally, we describe how these services could be converted into reusable design patterns to stimulate reuse in additional systems.system-aware cyber security','File system security'
'this paper describes the architecture of the gridsite system, which adds support for several grid security protocols to the apache web server platform. these include the globus gsi authentication system, grid access control language (gacl) access policy files, and distinguished name (dn) list and virtual organization membership service (voms) group memberships. particular emphasis is placed on how the architecture of gridsite has evolved during the past 3 years, how this has been influenced by operational experience with production systems, and how the project has led to new developments, such as gacl. finally, a description is given of how gridsite has been made to interoperate with other deployed security systems, both as producers and consumers of gridsite\'s authorization information. copyright &#169; 2005 john wiley & sons, ltd.the gridsite web-grid security system','File system security'
'with recent advances in electronic design files, computer aided design (cad) has become a common place to store the design knowledge. therefore, it is critical to secure the cad files so that such valuable intellectual property is not available to competitors by internal users. because most cad drawings are composed of a collection of files with various extensions, there exist problems associated with the processing speed and the accuracy of cad files encryption (decryption) using file based secure methods. in this study, an innovative idea of securing cad files based on the workplace against illegal piracy of design knowledge is presented. the proposed technology is to store all design files in the secure workplace which can be accessed by the authorized users and design applications only using application programming hooking at user level and system service table at kernel level. the technology is demonstrated in this paper using its implementation example in a civil engineering firm to verify it and cad files can be shared among users without a concern of its leakage to the competitors by internal user.the research of security system for sharing engineering drawings','File system security'
'in the paper we present an overview of saga security system, a security architecture in open distributed systems. an agent in saga security system is called a saga agent. the authorization model in saga security system (saga authorization model) supports the novel concept of a service path and provides uniform and flexible protection appropriate for advanced computational models such as object-oriented systems and cooperative agent systems. with respect to the security mechanism of saga security system, its key features are an access token and a security monitor. access tokens are implemented using public key technology and ensure integrity of request messages issued by saga agents. in addition, we can regard the security monitor of a saga agent as a reference monitor for the agent. security of a saga agent during its traversal over distributed environments is controlled by the security monitor integrated with the agent.the saga security system','File system security'
'transaction security system','File system security'
'this paper proposes three methods for choosing the transmission system expansion plan considering three reliability constraints, which are deterministic reliability criterion, probabilistic reliability criterion and security criterion based on n-&#945; contingency in order to give more successful market operation. the proposed method minimizes total investment cost it models the transmission system expansion problem as an integer programming one. the method solves for the optimal strategy using a branch and bound method that utilizes a network flow approach and the maximum flow-minimum cut set theorem. the 21 bus system case study results demonstrate that the proposed method is practical for solving the power system expansion planning problem subject to practical future uncertainties.transmission system expansion plans in view point of deterministic, probabilistic and security reliability criteria','File system security'
'unix operating system security','File system security'
'the global information technology (it) industry recognizes the need for standards to improve the quality and consistency of security for it products and services. as such, the international organization for standardization&#x002f; international electrotechnical commission (iso&#x002f;iec) 27000 series is focusing on the requirements, security controls, and implementation guidance for an organization\'s information security management system (isms). this guidance establishes general principles that can be used in various industries and government&semi; however, standardized techniques are also needed to identify, implement, and operate security controls as part of the isms life cycle. the bell labs security framework identifies both the minimal and differentiating security controls by decomposing an it product or service into a layered hierarchy of equipment and facilities groupings and examining the types of activities that occur at each layer in a standardized manner. furthermore, the bell labs security framework security dimensions provide the necessary mechanisms to implement and operate the selected controls. the bell labs security framework enhances the iso&#x002f;iec 27000 series by providing a comprehensive end-to-end approach to implementing it security. &#169; 2007 alcatel-lucent.using the bell labs security framework to enhance the iso 17799&#x002f;27001 information security management system','File system security'
'this paper presents a service-oriented platform for performing efficient and scalable on-line power system security analysis. this analysis is a complex, large-scale, and compute-intensive problem whose solution requires different distributed resources and functionalities. these can be both high-level services provided by organizations belonging to different functional areas of electric industry and low-level services provided by service providers on the internet. the platform described in the paper is able to integrate services of different kinds, such as real-time data acquisition from several sources, high computational power and data storage capabilities, to perform complex computations on large scale distributed systems. the paper discusses a web services-based implementation of the platform, which is centered on a bpel-modeled workflow enactor able to exploit a grid computing middleware to execute compute intensive code.web services workflow for power system security assessment','File system security'
'despite the fact that industry continues to rate confidentiality protection as the least important security goal for a commercial organisation, the cryptographic community has a fascination with developing new encryption technologies. it often seems that the majority of advances in general cryptologic theory are a result of research designed to improve our ability to transmit messages confidentially. the development of security models are a good example of this phenomenon. the earliest attempts to produce cryptographic schemes with some provable security guarantees centred on encryption technologies (shannon 1949; rabin 1979). the modern security model for confidentiality dates back to the early eighties (goldwasser---micali 1982; goldwasser---micali 1984) when the notion of &lt;em&gt;indistinguishability under chosen plaintext attacks&lt;/em&gt; (ind-cpa) was proposed. this was followed by the more advanced notions of ind-cca1 security (naor---yung 1990) and ind-cca2 security (rackoff---simon 1991) which are now so ubiquitous that they are often applied to new cryptographic primitives without thought. many people have forgotten that the elegant notion of ind-cca2 security is a simplification of the much more complex notion of &lt;em&gt;semantic security&lt;/em&gt; . in this invited talk, we\'ll consider the bedrock of cryptographic confidentiality: the notion of ind-cca2 security. we\'ll show by a series of examples that the simplifications that can be obtained in deriving the indistinguishability security notion from the semantic security notion for public key encryption can\'t always be derived for other types of public-key cryptography. for example, the ind-cca2 model for public-key encryption only considers a single user (public key), whereas the security model for a signcryption scheme must consider multiple users. the ind-cca2 model for public-key encryption only considers attacks against a single (challenge) ciphertext, whereas the security model for deterministic encryption must consider attacks against multiple (challenge) ciphertexts. the ind-cca2 model for public-key encryption only considers an attacker that is trying to determine some information about a message of some known length, whereas some results in plaintext awareness require the attacker be unable to determine any information about a message of an unknown length.   our ultimate aim will be to define a general model and set of rules for deriving a security notion for confidentiality for an arbitrary public-key primitive. a brief history of security models for confidentiality','Formal security models'
'formal methods have proved useful in the analysis of security protocols. in this paper, we propose a generic model for symbolic analyzing security protocols (gspm for short) that supports message passing semantics and constructs for modelling the behavior of protocol participants. gspm is simple, but it is expressive enough to express security protocols and properties in a precise and faithful manner. in order to address that the execution of a protocol generates infinitely many paths, we use symbolic method. based on gspm, it is shown how security properties such as confidentiality, authentication, non-repudiation, fairness and anonymity can be described.a generic model for symbolic analyzing security protocols','Formal security models'
'we have previously proposed an expressive uml-based language for constructing and transforming security-design models, which are models that combine design specifications for distributed systems with specifications of their security policies. here we show how the same framework can be used to analyze these models: queries about properties of the security policy modeled are expressed as formulas in uml\'s object constraint language and evaluated over the metamodel of the security-design language. we show how this can be done in a semantically precise and meaningful way and demonstrate, through examples, that this approach can be used to formalize and check non-trivial security properties of security-design models. the approach and examples presented have been implemented and checked in the securemova tool.a metamodel-based approach for analyzing security-design models','Formal security models'
'even experienced developers struggle to implement security policies correctly. for example, despite 15 years of development, standard java libraries still suffer from missing and incorrectly applied permission checks, which enable untrusted applications to execute native calls or modify private class variables without authorization. previous techniques for static verification of authorization enforcement rely on manually specified policies or attempt to infer the policy by code-mining. neither approach guarantees that the policy used for verification is correct. in this paper, we exploit the fact that many modern apis have multiple, independent implementations. our flow- and context-sensitive analysis takes as input an api, multiple implementations thereof, and the definitions of security checks and security-sensitive events. for each api entry point, the analysis computes the security policies enforced by the checks before security-sensitive events such as native method calls and api returns, compares these policies across implementations, and reports the differences. unlike code-mining, this technique finds missing checks even if they are part of a rare pattern. security-policy differencing has no intrinsic false positives: implementations of the same api must enforce the same policy, or at least one of them is wrong! our analysis finds 20 new, confirmed security vulnerabilities and 11 interoperability bugs in the sun, harmony, and classpath implementations of the java class library, many of which were missed by prior analyses. these problems manifest in 499 entry points in these mature, well-studied libraries. multiple api implementations are proliferating due to cloud-based software services and standardization of library interfaces. comparing software implementations for consistency is a new approach to discovering \"deep\" bugs in them.a security policy oracle','Formal security models'
'in this paper, we propose a novel threat model-driven security testing approach for detecting undesirable threat behavior at runtime. threats to security policies are modelled with uml (unified modeling language) sequence diagrams. from a design-level threat model we extract a set of threat traces, each of which is an event sequence that should not occur during the system execution. the same threat model is also used to decide what kind of information should be collected at runtime and to guide the code instrumentation. the instrumented code is recompiled and executed using test cases randomly generated. the execution traces are collected and analyzed to verify whether the aforementioned undesirable threat traces are matched. if an execution trace is an instance of a threat trace, security violations are reported and actions should be taken to mitigate the threat in the system. thus the linkage between models, code implementations, and security testing are extended to form a systematic methodology that can test certain security policies.a threat model driven approach for security testing','Formal security models'
'security labels convey information that is utilised to perform access control decisions, specify protective measures, and aid in the determination of additional handling restrictions required by security policies. in discussing security labelling, one of the most important aspects is to investigate access control models and obtain an appropriate technique for specifying the kind of security policies that are required. one problem with previous approaches to the specification of access control policies is that they are based on an idealisation of the real problem and give a first approximation: may or may not a subject access a given object? the binary, logical function is the essential starting point, but is generally insufficient to guide the hard decisions that are required by a variety of applications in the real world. focusing on the issues regarding security labelling, this paper first proposes a technique for expressing need-to-know policies that are regarded as the basis for security labelling and should be followed in the labelling process. then, based on the proposed lattice access control model dealing with both security levels and categories of objects, several security labelling principles are given. finally, we propose a dynamic model for security labelling that not only provides support for dynamic labelling within a system but also a functional base for the design and implementation of a security labelling system.access control models and security labelling','Formal security models'
'jfkr is a security protocol that establishes a shared encryption key between two participants. this paper briefly describes the different components of jfkr and the security property each component is intended to provide. it then describes an executable model, interleaving pieces of code to help the reader understand how the model represents the protocol specification. finally, it presents some theorems about the model. the contributions of this work include (1) an executable model for a key establishment protocol about which we can reason, (2) a model for an attacker that permits the injection, modification, and removal of messages between the participants, and (3) formalizations of a subset of desired security properties.an executable model for security protocol jfkr','Formal security models'
'assurance of security protocols needs particular attention. flaws in a protocol can devastate security of the applications that rely on it. analysis of the protocols is difficult and it is recommended that formal methods are employed to provide for higher levels of assurance. however, the formal methods can cover only a part of the scope of the problem. it is important that the formal models are valid representations of the protocol and that the application context is adequately represented. in the paper we present an analytical framework that integrates the object-oriented and formal modeling approaches. object models are used to capture the relevant aspects of the protocol and its security context and to communicate with the protocol designers. formal models are applied to verify the protocol security properties. applicability of the framework was demonstrated by several industrial case studies.an integrated framework for security protocol analysis','Formal security models'
'organizations face increasing challenges in addressing and preventing computer and network security incidents. there are financial consequences from security incidents. these include lost time and resources used during recovery, possible theft of personal and/or proprietary information, and reputational damage that may negatively impact stock prices or reduce consumer confidence in a company. being able to understand and predict trends in computer and network security incidents can aid an organization with resource allocation for prevention of such incidents, as well as evaluation of mitigation strategies. we look at using time series models with a large set of security incident data. we examine appropriateness of the data for modeling and consider needed transformations. parameter search and model selection criteria are discussed. then, forecasts from time series models are compared to forecasts from non-homogeneous poisson process (nhpp) software reliability growth (srg) models.analysis of computer security incident data using time series models','Formal security models'
'the core of the multilevel security policy is to divide information into different security levels, and to adopt different protection measures according to the security level. this policy is widely used in military field and business environment. this paper introduces three information security models based on multilevel security policy bell-la padula (blp) model, biba model and clark-wilson model. it emphatically expounds and compares the characteristics of the three models. at the end of this paper, the author analyzes the development trend of multilevel security models.analysis of security models based on multilevel security policy','Formal security models'
'we study and further develop two language-based techniques for analyzing security protocols. one is based on a typed process calculus; the other, on untyped logic programs. both focus on secrecy properties. we contribute to these two techniques, in particular by extending the former with a flexible, generic treatment of many cryptographic operations. we also establish an equivalence between the two techniques.analyzing security protocols with secrecy types and logic programs','Formal security models'
'mobile agents are a distributed computing paradigm based on mobile autonomous programs. mobile applications must balance security requirements with available security mechanisms in order to meet application level security goals. we introduce a trust framework to reason about application security requirements, trust expression, and agent protection mechanisms. we develop application security models that capture initial trust relationships and consider their use for mobile agent security.application security models for mobile agent systems','Formal security models'
'developing security-critical software correctly and securely is difficult. to address this problem, there has been a significant amount of work over the last 10 years on providing model-based development approaches based on the unified modeling language which aim to raise the trustworthiness of security-critical systems, some of them including tools allowing the user to check whether a uml model satisfies the relevant security requirements. however, when the requirements are not satisfied by a given model, it can be challenging for the user to determine which changes to do to the model so that it will indeed satisfy the security requirements. also, the fact that software continues to evolve on an ongoing basis, even after the implementation has been shipped to the customer, increases the challenge since in principle, the software has to be re-verified after each modification, requiring significant efforts. we present work on automated tool-support that exploits recent work on secure software evolution in the secure change project in order to support the security hardening of evolving uml models (within the context of the uml security extension umlsec).automated security hardening for evolving uml models','Formal security models'
'security attacks typically result from unintended behaviors or invalid inputs. security testing is labor intensive because a real-world program usually has too many invalid inputs. it is highly desirable to automate or partially automate security-testing process. this paper presents an approach to automated generation of security tests by using formal threat models represented as predicate/transition nets. it generates all attack paths, i.e., security tests, from a threat model and converts them into executable test code according to the given model-implementation mapping (mim) specification. we have applied this approach to two real-world systems, magento (a web-based shopping system being used by many online stores) and filezilla server (a popular ftp server implementation in c++). threat models are built systematically by examining all potential stride (spoofing identity, tampering with data, repudiation, information disclosure, denial of service, and elevation of privilege) threats to system functions. the security tests generated from these models have found multiple security risks in each system. the test code for most of the security tests can be generated and executed automatically. to further evaluate the vulnerability detection capability of the testing approach, the security tests have been applied to a number of security mutants where vulnerabilities are injected deliberately. the mutants are created according to the common vulnerabilities in c++ and web applications. our experiments show that the security tests have killed the majority of the mutants.automated security test generation with formal threat models','Formal security models'
'in many software applications, users access application data using graphical user interfaces (guis). there is an important, but little explored, link between visualization and security: when the application data is protected by an access control policy, the gui should be aware of this and respect the policy. for example, the gui should not display options to users for actions that they are not authorized to execute on application data. taking this idea one step further, the application gui should not just be security-aware, it should also be smart. for example, the gui should not display options to users for opening other widgets when these widgets will only display options for actions that the users are not authorized to execute on application data. we establish this link between visualization and security using a model-driven development approach. namely, we define and implement a many-models-to-model transformation that, given a security-design model and a gui model, makes the gui model both security-aware and smart.automatic generation of smart, security-aware gui models','Formal security models'
'a multilevel security (mls) model with credibility characteristics, the credibility-based bell-lapadula (cblp) security model, has been proposed to resolve the problem of trusted subjects&#8217; hidden security flaw and poor system usability in present mls systems in the previous paper of the authors. a sampling statistics method is proposed in this paper to evaluate the availability of the cblp model by obtaining the variation curve of the subjects&#8217; credibility and the rejection ratio of access operations. the validity of this method was achieved according to the strong law of large numbers and the central limit theorem. the sampling statistics results of the cblp model in specific scenarios showed that it is highly consistent with that of the formal analysis method and has lower computational complexity.availability analysis method of multilevel security models with credibility characteristics','Formal security models'
'video-based communications are becoming more important within domains, such as health care, that have stringent security and privacy requirements for data. vpn-based encryption is a now common part of organizational security architectures. it is expected that the time required to perform encryption at a vpn concentrator and decryption at the client side video may add significant overhead to the transmission of real-time video, which is already delay-sensitive. the real impacts of this overhead on quality of user experience are not well-understood. this paper describes experimental methods for measuring the network-level impacts of virtual private network (vpn) encryption on real-time video in wired and wireless networks. it also presents experimental results using these methods. this paper concludes that vpn-based encryption introduces significant latency to real-time video and that further analysis is warranted to characterize the transitive impacts of such latency on the quality of user experiencecharacterizing the impacts of vpn security models on streaming video','Formal security models'
'security is dependent on a mixture of interrelated concepts such as technical countermeasures, organizational policies, security procedures, and more. to facilitate rational decision making, these concepts need to be combined into an overall judgment on the current security posture, as well as potential future ones. decision makers are, however, faced with uncertainty regarding both what countermeasures that is in place, and how well different countermeasures contribute to mitigating attacks. this paper presents a security assessment framework using the bayesian statistics-based extended influence diagrams to combine attack graphs with countermeasures into defense graphs. the approach makes it possible to calculate the probability that attacks succeed based on an enterprise architecture model. the framework also takes uncertainties of the security assessment into consideration. moreover, using the extended influence diagram formalism the expected loss from each attack can be calculated.combining defense graphs and enterprise architecture models for security analysis','Formal security models'
'in this work, we overview some results concerning communication combinatorial properties in random intersection graphs and uniform random intersection graphs. these properties relate crucially to algorithmic design for important problems (like secure communication and frequency assignment) in distributed networks characterized by dense, local interactions and resource limitations, such as sensor networks. in particular, we present and discuss results concerning the existence of large independent sets of vertices whp in random instances of each of these models. as the main contribution of our paper, we introduce a new, general model, which we denote $/math$. in this model, v is a set of vertices and ? is a set of m vectors in r^m. furthermore, f is a probability distribution over the powerset 2^? of subsets of ?. every vertex selects a random subset of vectors according to the probability f and two vertices are connected according to a general intersection rule depending on their assigned set of vectors. apparently, this new general model seems to be able to simulate other known random graph models, by carefully describing its intersection rule.communication and security in random intersection graphs models','Formal security models'
'database security','Formal security models'
'the quest for designing secure and trusted software has led to refined software engineering methodologies that rely on tools to support the design process. automated reasoning mechanisms for requirements and software verification are by now a well-accepted part of the design process, and model driven architectures support the automation of the refinement process. we claim that we can further push the envelope towards the automatic exploration and selection among design alternatives and show that this is concretely possible for secure tropos, a requirements engineering methodology that addresses security and trust concerns. in secure tropos, a design consists of a network of actors (agents, positions or roles) with delegation/permission dependencies among them. accordingly, the generation of design alternatives can be accomplished by a planner which is given as input a set of actors and goals and generates alternative multi-agent plans to fulfill all given goals. we validate our claim with a case study using a state-of-the-art planner.designing security requirements models through planning','Formal security models'
'organizations and governments are becoming vulnerable to a wide variety of security breaches against their information infrastructure. the magnitude of this threat is evident from the increasing rate of cyber attacks against computers and critical infrastructure. weblogs, or blogs, have also rapidly gained in numbers over the past decade. weblogs may provide up-to-date information on the prevalence and distribution of various cyber security threats as well as terrorism events. in this paper, we analyze weblog posts for various categories of cyber security threats related to the detection of cyber attacks, cyber crime, and terrorism. existing studies on intelligence analysis have focused on analyzing news or forums for cyber security incidents, but few have looked at weblogs. we use probabilistic latent semantic analysis to detect keywords from cyber security weblogs with respect to certain topics. we then demonstrate how this method can present the blogosphere in terms of topics with measurable keywords, hence tracking popular conversations and topics in the blogosphere. by applying a probabilistic approach, we can improve information retrieval in weblog search and keywords detection, and provide an analytical foundation for the future of security intelligence analysis of weblogs.detecting cyber security threats in weblogs using probabilistic models','Formal security models'
'the distributed temporal logic dtl is an expressive logic, well suited for formalizing properties of concurrent, communicating agents. we show how dtl can be used as a metalogic to reason about and relate different security protocol models. this includes reasoning about model simplifications, where models are transformed to have fewer agents or behaviors, and verifying model reductions, where to establish the validity of a property it suffices to consider its satisfaction on only a subset of models. we illustrate how dtl can be used to formalize security models, protocols, and properties, and then present three concrete examples of metareasoning. first, we prove a general theorem about sufficient conditions for data to remain secret during communication. second, we prove the equivalence of two models for guaranteeing message-origin authentication. finally, we relate channel-based and intruder-centric models, showing that it is sufficient to consider models in which the intruder completely controls the network. while some of these results belong to the folklore or have been shown, mutatis mutandis, using other formalisms, dtl provides a uniform means to prove them within the same formalism. it also allows us to clarify subtle aspects of these model transformations that are often neglected or cannot be specified in the first place.distributed temporal logic for the analysis of security protocol models','Formal security models'
'caring for security at requirements engineering time is amessage that has finally received some attention recently.however, it is not yet very clear how to achieve thissystematically through the various stages of therequirements engineering process.the paper presents a constructive approach to themodeling, specification and analysis of application-specificsecurity requirements. the method is based on agoal-oriented framework for generating and resolvingobstacles to goal satisfaction. the extended frameworkaddresses malicious obstacles (called anti-goals) set up byattackers to threaten security goals. threat trees are builtsystematically through anti-goal refinement until leafnodes are derived that are either software vulnerabilitiesobservable by the attacker or anti-requirementsimplementable by this attacker. new security requirementsare then obtained as countermeasures by application ofthreat resolution operators to the specification of the anti-requirementsand vulnerabilities revealed by the analysis.the paper also introduces formal epistemic specificationconstructs and patterns that may be used to support aformal derivation and analysis process. the method isillustrated on a web-based banking system for whichsubtle attacks have been reported recently.elaborating security requirements by construction of intentional anti-models','Formal security models'
'since 9/11 and the creation of the u.s. patriot act, the intrusion of government surveillance into the lives of ordinary americans has become a topic of great concern to many citizens. while many americans view surveillance as a necessity in the name of national security, the government is not the only organization conducting surveillance. as technological capacity increases, an increasing number of employers are implementing technologies that allow them to maintain vigilance over the actions of their employees in the workplace. despite many attempts to implement surveillance technologies, there is little evidence that companies are any safer now than they were ten years ago. this paper demonstrates how system dynamics modeling can be utilized to help model the insider threat as a system. it provides analysis of the non-linear affect of decision making, assessing the 2nd, 3rd, and 4th order impacts of decisions, and demonstrates the important impact of delays in the system. a mathematical model is presented and simulations are conducted to determine the likely affect of company decisions and individual agent behavior.employing dynamic models to enhance corporate it security policy','Formal security models'
'existing security models require that information of a given security level be prevented from ``leaking\'\' into lower-security information. high-security applications must be demonstrably free of such leaks, but such demonstration may require substantial manual analysis. other authors have argued that the natural way to enforce these models automatically is with information-flow analysis, but have not shown this to be practicable for general purpose programming languages in current use. modern safety-critical systems can contain software components with differing safety integrity levels, potentially operating in the same address space. this case poses problems similar to systems with differing security levels; failure to show separation of data may require the entire system to be validated at the higher integrity level. in this paper we show how the information flow model enforced by the spark examiner provides support for enforcing these security and safety models. we describe an extension to the spark variable annotations which allows the specification of a security or safety level for each state variable, and an extension to the spark analysis which automatically enforces a given information flow policy on a spark program.enforcing security and safety models with an information flow analysis tool','Formal security models'
'security problems are increasing and, while existing systems are often found lacking, practical problems limit the adoption of more secure trusted systems. this paper describes a new operating system security model called vaults utilising cryptography to provide dramatically enhanced security over existing systems. this approach provides many of the benefits of trusted system designs, while being more intuitive and applicable to varying security requirements.enhanced security models for operating systems','Formal security models'
'first-order logic models of security for cryptographic protocols, based on variants of the dolev-yao model, are now well-established tools. given that we have checked a given security protocol &pi; using a given first-order prover, how hard is it to extract a formally checkable proof of it, as required in, e.g., common criteria at the highest evaluation level (eal7)? we demonstrate that this is surprisingly hard in the general case: the problem is non-recursive. nonetheless, we show that we can instead extract finite models m from a set s of clauses representing &pi;, automatically, and give two ways of doing so. we then define a model-checker testing m&thinsp;|=&thinsp;s, and show how we can instrument it to output a formally checkable proof, e.g., in coq. experience on a number of protocols shows that this is practical, and that even complex (secure) protocols modulo equational theories have small finite models, making our approach suitable.finite models for formal security proofs','Formal security models'
'single-sign-on (sso) protocols enable companies to establish a federated environment in which clients sign in the system once and yet are able to access to services offered by different companies. the oasis security assertion markup language (saml) 2.0 web browser sso profile is the emerging standard in this context. in this paper we provide formal models of the protocol corresponding to one of the most applied use case scenario (the sp-initiated sso with redirect/post bindings) and of a variant of the protocol implemented by google and currently in use by google\'s customers (the saml-based sso for google applications). we have mechanically analysed these formal models with satmc, a state-of-the-art model checker for security protocols. satmc has revealed a severe security flaw in the protocol used by google that allows a dishonest service provider to impersonate a user at another service provider. we have also reproduced this attack in an actual deployment of the saml-based sso for google applications. this security flaw of the saml-based sso for google applications was previously unknown.formal analysis of saml 2.0 web browser single sign-on','Formal security models'
'starting with the trusted computer system evaluation criteria (aka the \"orange book\"), the information security community within the us department of defense has been advocating formal methods for decades. others have followed suit, culminating in the appearance of the common criteria. the advantages of formal analysis seem self-evident. first, of the three things that are subject to certification -- people, process, and product -- product seems to be the most immediately relevant. second, if we focus on product, testing seems insufficient; as dijkstra famously noted, testing can reveal the presence flaws, but not their absence. this is especially true of security, where flaws may be intentionally constructed not to reveal themselves during normal testing. despite this, the acceptance of formal methods has been less than universal. in this talk i will discuss the history of formal methods, but with a focus on how that history has shaped our current situation. i\'ll also discuss what we need to do to make formal methods more appealing. this will involve the development of formal methods or new ways of using formal methods that will: have a more predictable, quantifiable impact on validation costs; support software engineering for nonstandard properties and multiple properties; incorporate untrusted software; and support flexible release strategies. i will also discuss a recent application of formal methods nrl undertook as part of the development of a new security device [1] and new directions formal methods must take if they are to be applicable to future systems. the focus here will be on their use in autonomous systems that incorporate nondeterministic learning algorithms.formal methods in security engineering','Formal security models'
'formal models for computer security','Formal security models'
'formal models of it security','Formal security models'
'this paper proposes a formalization and verification technique for security specifications, based on common criteria. generally, it is difficult to define reliable security properties that should be applied to validate an information system. therefore, we have applied security functional requirements that are defined in the iso/iec 15408 common criteria to the formal verification of security specifications. we formalized the security criteria of iso/iec 15408 and developed a process, using z notation, for verifying security specifications. we also demonstrate some examples of the verification instances using the theorem prover z/eves. in the verification process, one can verify strictly whether specifications satisfy the security criteria defined in iso/iec 15408.formal verification of security specifications with common criteria','Formal security models'
'the application of formal methods for rigorously validating cryptographic protocols has been getting increasing attention. the de facto standard for modeling such protocols in formal proof systems is the dolev-yao model that, e.g., uses abstract encryption instead of cryptographic encryption primitives. the dolev-yao model has been originally intended and successfully used for detecting flaws in many protocols. however, recent publications claim to perform actual proofs of security using this model, i.e., absence of any attack. we doubt this claim and challenge dolev-yao-based models as being oversimplified for establishing security proofs against arbitrary attacks.we substantiate our claim by an example protocol. this protocol has been proven secure in a dolev-yao-based model using formal methods. in a later publication, the protocol has been broken by describing a cryptographic attack. the attack was not detected in the formal analysis since any dolev-yao-based model only comprises a predefined set of adversary capabilities. the particular attack to break the protocol was not comprised.the only reliable long-term remedy is to proof resilience against all attacks (both known and unknown ones). recent approaches on cryptographic models of security have already made great progress towards this goal. unfortunately, proofs in these are more complex and harder to automate. on the short run, it therefore is appropriate to improve the quality of formal analysis without striving for complete proofs. this can be achieved by means of evolving a catalog of adversary capabilities. future formal analysis can then show resilience against any attack in this catalog. we initiate this discussion on an \"adversary capability catalog\" by providing a cryptographer\'s wish list. this list that points out several features which approaches based on the dolev-yao model or future extensions of it should cover in order to be effective for cryptographic protocol verification.from absence of certain vulnerabilities towards security proofs','Formal security models'
'present paper derives fuzzy economic models to evaluate the economic feasibility of information security investment. the net present value (npv), and discounted return on investment (droi) models are proposed for the execution of costbenefit analysis. since fuzzy results are in the form of a complex nonlinear representation, and do not always provide a totally ordered set in the same way that crisp numbers do, the current paper approximates the resulting fuzzy profitability indexes by a triangular fuzzy number initially, and then uses the mellin transform to obtain the means and variances of the triangle fuzzy numbers in order to determine their relative ranking in a decision-making process. the performances of the proposed models are verified by considering their application to a practical illustration, which were used in a previous literature. these investigations not only confirm that the results of the fuzzy economic models are consistent with those of the conventional crisp models, but also demonstrate that the proposed models are more flexible, intelligent and computationally efficient compared to the extension principle fuzzy mathematics approach. the developed models represent readily implemented feasibility analysis tools for use in the arena of uncertain economic decision-making.fuzzy economic decision-models for information security investment','Formal security models'
'the increasing complexity of software and it systems creates the necessity for research on technologies addressing current key security challenges. to meet security requirements in it infrastructures, a security engineering process has to be established. one crucial factor contributing to a higher level of security is the reliable detection of security vulnerabilities during security tests. in the presented approach, we observe the behavior of the system under test and introduce machine learning methods based on derived behavior metrics. this is a generic method for different test targets which improves the accuracy of the security test result of an automated security testing approach. reliable automated determination of security failures in security test results increases the security quality of the tested software and avoids costly manual validation.generic approach for security error detection based on learned system behavior models for automated security tests','Formal security models'
'risk assessment is often done by human experts, because there is no exact and mathematical solution to the problem. usually the human reasoning and perception process cannot be expressed precisely. this paper propose a light weight risk assessment system based on an hierarchical takagi-sugeno model designed using evolutionary algorithms. performance comparison is done with neuro-fuzzy and genetic programming methods. empirical results indicate that the techniques are robust and suitable for developing light weight risk assessment models, which could be integrated with intrusion detection and prevention systems.hierarchical takagi-sugeno models for online security evaluation systems','Formal security models'
'it becomes critical to address human adversaries\' bounded rationality in security games as the real-world deployment of such games spreads. to that end, the key contributions of this paper include: (i) new efficient algorithms for computing optimal strategic solutions using prospect theory and quantal response equilibrium; (ii) the most comprehensive experiment to date studying the effectiveness of different models against human subjects for security games. our new techniques outperform the leading contender for modelling human behavior in security games in experiment with human subjects.improved computational models of human behavior in security games','Formal security models'
'there exists a substantial amount of work on methods, techniques and tools for developing security-critical systems. however, these approaches focus on ensuring that the security properties are enforced during the initial system development and they usually have a significant cost associated with their use (in time and resources). in order to enforce that the systems remain secure despite their later evolution, it would be infeasible to re-apply the whole secure software development methodology from scratch. this work presents results towards addressing this challenge in the context of the uml security extension umlsec. we investigate the security analysis of umlsec models by means of a changespecific notation allowing multiple evolution paths and sound algorithms supporting the incremental verification process of evolving models. the approach is validated by a tool implementation of these verification techniques that extends the existing umlsec tool support.incremental security verification for evolving umlsec models','Formal security models'
'security assessment is largely ad hoc today due to its inherent complexity. the existing methods are typically experimental in nature highly dependent of the assessor\'s experience, and the security metrics are usually qualitative. we propose to address the dual problems of experimental analysis and qualitative metrics by developing two complementary approaches for security assessment: (1) analytical modeling, and (2) metrics-based assessment. to avoid experimental evaluation, we put forward a formal model that permits the accurate and scientific analysis of different security attributes and security flaws. to avoid qualitative metrics leading to ambiguous conclusions, we put forward a collection of mathematical formulas based on which quantitative metrics can be derived. the vulnerability analysis model responses to the need for a theoretical foundation for modeling information security, and security metrics are the cornerstone of risk analysis and security management. in addition to the security analysis approach, we discuss security testing methods as well. a relative complete coverage (rcc) principle is proposed along with an example of applying the rcc principle. the innovative ideas proposed in this paper include a hierarchical multi-level modeling approach to modeling vulnerability using model composition and refinement techniques, a data-centric, quantitative metrics mechanism, and multidimensional assessment capturing both process and product elements in a formalized framework.information security models and metrics','Formal security models'
'risk assessment is important in assessing the security states in information security. this paper proposed to use markov models to assess the risk of information security. the simulation results were shown using different distributions.information security risk assessment using markov models','Formal security models'
'large, distributed application plays an increasing central role in today\'s information technology environment. the diversity and openness of these systems have given raise to questions of trust, certification and security. this paper deals with the up gradation of security certificate of a component. as security certification is necessary for third-party components, taking certification from the beginning whenever a component is updated turns out to be a redundant process. it leads to wastage of time especially in large component based software systems. to remove that redundant certification we propose a certdriver. the proposed certdriver takes the component, its required security properties as security goals and version number specified by user as input and gives output a certified component providing its security in terms of percentages. the whole process centers on version number. in the whole process, it will also take the help of test cases which can be useful while finding out errors in the component. the aim of this paper to examine certification, trust and security related questions and try to find out possible solutions.intra-component security certification','Formal security models'
'in today&#226;&#128;&#153;s software development process, security related design decisions are rarely made early in the overall process. even if security is considered early, this means that in most cases a more-or-less encompassing security requirement analyses is made, based on this analysis best-practices, ad-hocdesign decisions or individual expertise is used to integrate security during the development process or after weaknesses are found after the deployment. this paper introduces security building block models which are used to build security related components, namely security building blocks. these security building blocks represent concrete security solutions, so called security properties, introduced in other publications of the sec futur project. the goal of this approach is to provide already defined and tested security related software components, which can be used early in the overall development process, to support security-design-decision already while modeling the software-system. the paper shortly describes this new security engineering process with its requirement analysis and definition of security properties and how the security building block model fits into this approach. additionally the security building block model is presented in detail. all artifacts and relationships of the model are described. short examples finish up the paper to show the creation of the security building blocks and their interactions with other software components.introducing security building block models','Formal security models'
'we present a formalism for the automatic verification of security protocols based on multi-agent systems semantics. we give the syntax and semantics of a temporal-epistemic securityspecialised logic and provide a lazy-intruder model for the protocol rules that we argue to be particularly suitable for verification purposes. we exemplify the technique by finding a (known) bug in the traditional nspk protocol.ldyis: a framework for model checking security protocols','Formal security models'
'locality abstraction and security models in a mobile agent environment','Formal security models'
'in this chapter i present a process algebraic approach to the modelling of security properties and policies. i will concentrate on the concept of secrecy, also known as confidentiality, and in particular on the notion of non-interference. non-interference seeks to characterise the absence of information flows through a system and, as such, is a fundamental concept in information security.a central thesis of these lectures is that, viewed from a process algebraic point of view, the problem of characterising non-interference is essentially equivalent to that of characterising the equivalence of processes. the latter is itself a fundamental and delicate question at the heart of process algebra and indeed theoretical computer science: the semantics of a process is intimately linked to the question of which processes should be regarded as equivalent.we start, by way of motivation and to set the context, with a brief historical background. a much fuller exposition of security policies in the wider sense, embracing properties other than secrecy, can be found in the chapter by pierangela samarati in this volume. we then cover some elements of process algebra, in particular csp (communicating sequential processes), that we need and present a formulation of noninterference, along with some more operational presentations of process algebra, including the idea of bi-simulation. i argue that the classical notion of unwinding found in the security literature is really just bisimulation in another guise.finally, i propose some generalisations of the process algebraic formulations designed to encompass a richer class of policies and examples.mathematical models of computer security','Formal security models'
'in computer security, risk communication refers to informing computer users about the likelihood and magnitude of a threat. efficacy of risk communication depends not only on the nature of the risk, but also on the alignment between the conceptual model embedded in the risk communication and the user\'s mental model of the risk. the gap between the mental models of security experts and non-experts could lead to ineffective risk communication. our research shows that for a variety of the security risks self-identified security experts and non-experts have different mental models. we propose that the design of the risk communication methods should be based on the non-expert mental models.mental models of security risks','Formal security models'
'it is quite necessary that an organization\'s information network should be equipped with a proper security system based on its scale and importance. one of the effective methods is to use the simulation model for deciding which security policy and mechanism is appropriate for the complex network. the need arises for systems to coordinate with one another, to manage diverse attacks across networks. the coordination issue is the essential problem since it is beyond the scope of any one ids (intrusion detection system) to deal with the intrusions. this paper shows a modeling and simulation of network security in which the multi-security agents coordinate by sharing attacker\'s information for the effective detection of the intrusion.modeling and simulation of distributed security models','Formal security models'
'this paper discusses progress in the verification of security protocols. focusing on a small, classic example, it stresses the use of program-like representations of protocols, and their automatic analysis in symbolic and computational models. models and proofs of protocol security','Formal security models'
'measurement units and knowledge of security properties are hardly known. this process causes the complex systems decomposition into simpler and smaller systems thus allowing the estimative of properties that will help the understanding and measurement of software systems security properties. this process provides the security model and the score of security attributes priority is calculated by ahp methodology. a security model example to illustrate this approach is presented.models for measuring access security of web application','Formal security models'
'very often, risk assessment in security systems is often done by human experts, because there is no exact and mathematical solution to the problem. usually the human reasoning and perception process cannot be expressed precisely. different people have different opinions about risk and the association of its dependent variables. we first present the role of fuzzy inference methods to develop intelligent online risk assessment models. we further illustrate the optimization of fuzzy inference systems using neural learning and evolutionary learning for using such models in an online environment. all the developed models are used in an intrusion detection/prevention system for online risk assessment. finally, we present genetic programming models that could combine both intrusion detection and risk assessment and easily deployed in a mobile environment. nature inspired online real risk assessment models for security systems','Formal security models'
'network security policy models','Formal security models'
'this paper identifies some of the features of ipv6 technology that provide security and privacy benefits to internet users and commercial network operators. it is clearly shown that deployment of ipv6 technology is a critical step on the path towards a more secure and trustworthy networking infrastructure for the future.new internet security and privacy models enabled by ipv6','Formal security models'
'a unified view of a wide range of adversary classes and composition principles for reasoning about security properties of systems are cornerstones of a science of security. they provide a systematic basis for security analysis by explaining and predicting attacks on systems.on adversary models and compositional security','Formal security models'
'group key exchange (gke) protocols can be used to guarantee confidentiality and authentication in group applications. the paradigm of provable security subsumes an abstract formalization (security model) that considers the protocol environment and identifies its security goals. the first security model for gke protocols was proposed by bresson, chevassut, pointcheval, and quisquater in 2001, and has been subsequently applied in many security proofs. their definitions of ake-security (authenticated key exchange; a.k.a. indistinguishability of the key) and ma-security (mutual authentication) became meanwhile standard. in this paper we analyze the bcpq model and some of its variants and identify several risks resulting from its technical core construction - the notion of partnering. consequently, we propose a revised model extending ake- and ma-security in order to capture attacks by malicious participants and strong corruptions. then, we turn to generic solutions (known as compilers) for ake- and ma-security in bcpq-like models. we describe a compiler c-ama which provides ake- and ma-security for any gke protocol, under standard cryptographic assumptions, that eliminates some identified limitations in existing compilers.on security models and compilers for group key exchange protocols','Formal security models'
'on the evolution of adversary models in security protocols','Formal security models'
'we make fine-grained distinctions on the security models for provably secure ring signature schemes. currently there are two commonly used security models which are specified by rivest et al. [1] and abe et al. [1]. they offer different levels of security. in this paper, we introduce a new but compatible model whose security level can be considered to be lying in between these two commonly used models. it is important to make fine-grained distinctions on the security models because some schemes may be secure in some of the models but not in the others. in particular, we show that the bilinear map based ring signature scheme of boneh et al. [4], which have been proven secure in the weakest model (the one specified by rivest et al. [15]), is actually insecure in stronger models (the new model specified by us in this paper and the one specified by abe et al. [1]). we also propose a secure modification of their scheme for each of the two stronger models. in addition, we propose a threshold ring signature scheme using bilinear maps and show its security against adaptive adversaries in the strongest model defined in this paper. throughout the paper, we carry out all of the security analyses under the random oracle assumption.on the security models of (threshold) ring signature schemes','Formal security models'
'techniques to reduce power dissipation for embedded systems have recently come into sharp focus in the technology development. among these techniques, dynamic voltage scaling (dvs), power gating (pg), and multiple-domain partitioning are regarded as effective schemes to reduce dynamic and static power. in this paper, we investigate the problem of power-aware scheduling tasks running on a scalable encryption processor, which is equipped with heterogeneous distributed soc designs and needs the effective integration of the elements of dvs, pg, and the scheduling for correlations of multiple domain resources. we propose a novel heuristic that integrates the utilization of dvs and pg and increases the total energy-saving. furthermore, we propose an analytic model approach to make an estimate about its performance and energy requirements between different components in systems. these proposed techniques are essential and needed to perform dvs and pg on multiple domain resources that are of correlations. experiments are done in the prototypical environments for our security processors and the results show that significant energy reductions can be achieved by our algorithms.power-aware scheduling for parallel security processors with analytical models','Formal security models'
'information risk security management is an area that is constantly moving to respond to new threats, standards and technologies. security is now a part of information risk management, which in turn has a place in the overall business risk management strategy. the security model can help with explaining why security is important, and can support justifications for that \'rather expensive\' piece of technology, depending on the point of view, security policy and business appetite for risk.practical application of information security models','Formal security models'
'recent research in security and protocol verification has shown an important need for probabilistic formal concurrent models. indeed, the use of probabilities in formal models allows to define and check quantitative properties which are usefull for a lot of applications, such as probabilistic anonymity, failures or information leakage. several recent research showed very interesting situations for these properties [1]. research on probabilistic models is also a topic of interest from the mathematical point of view. topics include relations between probability and non-determinism, expressivity, processus equivalence and denotational semantics. research domains for these topics include probabilistic process algebras, concurrent constraint programming and domain theory. interesting references can be found in [2,3]. probabilistic and concurrent models for security','Formal security models'
'risk assessment is often done by human experts, becausethere is no exact and mathematical solution to the problem.usually the human reasoning and perception process cannotbe expressed precisely. this paper propose a geneticprogramming approach for risk assessment. preliminaryresults indicate that genetic programming methods are robustand suitable for this problem when compared to otherrisk assessment models.programming risk assessment models for online security evaluation systems','Formal security models'
'cloud computing has become a hot topic both in research and in industry, and when making decisions on deploying/adopting cloud computing related solutions, security has always been a major concern. this article summarizes security related issues in cloud computing and proposes five service deployment models to address these issues. the proposed models provide different security related features to address different requirements and scenarios and can serve as reference models for deployment.reference deployment models for eliminating user concerns on cloud security','Formal security models'
'with the development of national economy, the construction of highway network makes astounding advances. the province highway network is evolving after some years&#8217; construction. with the formation of the road network, the security issue of highway toll network is becoming prominent. in this paper, the status and the network security issue of the current highway toll network charge, combined with zhejiang expressway, are discussed and a security model for the highway toll network is established, and the level of network security prevention system is proposed.research on security models of highway toll network','Formal security models'
'in this paper, we introduce a notion of restricted revocable delegation and study its consequences in language-based security. in particular, we add this notion by means of delegate and revoke commands to a simple imperative programming language. we then define an operational semantics for our programming language, in the natural semantics style of gilles kahn. we briefly discuss our initial ideas about the security properties of the semantics, which are extensions of existing variations of the renowned non-interference property, e.g., in the context of delimited information release.restricted delegation and revocation in language-based security','Formal security models'
'in this paper, we explore the technologies behind the security models applied to distributed data access in a grid environment. our goal is to study a security model allowing data integrity, confidentiality, authentication and authorization for vo users. we split the process for data access in three levels: grid authentication, grid authorization, local enforcement. for each level, we introduce at least one possible technological solution. finally, we show our vision of a soa oriented security framework. this work is developed as part of the coregrid network of excellence, for the institute on knowledge and data management.review of security models applied to distributed data access','Formal security models'
'in this paper we review existing approaches for the safety and security analysis of object-oriented software designs, and identify ways in which these approaches can be improved and made more rigorous.safety and security analysis of object-oriented models','Formal security models'
'process algebraic specifications of distributed systems are increasingly being targeted at identifying security primitives well-suited as high-level programming abstractions, and at the same time adequate for security analysis and verification. drawing on our earlier work along these lines [bugliesi, m. and r. focardi, language based secure communication, in: proceedings of the 21st ieee computer security foundations symposium, csf 2008, pittsburgh, pennsylvania, 23-25 june 2008 (2008), pp. 3-16], we investigate the expressive power of a core set of security and network abstractions that provide high-level primitives for the specifications of the honest principals in a network as well as the lower-level adversarial primitives that must be assumed available to an attacker. we analyze various bisimulation equivalences for security, arising from endowing the intruder with (i) different adversarial capabilities and (ii) increasingly powerful control on the interaction among the distributed principals of a network. by comparing the relative strength of the bimimulation equivalences we obtain a direct measure of the discriminating power of the intruders, hence of the expressiveness of the corresponding models.security abstractions and intruder models (extended abstract)','Formal security models'
'a new service model of public wireless internet access, called autonomous distributed public wireless internet access, is presented. in the service model any volunteer with broadband internet access lines can provide his access points for public service without any fear of malicious use. a user of such service is assumed to have his own account on a authentication server at home in the internet, and all the internet access through any of those access points can be treated as if it is from the home. in this paper, we present how the autonomous distributed internet access services can be securely provided with the combinations of two aspects: treatment of authentication transactions at access points and data path of communication transaction.security analysis on public wireless internet service models','Formal security models'
'building secure systems is a difficult job for most engineers since it requires in-depth understanding of security aspects. this task, however, can be assisted by capturing security knowledge in a particular domain and reusing the knowledge when designing applications. we use this strategy and employ an information security ontology to represent the security knowledge. the ontology is associated with system designs which are modelled in collaborative building blocks specifying the behaviour of several entities. in this paper, we identify rules to be applied to the elements of collaborations in order to identify security assets present in the design. further, required protection mechanisms are determined by applying a reasoner to the ontology and the obtained assets. we exemplify our approach with a case study from the smart metering domain.security asset elicitation for collaborative models','Formal security models'
'supporting distributed, research collaborations is a fundamental demand of e-research infrastructures (e-infrastructures). to be successful, e-infrastructures must address the needs of all parties involved including end user researchers and associated stakeholders, e.g. organizations that make resources available. these needs often translate into ensuring the security and integrity of systems and data sets used for research purposes. whilst a cornerstone of e-research has been to support single sign-on, i.e. where users are not required to provide multiple username/passwords, the reality is that most single sign-on solutions have been based around authentication-oriented only models based on public key infrastructures. for many researchers and organizations, finer-grained access control (authorization) is essential. such authorization solutions typically depend on delivery of security attributes that determine the privileges of individuals that can subsequently be used to determine their access requests to organizational resources. in this paper we identify attribute delivery patterns that support different authorization-oriented collaborative models. these patterns are currently being explored within the context of the australian urban research infrastructure network (aurin--www.aurin.org.au).security attribute aggregation models for e-research collaborations','Formal security models'
'ensuring that software protects its users\' privacy has become an increasingly pressing challenge. requiring software to be certified with a secure type system is one enforcement mechanism. protecting privacy with type systems, however, has only been studied for programs written entirely in a single language, whereas software is frequently implemented using multiple languages specialized for different tasks. this paper presents an approach that facilitates reasoning over composed languages. it outlines sufficient requirements for the component languages to lift privacy guarantees of the component languages to well-typed composed programs, significantly lowering the burden necessary to certify that such composite programs safe. the approach relies on computability and security-level separability. this paper defines completeness with respect to secure computations and formally establishes conditions sufficient for a security-typed language to be complete. we demonstrate the applicability of the results with a case study of three seminal security-typed languages.security completeness','Formal security models'
'as messaging middleware technology matures, users demand increasingly many features, leading to modular middleware architectures. however, extra complexity increases the risk of a security breach, arising from a vulnerability in one module or misconfiguration of the module linkages. this position paper presents a framework for enforcing security policies between middleware modules, which simultaneously facilitates co-design of application and middleware security. for example, a healthcare application might require (1) all clinical data to be encrypted in transit, (2) a log of all messages sent and delivered (revealing no disclosive patient information), and (3) parameterised role based access control on message delivery. in our framework, we can satisfy all of these requirements, even when each feature is implemented as a separate extension module: extensions tag events with meta-data, and this meta-data guides the enforcement of the security policy. exposing this meta-data to applications can help to unite application and middleware security policy.security for middleware extensions','Formal security models'
'planning information security investment is somewhere between art and science. this paper reviews and compares existing scientific approaches and discusses the relation between security investment models and security metrics. to structure the exposition, the high-level security production function is decomposed into two steps: cost of security is mapped to a security level, which is then mapped to benefits. this allows to structure data sources and metrics, to rethink the notion of security productivity, and to distinguish sources of indeterminacy as measurement error and attacker behavior. it is further argued that recently proposed investment models, which try to capture more features specific to information security, should be used for all strategic security investment decisions beneath defining the overall security budget.security metrics and security investment models','Formal security models'
'with the widespread use of electronic health record (ehr), building a secure ehr sharing environment has attracted a lot of attention in both healthcare industry and academic community. cloud computing paradigm is one of the popular healthit infrastructure for facilitating ehr sharing and ehr integration. in this paper we discuss important concepts related to ehr sharing and integration in healthcare clouds and analyze the arising security and privacy issues in access and management of ehrs. we describe an ehr security reference model for managing security issues in healthcare clouds, which highlights three important core components in securing an ehr cloud. we illustrate the development of the ehr security reference model through a use-case scenario and describe the corresponding security countermeasures and state of art security techniques that can be applied as basic security guards.security models and requirements for healthcare application clouds','Formal security models'
'digital watermarking, traditionally modeled as communication with side information, is generally considered to have important potential applications in various scenarios such as digital rights managements. however, the current literature mainly focuses on robustness, capacity and imperceptibility. there lacks systematic formal approach in tackling secure issues of watermarking. one one hand, the threat models in many previous works are not sufficiently established, which result in somewhat superficial or even flawed security analysis. on the other hand, there lacks a rigorous model for watermarking in general that allows useful analysis in practice. there has been some efforts in clearing the threat models and formulate rigorous watermarking models. however, there are also many other cases where security issues are lightly or incorrectly treated. in this paper, we survey various security notions and models in previous work, and discuss possible future research directions.security models of digital watermarking','Formal security models'
'future security models will rely on system dynamics and agent technologies.security models','Formal security models'
'we formalize the notion of several weakened random oracle models in order to capture which property of a hash function is crucial to prove the security of a cryptographic scheme. in particular, we focus on augmenting the random oracle with additional oracles that respectively return collisions, secondpreimages, and first-preimages. we study the security of the full domain hash signature scheme, as well as three variants thereof in the weakened random oracle models, leading to a separation result.security of digital signature schemes in weakened random oracle models','Formal security models'
'liskov proposed several weakened versions of the random oracle model, called weakened random oracle models (wroms), to capture the vulnerability of ideal compression functions, which are expected to have the standard security of hash functions, i.e., collision resistance, second-preimage resistance, and one-wayness properties. the wroms offer additional oracles to break such properties of the random oracle. in this paper, we investigate whether public-key encryption schemes in the random oracle model essentially require the standard security of hash functions by the wroms. in particular, we deal with four wroms associated with the standard security of hash functions; the standard, collision tractable, second-preimage tractable, first-preimage tractable ones (rom, ct-rom, spt-rom, and fpt-rom, respectively), done by numayama et al. for digital signature schemes in the wroms. we obtain the following results: (1) the oaep is secure in all the four models. (2) the encryption schemes obtained by the fujisaki-okamoto conversion (fo) are secure in the spt-rom. however, some encryption schemes with fo are insecure in the fpt-rom. (3) we consider two artificial variants wfo and dfo of fo for separation of the wroms in the context of encryption schemes. the encryption schemes with wfo (dfo, respectively) are secure in the ct-rom (rom, respectively). however, some encryption schemes obtained by wfo (dfo, respectively) are insecure in the spt-rom (ct-rom, respectively). these results imply that standard encryption schemes such as the oaep and fo-based one do not always require the standard security of hash functions. moreover, in order to make our security proofs complete, we construct an efficient sampling algorithm for the binomial distribution with exponentially large parameters, which was left open in numayama et al.&#8217;s paper.security of encryption schemes in weakened random oracle models','Formal security models'
'we observe that the definitions of security in the computational complexity proof models of bellare &#38; rogaway (1993) and canetti &#38; krawczyk (2001) require two partners in the presence of a malicious adversary to accept the same session key, which we term a key sharing requirement. we then revisit the bellare&#8211;rogaway three-party key distribution (3pkd) protocol and the jeong&#8211;katz&#8211;lee two-party authenticated key exchange protocol $\\mathcal{ts}2$, which carry claimed proofs of security in the canetti &#38; krawczyk (2001) model and the bellare &#38; rogaway (1993) model respectively. we reveal previously unpublished flaws in these protocols where we demonstrate that both protocols fail to satisfy the definition of security in the respective models. we present a new 3pkd protocol as an improvement with a proof of security in the canetti &#38; krawczyk (2001) model and a simple fix to the specification of protocol $\\mathcal{ts}2$. we also identify several variants of the key sharing requirement and present a brief discussion.security requirements for key establishment proof models','Formal security models'
'web applications are complex and face a massive amount of sophisticated attacks. since manually testing web applications for security issues is hard and time consuming, automated testing is preferable. in model-based testing, test cases are often generated using structural criteria. since such test cases do not directly target security properties, this ph.d thesis proposes to use a fault model for generating tests for web applications. faults are represented as known source code vulnerabilities that, by using respective mutation operators at the model level, are injected into models of a system under validation to generate \"interesting\" test cases. to achieve this, advantages of penetration testing are combined with model-checkers dedicated to security analysis. to find attacks on real systems the gap between an abstract attack trace output by a model-checker and a penetration test needs to be addressed. this ph.d thesis contributes with a semi-automatic methodology to turn abstract attack traces operational.security testing with fault-models and properties','Formal security models'
'today, cloud computing is rising strongly, presenting itself to the market by its main service models, known as iaas, paas and saas, that offer advantages in operational investments by means of on-demand costs, where consumers pay by resources used. in face of this growth, security threats also rise, compromising the confidentiality, integrity and availability of the services provided. our work is a systematic mapping where we hope to present metrics about publications available in literature that deal with some of the seven security threats in cloud computing, based in the guide entitled \"top threats to cloud computing\" from the cloud security alliance (csa). in our research we identified the more explored threats, distributed the results between fifteen security domains and identified the types of solutions proposed for the threats. in face of those results, we observed that publications in the literature mostly show a certain tendency as the proposals presented for threats involved. however, in some cases there is some variation, it motivated us to carry out an analysis of standard deviation in the results obtained in our protocol. based on these data, we present our conception regarding this behavior.security threats in cloud computing models','Formal security models'
'we study the vulnerability reports in the common vulnerability and exposures (cve) database by using topic models on their description texts to find prevalent vulnerability types and new trends semi-automatically. in our study of the 39,393 unique cves until the end of 2009, we identify the following trends, given here in the form of a weather forecast: php: declining, with occasional sql injection. buffer overflows: flattening out after decline. format strings: in steep decline. sql injection and xss: remaining strong, and rising. cross-site request forgery: a sleeping giant perhaps, stirring. application servers: rising steeply.security trend analysis with cve topic models','Formal security models'
'this paper presents a study that uses extensive analysis of real security vulnerabilities to drive the development of: 1) runtime techniques for detection/masking of security attacks and 2) formal source code analysis methods to enable identification and removal of potential security vulnerabilities. the presentation will describe the hardware architecture of a reliability and security engine (rse) that embodies the proposed techniques, to provide run-time checking at the processor level.security vulnerabilities','Formal security models'
'semiring-based constraint models and frameworks have been extensively used in literature to optimize different security-related metrics, in order to represent trust scores, levels of security and, in general, quantitative information on shared resources to be securely managed. in this tutorial, we summarize four approaches that show an application of these formal models to different security-related problems, as access control list-like rights, policy-based access with weighted credentials, propagation of trust on trust-networks, and the cascade vulnerability problem.semiring-based constraint models and frameworks for security-related scenarios','Formal security models'
'this paper describes an approach to model complex applications by modeling application requirements separately from security requirements in use case models. by careful separation of concerns, the security requirements are captured in security use cases separately from the application requirements, which are captured in application use cases. the approach reduces system complexity caused by mixing security requirements with business application requirements with the goal of making complex systems more maintainable. furthermore, the security use cases can be reused by other software applications. this paper describes how the application and security concerns are modeled separately, and how they can be woven together into an application.separating application and security concerns in use case models','Formal security models'
'separating some splicing models','Formal security models'
'model-carrying code and security-by-contract have proposed to augment mobile code with a claim on its security behavior that could be matched against a mobile platform policy before downloading the code. in order to capture realistic scenarios with potentially infinite transitions (e.g. \"only connections to urls starting with https\") we have proposed to represent those policies with the notion of automata modulo theory (amt), an extension of buchi automata (ba), with edges labeled by expressions in a decidable theory. our objective is the run-time matching of the mobile\'s platform policy against the midlet\'s security claims expressed as amt. to this extent the use of on-the-fly product and emptiness test from automata theory may not be effective. in this paper we present an algorithm extending fair simulation between b&#252;chi automata that can be more efficiently implemented.simulating midlet\'s security claims with automata modulo theory','Formal security models'
'ids (intrusion detection system) plays a vital role in network security in that it monitors system activities to identity unauthorized use, misuse or abuse of computer and network system. for the simulation of ids a model has been constructed based on the devs (discrete event system specification) formalism. with this model we can simulate whether the intrusion detection, which is a core function of ids, is effectively done under various different conditions. as intrusions become more sophisticated, it is beyond the scope of any one ids to deal with them. thus we placed multiple ids agents in the network where the information helpful for detecting the intrusions is shared among these agents to cope effectively with attackers. each agent cooperates through the bba (black board architecture) for detecting intrusions. if an agent detects intrusions, it transfers attacker\'s information to a firewall. using this mechanism attacker\'s packets detected by ids can be prevented from damaging the network.simulation of network security with collaboration among ids models','Formal security models'
'this paper presents a tool called asm-spv (abstract state machines-security protocols verifier) for verifying security protocols by model checking. in asm-spv, a security protocol is modeled by coreasm language which is an executable asm (abstract state machines) language. then a modified coreasm engine takes the coreasm model of the protocol to build state space on-demand. furthermore, security properties of the protocol are described as ctl (computation tree logic) formulas and an adapted model checking algorithm is introduced to check whether the coreasm model satisfies a given ctl formula or not. in this paper, we show the effectiveness of asm-spv with regard to memory consumption and speed of generating states compared with another coreasm based model checker [mc]square.some improvements on model checking coreasm models of security protocols','Formal security models'
'security automata are a variant of b&#252;chi automata used to specify security policies that can be enforced by monitoring system execution. in this paper, we propose using csp-oz, a specification language combining communicating sequential processes (csp) and object-z (oz), to specify security automata, formalize their combination with target systems, and analyze the security of the resulting system specifications. we provide theoretical results relating csp-oz specifications and security automata and show how refinement can be used to reason about specifications of security automata and their combination with target systems. through a case study, we provide evidence for the practical usefulness of this approach. this includes the ability to specify concisely complex operations and complex control, support for structured specifications, refinement, and transformational design, as well as automated, tool-supported analysis.specifying and analyzing security automata using csp-oz','Formal security models'
'the jisc-funded shintau project has produced an extension to the shibboleth profile which allows a user to link information from more than one idp together utilising a custom linking service (ls). this paper describes both the application and independent evaluation of this software by the nationale-science centre (nesc) at the university of glasgow within the context of the esrc-funded data management throug he-social science (dames) project.supporting federated multi-authority security models','Formal security models'
'the java security model has undergone considerable evolution since its initial implementation. however, due to its historical focus on securing machines against attack from hostile java applications, it has neglected support for securing \"real world\" applications. we suggest that in order to support \"real world\" security the ability to insert checks into compiled code in a principled way and high-level abstract security models are required.we briefly review the evolution of the java security model, outline the requirements for supporting \"real world\" security for applications, discuss whether enterprise java beans satisfy these requirements, introduce our approach to meeting these requirements and discuss our current work.supporting real world security models in java','Formal security models'
'developing security-critical systems in a way that makes sure that the developed systems actually enforce the desired security requirements is difficult, as can be seen by many security vulnerabilities arising in practice on a regular basis. part of the difficulty is the transition from the security requirements analysis to the design, which is highly nontrivial and error-prone, leaving the risk of introducing vulnerabilities. unfortunately, existing approaches bridging this gap largely only provide informal guidelines for the transition from security requirements to secure design. we present a method to systematically develop structural and behavioral umlsec design models based on security requirements. each step of our method is supported by model generation rules expressed as pre- and postconditions using the formal specification language ocl. moreover, we present a concept for a case tool based on the model generation rules. thus, applying our method to generate umlsec design models supported by this tool and based on previously captured and analyzed security requirements becomes systematic, less error-prone, and a more routine engineering activity. we illustrate our method by the example of a patient monitoring system.systematic development of umlsec design models based on security requirements','Formal security models'
'this paper is focussed on the notion of a formal model of security policy (fmsp). this kind of model is essential when reasoning about the security of information technology devices like a specific it-product or it-system. without an unambiguous definition of what security means, it is impossible to say whether a product is really secure.the new topicality of using formal models of security policy within the security engineering process','Formal security models'
'multiple-prerequisite graph (mp graph) is a type of attack graph that has been developed to help defending large scale enterprise network. as a middle-level attack graph, it has its unique advantages. however, quantitative security evaluations based on mp graph has not been proposed yet. in this paper, we present two stochastic models for quantitative security evaluation using mp graphs. these models are constructed based on the use of markov decision process to model the attacker&#8217;s behaviors. the network administrators can use these two models respectively to evaluate security metrics at network designing stage and network defending stage.two stochastic models for security evaluation based on attack graph','Formal security models'
'a number of programming languages use rich type systems to verify security properties of code. some of these languages are meant for source programming, but programs written in these languages are compiled without explicit security proofs, limiting their utility in settings where proofs are necessary, e.g., proof-carrying authorization. others languages do include explicit proofs, but these are generally lambda calculi not intended for source programming, that must be further compiled to an executable form. a language suitable for source programming backed by a compiler that enables end-to-end verification is missing. in this paper, we present a type-preserving compiler that translates programs written in fine, a source-level functional language with dependent refinements and affine types, to dcil, a new extension of the .net common intermediate language. fine is type checked using an external smt solver to reduce the proof burden on source programmers. we extract explicit lcf-style proof terms from the solver and carry these proof terms in the compilation to dcil, thereby removing the solver from the trusted computing base. explicit proofs enable dcil to be used in a number of important scenarios, including the verification of mobile code, proof-carrying authorization, and evidence-based auditing. we report on our experience using fine to build reference monitors for several applications, ranging from a plugin-based email client to a conference management server.type-preserving compilation of end-to-end verification of security enforcement','Formal security models'
'multicore computers implementing weak memory models are mainstream, yet type-based analyses of these models remain rare. we help fill this gap. we not only prove the soundness of a type system for a weak execution model, but we also show that interesting properties of that model can be embedded in the types themselves. we argue that correspondence assertions can be used in a programming discipline that captures happens-before relationships, which are the basis for reasoning about weak memory in java. this programming discipline is flexible and can be statically enforced. we present several examples from java.util.concurrent and prove the static semantics sound with respect to an execution model based on java\'s memory model.types for relaxed memory models','Formal security models'
'the relationships between the work products of a security engineering process can be hard to understand, even for persons with a strong technical background but little knowledge of security engineering. market forces are driving software practitioners who are not security specialists to develop software that requires security features. when these practitioners develop software solutions without appropriate security-specific processes and models, they sometimes fail to produce effective solutions.we have adapted a proven object-oriented modeling technique, use cases, to capture and analyze security requirements in a simple way. we call the adaptation an abuse case model. its relationship to other security engineering work products is relatively simple, from a user perspective.using abuse case models for security requirements analysis','Formal security models'
'this paper is aimed at formally specifying and validating security-design models of an information system. it combines graphical languages and formal methods, integrating specification languages such as uml and an extension, secureuml, with the z language. the modeled system addresses both functional and security requirements of a given application. the formal functional specification is built automatically from the uml diagram, using our roz tool. the secure part of the model instanciates a generic security-kernel written in z, free from applications specificity, which models the concepts of rbac (role-based access control). the final modeling step creates a link between the functional model and the instanciated security kernel. validation is performed by animating the model, using the jaza tool. our approach is demonstrated on a case-study from the health care sector where confidentiality and integrity appear as core challenges to protect medical records.validation of security-design models using z','Formal security models'
'security for applications running on mobile devices is important. in this paper we present expressos, a new os for enabling high-assurance applications to run on commodity mobile devices securely. our main contributions are a new os architecture and our use of formal methods for proving key security invariants about our implementation. in our use of formal methods, we focus solely on proving that our os implements our security invariants correctly, rather than striving for full functional correctness, requiring significantly less verification effort while still proving the security relevant aspects of our system. we built expressos, analyzed its security, and tested its performance. our evaluation shows that the performance of expressos is comparable to an android-based system. in one test, we ran the same web browser on expressos and on an android-based system, and found that expressos adds 16\% overhead on average to the page load latency time for nine popular web sites.verifying security invariants in expressos','Formal security models'
'security plays a predominant role in software engineering. nowadays, security solutions are generally added to existing software either as an afterthought, or manually injected into software applications. however, given the complexity and pervasiveness of today\'s software systems, the current practices might not be completely satisfactory. in most cases, security features remain scattered and tangled throughout the entire software, resulting in complex applications that are hard to understand and maintain. in this paper, we propose an aspect-oriented modeling approach to systematically integrate security solutions into software during the early phases of the software development life cycle. first, we present the security design weaving approach, as well as the uml profile needed for specifying security aspects. then, we illustrate the approach through an example for injecting the design-level security aspects into base models.weaving security aspects into uml 2.0 design models','Formal security models'
'currently, research on reverse engineering and automated design pattern detection focuses mostly on some of the programming languages such as java and c/c++, with marginal interest in the .net area. in this paper, we present a tool for analyzing .net executables for architecture reconstruction in general and design pattern detection in particular. the tool extracts the meaningful information from .net systems and produces an output compatible with the design pattern detection tool we are developing for java software, tool called marple (metrics and architecture reconstruction plug-in for eclipse)..net reverse engineering with marple','Hardware reverse engineering'
'this paper describes a collaborative structured demonstration of reverse engineering tools that was presented at a working session at wcre 2001 in stuttgart, germany. a structured demonstration is a hybrid tool evaluation technique that combines elements from experiments, case studies, technology demonstrations, and benchmarking. the essence of the technique is to facilitate learning about software engineering tools using a common set of tasks. the collaborative experience discussed at wcre involved several peer and complementary technologies that were applied in concert to solve a real life reverse engineering problem. for the most part, the tool developers themselves applied their own tools to this problem. preliminary results have shown to the research community that we still have much to learn about our tools and how they can be applied as part of a reverse engineering and reengineering process. consequently, the participants agreed to continue participation in this demonstration beyond the wcre event.a collaborative demonstration of reverse engineering tools','Hardware reverse engineering'
'reverse engineering tools support software engineers in the process of analyzing and understanding complex software systems during maintenance activities. the functionality of such tools varies from editing and browsing capabilities to the generation of textual and graphical reports. there are several commercial reverse engineering tools on the market providing different capabilities and supporting specific source code languages. we evaluated four reverse engineering tools that analyze c source code: refine/c, imagix4d, sniff+, and rigi. we investigated the capabilities of these tools by applying them to a commercial embedded software system as a case study. we identified benefits and shortcomings of these four tools and assessed their applicability for embedded software systems, their usability, and their extensibility.a comparison of four reverse engineering tools','Hardware reverse engineering'
'to group related things together (for example to form subsystems); researchers in reverse engineering are looking for algorithms that create meaningful groups. one such algorithm, concept analysis, received a lot of interest recently. it creates a lattice of concepts, which have some advantages over the more traditional tree of clusters from clustering algorithms.we will argue that the main interest of concept analysis lies in the concepts themselves and can be disconnected from the particular structure (the lattice of concepts) in which the concepts are usually arranged. we will compare concept analysis to various other algorithms trying to select the most important concepts contained in a set of entities.our main conclusion is that although it have advantages, the lattice of concepts suffer from a major drawback that other constructs do not have: it returns much more information (concepts) than what it was given in input (a set of entities describing some software system).a comparison of graphs of concept for reverse engineering','Hardware reverse engineering'
'there is a growing number of reconfigurable architectures that combine the advantages of a hardwired implementation (performance, power consumption) with the advantages of a software solution (flexibility, time to market). today, there are devices on the market that can be dynamically reconfigured at run-time within one clock cycle. but the benefits of these architectures can only be utilized if applications can be mapped efficiently. in this paper we describe a design environment that takes into account the three aspects architecture, compiler, and applications, and we present the basic techniques that we use to realize the compiler.a design environment for processor-like reconfigurable hardware','Hardware reverse engineering'
'a new method of reverse engineering for fast, simple and interactive acquisition and reconstruction of a virtual three-dimensional (3d) model is presented. we propose an active stereo acquisition system, which makes use of two infrared cameras and a wireless active-pen device, supported by a reconstruction method based on subdivision surfaces. in the 3d interactive hand sketching process the user draws and refines the 3d style-curves, which characterize the shape to be constructed, by simply dragging the active-pen device; then the system automatically produces a low-resolution mesh that is naturally refined through subdivision surfaces. several examples demonstrate the ability of the proposed advanced design methodology to produce complex 3d geometric models by the interactive and iterative process that provides the user with a real-time visual feedback on the ongoing work.a fast interactive reverse-engineering system','Hardware reverse engineering'
'the field programmable digital signal processor (fpdsp) architecture is intended to allow application specific dsp filtering at moderate sample rates where the ability to rapidly modify the filter characteristics can be used to an advantage. applications that the fpdsp will be best suited for are rapid prototyping of filters, audio applications, and to evaluate the potential advantages of run-time reconfiguration. the system architecture is based on an input pipelined least significant bit first bit-serial two\'s complement arithmetic. it performs digital signal processing by using programmable bit-serial signal processing units and programmable interconnect. the bit-serial processing units implement simple arithmetic operations: summation, multiplication and division by powers of two, and multiplication by negative one. the programmable unit also has variable bit-delays to time-align bit-serial words and also generates the control signals for the arithmetic operations internally. by combining the functions of these programmable units, a 2nd order recursive filter has been built and tested to verify the functionality of the fpdsp.a field programmable bit-serial digital signal processor','Hardware reverse engineering'
'a form driven object-oriented reverse engineering methodology','Hardware reverse engineering'
'as a program evolves, it becomes increasingly difficult to understand and reason about changes in the source code. eventually, if enough changes are made, reverse engineering and design recovery techniques must be used in order to understand the current behavior of a system. in this context, the effective use of complementary approaches can facilitate program and system understanding by taking advantage of the relative benefits of different approaches.this paper presents an approach to reverse engineering that combines the use of both informal and formal methods and describes a case study project involving the reverse engineering of a mission control system used by the nasa jet propulsion laboratory to command unmanned spacecraft.a formal approach for reverse engineering','Hardware reverse engineering'
'when assessing the quality and maintainability of large c++ code bases, tools are needed for extracting several facts from the source code, such as: architecture, structure, code smells, and quality metrics. moreover, these facts should be presented in such ways so that one can correlate them and find outliers and anomalies. we present solidfx, an integrated reverse-engineering environment (ire) for c and c++. solidfx was specifically designed to support code parsing, fact extraction, metric computation, and interactive visual analysis of the results in much the same way ides and design tools offer for the forward engineering pipeline. in the design of solidfx, we adapted and extended several existing code analysis and data visualization techniques to render them scalable for handling code bases of millions of lines. in this paper, we detail several design decisions taken to construct solidfx. we also illustrate the application of our tool and our lessons learnt in using it in several types of analyses of real-world industrial code bases, including maintainability and modularity assessments, detection of coding patterns, and complexity analyses.a framework for reverse engineering large c++ code bases','Hardware reverse engineering'
'after developing several workbenches for reverse engineering various assembly languages, we developed a portable assembly language reverse engineering environment (pare). the individual workbenches and the portable version are described. the language independent method (lim) of programming used to develop the portable workbench is discussed, and issues specific to processing assembly languages as opposed to a third generation language are addressed.a portable assembler reverse engineering environment (pare)','Hardware reverse engineering'
'agile manufacturing enables an enterprise to introduce new products rapidly into the competitive markets. reverse engineering offers tremendous advantages over product development. the paper proposes the combined agile reverse engineering approach for the companies seeking agile manufacturing. the paper will outline a practical framework and characterise the agile reverse engineering and its applications.a proposed framework for agile reverse engineering','Hardware reverse engineering'
'querying source code is an essential aspect of a variety of software engineering tasks such as program understanding, reverse engineering, program structure analysis, and program flow analysis. in this paper, we present and demonstrate the use of an algebraic source code query technique that blends expressive power with query compactness. the query framework of source code algebra, or sca, permits users to express complex source code queries and views as algebraic expressions. queries are expressed on an extensible, object-oriented database that stores program source code. the sca algebraic approach offers multiple benefits such as an applicative query language, high expressive power, seamless handling of structural and flow information, clean formalism, and potential for query optimization. we present a case study where sca expressions are used to query a program in terms of program organization, resource flow, control flow, metrics, and syntactic structure. our experience with an sca-based prototype query processor indicates that an algebraic approach to source code queries combines the benefits of expressive power and compact query formulation.a query algebra for program databases','Hardware reverse engineering'
'the semantic web is gaining increasing interest to fulfill the need of sharing, retrieving, and reusing information. since web pages are designed to be read by people, not machines, searching and reusing information on the web is a difficult task without human participation. to this aim adding semantics (i.e meaning) to a web page would help the machines to understand web contents and better support the web search process. one of the latest developments in this field is google\'s rich snippets, a service for web site owners to add semantics to their web pages. in this paper we provide a structured approach to automatically annotate a web page with rich snippets rdfa tags. exploiting a data reverse engineering method, combined with several heuristics, and a named entity recognition technique, our method is capable of recognizing and annotating a subset of rich snippets\' vocabulary, i.e., all the attributes of its review concept, and the names of the person and organization concepts. we implemented tools and services and evaluated the accuracy of the approach on real e-commerce web sites.a reverse engineering approach for automatic annotation of web pages','Hardware reverse engineering'
'a reverse engineering process for design level document production from ada code','Hardware reverse engineering'
'this paper reviews the progress-to-date of the application of program reverse engineering technologies to a large-scale legacy software product. basic reverse engineering concepts and a project overview are outlined, followed by a description of the legacy software product, the reverse engineering toolkit used, and analysis and discussion of the experiences so far. future research directions and summary comments are then detailed.a software reverse engineering experience','Hardware reverse engineering'
'the majority of documents on the web are written in html, constituting a huge amount of legacy data: all documents are formatted for visual purposes only and with different styles due to diverse authorships and goals and this makes the process of retrieval and integration of web contents difficult to automate. we provide a contribution to the solution of this problem by proposing a structured approach to data reverse engineering of data-intensive web sites. we focus on data content and on the way in which such content is structured on the web. we profitably use a web data model to describe abstract structural features of html pages and propose a method for the segmentation of html documents in special blocks grouping semantically related web objects. we have developed a tool based on this method that supports the identification of structure, function, and meaning of data organized in web object blocks. we demonstrate with this tool the feasibility and effectiveness of our approach over a set of real web sites. a structured approach to data reverse engineering of web applications','Hardware reverse engineering'
'most current reverse engineering techniques start with an analysis of the system\'s source code to derive structural information, based on compiler technology. as a consequence of the maturity of the field, several formal program models exist that have allowed the automatic generation of language processing front-end. however, the software engineer has to code the data structures that implement the program model and the algorithms that implement the desired analysis. thus, while the domain of code analysis is well understood, economic convenience leads very often to rigid code analyzers that perform a fixed set of analyses and produce standard reports that users can only marginally customize.we have implemented a system for developing code analyzers that uses a unique database to store both a no-loss fine-grained intermediate representation and the analyses\' results. the analyzers are automatically generated from a very high-level specification of the desired analyses expressed in a domain-oriented language. we use an algebraic representation, called , as the user-visible intermediate representation. analyzers are specified in a logic-based language, called , which allows the specification of an analysis in the form of a traversal of an algebraic expression, with accesses to, and stores of, the database information the algebraic expression indexes. a foreign language interface allows the analyzers to be embedded into c programs to facilitate interoperation with other tools.a system for generating reverse engineering tools','Hardware reverse engineering'
'reverse engineering a program constructs a high-levelrepresentation suitable for various software developmentpurposes such as documentation or reengineering.unfortunately however, there are no establishedguidelines to assess the adequacy of such a representation.we propose two such criteria, completenessand accuracy, and show how they can be determinedduring the course of reversing the representation.a representation is successfully reversed when itis given as input to a suitable code generator, and aprogram equivalent to the original is produced. to explorethis idea, we reverse engineer a small but complexnumerical application, represent our understandingusing algebraic specifications, and then use a codegenerator to produce code from the specification. wediscuss the strengths and weaknesses of the approachas well as alternative approaches to reverse engineeringadequacy.adequate reverse engineering','Hardware reverse engineering'
'abstract: reverse engineering tools aimed at facilitating software maintenance suffer from low adoption. many are developed, but few are used by software engineers in performing their maintenance work. we introduce an approach for tool design that is aimed at increasing the adoptability potential of tools. our approach is based on applying cognitive analysis to identify cognitive overloads during software maintenance. a software solution is developed to target the cognitive difficulties in order to alleviate the overloads. the approach is described in the context of the implementation of a reverse engineering tool we call dynasee. dynasee is a tool that processes and visualises routine traces. we describe how dynasee addresses a specific set of cognitive difficulties.adoption of reverse engineering tools','Hardware reverse engineering'
'web applications are the legacy software of the future. the web application reverse-engineering process becomes necessary in order to facilitate the maintenance and evolution. this paper presents an approach to recover the architecture of web applications. the approach generates uml models from existing web applications through static and dynamic techniques. uml diagrams are extracted to depict the static, dynamic and behavioral aspect of web applications. finally, the architecture of the tool is described.an approach for reverse engineering of web applications','Hardware reverse engineering'
'this paper presents an industrial example of database reverse engineering. the example has been abridged so that it fits within a paper. also some of the field names have been disguised as a courtesy to the source company. nevertheless, the example is real and illustrates the kinds of mistakes and poor design that are often found in practice.an industrial example of database reverse engineering','Hardware reverse engineering'
'in this paper, we evaluate the benefits of applying aspect-oriented software development techniques in the context of a large-scale industrial embedded software system implementing a number of crosscutting concerns. additionally, we assess the feasibility of automatically extracting these crosscutting concerns from the source code. in order to achieve this, we present an approach for reverse engineering aspects from an ordinary application automatically. this approach incorporates both a concern verification and an aspect construction phase. our results show that such automated support is feasible, and can lead to significant improvements in source code quality.an initial experiment in reverse engineering aspects','Hardware reverse engineering'
'intermediate representations (ir) are a key issue both for compilers as well as for reverse engineering tools to enable efficient analyses. research in the field of compilers has proposed many sophisticated irs that can be used in the domain of reverse engineering, especially in the case of deep analyses, but reverse engineering has also its own requirements for intermediate representations not covered by traditional compiler technology this paper discusses requirements of irs for reverse engineering. it shows then how extending and integrating existing irs can meet most of these requirements. these extensions include a generalized ast and a mechanism supporting multiple views on programs. moreover, the paper shows how these views can efficiently be implemented.an intermediate representation for reverse engineering analyses','Hardware reverse engineering'
'in this work we present techniques and tools that enable effective reverse engineering procedures for web applications that were developed using the promising asp.net technology. we deal with model-driven development in its reverse aspect by implementing reverse engineering methods. our implemented methods model web applications using a well-known, web oriented and robust language, namely webml. this is, to the authors\' best knowledge, a novel re-engineering transformation. in this paper we propose a method to reverse engineer web applications in order to extract their conceptual model using webml notation. moreover, we present an efficient tool we have developed in order to implement the proposed method, along with a study of the application of our tool to an exemplar, content-management web application. the overall results are quite encouraging and indicate that our approach is efficient.application modeling using reverse engineering techniques','Hardware reverse engineering'
'this paper summarizes recent research at edinburgh university on applying domain knowledge of standard shapes and relationships to solve or improve reverse engineering problems. the problems considered are how to enforce known relationships when data fitting, how to extract features even in noisy data, how to get better shape parameter estimates and how to infer data about unseen features.applying knowledge to reverse engineering problems','Hardware reverse engineering'
'assertions had their origin in program verification. for the systems developed in industry, construction of assertions and their use in showing program correctness is a near-impossible task. however, they can be used to show that some key properties are satisfied during program execution. in this paper we first present a survey of the special roles that assertions can play in object oriented software construction. we then analyse such assertions by relating them to the case study of an automatic surveillance system. in particular, we address the following two issues: what types of assertions can be used most effectively in the context of object oriented software? how can you discover them and where should they be placed? during maintenance, both the design and the software are continuously changed. these changes can mean that the original assertions, if present, are no longer valid for the new software. can we automatically derive assertions for the changed software?assertions in object oriented software maintenance','Hardware reverse engineering'
'understanding the command-and-control (c&c) protocol used by a botnet is crucial for anticipating its repertoire of nefarious activity. however, the c&c protocols of botnets, similar to many other application layer protocols, are undocumented. automatic protocol reverse-engineering techniques enable understanding undocumented protocols and are important for many security applications, including the analysis and defense against botnets. for example, they enable active botnet infiltration, where a security analyst rewrites messages sent and received by a bot in order to contain malicious activity and to provide the botmaster with an illusion of successful and unhampered operation. in this work, we propose a novel approach to automatic protocol reverse engineering based on dynamic program binary analysis. compared to previous work that examines the network traffic, we leverage the availability of a program that implements the protocol. our approach extracts more accurate and complete protocol information and enables the analysis of encrypted protocols. our automatic protocol reverse-engineering techniques extract the message format and field semantics of protocol messages sent and received by an application that implements an unknown protocol specification. we implement our techniques into a tool called dispatcher and use it to analyze the previously undocumented c&c protocol of megad, a spam botnet that at its peak produced one third of the spam on the internet.automatic protocol reverse-engineering','Hardware reverse engineering'
'malware authors have recently begun using emulation technology to obfuscate their code. they convert native malware binaries into bytecode programs written in a randomly generated instruction set and paired with a native binary emulator that interprets the bytecode. no existing malware analysis can reliably reverse this obfuscation technique.in this paper, we present the first work in automatic reverse engineering of malware emulators. our algorithms are based on dynamic analysis. we execute the emulated malware in a protected environment and record the entire x86 instruction trace generated by the emulator. we then use dynamic data-flow and taint analysis over the trace to identify data buffers containing the bytecode program and extract the syntactic and semantic information about the bytecode instruction set. with these analysis outputs, we are able to generate data structures, such as control-flow graphs, that provide the foundation for subsequent malware analysis. we implemented a proof-of-concept system calledrotalume and evaluated it using both legitimate programs and malware emulated by vmprotect and code virtualizer. the results show that rotalume accurately reveals the syntax and semantics of emulated instruction sets and reconstructs execution paths of original programs from their bytecode representations.automatic reverse engineering of malware emulators','Hardware reverse engineering'
'designer\'s productivity has become the key-factor of the development of electronic systems. an increasing application of design data reuse is widely recognized as a promising technique to master future design complexities. since the intellectual property of a design is more and more kept in software-like hardware description languages (hdl), successful reuse depends on the availability of suitable hdl reverse engineering tools. this paper introduces new concepts for an integrated hdl reverse engineering tool-set and presents an implemented evaluation prototype for vhdl designs. starting from an arbitrary collection of hdl source code files, several graphical and textual views on the design description are automatically generated. the tool-set provides novel hypertext techniques, expressive graphical code representations, a user-defined level of abstraction, and interactive configuration mechanisms in order to facilitate the analysis, adoption and upgrade of existing hdl designs.basic concepts for an hdl reverse engineering tool-set','Hardware reverse engineering'
'best fit translational and rotational surfaces for reverse engineering shapes','Hardware reverse engineering'
'regulatory networks are complex networks. this paper addresses the challenge of modelling these networks. the boolean representation is chosen and supported as a suitable representation for an abstract approach. in in-silico experiments, two different bio-inspired techniques are applied to the reverse engineering of a boolean regulatory network: as a search method a genetic algorithm is applied and an indirect method based on artificial development and tuned to this application, is proposed. both methods are challenged at reverse engineering a known network - the yeast cell-cycle network model. presented results show that they are both successful in reverse engineering the considered network.bio-inspired reverse engineering of regulatory networks','Hardware reverse engineering'
'boundary curves in reverse engineering','Hardware reverse engineering'
'significant time is spent by companies trying to reproduce and fix the bugs that occur for released code. to assist developers, we propose the bugnet architecture to continuously record information on production runs. the information collected before the crash of a program can be used by the developers working in their execution environment to deterministically replay the last several million instructions executed before the crash. bugnet is based on the insight that recording the register file contents at any point in time, and then recording the load values that occur after that point can enable deterministic replaying of a program&#253;s execution. bugnet focuses on being able to replay the application&#253;s execution and the libraries it uses, but not the operating system. but our approach provides the ability to replay an application&#253;s execution across context switches and interrupts. hence, bugnet obviates the need for tracking program i/o, interrupts and dma transfers, which would have otherwise required more complex hardware support. in addition, bugnet does not require a final core dump of the system state for replaying, which significantly reduces the amount of data that must be sent back to the developer.bugnet','Hardware reverse engineering'
'we present cager: a novel framework for converting animated 3d shape sequences into compact and stable cage-based representations. given a raw animated sequence with one-to-one point correspondences together with an initial cage embedding, our algorithm automatically generates smoothly varying cage embeddings which faithfully reconstruct the enclosed object deformation. our technique is fast, automatic, oblivious to the cage coordinate system, provides controllable error and exploits a gpu implementation. at the core of our method, we introduce a new algebraic algorithm based on maximum volume sub-matrices (maxvol) to speed up and stabilize the deformation inversion. we also present a new spectral regularization algorithm that can apply arbitrary regularization terms on selected subparts of the inversion spectrum. this step allows to enforce a highly localized cage regularization, guaranteeing its smooth variation along the sequence. we demonstrate the speed, accuracy and robustness of our framework on various synthetic and acquired data sets. the benefits of our approach are illustrated in applications such as animation compression and post-editing. &#169; 2012 wiley periodicals, inc.cager: cage-based reverse engineering of animated 3d shapes','Hardware reverse engineering'
'capsule oriented reverse engineering for software reuse','Hardware reverse engineering'
'the process of reverse engineering allows attackers to understand the behavior of software and extract proprietary algorithms and data structures (e.g. cryptographic keys) from it. code obfuscation is frequently employed to mitigate this risk. however, while most of today\'s obfuscation methods are targeted against static reverse engineering, where the attacker analyzes the code without actually executing it, they are still insecure against dynamic analysis techniques, where the behavior of the software is inspected at runtime. in this paper, we introduce a novel code obfuscation scheme that applies the concept of software diversification to the control flow graph of the software to enhance its complexity. our approach aims at making dynamic reverse engineering considerably harder as the information an attacker can retrieve from the analysis of a single run of the program with a certain input, is useless for understanding the program behavior on other inputs. based on a prototype implementation we show that our approach improves resistance against both static disassembling tools and dynamic reverse engineering at a reasonable performance penalty.code obfuscation against static and dynamic reverse engineering','Hardware reverse engineering'
'the commenter disagrees with the arguments in the above-titled article by p. samuelson (ibid., vol.7, no.4, p.90-6, jan. 1990) that reverse-engineering programs should be legal and that the unauthorized copies of the program created in decompilation are just `incidental\' and thus do not infringe the program\'s copyright. he also disagrees with samuelson\'s contention that there is a trend for the courts to find copyright infringement only where the program that results from the reverse engineering is substantially similar in expression to the decompiled program. he presents a detailed argument to support his viewpoint. samuelson presents additional information is support of her positioncomments, with reply, on `reverse-engineering someone else\'s software','Hardware reverse engineering'
'aperture transformation reduces metastability by trading unsafe edge arrival times with control signal metastability. we improve this further by providing 1) a new runt-free schmitt trigger-based hardening technique using a controlled clock signal and 2) a new detection/compensation technique that overwrites controlled-clock metastability using set&clear controls.cutting metastability using aperture transformation','Hardware reverse engineering'
'in the field of machine vision system, to evaluate the precision of the collected data whether to meet our needs is very important. this paper presents a new measuring system employing the technology of binocular parallax and liner laser scanning to pick up point cloud on the object surface, and then analyzes the errors in measurement system, finally verifies the accuracy of data acquisition.data acquisition and error analysis in reverse engineering','Hardware reverse engineering'
'database reverse engineering (dbre) attempts to recover the technical and semantic specifications of he persistent data of information systems. dependencies between records (data dependency) form a major class that need to be recovered. since most of these dependencies are not supported by the dbms, (foreign keys are the main exception, at least in modern relational dbms), they have not be explicitly declared in the database schema. careless reverse engineering will inevitably ignore them, leading to poor quality conceptual schema. several information sources can contribute to the elicitation of these hidden dependencies. the program source code has long been considered the richest, but also the most complex, of them. in this paper, we analyze and compare, through their respective quality and cost, different program understanding techniques that can be used to elicit data dependencies.data dependency elicitation in database reverse engineering','Hardware reverse engineering'
'data reverse engineering is a rapidly growing field, which is sometimes misunderstood. in our effort to promote the realization that data reverse engineering is a valuable and essential part of reverse engineering and is now finding its place in software engineering, we present a summary of the history of data reverse engineering. in this paper, a definition of data reverse engineering, a summary of the history of what has been done and published, a statement about the state-of-the-art today, and a comment on the future of dre is presented.data reverse engineering','Hardware reverse engineering'
'database reverse engineering is a complex activity that can be modeled as a sequence of two major processes, namely data structure extraction and data structure conceptualization. the first process consists in reconstructing the logical - that is, dbms-dependent - schema, while the second process derives the conceptual specification of the data from this logical schema. this paper concentrates on the first process, and more particularly on the reasonings and the decision process through which the implicit and hidden data structures and constraints are elicited from various sources.data structure extraction in database reverse engineering','Hardware reverse engineering'
'database reverse engineering','Hardware reverse engineering'
'development of a tool-set for remote and partial reconfiguration of fpgas','Hardware reverse engineering'
'this paper describes the development of a customised reverse engineering system in which a 3d digitiser &#40;microscribe&#45;3dx&#41; has been integrated with a computer&#45;aided design &#40;cad&#41; system &#40;pro&#47;engineer&#41;. the application programme written in c language enables a real&#45;time input from the digitiser to pro&#47;engineer. two application programming interfaces &#40;apis&#41; were used&#58; the microscribe&#45;3d software development kit &#40;sdk&#41; and pro&#47;toolkit. the former allows the user to develop an integrated system through intuitive and high&#45;level function calls to the digitiser. the latter enables a user to customise a pro&#47;engineer environment. this system offers an intuitive and user&#45;friendly means for reverse engineering. it also helps to shorten the whole reverse engineering process. this is because the digitised data can be displayed and edited in real time, so that early identification and exclusion of the undesired and incorrect data are made possible.development of an integrated reverse engineering system','Hardware reverse engineering'
'developing a dynamically adaptive system (das) requires a developer to identify viable target systems that can be adopted by the das at runtime in response to specific environmental conditions, while satisfying critical properties. this paper describes a preliminary investigation into using digital evolution to automatically generate models of viable target systems. in digital evolution, a population of selfreplicating computer programs exists in a user-defined computational environment and is subject to instruction-level mutations and natural selection. these \"digital organisms\" have no built-in ability to generate a model - each population begins with a single organism that only has the ability to self-replicate. in a case study, we demonstrate that digital evolution can be used to evolve known state diagrams and to further evolve these diagrams to satisfy system critical properties. this result shows that digital evolution can be used to aid in the discovery of the viable target systems of a das.digitally evolving models for dynamically adaptive systems','Hardware reverse engineering'
'we continue to be surprised by the variability reverse engineering problems. when we tackle new problems, we often encounter situations we have not seen before. for these different situations, we have to adjust our reverse engineering techniques, level of effort, and expectations. this paper characterizes dimensions of variation for reverse engineering of databases.dimensions of data ase reverse engineering','Hardware reverse engineering'
'in reverse engineering a physical object is digitally re-constructedfrom a set of boundary points. in the segmentation phase these points are grouped into subsets to faclitate consecutive steps as surface fitting. in this paperwe present a step segmentation method with subsequentclassification of simple algebraic surfaces. our memod isdirect in me sense diat it operates directly on the point setin contrast to othier approaches that are based on a triangulationof the data set.the segmentation process involves a fast algorithm for.k-nearest neighbors search and an estimation of first andsecond order surface properties. the first order segmentation,that is based on normal vectors, provides an initialsubdivision of the surface and detects sharp edges aswell as flat or highly curved areas. one of the main featuresof our method is to proceed by alternating the stepsof segmentation and normal vector estimation. the secondorder segmentation subdivides me surface accordingto principal curvatures and provides a sufficient foundationfor the classification of simple algebraic surfaces. ifthe boundary of dle original object contains such surfacesthe segmentation is optimized based on the result of a surfacefitting procedure.direct segmentation for reverse engineering','Hardware reverse engineering'
'program transformations have been advocated as a method for accomplishing reverse engineering. the hypothesis is that the original source code can be progressively transformed into alternative forms, but with the same semantics. at the end of the process, an equivalent program is acquired, but one which is much easier to understand and more maintainable.we have been undertaking an extensive programme of research over twelve years into the design and development of transformations for the support of software maintenance. the paper very briefly explains the theory, practice and tool support for transformational systems, but does not present new theoretical results. the main results are on an analysis of the strengths and weaknesses of the approach, based on experience with case studies and industrial applications. the evaluation framework used (called dere) is that presented in [5]. it is hoped that the results will be of benefit to industry, who might be considering using the technology; and to other researchers, interested in addressing the open problems.the overall conclusion is that transformations can help in the bottom-up analysis and manipulation of source code at approximately the 3gl level, and have proved successful in code migration, but need to be complemented by other top-down techniques to be useful at higher levels of abstraction or in more ambitious re-engineering projects.do program transformations help reverse engineering?','Hardware reverse engineering'
'document reverse engineering','Hardware reverse engineering'
'domain analysis and reverse engineering','Hardware reverse engineering'
'domain-retargetable reverse engineering ii','Hardware reverse engineering'
'for pt. ii see international conference on software maintenance (icsm \'94), p. 336-342. the paper describes ongoing work on a domain-retargetable reverse engineering environment which is used to aid the structural understanding of large information spaces. in particular, it presents a layered modeling approach to representing three classes of artifacts manipulated during the reverse engineering process. the approach provides a practical and extensible method of integrating existing tools and techniques into a reverse engineering environment by leveraging results from other areas, such as relational databases, hypertext, and conceptual modeling.domain-retargetable reverse engineering. iii. layered modeling','Hardware reverse engineering'
'domain-retargetable reverse engineering','Hardware reverse engineering'
'dynamic reverse engineering of java software','Hardware reverse engineering'
editorial,'Hardware reverse engineering'
'editorial: ring in the 00, ring in the new','Hardware reverse engineering'
'this tutorial will aim at providing practical guidelines on how to conduct empirical evaluations of software maintenance and reengineering approaches and tools. the tutorial will describe how to conduct empirical evaluations carried out involving human subjects (i.e., developers), and then will provide a \"by-example\'\' introduction to the main statistical procedures to be used for analyzing empirical study results.empirical studies in reverse engineering and maintenance','Hardware reverse engineering'
'starting with the aim of modernizing legacy systems, often written in old programming languages, reverse engineering has extended its applicability to virtually every kind of software system. moreover, the methods originally designed to recover a diagrammatic, high-level view of the target system have been extended to address several other problems faced by programmers when they need to understand and modify existing software. the authors\' position is that the next stage of development for this discipline will necessarily be based on empirical evaluation of methods. in fact, this evaluation is required to gain knowledge about the actual effects of applying a given approach, as well as to convince the end users of the positive cost---benefit trade offs. the contribution of this paper to the state of the art is a roadmap for the future research in the field, which includes: clarifying the scope of investigation, defining a reference taxonomy, and adopting a common framework for the execution of the experiments.empirical studies in reverse engineering','Hardware reverse engineering'
'buffer overflows have been the most common form of security vulnerability in the past decade. a number of techniques have been proposed to address such attacks. some are limited to protecting the return address on the stack; others are more general, but have undesirable properties such as large overhead and false warnings. the approach described in this paper uses legality assertions, source code assertions inserted before each subscript and pointer dereference that explicitly check that the referencing expression actually specifies a location within the array or object pointed at run time. a transformation system is developed to analyze a program and annotate it with appropriate assertions automatically. this approach detects buffer vulnerabilities in both stack and heap memory as well as potential buffer overflows in library functions. runtime checking through using automatically inferred assertions considerably enhances the accuracy and efficiency of buffer overflow detection. a number of example buffer over-flow-exploitingc programs are used to demonstrate the effectiveness of this approach.enhancing security using legality assertions','Hardware reverse engineering'
'much of the knowledge about software systems is implicit, and therefore difficult to recover by purely automated techniques. architectural layers and the externally visible features of software systems are two examples of information that can be difficult to detect from source code alone, and that would benefit from additional human knowledge. typical approaches to reasoning about data involve encoding an explicit meta-model and expressing analyses at that level. due to its informal nature, however, human knowledge can be difficult to characterize up-front and integrate into such a meta-model. we propose a generic, annotation-based approach to capture such knowledge during the reverse engineering process. annotation types can be iteratively defined, refined and transformed, without requiring a fixed meta-model to be defined in advance. we show how our approach supports reverse engineering by implementing it in a tool called &lt;emphasis fontcategory=\"sansserif\"&gt;metanool&lt;/emphasis&gt;and by applying it to (i) analyzing architectural layering, (ii) tracking reengineering tasks, (iii) detecting design flaws, and (iv) analyzing features.enriching reverse engineering with annotations','Hardware reverse engineering'
'eval endows javascript developers with great power. it allows developers and end-users, by turning text into executable code, to seamlessly extend and customize the behavior of deployed applications as they are running. with great power comes great responsibility, though not in our experience. in previous work we demonstrated through a large corpus study that programmers wield that power in rather irresponsible and arbitrary ways. we showed that most calls to eval fall into a small number of very predictable patterns. we argued that those patterns could easily be recognized by an automated algorithm and that they could almost always be replaced with safer javascript idioms. in this paper we set out to validate our claim by designing and implementing a tool, which we call evalorizer, that can assist programmers in getting rid of their unneeded evals. we use the tool to remove eval from a real-world website and validated our approach over logs taken from the top 100 websites with a success rate over 97\% under an open world assumption.eval begone!','Hardware reverse engineering'
'better understanding manual reverse engineering can make it and any associated systems reengineering more effective. we reverse engineered a version of a system (referred to as \"bos/x\") in support of a broader reengineering effort. system reengineering goals and other circumstances dictated a focused, limited duration, manual reverse engineering exercise. this presented an opportunity to study the bos/x reverse engineering separately from other reengineering activities. we studied the bos/x reverse engineering, the results achieved, and some limited reverse engineering metrics. this paper describes the: systems reengineering context; circumstances preventing application of automated techniques and motivating manual reverse engineering; reverse engineering process developed; bos/x reverse engineering goals; evolution of the reverse engineering products; re-verse engineering results; resources required to produce the results; and an evaluation of the reverse engineering effectiveness. combined, these results may be used as measures - standards of comparison - that can be studied further - for example to determine potential areas for future automation application.experiences reverse engineering manually','Hardware reverse engineering'
'extracting semantic metadata and its visualization','Hardware reverse engineering'
'modular exponentiation is a cornerstone operation to several public-key cryptosystems. it is performed using successive modular multiplications. this operation is time consuming for large operands, which is always the case in cryptography. for software or hardware fast cryptosystems, one needs thus to reduce the total number of modular multiplication required. existing methods attempt to reduce this number by partitioning the exponent in constant or variable size windows. in this paper, we propose a fast and compact reconfigurable hardware for computing modular exponentiation using the m-ary method. the cryptographic hardware is low-cost and concise and therefore can be embedded in almost all electronic devices that use encrypted data.fast reconfigurable hardware for the m-ary modular exponentiation','Hardware reverse engineering'
'this paper addresses the reconstruction of an object shape model from a set of digitized profiles, or scanlines. the reconstruction is approached in two main phases. firstly, a hierarchical simplification of the original data set is performed which is aimed at discarding irrelevant data and at providing different levels of detail of the data set. secondly, a shape signature is computed to characterize the shape of each profile and to reconstruct important feature lines. feature lines can be used to delimitate meaningful surface patches on the reconstructed mesh (segmentation). even if the proposed approach is presented in the specific context of reverse engineering, its application and usefulness is more general as it will be discussed for the geographical domain.feature lines reconstruction for reverse engineering','Hardware reverse engineering'
'the current ontology matchers usually expect the ontologies to be described with the same semantic granularity. in this work we address the issue of enriching a geographic ontology by inferring topological from its instances. moreover, we present a method for (re)constructing the ontology taxonomy by combining some reverse engineering techniques and the comparison with a reference ontology. this allows the homogenization of the semantic granularity of the ontologies to be matched in a later step. our proposal goes a little bit further and also enables the complete (re)definition of the ontology concepts in case of only the instances are available, by extracting all the needed information from the instances\' owl tags.geo-ontology enrichment through reverse engineering','Hardware reverse engineering'
'geometric dimensioning and tolerancing constitutes the dominant approach for design and manufacture of mechanical parts that control inevitable dimensional and geometric deviations within appropriate limits. position tolerance is a critical geometric tolerance very frequently used in industry. its designation requires size data in conjunction with appropriate datums and location coordinates for the position. in reverse engineering, where typically relevant engineering information does not exist, conventional, human-based, trial and error approach for the allocation of positional tolerances requires much effort and time and offers no guarantee for the generation of the best results. this is mainly due to the large number of possible data combinations and the applicable relationships that have to be developed and processed. a methodology that aims towards the systematic solution of this problem in reasonable computing time and provides realistic and industry approved results is presented, demonstrated and discussed in the paper.geometrical position tolerance assignment in reverse engineering','Hardware reverse engineering'
'getting your network ready for year 2000','Hardware reverse engineering'
'in this paper a reengineering approach is presented which uses graph transformations as formal background. the term reengineering describes any kind of activities concerned with the renewal and improvement of existing (software) applications. for this purpose the structure of an application has to be recovered to get information about relevant components and their relations. in this context graph rewriting systems are used to specify objects and relations and to determine possible changes in order to improve the system.graph-based reverse engineering and reengineering tools','Hardware reverse engineering'
'developing sustainable approaches to manufacture is a critical global problem concerned by people with a lack of resource on planet earth. an increasing interest in product take-back, product recovery and the redistribution has been concerned increasingly. such as green remanufacturing, in particular, has a strong history of product recovery operations in the automotive sector. it can be both more profitable and less harmful to the environment compare with conventional manufacturing. however, the application of green remanufacturing in structural machinery didn&#8217;t draw attention. this paper provides the background of green remanufacturing together with the way of product recovery reverse engineering theory. such as, study target, content, methods, design order, key technology and application of reverse engineering. this method can measure and evaluate wear of machine parts which have complicated curved surface quantitatively, accurately and visualization ally. reverse engineering methods and technologies play an important role in many software engineering tasks of product model. the significance of this research contributes to theory, to industry and to future research in the field.green remanufacturing engineering in structural machinery based on reverse engineering','Hardware reverse engineering'
'this assignment is designed for a computer networking course with some emphasis on sockets programming. in the assignment students are encouraged to reverse engineer the protocol for a server which maintains their grades for the assignment. the initial grade for each student is set to zero. the reverse engineering task requires application level protocol analysis and decomposition.hack this! reverse engineering a network server','Hardware reverse engineering'
'protecting application software from reverse engineering and piracy is key to ensuring the integrity of intellectual property and critical infrastructures. unorthodox protection strategies can help mitigate these types of attacks. such strategies must include random, dynamic protections to complicate the ease with which attackers can overcome standard approaches.hindering reverse engineering','Hardware reverse engineering'
'no abstract availableinterpreting reverse-engineering law','Hardware reverse engineering'
'investigating reverse engineering technologies for the cas program understanding project','Hardware reverse engineering'
'reverse engineering-the process of looking at lower levels of abstraction to understand higher levels-is not limited to decompiling programs, as many believe. it is an excellent way to pinpoint what you need to build an interface or change a system to reflect new business goals. but unraveling someone\'s code opens up a legal can of worms, and as reverse engineering becomes more popular, some people are taking time out from their y2k worries to say, &amp;ldquo;is this legal?&amp;rdquo; this in and of itself is not new-the law seems consistently to be an afterthought to solving technological problems-but in this case, the current preoccupation with reverse engineering may cause the two disciplines to finally stop and consider each other. the courts may realize that if we want to achieve a global electronic society, the law has to make it easier for systems to become interoperable, correct, and secure. software and system developers may realize that long-standing legal principles can actually work for them, not tie their hands and, more important, that they can influence the laws being made. unfortunately, we seem to be taking one step forward and two steps back in trying to make this happen. laws in four key regions-the us, eu, japan, and australia-reveal foundational inconsistencies in attitudes about reverse engineering. and recent us legislation, both enacted and proposed, is conflictedis reverse engineering always legal?','Hardware reverse engineering'
'legacy and future of data reverse engineering','Hardware reverse engineering'
'researchers that are involved in web site reverse engineering are often not aware of potential legal implications of using someone else\'s web site for experimentation. even if researchers are concerned with legal problems, there is little guidance available. this paper explores the legality of web site reverse engineering with the intent to raise awareness among researchers about this issue. the discussed legal issues encompass copyright, contract, and trespass law.legal concerns of web site reverse engineering','Hardware reverse engineering'
'reverse engineering of data has been performed in one form or another for over twenty-five years. in this paper we describe the lessons learned in data reverse engineering (dre) as contributed in a survey of data reverse engineers. interesting is the fact that some of the lessons learned tell us how we are doing in the process of initial database design as well as how difficult the dre process really is. it is hoped that from these lessons learned, we can assist in the suggestion of the next steps that are needed in the dre area and promote discussion among the dre community.lessons learned in data reverse engineering','Hardware reverse engineering'
'currently there exist no formal or structured method for analyzing malware, the implications of a hodgepodge method leads to inconsistencies and incomplete findings of analyzed malware. malware analysis and reverse engineering (mare) is a methodology that introduces a structured approach to malware analysis. a structured approach leads to a more consistent and complete analysis report. the mare methodology is designed to take a malware analyst from the moment of detecting malware to the end of fully grasping the analyzed malware\'s full functionality and capabilities. the mare methodology presents helpful tools and more importantly, a structured process flow to help analyst better understand how to analyze malware. the malware defense (m.d.) timeline maps out our ultimate research goal and presents where we are currently at in reaching that goal - eliminate the threat of malware. finally, the applicability of mare to the judicial system and teaching is discussed.malware analysis reverse engineering (mare) methodology & malware defense (m.d.) timeline','Hardware reverse engineering'
'managers of large software systems face enormous challenges when it comes to making informed project-related decisions. they require a high-level understanding of the entire system and in-depth information on selected components. unfortunately, many software systems are so complex and/or old that such information is not readily available. reverse engineering---the process of extracting system abstractions and design information from existing software systems---can provide some of this missing information. this paper outlines how a software system can benefit from reverse engineering, and describes how management personnel can use the information provided by this process as an aid in making informed decisions related to large software projects.management decision support through reverse engineering technology','Hardware reverse engineering'
'this paper proposes a reverse engineering-based method for mass measurement of bulk materials with an irregular shape. through repeated measurements, the multi-view point cloud data of a material stack surface can be obtained and the 3d surface model can be reconstructed after point cloud data registration. the volume of the stack can be calculated through image acquisition, data analysis and 3d reconstruction. after measuring the density of a material stack sample, the mass can be calculated through a combination of volume and density. an experiment using a rice stack shows that the measuring error of the system is lower than one percent. it is suitable for the mass measurement of grain stacks and other material stacks as well.mass measurement method of bulk materials based on reverse engineering','Hardware reverse engineering'
'although metrics have been used in the whole process of software engineering with the ostensible purpose of improving the quality of software, little thought appears to have been given to the use of metrics for reverse engineering. not surprisingly, there are remarkably few metrics tools available for this purpose. the aim of this study was to investigate the provision of metrics suitable for measuring aspects of software for the process of reverse engineering. it should be noted, however, that the use of these metrics will help not only in reverse engineering for software maintenance but also in re-engineering in software evolution.measuring abstractness for reverse engineering in a re-engineering tool','Hardware reverse engineering'
'an emerging approach to multi-device application development requires developers to build an abstract semantic model that is translated into specific implementations for web browsers, pdas, voice systems and other user interfaces. specifying abstract semantics can be difficult for designers accustomed to working with concrete screen-oriented layout. we present an approach to model recovery: inferring semantic models from existing applications, enabling developers to use familiar tools but still reap the benefits of multi-device deployment. we describe more, a system that converts the visual layout of html forms into a semantic model with explicit captions and logical grouping. we evaluate mores performance on forms from existing web applications, and demonstrate that in most cases the difference between the recovered model and a hand-authored model is under 5\%more for less','Hardware reverse engineering'
more,'Hardware reverse engineering'
'when planning to move a legacy style application to the cloud various challenges arise. the potential size and complexity of such a project might especially discourage small or medium companies trying to benefit from the advantages the cloud promises. in addition, the field they have to address is still young and very dynamic and related technologies are rapidly changing. based on on-going work in the context of the modaclouds eu project, this paper describes an evolutionary, iterative approach to accomplish the migration of an existing application to a cloud based environment. model based techniques are used to support the steps of this transition process by providing a baseline for the development of appropriate deployment architectures and the selection of suitable cloud providers. in addition they provide necessary abstractions in order to be less dependent on a specific technology stack or cloud provider. in order to show how we imagine the developed approach to be applied in practice we describe an existing traditional 3-tier application based on the meta-modeling platform adoxx and how it could be moved to the cloud from the perspective of a medium-sized software manufacturing company.moving an application to the cloud','Hardware reverse engineering'
'the transmission control protocol internet protocol (tcp/ip) [1] suite is widely used to interconnect computing facilities in modern network environments. however, there exist several security vulnerabilities in the tcp specification and additional weaknesses in a number of its implementations. these vulnerabilities may enable an intruder to \"attack\" tcp-based systems, allowing him/her to \"hijack\" a tcp connection or cause denial of service to legitimate users. we analyze tcp code via a \"reverse engineering\" technique called \"slicing\" to identify several of these vulnerabilities, especially those that are related to the tcp state-transition diagram. we discuss many of the paws present in the tcp implementation of many widely used operating systems, such as sunos 4.1.3, svr4, and ultrix 4.3. we describe the corresponding tcp attack \"signatures\"(including the well-known 1994 christmas day mitnick attack) and provide recommendations to improve the security state of a tcp-based system, e.g., incorporation of a \"timer escape route\" from every tcp state.network security via reverse engineering of tcp code','Hardware reverse engineering'
'no more software reverse engineering','Hardware reverse engineering'
'various research initiatives try to utilize the operational principles of organisms and brains to develop alternative, biologically inspired computing paradigms and artificial cognitive systems. this article reviews key features of the standard method applied to complexity in the cognitive and brain sciences, i.e. decompositional analysis or reverse engineering. the indisputable complexity of brain and mind raise the issue of whether they can be understood by applying the standard method. actually, recent findings in the experimental and theoretical fields, question central assumptions and hypotheses made for reverse engineering. using the modeling relation as analyzed by robert rosen, the scientific analysis method itself is made a subject of discussion. it is concluded that the fundamental assumption of cognitive science, i.e. complex cognitive systems can be analyzed, understood and duplicated by reverse engineering, must be abandoned. implications for investigations of organisms and behavior as well as for engineering artificial cognitive systems are discussed.on reverse engineering in the cognitive and brain sciences','Hardware reverse engineering'
'this is a position paper. we articulate the notion of reverse engineering of vendor databases and argue that this is a compelling technology that organizations should be routinely practicing. we take the perspective of a large organization that is a consumer of software (and not the perspective of a software vendor). we frequently reverse engineer vendor databases in our consulting practice and the technology has been enthusiastically received by industry.on reverse engineering of vendor databases','Hardware reverse engineering'
'parsing descriptive programming languages, such as extendable markup language (xml) and unified modeling language (uml), has been an active area of research. lam, ding, and liu claim that among all important phases of xml (e.g., parsing, access, modification, and serialization), parsing is the most time-consuming one. that motivates investigation of efficient parsing techniques with applications in many computer science areas, including reverse engineering.while there are many works on parsing xml, there is still room for research about uml parsing. uml parsing is still challenging because uml deals with graphical representations, such as class icons, class diagrams, sequence diagrams, state diagrams, and not text representations that are input for traditional parsers. reverse engineering is an important sub-area in software engineering and it basically means obtaining the uml specification from a source program described in an (object-oriented) programming language. this paper describes a new application of a non-traditional parallel parsing technique for a uniform petal language (upl) program. our bidirectional parser takes as input a upl program, that is, a text representation of a uml specification. the benefits of parsing a upl program are checking the correctness of the uml specification and obtaining the productions that lead to that upl program. consequently, the upl program can be generated from the associated set of productions. the set of productions associated to a correct upl program is a very condensed way to store it. keeping a specification as minimum as possible, but expressive, is an important concept of reverse engineering. we have implemented a bidirectional parser for context free languages, including upl, in java programming language, called parsing uniform petal language (pupal), and compare it with cup, a state-of-the-art parser for java designed by hudson, flannery, and ananian. the experimental results considered several upl programs and concluded that pupal performs better than cup.parallel parsing-based reverse engineering','Hardware reverse engineering'
'in a system development trajectory (or a life-cycle model) fostered by the \"traditional telecommunications\" community, it is accepted that implementations are derived from specifications that are assumed correct. these (formal) specifications can then serve as a reference for designing further realizations and for checking their correctness. in a fast development trajectory, characteristic of \"new telecommunications\", specifications are often fragmentary, informal, or never produced (or disclosed). we propose a reverse-engineering method for specification recovery that is based on passive testing of an implementation that is assumed correct. we illustrate the application of the method with a case study - recovery of the specification of the iax2 protocol for voip systems. it is argued that the method can be instrumental in bridging the gap between the approaches and methods used by both research communities.passive testing for reverse engineering','Hardware reverse engineering'
'my dissertation proposes a vision in which anybody can modify any interface of any application. realizing this vision is difficult because of the rigidity and fragmentation of current interfaces. specifically, rigidity makes it difficult or impossible for a designer to modify or customize existing interfaces. fragmentation results from the fact that people generally use many different applications built with a variety of toolkits. each is implemented differently, so it is difficult to consistently add new functionality. as a result, researchers are often limited to demonstrating new ideas in small testbeds, and practitioners often find it difficult to adopt and deploy ideas from the literature. in my dissertation, i propose transcending the rigidity and fragmentation of modern interfaces by building upon their single largest commonality: that they ultimately consist of pixels painted to a display. building from this universal representation, i propose pixel-based interpretation to enable modification of interfaces without their source code and independent of their underlying toolkit implementation.pixel-based reverse engineering of graphical interfaces','Hardware reverse engineering'
'reverse engineering systems hold great promise in aiding developers regain control over long-lived software projects whose architecture has been allowed to \"drift\". however, it is well known that these systems have relative strengths and weaknesses, and to date relatively little work has been done on integrating various subtools within other reverse engineering systems. the design of a common interchange format for data used by reverse engineering tools is therefore of critical importance.in this position paper, we describe some of our previous work with taxform (tuple attribute exchange format) [2,6], and in integrating various \"fact extractors\" into the pbs reverse engineering system. for example, we have recently created translation mechanisms that enable the acacia system\'s c and c++ extractors to be used within pbs, and we have used these mechanisms to create software architecture models of two large software systems: the mozilla web browser (2.2 mloc of c++ and c) and the vim text editor (150 kloc of c) [6]. we also describe our requirements for an exchange format for reverse engineering tools and some problems that must be resolved.practical data exchange for reverse engineering frameworks','Hardware reverse engineering'
'process reverse engineering for bpr','Hardware reverse engineering'
'program and interface slicing for reverse engineering','Hardware reverse engineering'
'program understanding in databases reverse engineering','Hardware reverse engineering'
'research on protocol reverse engineering is of great significance in network security applications. this paper firstly describes the existing protocol reverse engineering technologies, and then detailedly analyses their advantages and disadvantages. finally, a new approach of unknown protocol reverse extraction based on dynamorio is proposed, adopting both dynamic binary analysis and dynamic taint analysis techniques to extract protocol format from the data flow information revealed by the protocol application while processing the protocol data.protocol reverse engineering based on dynamorio','Hardware reverse engineering'
'in this paper, we will present the results of protocol reverse engineering for facebook protocol, which includes facebook message format and its exchange sequences. in order to learn message exchange sequences, we will rely on passive traffic monitoring approach which is based on packet capturing and behavior analysis. and facebook message formats are deduced from the source program code trace on a test facebook client that was developed for generating probing messages based on facebook open graph api. deduced facebook message formats are described in the form of constraint http message specification, and facebook message exchange sequences are depicted in sequence diagrams with objects of the facebook client and facebook servers.protocol reverse engineering to facebook messages','Hardware reverse engineering'
'the aim of reverse engineering is to convert an unstructured representation of a geometric object, emerging e.g. from laser scanners, into a natural, structured representation in the spirit of cad models, which is suitable for numerical computations. therefore we present a user-controlled, as isometric as possible parameterization technique which is able to prescribe geometric features of the input and produces high-quality quadmeshes with low distortion. starting with a coarse, user-prescribed layout this is achieved by using affine functions for the transition between non-orthogonal quadrangular charts of a global parameterization. the shape of each chart is optimized non-linearly for isometry of the underlying parameterization to produce meshes with low edge-length distortion. to provide full control over the meshing alignment the user can additionally tag an arbitrary subset of the layout edges which are guaranteed to be represented by enforcing them to lie on iso-lines of the parameterization but still allowing the global parameterization to relax in the direction of the iso-lines.quadrangular parameterization for reverse engineering','Hardware reverse engineering'
'qualitative reverse engineering','Hardware reverse engineering'
'we propose a new paradigm to query information about programs, namely query by outlines. this paradigm relies on an outlining model that conceptually describe units of code according to the computations they perform. outlines are automatically constructed by our system prisme for c and lisp programs. currently, both our model and our system are restricted to loops.qbo is a prototype tool that implements the query by outline paradigm. it proposes to browse the loops of a program directly through their outline, and allows to restrict these loops to browse with queries expressed as constraints on the outlines. thus it enables to answer questions such as \"where is this variable modified?\", \"where is this kind of computation performed?\", or \"are there many places where this computation is performed?\".in this paper, we sketch our outlining model, introduce qbo and argue that query by outline is a helpful paradigm to manage programs.query by outlines','Hardware reverse engineering'
'reportal is an existing web-based reverse engineering portal web site that provides access to a suite of reverse engineering and program comprehension tools via a web browser. this abstraction was intended to allow ease of system maintenance by adding and upgrading tools without involving the end user. however, the software tools and server technologies used became deprecated so quickly that it was not possible to take full advantage of the architectural vision. using a service-oriented architecture, we abstract the process flow of the system from the underlying tools, enabling a wizard-style method of adding services to the system, and facilitating ore \"hands-off\" maintenance. in this paper, we describe the challenges and benefits of this architectural migration.re-engineering a reverse engineering portal to a distributed soa','Hardware reverse engineering'
'multimedia applications are currently being deployed over dynamic and heterogeneous environments. they operate on devices with diverse computation capabilities in terms of processing, storage and display capabilities. in such environments, resource availability is subject to unexpected fluctuations (such as network bandwidth, battery life-time, cpu, etc.). this obliges applications to reconfigure their behavior accordingly in order to maintain an acceptable quality of service (qos). this paper addresses this aspect and describes a framework for a reconfiguration-based qos management in multimedia streaming applications. our objective is to relieve developers from dealing with the complexity of multimedia applications and their reconfiguration requirements. the framework provides a high-level specification language enabling an easy description of applications and their reconfiguration policies. a powerful component-based implementation is used for mapping specifications into run-time applications. several experiments in different scenarios are reported to argue our design choices, and to show the impact of reconfigurations on the perceived qos.reconfiguration-based qos management in multimedia streaming applications','Hardware reverse engineering'
'approaches to relational database reverse engineering often expect that the input has desirable characteristics and that it is complete; they also often fail to provide formal guarantees that their results are faithful to the initial input. both of these problems can be addressed by using an incremental approach based on a formally defined target model. the incremental approach we propose here quickly produces an initial model instance that is provably equivalent to the original relational database, which is assumed to be correct but may lack desirable characteristics and may be incomplete. the approach then proceeds incrementally using provably correct transformations. these incremental transformations allow for user interaction to provide needed information that may be missing or hard to obtain because the input lacks some desirable characteristics.relational database reverse engineering','Hardware reverse engineering'
'this paper proposes a general architecture for information systems (or data-centered applications) reverse engineering case environments. recovering the specifications of such applications requires recovering first those of their data, i.e. database reverse engineering (dbre). first, the paper describes a generic dms-independent dbre methodology, then it analyses the main characteristics of dbre activities in order to collect a set of minimum or desired requirements. finally, it describes the main features of an operational case tool developed according to these requirements. this study and these developments are being carried out as part of the db-main and db-process projects.requirements for information system reverse engineering support','Hardware reverse engineering'
'research challenges in the reverse engineering community','Hardware reverse engineering'
'in this paper, analysis is presented aiming at the problem of plastic operation which is usually finished depending on doctor\'s experience only. based on it, reverse engineering of freeform surface is studied and applied for plastic operation. the sufferer\'s freeform surface is measured and reconstructed to digital model. and the surgery result could be emulated beforehand. breast plastic is chosen as example to test the technical application. the sufferer could consult the surgery operation with doctor by using reverse engineering. and the sufferer\'s desire could be satisfied and the successful probability of surgery be improved.research on reverse engineering for plastic operation','Hardware reverse engineering'
'the unified modeling language (uml) provides a graphical notation to express the design of object-oriented software systems and has become the de facto industry standard for software design. however uml lacks precise semantics and is semi-formal. formal specification languages are intended to provide precise and complete models for proposed software systems. many researchers have done a lot of work in translating uml models into formal models to validate uml models. but in this paper, we discuss the reverse engineering problem, that is, when the formal models are validated and corrected, how to reverse them to uml models. we think this problem is more meaningful for software engineer. this paper presents a method that translates formal models into uml models by xmi and its schema, and then testifies the feasibility and correctness of the reverse method by unifying theories of programming (utp)research on reverse engineering from formal models to uml models','Hardware reverse engineering'
'in this paper, we present a framework for reverse engineeringallowing the integration and interaction of differentanalysis and visualization tools. the framework architecturethat we propose uses a dynamic type system to guaranteethe proper exchange of data between the tools and aset of wrapper classes to handle their communication. thisallows for an easy and secure integration of tools that haveoriginally not been designed to work together. in this sense,existing tools can be (re-)used and integrated. as a proofof concept we also present our own instantiation of the proposedframework architecture.reuse in reverse engineering','Hardware reverse engineering'
'reverse data engineering technology for visual database design','Hardware reverse engineering'
'an effort to recover an entity-relationship model from an existing medical database is described. this effort examines the applicability of a variety of existing reverse engineering techniques. problems and limitations encountered with these techniques are discussed. an alternative approach that avoids these limitations is presented, together with examples from the medical database that illustrate the applicability of this approach. based on this experience, suggestions are made for future research directions related to reverse engineering of relational databases.reverse engineering a medical database','Hardware reverse engineering'
'reverse engineering ada into hood','Hardware reverse engineering'
'the key to applying computer-aided software engineering to the maintenance and enhancement of existing systems lies in applying reverse-engineering approaches. however, there is considerable confusion over the terminology used in both technical and marketplace discussions. the authors define and relate six terms: forward engineering, reverse engineering, redocumentation, design recovery, restructuring, and reengineering. the objective is not to create new terms but to rationalize the terms already in use. the resulting definitions apply to the underlying engineering processes, regardless of the degree of automation applied.reverse engineering and design recovery','Hardware reverse engineering'
'reverse engineering and reengineering','Hardware reverse engineering'
'since the 1980s, software maintenance started to attract attention. some progress has been made in using formal methods on software maintenance, especially on reverse engineering. this paper attempts to summarise some major advances in this area over the last one and a half decades. firstly, we introduce program transformation techniques for software development and review the techniques used for software maintenance. we then describe a method for reverse engineering and reusing cobol programs using program transformations. finally, we suggest a future investigation direction for this work.reverse engineering and reusing cobol programs','Hardware reverse engineering'
'to facilitate research in the field of reverse engineering and system renovation we have compiled an annotated bibliography. we put the contributions not only in alphabetical order but also grouped by topic so that readers focusing on a certain topic can read their annotations in the alphabetical listing. we also compiled an annotated list of pointers to information about reverse engineering and system renovation that can be reached via internet. for the sake of ease we also incorporated a brief introduction to the field of reverse engineering.reverse engineering and system renovation&#8212;an annotated bibliography','Hardware reverse engineering'
'recent actions against reverse engineering threaten to remove this valuable practice from the computing profession\'s toolkit. unless we develop better communication between the legal and computing professions, we will continue to suffer the consequences of legislation such as the usa\'s digital millennium copyright act (dmca). by making the distribution of research results more difficult, such legislation stifles the sharing of ideas, causing an uncertainty that can lead to less innovation and fewer benefits to societyreverse engineering and the computing profession','Hardware reverse engineering'
'reverse engineering the variability of an existing system is a challenging activity. the architect knowledge is essential to identify variation points and explicit constraints between features, for instance in feature models (fms), but the manual creation of fms is both time-consuming and error-prone. on a large scale, it is very difficult for an architect to guarantee that the resulting fm is consistent with the architecture it is associated with. in this paper, we present a comprehensive, tool supported process for reverse engineering architectural fms. we develop automated techniques to extract and combine different variability descriptions of an architecture. then, alignment and reasoning techniques are applied to integrate the architect knowledge and reinforce the extracted fm. we illustrate the process when applied to a representative software system and we report on our experience in this context.reverse engineering architectural feature models','Hardware reverse engineering'
'abstract: this is an applied research report on the exploitation of reverse engineering technology acquired through the esprit docket project. it describes three efforts to implement this technology in business applications at the bremer warehouse company, the technical university of manchester, and the union bank of switzerland. in all three instances, reverse engineering was used to extract knowledge from existing source code and database structures, in order to build a bridge from the current operating software to a planned case environment. in all three instances, automated tools developed within the scope of the docket project were used. the report shows that much remains to be done to really fulfil the requirements of industry.reverse engineering as a bridge to case','Hardware reverse engineering'
'the undergraduate program in the materials science and engineering department at the university of florida requires its junior students to take the course &#8220;analysis of the structure of materials&#8221;. this course provides students the opportunity to disassemble an engineered product, characterize and analyze the structure of its components, and correlate the structures with properties and processing addressing the criteria most likely used for materials selection by the manufacturers. in addition to the traditional reverse engineering approach, the students must select at least one component and re-create the life cycle of the material/component. the analysis requires collection and analysis of data from emissions, and waste generated from each step of the mining, processing, manufacturing, fabrication, distribution, use, and typical disposal of the product. the students are required to estimate product impacts on the waste stream, environment and society. the final presentation must also include recommendations from the students to create the same product in a more environmentally responsible way.reverse engineering as an educational tool for sustainability','Hardware reverse engineering'
'reverse engineering based on metrics and program visualization','Hardware reverse engineering'
'this paper presents some preliminary results on applying information retrieval and knowledge-mining techniques to reverse engineering of legacy systems. in order to support a dynamic environment, we take an approach of integrating lightweight tools. instead of forcing q user to use a fixed environment, our approach provides a basic information repository, which manages information extracted from the documentation and source code. the system stores this information in a graph structure; it supports navigation through the repository, and modification of its structure and annotation. preliminary evaluation of the proposed approach on the small-size software system is encouraging.reverse engineering by mining dynamic repositories','Hardware reverse engineering'
'captchas are automated turing tests used to determine if the end-user is human and not an automated program. users are asked to read and answer visual captchas, which often appear as bitmaps of text characters, in order to gain access to a low-cost resource such as webmail or a blog. captchas are generated by software and the structure of a captcha gives hints to its implementation. thus due to these properties of image processing and image composition, the process that creates captchas can often be reverse engineered. once the implementation strategy of a family of captchas has been reverse engineered the captcha instances may be solved automatically by leveraging weaknesses in the creation process or by comparing a captcha\'s output against itself. in this paper, we present a case study where we reverse engineer and solve real-world captchas using simple image processing techniques such as bitmap comparison, thresholding, fill-flood segmentation, dilation, and erosion. we present black-box and white-box methodologies for reverse engineering and solving captchas. as well we provide an open source toolkit for solving captchas that we have used with a success rates of 99, 95, 61, 30\%, and 27\% on hundreds of captchas from five real-world examples.reverse engineering captchas','Hardware reverse engineering'
'we study the following problem: given a database d with schema g and an output table out, compute a join query q that generates out from d. a simpler variant allows q to return a superset of out. this problem has numerous applications, both by itself, and as a building block for other problems. related prior work imposes conditions on the structure of q which are not always consistent with the application, but simplify computation. we discuss several natural sql queries that do not satisfy these conditions and cannot be discovered by prior work. in this paper, we propose an efficient algorithm that discovers queries with arbitrary join graphs. a crucial insight is that any graph can be characterized by the combination of a simple structure, called a star, and a series of merge steps over the star. the merge steps define a lattice over graphs derived from the same star. this allows us to explore the set of candidate solutions in a principled way and quickly prune out a large number of infeasible graphs. we also design several optimizations that significantly reduce the running time. finally, we conduct an extensive experimental study over a benchmark database and show that our approach is scalable and accurately discovers complex join queries.reverse engineering complex join queries','Hardware reverse engineering'
'legacy applications are still widely spread. if a need to change deployment or update its functionality arises, it becomes difficult to estimate the performance impact of such modifications due to absence of corresponding models. in this paper, we present an extendable integrated environment based on eclipse developed in the scope of the q-impress project for reverse engineering of legacy applications (in c/c++/java). the q-impress project aims at modeling quality attributes (performance, reliability, maintainability) at an architectural level and allows for choosing the most suitable variant for implementation of a desired modification. the main contributions of the project include i) a high integration of all steps of the entire process into a single tool, a beta version of which has been already successfully tested on a case study, ii) integration of multiple research approaches to performance modeling, and iii) an extendable underlying meta-model for different quality dimensions.reverse engineering component models for quality predictions','Hardware reverse engineering'
'the definition and method of reverse engineering are introduced. the application of pro/e software in re and model building are also discussed. the application of re is described with example.reverse engineering design','Hardware reverse engineering'
'integrated circuits (ics) are now designed and fabricated in a globalized multi-vendor environment making them vulnerable to malicious design changes, the insertion of hardware trojans/malware and intellectual property (ip) theft. algorithmic reverse engineering of digital circuits can mitigate these concerns by enabling analysts to detect malicious hardware, verify the integrity of ics and detect ip violations. in this paper, we present a set of algorithms for the reverse engineering of digital circuits starting from an unstructured netlist and resulting in a high-level netlist with components such as register files, counters, adders and subtracters. our techniques require no manual intervention and experiments show that they determine the functionality of more than 51\% and up to 93\% of the gates in each of the practical test circuits that we examine.reverse engineering digital circuits using functional analysis','Hardware reverse engineering'
'semantic web service (sws) enrich web service technology with formal, ontology-based descriptions of service functionalities and capabilities at the semantic level, thus enabling semantic-based discovery, composition, dynamic binding and orchestration.several solutions were proposed for the specification of the semantic web services, in this paper, we propose a reverse engineering based approach to specify web service according to the web service modeling ontology (wsmo).reverse engineering existing web service applications','Hardware reverse engineering'
'successful software evolves, more and more commonly, from a single system to a set of system variants tailored to meet the similiar and yet different functionality required by the distinct clients and users. software product line engineering (sple) is a software development paradigm that has proven effective for coping with this scenario. at the core of sple is variability modeling which employs feature models (fms) as the de facto standard to represent the combinations of features that distinguish the systems variants. reverse engineering fms consist in constructing a feature model from a set of products descriptions. this research area is becoming increasingly active within the sple community, where the problem has been addressed with different perspectives and approaches ranging from analysis of configuration scripts, use of propositional logic or natural language techniques, to ad hoc algorithms. in this paper, we explore the feasibility of using evolutionary algorithms (eas) to synthesize fms from the feature sets that describe the system variants. we analyzed 59 representative case studies of different characteristics and complexity. our exploratory study found that fms that denote proper supersets of the desired feature sets can be obtained with a small number of generations. however, reducing the differences between these two sets with an effective and scalable fitness function remains an open question. we believe that this work is a first step towards leveraging the extensive wealth of search-based software engineering techniques to address this and other variability management challenges.reverse engineering feature models with evolutionary algorithms','Hardware reverse engineering'
'feature models describe the common and variable characteristics of a product line. their advantages are well recognized in product line methods. unfortunately, creating a feature model for an existing project is time-consuming and requires substantial effort from a modeler. we present procedures for reverse engineering feature models based on a crucial heuristic for identifying parents - the major challenge of this task. we also automatically recover constructs such as feature groups, mandatory features, and implies/excludes edges. we evaluate the technique on two large-scale software product lines with existing reference feature models--the linux and ecos kernels--and freebsd, a project without a feature model. our heuristic is effective across all three projects by ranking the correct parent among the top results for a vast majority of features. the procedures effectively reduce the information a modeler has to consider from thousands of choices to typically five or less.reverse engineering feature models','Hardware reverse engineering'
'despite the advancement of xml, the majority of documents on the web is still marked up with html for visual rendering purposes only, thus building a huge amount of \"legacy\" data. in order to facilitate querying web based data in a way more efficient and effective than just keyword based retrieval, enriching such web documents with both structure and semantics is necessary.this paper describes a novel approach to the integration of topic specific html documents into a repository of xml documents. in particular, we describe how topic specific html documents are transformed into xml documents. the proposed document transformation and semantic element tagging process utilizes document restructuring rules and minimum information about the topic in form of concepts. for the resulting xml documents, a majority schema is derived that describes common structures among the documents in the form of a dtd.we explore and discuss different techniques and rules for document conversion and majority schema discovery. we finally demonstrate the feasibility and effectiveness of our approach by applying it to a set of resume html documents gathered by a web crawler.reverse engineering for web data','Hardware reverse engineering'
'we describe a new reverse engineering technology to generate a narrative specification used by real-world maintainers which facilitates the understanding of business procedures in existing cobol programs. it defines business process logic and how to recognize it in a program. it also defines how to generate narrative specifications based on this process logic. we also discuss a specification-based maintenance support system; proof-correction marking and parallel scrolling. evaluations were made in collaboration with more than six software maintainers by inspecting 500 generated specifications from real programs of financial firms. the evaluation suggests that generated specifications are helpful in clarifying the program in terms of business procedures and in locating statements to be changed during software maintenance.reverse engineering from cobol to narrative specification','Hardware reverse engineering'
'segmenting garments from humanoid meshes or point clouds is a challenging problem with applications in the textile industry and in model based motion capturing. in this work we present a physically based template-matching technique for the automatic extraction of garment dimensions from 3d meshes or point clouds of dressed humans. the successfull identification of garment dimensions also allows the semantic segmentation of the mesh into naked and dressed parts.reverse engineering garments','Hardware reverse engineering'
'as a result of the ubiquity and popularity of smart phones, the number of third party mobile applications is explosively growing. with the increasing demands of users for new dependable applications, novel software engineering techniques and tools geared towards the mobile platform are required to support developers in their program comprehension and analysis tasks. in this paper, we propose a reverse engineering technique that automatically (1) hooks into, dynamically runs, and analyzes a given ios mobile application, (2) exercises its user interface to cover the interaction state space and extracts information about the runtime behaviour, and (3) generates a state model of the given application, capturing the user interface states and transitions between them. our technique is implemented in a tool called icrawler. to evaluate our technique, we have conducted a case study using six open-source iphone applications. the results indicate that icrawler is capable of automatically detecting the unique states and generating a correct model of a given mobile application.reverse engineering ios mobile applications','Hardware reverse engineering'
'reverse engineering is focused on the challenging task of understanding legacy program code without having suitable documentation. using a transformational forward engineering perspective, we gain the insight that much of this difficulty is caused by design decisions made during system development. such decisions ``hide\'\' the program functionality and performance requirements in the final system by applying repeated refinements through layers of abstraction, and information--spreading optimizations, both of which change representations and force single program entities to serve multiple purposes. to be able to reverse engineer, we essentially have to reverse these design decisions. following the transformational approach we can use the transformations of a forward engineering methodology and apply them \"backwards\" to reverse engineer code to a more abstract specification. since most existing code was not generated by transformational synthesis, this produces a plausible formal transformational design rather than the original authors\' actual design. a by-product of the transformational reverse engineering process is a design database for the program that then can be maintained to minimize the need for further reverse engineering during the remaining lifetime of the system. a consequence of this perspective is the belief that plan recognition methods are not sufficient for reverse engineering. as an example, a small fragment of a real--time operating system is reverse-engineered using this approach.reverse engineering is reverse forward engineering','Hardware reverse engineering'
'legacy systems constitute valuable assets to the organizations that own them. however, due to the development of newer and faster hardware platforms and the invention of novel interface styles, there is a great demand for their migration to new platforms. in this paper, we present a method for reverse engineering the system interface that consists of two tasks. based on traces of the users interaction with the system, the ``interface mapping\'\' task constructs a ``map\'\' of the system interface, in terms of the individual system screens and the transitions between them. the subsequent ``task and domain modeling\'\' task uses the interface map and task-specific traces to construct an abstract model of a user\'s task as an information exchange plan. the task model specifies the screen transition diagram that the user has to traverse in order to accomplish the task in question, and the flow of information that the user exchanges with the system at each screen. this task model is later used as the basis for specifying a new graphical user interface tailored to the task in question.reverse engineering legacy interfaces','Hardware reverse engineering'
'abstract: we demonstrate how the data management techniques known as on-line analytical processing, or olap, can be used to enhance the sophistication and range of software reverse engineering tools. this is the first comprehensive examination of the similarities and differences in these tasks both in how olap techniques meet (or fail to meet) the needs of reverse engineering and in how reverse engineering can be recast using data analysis. to permit the seamless integration of these technologies, we extend a multidimensional data model to manage dynamically changing dimensions (over which data can be aggregated). we use a case study of the apache web server to show how our solutions permit an integrated view of data ranging from low level program analysis information to abstract, aggregate information. these high-level abstractions may be provided either by humans (perhaps using a visualization tool) or directly from reverse engineering tools or data mining techniques.reverse engineering meets data analysis','Hardware reverse engineering'
'for various reasons, many people suffer from the bone defects, and how to solve this problem has become a hot topic internationally in the field of tissue engineering. nowadays, the doctor diagnoses the state of the bone defect illness mainly depending on observing the ct images of patient; it mainly depends on the technique and experience of the doctor. in order to solve this problem, this paper focuses on using the reverse engineering technology to make the skull-repair-technology efficient. during this process, we can construct the repair-sheet that fits the skull surface and then mold it. this method can shorten the time of operation, reduce the risk of operation.reverse engineering methodology in broken skull surface model reconstruction','Hardware reverse engineering'
'analysis of molecular interaction networks is pervasive in systems biology. this research relies almost entirely on graphs for modeling interactions. however, edges in graphs cannot represent multiway interactions among molecules, which occur very often within cells. hypergraphs may be better representations for networks having such interactions, since hyperedges can naturally represent relationships among multiple molecules. here, we propose using hypergraphs to capture the uncertainty inherent in reverse engineering gene-gene networks. some subsets of nodes may induce highly varying subgraphs across an ensemble of networks inferred by a reverse engineering algorithm. we provide a novel formulation of hyperedges to capture this uncertainty in network topology. we propose a clustering-based approach to discover hyperedges. we show that our approach can recover hyperedges planted in synthetic data sets with high precision and recall, even for moderate amount of noise. we apply our techniques to a data set of pathways inferred from genetic interaction data in s. cerevisiae related to the unfolded protein response. our approach discovers several hyperedges that capture the uncertain connectivity of genes in relevant protein complexes, suggesting that further experiments may be required to precisely discern their interaction patterns. we also show that these complexes are not discovered by an algorithm that computes frequent and dense subgraphs.reverse engineering molecular hypergraphs','Hardware reverse engineering'
'a significant part of the modern software systems are designed and implemented as object-oriented distributed applications, addressing the needs of a globally-connected society. while they can be analyzed focusing only on their object-oriented nature, their understanding and quality assessment require very specific, technology-dependent analysis approaches. this doctoral dissertation describes a methodology for understanding object-oriented distributed systems using a process of reverse engineering driven by the assessment of their technological and domain-specific particularities. the approach provides both system-wide and class-level characterizations, capturing the architectural traits of the systems, and assessing the impact of the distribution-aware features throughout the application. the methodology describes a mostly-automated analysis process fully supported by a tools infrastructure, providing means for detailed understanding of the distribution-related traits and including basic support for the potentially consequent system restructuring.reverse engineering object-oriented distributed systems','Hardware reverse engineering'
'in this paper, the workflow to reverse engineer the bamboo-paper-binding frame of the chinese folk handicraft - southern style lion head will be described. the problems encountered during the reverse engineering process are summarized. finally how to apply knowledge of structural relationship to improve the reverse engineering problems is proposed.reverse engineering of a bamboo-net handicraft','Hardware reverse engineering'
'the reap project at interglossa is developing tools to support maintenance and reverse engineering of assembly language programs, concentrating on well-engineered hand-coded programs. abstraction of assembly programs takes place in the context of a selected `engineering model\' which includes the definition of the instruction set semantics but also constraints on the programs similar to those found in abis. the process of translation takes the form of a large-scale inductive demonstration that the program meets the constraints of the `engineering model\' as the translated abstraction is produced. an engineer\'s interface makes this manifest to the engineer supervising the translation. this approach can in principle handle programs whose models include a disciplined use of code self-modification or dynamic register bank switching. as intermediate language for the major analyses involved we use a representation based on the xandf x/open standard originating from the uk defence research agency. xandf is a standard for architecture neutral program representation which will permit support for analyses of portability. concurrency is not yet covered but recent advances show how xandf can be extended to encompass concurrency and distribution. we illustrate the effectiveness of the tools with examples taken from live intel 8051 and zilog z80 systems.reverse engineering of assembler programs','Hardware reverse engineering'
'reverse engineering of gene regulatory networks (grns) is one of the most challenging tasks in systems biology and bioinformatics. it aims at revealing network topologies and regulation relationships between components from biological data. owing to the development of biotechnologies, various types of biological data are collected from experiments. with the availability of these data, many methods have been developed to infer grns. this paper firstly provides an introduction to the basic biological background and the general idea of grn inferences. then, different methods are surveyed from two aspects: models that those methods are based on and inference algorithms that those methods use. the advantages and disadvantages of these models and algorithms are discussed. &#x00a9; 2012 wiley periodicals, inc. &#169; 2012 wiley periodicals, inc.reverse engineering of gene regulatory networks from biological data','Hardware reverse engineering'
'the discovery of gene regulatory networks is a major goal in the field of bioinformatics due to their relevance, for instance, in the development of new drugs and medical treatments. the idea underneath this task is to recover gene interactions in a global and simple way, identifying the most significant connections and thereby generating a model to depict the mechanisms and dynamics of gene expression and regulation. in the present paper we tackle this challenge by applying a genetic algorithm to boolean-based networks whose structures are inferred through the optimization of a tsallis entropy function, which has been already successfully used in the inference of gene networks with other search schemes. additionally, wisdom of crowds is applied to create a consensus network from the information contained within the last generation of the genetic algorithm. results show that the proposed method is a promising approach and that the combination of criterion function based on tsallis entropy with an heuristic search such as genetic algorithms yields networks up to 50\% more accurate when compared to other boolean-based approaches.reverse engineering of grns','Hardware reverse engineering'
'web applications have gained significant popularity. relevant technologies, however, are to a great extent still immature and in constant evolution. this means many current applications are subject to constant change to keep up with the technology, leading to a degradation of application quality, both from an implementation and a usage perspective. in this context, tools that enable reasoning about the quality of the application from its source code can have a significant role. this paper reports on our preliminary work on reverse engineering the user interface layer of web applications directly from source code. its applicability to gwt is described through two examples.reverse engineering of gwt applications','Hardware reverse engineering'
'in mobile applications, the application lifecycle consists of the process-related states (e.g. suspended, ready, running) and the transitions between them. a faulty or insufficient implementation of the mobile application lifecycle can be the source of many problematic faults, e.g. loss of data. thus for a software developer, understanding and mastering the mobile application lifecycle is essential for high quality software. in our work with various mobile platforms, we found that the given lifecycle models and corresponding documentation are often inconsistent, incomplete and incorrect. in this paper we present a way to reverse-engineer application lifecycles of mobile platforms by testing. within a case study we apply the presented concept to three mobile platforms: android, ios and java me. we further show how developers of mobile applications can use our results to get correct lifecycle models for these platforms.reverse engineering of mobile application lifecycles','Hardware reverse engineering'
'during software evolution, programmers devote most of their effort to the understanding of the structure and behavior of the system. for object-oriented code, this might be particularly hard, when multiple, scattered objects contribute to the same function. design views offer an invaluable help, but they are often not aligned with the code, when they are not missing at all.this tutorial describes some of the most advanced techniques that can be employed to reverse engineer several design views from the source code. the recovered diagrams, represented in uml (unified modeling language), include class, object, interaction (collaboration and sequence), state and package diagrams. a unifying static code analysis framework used by most of the involved algorithms is presented at the beginning of the tutorial. a single running example is referred all over the presentation. trade-offs (e.g., static vs. dynamic analysis), limitations and expected benefits are also discussed.reverse engineering of object oriented code','Hardware reverse engineering'
'communication protocols determine how network components interact with each other. therefore, the ability to derive a specification of a protocol can be useful in various contexts, such as to support deeper black-box testing or effective defense mechanisms. unfortunately, it is often hard to obtain the specification because systems implement closed (i.e., undocumented) protocols, or because a time consuming translation has to be performed, from the textual description of the protocol to a format readable by the tools. to address these issues, we propose a new methodology to automatically infer a specification of a protocol from network traces, which generates automata for the protocol language and state machine. since our solution only resorts to interaction samples of the protocol, it is well-suited to uncover the message formats and protocol states of closed protocols and also to automate most of the process of specifying open protocols. the approach was implemented in a tool and experimentally evaluated with publicly available ftp traces. our results show that the inferred specification is a good approximation of the reference specification, exhibiting a high level of precision and recall.reverse engineering of protocols from network traces','Hardware reverse engineering'
'reverse engineering of relational database applications','Hardware reverse engineering'
'reverse engineering of relational database physical schema','Hardware reverse engineering'
'reverse engineering of software','Hardware reverse engineering'
'tophat is a fast splice junction mapper for next generation sequencing analysis, a technology for functional genomic research. next generation sequencing technology allows more accurate analysis increasing data to elaborate, this opens to new challenges in terms of development of tools and computational infrastructures. we present a solution that cover aspects both software and hardware, the first one, after a reverse engineering phase, provides an improvement of algorithm of tophat making it parallelizable, the second aspect is an implementation of an hybrid infrastructure: grid and virtual grid computing. moreover the system allows to have a multi sample environment and is able to process automatically totally transparent to user.reverse engineering of tophat','Hardware reverse engineering'
'in the present work, we outline a reverse engineering approach for uml specifications in form of class diagrams from java bytecode. after a brief introduction to the subject we present some analyses which go beyond mere enumeration of methods and fields. a glance onto some related work shows that there seems to be no pat solution for the reverse engineering of the more difficult class diagram elements. we sketch our method of determining association multiplicities, being, in a sense, representative of our approach in general: \"intuitive\" analyses, producing results that can be understood by a programmer when inspecting the source code of a given class. finally, we introduce a tool that implements this work and we apply it onto a small real life example, discussing the results it gave.reverse engineering of uml specifications from java programs','Hardware reverse engineering'
'we propose a novel technique for recovering certain elements of the uml model of a software system. these include relationships between use cases as well as class roles in collaborations that realize each use case, identifying common functionality and thus establishing a hierarchical view of the model. the technique is based on dynamic analysis of the system for the selected test cases that cover relevant use cases. the theory of formal concept analysis is applied to obtain classification of model elements, obtained by a static analysis of code, in terms of use case realitations.reverse engineering of use case realizations in uml','Hardware reverse engineering'
'despite growth in the popularity of desktop systems, web applications, and mobile computing, mainframe systems remain the dominant force in large-scale enterprise computing. although they\'re sometimes referred to as \"the dinosaurs of computing,\" even mainframe systems must adapt to changing circumstances to survive. although reverse-engineering and reengineering techniques can help identify and achieve these adaptations, current techniques are mainly geared mainly toward more modern environments, languages, and platforms. it remains to be seen whether successful techniques can be easily transferred to a mainframe context. this article reports on the application of two proven reverse-engineering techniques (software visualization and feature location) in the context of mainframe systems. the authors conclude that these techniques remain viable but become very labor intensive when implemented on a per-project basis.reverse engineering on the mainframe','Hardware reverse engineering'
'this is a questionnaire on program understanding and reverse engineering. it may be filled out manually or on-line. the results of the questionnaire will be used to guide the research of the two authors, both of whom are ph.d. students working in this area. copies of the resulting report will be mailed to all who participate, and a summary of the results will be published in an appropriate forum.reverse engineering questionnaire','Hardware reverse engineering'
'reverse engineering requirements for process-control software','Hardware reverse engineering'
'an important application of binary-level reverse engineering is in reconstructing the internal logic of computer malware. most malware code is distributed in encrypted (or \"packed\") form, at runtime, an unpacker routine transforms this to the original executable form of the code, which is then executed. most of the existing work on analysis of such programs focuses on detecting unpacking and extracting the unpacked code. however, this does not shed any light on the functionality of different portions of the code so obtained, and in particular does not distinguish between code that performs unpacking and code that does not, identifying such functionality can be helpful for reverse engineering the code. this paper describes a technique for identifying and extracting the unpacker code in a self-modifying program. our algorithm uses offline analysis of a dynamic instruction trace both to identify the point(s) where unpacking occurs and to identify and extract the corresponding unpacker code.reverse engineering self-modifying code','Hardware reverse engineering'
'this poster presents our experience incorporating reverse engineering techniques on software engineering education to enhance the productivity of students\' team projects. in this approach, the students were provided with an existing working software by the instructor. they were asked to evaluate the software, use the knowledge from the software\'s behavior and the application domain to define the task scenarios that reflect the interaction pattern and the services that the system provides to its environment. consequently, the extracted patterns were used to develop the architectural design of the software system and rebuild a modified version of the software.reverse engineering technique to enhance software engineering education','Hardware reverse engineering'
'web systems evolved in the last years starting from static websites to web applications, up to ajax-based rich internet applications (rias). reverse engineering techniques followed the same evolution, too. the authors and many other wse contributors proposed a lot of innovative and effective ideas providing important advances in the reverse engineering field. in this paper, we will show the historical evolution of reverse engineering approaches for web systems with particular attention to the ones presented in the wse events.reverse engineering techniques','Hardware reverse engineering'
'reverse engineering the bazaar','Hardware reverse engineering'
'reverse engineering the brain','Hardware reverse engineering'
'if avatar is the bright future of cinema, a great deal of that dazzle is going to come from weta digital, the firm that created most of the movie\'s oscar-winning visual effects.reverse engineering the human face','Hardware reverse engineering'
'to provide insight into internet operation and performance, recent efforts have measured various aspects of the internet, developing and improving measurement tools in the process. in this paper, we argue that these independent advances present the community with a startling opportunity: the collaborative reverse-engineering of the internet. by this, we mean annotating a map of the internet with properties such as: client populations, features and workloads; network ownership, capacity, connectivity, geography and routing policies; patterns of loss, congestion, failure and growth; and so forth. this combination of properties it greater than the sum of its parts, and exposes the attributes of network design easily overlooked by simpler, uncorrelated models. we argue that reverse engineering the internet is feasible based on continuing improvements in measurement techniques, the potential to infer new properties from external measurements, and an accounting of the resources required to complete the process.reverse engineering the internet','Hardware reverse engineering'
'the vision system is perhaps the most well understood part of the neocortex. the input from the eyes consists of a set of images made up of pixels that are densely packed in the fovea and less so in the periphery. each pixel is represented by a vector of attributes such as color, brightness, spatial and temporal derivatives. pixels from each eye are registered in the lateral geniculate nucleus and projected to the cortex where they are processed by a hierarchy of array processors that detect features and patterns and compute their attributes, state, and relationships. these array processors consist of cortical computational units (ccus) made up of cortical hypercolumns and their underlying thalamic and other subcortical nuclei. each ccu is capable of performing complex computational functions and communicating with other ccus at the same and higher and lower levels. the entire visual processing hierarchy generates a rich, colorful, dynamic internal representation that is consciously perceived to be external reality. it is suggested that it may be possible to reverse engineer the human vision system in the near future [1].reverse engineering the vision system','Hardware reverse engineering'
'the growth of the world wide web and the accelerated development of web sites and associated web technologies has resulted in a variety of maintenance problems. the maintenance problems associated with web sites and the www are examined. it is argued that currently web sites and the www lack both data abstractions and structures that could facilitate maintenance. a system to analyse existing web sites and extract duplicated content and style is described here. in designing the system, existing reverse engineering techniques have been applied, and a case for further application of these techniques is made in order to prepare sites for their inevitable evolution in future.reverse engineering to achieve maintainable www sites','Hardware reverse engineering'
'two recent federal appellate decisions, one involving nintendo and atari, the other involving sega and accolade, are discussed. in both cases, the appeals courts endorsed reverse engineering in principle, where it was necessary to gain access to the ideas of a copyrighted computer program or was otherwise supported by a legitimate purpose. but just what purposes will legitimize reverse engineering under copyright law remains murky, once one goes beyond the factual context of the specific cases before both courts-a lock-and-key security system (interface) designed to keep unauthorized software out of a hardware platformreverse engineering to develope competitive software','Hardware reverse engineering'
'sales configurators are widespread web applications. although such applications have specific common characteristics (e.g., they manage options governed by constraints, they enforce a configuration process.), they are usually developed in an unspecific way, that is, like any other web application. proceeding this way leads to configurators that are sub optimal in reliability, efficiency and maintainability. this phd thesis is concerned with the reverse-engineering of web sales configurators. it aims to develop a consistent set of methods, languages and tools to semi-automatically extract configuration-specific data from a web configurator. such data is stored in formal models (e.g., variability models, process models). these models can later be used for verification purposes (e.g., checking the completeness and correctness of the configuration constraints) as well as input for generative techniques (e.g., to re-engineer legacy configurators). more precisely, our two main research questions are: (1) how to extract variability data from the unstructured or semi-structured web pages of a sales configurator? (2) how to extract such data from the dynamic content created when the configurator is executing? the accuracy of the extracted data and the scalability of the delivered tools are major concerns. the phd thesis is meant to be completed within the coming year.reverse engineering web sales configurators','Hardware reverse engineering'
'this paper proposes a scanning procedure using an structured light system (sls). to respond to the increasing needs of reverse engineering, it becomes very important to achieve high efficiency of measuring procedure. as an sls can measure only the visible area from a specific direction, it is necessary to scan multiple times to obtain a complete model. however, it is more desirable to minimize the number of scanning directions to save the measuring time and the amount of required memory. the proposed procedure identifies missing areas from multiple range images (scanning data), and attempts to locate additional scanning orientations to fill the missing areas. the core part of the proposed procedure is the identification of missing areas from multiple range images. since the identification of missing areas may be called repeatedly during the scanning procedure, it is very important to improve the efficiency. to satisfy the requirement, this paper proposes a new method for identifying missing areas using a graphic hardware. once missing areas are identified, the proposed procedure computes additional scanning orientations by using the visibility information of missing areas. as the proposed procedure is based on well-known 2d geometric algorithms, it is very efficient.reverse engineering with a structured light system','Hardware reverse engineering'
'with the emergence of petri nets in practical applications the need to reverse-engineer them arises. folding based reverse-engineering techniques are crucial for petri nets. but after a translation step they offer novel analysis capabilities for other systems. such a translation makes petri nets a powerful and intuitive engineering metaphor outside their traditional strength for concurrency.we present a folding-based algorithm which transforms an unstructured flat net into a colored net. in reverse engineering terms, it recovers a high-level design, a structured specification and a data model from an existing system. both the algorithm and the translation to petri nets allow many variations for adaptation to different tasks. moreover, the cost is almost linear, thus ensuring scalability.reverse engineering with petri nets','Hardware reverse engineering'
'a great number of existing xml documents in various domain such as electrical business have to be maintained in order to constantly adapt to a dynamically changing environment to keep pace with business needs. a dtd or xml schema in its current textual form commonly lacks clarity and readability, which makes the maintenance process tedious and error-prone. this paper presents an approach to reverse engineering the xml documents to conceptual model, which makes the xml documents more close to real world and business needs, let the designers quickly gain a picture of the overall structure of xml documents in order to improve its quality, increase the maintainability and reusability. in this paper, the conceptual model is described by uml class diagram, a three-level model is defined, and a novel approach for extracting various structure and semantic information from existing dtd is given, especially the inheritance structure can be inferred from the dtd structure.reverse engineering xml','Hardware reverse engineering'
'reverse engineering, re-engineering, and conversion','Hardware reverse engineering'
'reverse engineering','Hardware reverse engineering'
'reverse engineering: progress along many dimensions','Hardware reverse engineering'
'in recent years, several national and community-driven conference rankings have been compiled. these rankings are often taken as indicators of reputation and used for a variety of purposes, such as evaluating the performance of academic institutions and individual scientists, or selecting target conferences for paper submissions. current rankings are based on a combination of objective criteria and subjective opinions that are collated and reviewed through largely manual processes. in this setting, the aim of this paper is to shed light into the following question: to what extent existing conference rankings reflect objective criteria, specifically submission and acceptance statistics and bibliometric indicators? the paper specifically considers three conference rankings in the field of computer science: an australian national ranking, a brazilian national ranking and an informal community-built ranking. it is found that in all cases bibliometric indicators are the most important determinants of rank. it is also found that in all rankings, top-tier conferences can be identified with relatively high accuracy through acceptance rates and bibliometric indicators. on the other hand, acceptance rates and bibliometric indicators fail to discriminate between mid-tier and bottom-tier conferences.reverse-engineering conference rankings','Hardware reverse engineering'
'device drivers today lack two important properties: guaranteed safety and cross-platform portability. we present an approach to incrementally achieving these properties in drivers, without requiring any changes in the drivers or operating system kernels. we describe reveng, a tool for automatically reverse-engineering a binary driver and synthesizing a new, safe and portable driver that mimics the original one. the operating system kernel runs the trusted synthetic driver instead of the original, thus avoiding giving untrusted driver code kernel privileges. initial results are promising: we reverse-engineered the basic functionality of network drivers in linux and windows based solely on their binaries, and we synthesized safe drivers for linux. we hope reveng will eventually persuade hardware vendors to provide verifiable formal specifications instead of binary drivers; such specifications can be used to automatically synthesize safe drivers for every desired platform.reverse-engineering drivers for safety and portability','Hardware reverse engineering'
'this paper reverse-engineers backoff-based random-access mac protocols in ad-hoc networks. we show that the contention resolution algorithm in such protocols is implicitly participating in a non-cooperative game. each link attempts to maximize a selfish local utility function, whose exact shape is reverse-engineered from the protocol description, through a stochastic subgradient method in which the link updates its persistence probability based on its transmission success or failure. we prove that existence of a nash equilibrium is guaranteed in general. then we establish the minimum amount of backoff aggressiveness needed, as a function of density of active users, for uniqueness of nash equilibrium and convergence of the best response strategy. convergence properties and connection with the best response strategy are also proved for variants of the stochastic-subgradient-based dynamics of the game. together with known results in reverse-engineering tcp and bgp, this paper further advances the recent efforts in reverse-engineering layers 2-4 protocols. in contrast to the tcp reverse-engineering results in earlier literature, mac reverse-engineering highlights the non-cooperative nature of random access.reverse-engineering mac','Hardware reverse engineering'
'reverse-engineering a commercial client-server system from peoplesoft yielded a valuable resource and proved to be cost-effective. the authors describe the motivations for, approach to, and results of this project, commissioned by the commonwealth of virginia\'s governmentreverse-engineering new systems for smooth implementation','Hardware reverse engineering'
'scanning or soft keyboards are alternatives to physical computer keyboards that allow users with motor disabilities to compose text and control the computer using a small number of input actions. in this paper, we present the reverse huffman algorithm (rha), a novel information theoretic method that extracts a representative latent probability distribution from a given scanning keyboard design. by calculating the jensen-shannon divergence (jsd)between the extracted probability distribution and the probability distribution that represents the body of text that will be composed by the scanning keyboard, the efficiency of the design can be predicted and designs can be compared with each other. thus, using rhs provides a novel a priori context-aware method for reverse-engineering scanning keyboards.reverse-engineering scanning keyboards','Hardware reverse engineering'
'the author covers the legal issues of reverse-engineering someone else\'s software, explaining what reverse-engineering activities the courts have found to be acceptable and what legal applications are for the knowledge you gained from reverse engineering. she also defines \'reverse engineering\' and presents two theories regarding its use: the strict-constructionist theory, which holds that reverse-engineering copyrighted software is always illegal, and the pragmatist theory, which takes a much more liberal view of the fair-use privilege.reverse-engineering someone else\'s software','Hardware reverse engineering'
'the goal of reverse-engineering the human brain, starting with the auditory pathway, requires three essential ingredients: neuroscience knowledge, a sufficiently capable computing platform, and a long-term funding source. by 2003, the neuroscience community had a good understanding of the characterization of sound which is carried out in the cochlea and auditory brainstem, and 1.4 ghz single-core computers with xga displays were fast enough that it was possible to build computer models capable of running and visualizing these processes in isolation at near biological resolution in real-time, and it was possible to raise venture capital funding to begin the project. by 2008, these advances had permitted the development of products in the area of two-microphone noise reduction for mobile phones, leading to viable business by 2010, thus establishing a self-sustaining funding source to continue the work into the next decade 2010-2020. by 2011, advances in fmri, multi-electrode, and behavioral studies have illuminated the cortical brain regions responsible for separating sounds in mixtures, understanding speech in quiet and in noisy environments, producing speech, recognizing speakers, and understanding and responding emotionally to music. 2ghz computers with 8 virtual cores and hd displays now permit models of these advanced auditory brain processes to be simulated and displayed simultaneously in real-time, giving a rich perspective on the concurrent and interacting representations of sound and meaning which are developed and maintained in the brain, and exposing a deeper generality to brain architecture than was evident a decade earlier. while there is much still to be discovered and implemented in the next decade, we can show demonstrable progress on the scientifically ambitious and commercially important goal of reverse-engineering the human auditory pathway.reverse-engineering the human auditory pathway','Hardware reverse engineering'
'this research focuses on the experimental evaluation and refinement of a set of scalable visualizations that show promise for improving the overall software process. the roles which visualizations may play during reverse engineering and maintenance are of particular interest. this research also investigates the manner in which software professionals use software visualizations in practice. partners from government and industry have agreed to participate by providing access to appropriate production software. this paper describes a framework for evaluating visual techniques and investigating their practical use and application,scalable visualizations to support reverse engineering','Hardware reverse engineering'
'schema transformation techniques for database reverse engineering','Hardware reverse engineering'
'in this paper we describe a two step process for reverse engineering the software architecture of a system directly from its source code. the first step involves clustering the modules from the source code into abstract structures called subsystems. the second step involves reverse engineering the subsystem-level relations using a formal (and visual) architectural constraint language. we use search techniques to accomplish both of these steps, and have implemented a suite of integrated tools to support the reverse engineering process. through a case study, we demonstrate how our tools can be used to extract the software architecture of an open-source software package from its source code without having any a priori knowledge about its design.search based reverse engineering','Hardware reverse engineering'
'segmentation and surface fitting in reverse engineering','Hardware reverse engineering'
'software reverse engineering','Hardware reverse engineering'
'this article describes the bauhaus tool suite as a concrete example for software visualization in reverse engineering, re-engineering, andsoft ware maintenance. results from a recent survey on software visualization in these domains are reported. according to this survey, bauhaus can indeed be considered a typical representative of these domains regarding the way software artifacts are visualized. specific requirements for software visualizations are drawn from both the specific example and the survey.software visualization for reverse engineering','Hardware reverse engineering'
'lack of precise specification is a well-known problem in the software industry. this article covers some peculiar aspects of the problem and its causes in the automotive software industry. the authors describe how the situation motivates engineers to grasp reverse-engineering methodologies to comprehend third-party components. theyeveloped a novel approach for reverse-engineering components, which they applied to a recent project on testing embedded systems of a modern vehicle.specification inference using systematic reverse-engineering methodologies','Hardware reverse engineering'
'recovering the semantic description of file and database structures is an important aspect of business application reverse engineering. it includes a particularly delicate activity, namely data structure extraction, i.e. finding the exact data structures and integrity constraints of the database. this process is made more complex than generally expected due to the fact that these structures and constraints often are not explicitly defined, but are translated into implicit constructs, controlled and managed through procedural code or user interface protocol for instance. this paper describes the problem of implicit structure elicitation. it proposes an analysis of this phenomenon, and of the techniques and heuristics that can be used in the elicitation process. it develops a set of efficient techniques and a strategy for the elicitation of one of the most common implicit construct, namely the foreign key. the paper also explains how db-main, a general-purpose database reverse engineering case tool, can help analysts elicit implicit constructs, and specifically foreign keys.structure elicitation in database reverse engineering','Hardware reverse engineering'
'this paper introduces a new method of surface simplification based on calculating changing rate of surface. we apply the adapted butterfly subdivision method to determine the new vertex positions of simplified mesh, making the new vertices lie on an optimized limiting surface. we test our method on the rabbit mesh, and it is very effective to hold geometric features.study a new mesh simplification method in reverse engineering','Hardware reverse engineering'
'the paper realized virtual design by constructing virtual design expert database using reverse engineering methods. through systematic analysis of common mechatronic system, virtual design problems based on reverse engineering were studied. the process of reverse design were divided into system reverse and part reverse. because of different principles and characteristics of two processes, different modeling methods were used. integrated model structure of system reverse of virtual design was studied. system reverse model, basic information model of part, assembly model, relationship models of location, connection, movement were established. architecture of reverse virtual design of part entity, key techniques of 3d modeling and reconstruction of part were studied.study of virtual design based on reverse engineering','Hardware reverse engineering'
'this paper reports on an experience in teaching database reverse engineering. we found a graduated sequence of case studies to be effective. our evidence is anecdotal, but we believe the observations will be helpful for improving teaching techniques.teaching database reverse engineering','Hardware reverse engineering'
'the automatic restructuring of cobol','Hardware reverse engineering'
'  this paper describes three categories of canonical activities that are characteristic of reverse engineering for program understanding. the activities are data gathering, knowledge management, and information exploration. all tasks carried out by a software engineer during a program understanding exercise can be mapped to a composition of one or more of these canonical activities. the design space formed by the canonical activities can be used to classify the capabilities provided by individual support mechanisms using a common vocabulary. a descriptive model that categorizes important support mechanism features based on a hierarchy of attributes is used to structure the canonical activities.the canonical activities of reverse engineering','Hardware reverse engineering'
'it may seem hard to choose between two software products, but studying their databases can reveal a clear winner and save you years of grief. the most productive way to reverse engineer a database is to build a model that conveys the software\'s scope and intent. for a clean database, you\'ll most likely want to prepare a model; for a flawed and poorly documented database, it may be best to stop after studying the style and quality of the database structure. at omt associates, we generally construct models-usually expressed as unified modeling language (uml) object models. we typically organize reverse engineering into three phases-implementation recovery, design recovery, and analysis recovery. reverse engineering is more than just a process for studying databases; it has profound implications for software development in general. you might view the reverse engineering of vendor databases as a creative response to the so-called software crisis. the current approach of preaching methodology and discipline isn\'t working, software quality and development productivity still badly lag behind our desires. database reverse engineering gives us both a carrot and a stick. the flaws and excellence of various products become more obvious and more heavily influence product success and failure. for a large corporation, the cost of the evaluation (as little as a few person-weeks) is trivial compared to the millions spent buying and deploying new softwarethe case for reverse engineering','Hardware reverse engineering'
'almost all software contracts that are not open-source contain broad bans on reverse engineering, but as far as we can tell, almost all professional software development does reverse engineering to some degree. this is a fundamental, unresolved conflict. every student and practitioner of software of engineering will face reverse engineering issues and they will have to make their own decisions about what is fair and reasonable in their situation, what risks they are willing to accept, and what corporate policies they should follow, support, or challenge. the industry is polarized and it will probably be a decade or more before the next generation of leadership revisits this conflict in a constructive way. for now, the statutes and the courts offer insufficient guidance. we plan to highlight the the issues of the law and reverse engineering through examples. each exemplar case will be drawn either from an actual lawsuit or from a technical advance in reverse engineering. rather than telling particiants what to do, we will lay out the factors that we think they should consider.the law and reverse engineering','Hardware reverse engineering'
'the local impact of the law suit against advanced micro devices inc. that was won by brooktree corp. in establishing just where copying ends and reverse engineering begins, and how to prove the difference, is discussed. in particular, to stay within the bounds of the chip protection act, the chip copier must prove some innovation was added, and must produce a paper trailthe law on reverse engineering','Hardware reverse engineering'
'  reverse engineering of computer software has assumed greater importance in recent years because of the need to examine legacy code to remove the year 2000 bug. there are different types of reverse engineering based on the level of abstraction of the code to be reengineered&semi; machine code, assembly code, source code or even case code. we describe the different types of reverse engineering and the extent of copyright protection for software. the most common uses of reverse engineering are described. this provides for a comparative overview of the legal standing on reverse engineering at the international level. we propose challenges to the global electronic community in relation to existing and future legislation in the area of reverse engineering and protection of digital works.the legal status of reverse engineering of computer software','Hardware reverse engineering'
'a truly grand challenge for science in general, and for computer architects and designers in particular, is to understand the mammalian brain\'s computing paradigm and then construct a computing device that embodies that paradigm. although computer designers have a potential role to play in solving this grand challenge, it is up to us to define that role. from a computer designer\'s perspective, i will illustrate the current understanding of the brain\'s computational paradigm by describing several examples from experimental neuroscience. i will suggest an architecture hierarchy and discuss issues that arise when translating from the complex, asynchronous, electro-chemical device, which is the brain, to a synchronous digital device capable of performing computation in a similar manner. this translation presents many difficult challenges that will require science-inspired insight and discovery, added to the challenges of engineering a very large, unconventional digital system. but, as difficult as they may be, these challenges provide almost unlimited opportunities for forward-looking, risk-taking computer architects and designers.the role of computer designers in reverse-engineering the brain','Hardware reverse engineering'
'research in maintenance and reengineering has flourished and evolved into a central part of software engineering research worldwide. in this paper, we have a look at this research community through the publications of its members in several international conferences. we analyze our results using various graph and text mining techniques. we contrast our findings to other research communities.the small world of software reverse engineering','Hardware reverse engineering'
'thermoforming is commonly used to produce shaped plastic sheets for packaging consumer products. the conventional method of designing and making thermoforming moulds is laborious and time consuming. a method based on a reverse engineering approach and thermoforming feature concept is proposed. the method involves the use of a self-developed device to digitise the surface of a product. a cad model that corresponds to the thermoforming mould of the product is then constructed by using the digitised data. the construction of the mould surface is based on the concept of a defined set of thermoforming mould features. a modified laplacian smoothing technique is applied to process the digitised data for generating the thermoforming mould surfaces. several examples are used to explain the working principle and demonstrate the viability of the proposed method.thermoforming mould design using a reverse engineering approach','Hardware reverse engineering'
'currently, the demand for the reverse engineering has been growing significantly. the need of different business sectors to adapt their systems to web or to use other technologies is stimulating the research for methods, tools and infrastructures that support the evolution of existing applications. in this paper, we present the main research trends on reverse engineering, and discuss how should be an efficient reverse engineering approach, aiming at higher reuse levels.towards an effective approach for reverse engineering','Hardware reverse engineering'
'model driven development suggests to make models the main artifact in software development. to get executable models in most cases code generation to a \"traditional\" programming language like e.g. java is used. to obtain customizable code generation template-based approaches are applied, commonly. so, to adapt the generated code to platform specific needs templates are modified by the user. after code generation, in real world application the generated code is often changed e.g. by refactorings. to keep the code and the model synchronous reverse engineering is needed. many approaches use a java parser and a mapping from the java parse tree to the uml model for this task. this causes maintenance issues since every change to a template potentially results in a change to this parse tree - model mapping. to tackle this maintenance problem our solution does not use a common language parser but uses the templates as a grammar to parse the generated code, instead. this way changes to the templates are automatically taken into account in the reverse engineering step. our approach has been implemented and tested in the fujaba case tool as a part of the model and template-based code generator codegen2 [11].towards roundtrip engineering - a template-based reverse engineering approach','Hardware reverse engineering'
'network protocol reverse engineering (npre) has played an increasing role in honeypot operations. it allows to automatically generate statemodels and scripts being able to act as realistic counterpart for capturing unknown malware. this work proposes a novel approach in the field of npre. by passively listening to network traces, our system automatically derives the protocol state machines of the peers involved allowing the analyst to understand its intrinsic logic. we present a new methodology to extract the relevant fields from arbitrary binary protocols to construct a statemodel. we prove our methodology by deriving the statemachine of documented protocols arp, dhcp and tcp. we then apply it to kademlia, the results show the usefulness to support binary reverse engineering processes and detect a new undocumented feature.traffic to protocol reverse engineering','Hardware reverse engineering'
'there\'s reverse engineering to understand, and then there\'s reverse engineering to copy. counterfeiting is a very old human temptation, but it is keeping up with the digital world very well indeed. putting aside ordinary movie piracy, we thought that for this issue we\'d just compare some counterfeiting metrics, old and new. putting the punchline right up front, counterfeiting matters in information technology (it)&#8212;and it might soon be where counterfeiting matters most.type ii reverse engineering','Hardware reverse engineering'
'the unified modeling language (uml) is widely used as a high level object oriented specification language. in this paper we present a novel approach in which reverse engineering is performed using uml as the modelling language used to achieve a representation of the implemented system. the target is the core logic of a complex critical railway control system, which was written in an application specific legacy language. uml perfectly suited to represent the nature of the core logic, made up by concurrent and interacting processes, using a bottom-up approach and proper modeling rules. each process, in fact, was strictly related to the management of a physically (resp. logically) well distinguished railway device (resp. functionality). the obtained model deeply facilitated the static analysis of the logic code, allowing for at a glance verification of correctness and compliance with higher-level specifications, and opened the way to refactoring and other formal analyses.uml based reverse engineering for the verification of railway control logics','Hardware reverse engineering'
'reverse engineering is the filthy end of the security industry; it is the business of extracting information from a program when the source is unavailable. reversing is often necessary when performing a security audit on a product that relies on third-party software such as a library. security engineers also reverse to reason about the latest malicious programs and devise antivirus software. security engineers (and malicious hackers) do not attempt to reverse assembler into, say c, which is the traditional aspiration in reversing, but merely to understand the code to sufficient depth to locate a vulnerability. untangling reverse engineering with logic and abstraction','Hardware reverse engineering'
'first page of the articleus court establishes protection against reverse engineering','Hardware reverse engineering'
'capturing design knowledge in large software systems reduces the complexity of understanding and reusing these systems. model driven engineering (mde) is seen by many as the new trend to cope with software complexity. mde promotes the notion of modeling and model transformations in model-driven development. in this paper, we propose an approach that utilizes ontological modeling and reasoning for recovering design pattern information from source code. we thus provide a formal representation of the conceptual knowledge found in source code and match it to similar representation of design patterns. this proper matching is the basis for applying semantic reasoning to infer design pattern instances. we have tested our approach on multiple open source application frameworks. the results we obtained are promising and show an improvement in terms of recall and precision.using ontology reasoning for reverse engineering design patterns','Hardware reverse engineering'
'using queries to improve database reverse engineering','Hardware reverse engineering'
'in thread-level speculation (tls), speculative tasks generate memory state that cannot simply be combined with the rest of the system because it is unsafe. one way to deal with this difficulty is to allow speculative state to merge with memory but back up in an undo log the data that will be overwritten. such undo log can be used to roll back to a safe state if a violation occurs. this approach is said to use future main memory (fmm), as memory keeps the most speculative state.while the aggressive approach of fmm systems often delivers better performance than more conservative approaches, it also requires additional hardware support. to simplify the design of fmm systems, this paper proposes a software-only design for the undo log system. we show that an fmm system with software logging is a good design point: the design has less implementation complexity than an fmm system with hardware logs, and it only reduces performance moderately. in particular, in a simulated 16-processor machine, applications take only 10\% longer to execute than if the system had the logging system fully implemented in hardware.using software logging to support multi-version buffering in thread-level speculation','Hardware reverse engineering'
'the field of reverse engineering, originally tied to the analysis and restructuring of legacy systems, proved to be equally effective in supporting the evolution of modern software systems (e.g., oo code, web applications, etc.). correspondingly, a high number of techniques and tools have been developed to address the program comprehension needs of the programmers facing maintenance tasks on any kind of software. so far, the validation of the proposed approaches consisted mainly of proofs of concepts and limited case studies. the aim of this workshop was to assess the role of the empirical studies in the future developments of reverse engineering. knowledge on the actual effectiveness of the available techniques and tools can be gained only through controlled experimentation. in this workshop, the scope of investigation of such studies was considered and a (provisional) reference taxonomy of tools and techniques was discussed. then, the main features of the empirical studies specifically designed to validate reverse engineering tools or techniques have been thoroughly examined.workshop on empirical studies in reverse engineering','Hardware reverse engineering'
'we have designed and developed a 3d digital lego system as an education tool for teaching security protocols effectively in information assurance courses (lego is a trademark of the lego group. here, we use it only to represent the pieces of a construction set.). our approach applies the pedagogical methods learned from toy construction sets by treating security primitives as lego pieces and protocols as construction results. simulating the lego toys, the digital legos use matching shapes to help students understand the relationships among security primitives and protocols. specifically, we present a flexible lego generation method that can use various intuitive shapes to represent abstract and complex security protocols. our design allows easy generation of new lego sets and creation of different course materials. the integrated system also provides 3d interaction methods that simulate the real lego building experience. for selected security courses, we have designed sample demonstrations and experiments for a set of important protocols. the initial evaluation results show encouraging feedback from students on using digital legos in introductory security courses.3d digital legos for teaching security protocols','Hardware-based security protocols'
'this paper considers a new security protocol paradigm whereby principals negotiate and on-the-fly generate security protocols according to their needs. when principals wish to interact then, rather than offering each other a fixed menu of \'known\' protocols, they negotiate and, possibly with the collaboration of other principles, synthesise a new protocol that is tailored specifically to their current security environment and requirements. this approach provides a basis for autonomic security protocols. such protocols are self-configuring since only principal assumptions and protocol goals need to be a-priori configured. the approach has the potential to survive security compromises that can be modelled as changes in the beliefs of the principals. a compromise of a key or a change in the trust relationships between principals can result in a principal self-healing and synthesising a new protocol to survive the event.a collaborative approach to autonomic security protocols','Hardware-based security protocols'
'an ip-based multimedia communication system can be roughly divided into two planes: a signaling plane and a media plane. the signaling plane provides the necessary functions for setting up, controlling and terminating the multimedia sessions. the media plane provides the support for transporting the media content (audio, video, text or applications). security mechanisms in the signaling planes address aspects related to user authentication, authorization or annonymization as well as the protection of the signaling messages against eavesdropping, interception and manipulation. the security aspects relevant to the media plane concern the encrypting of the media traffic as well as the efficient and secure exchange of the necessary keying material. this paper provides a comparative analysis of the security aspects of the most representative key exchange protocols designed for voip communication, namely dtls, zrtp, mikey and sdes. in this context, the key exchange protocols are described in relation to various authentication mechanisms and signaling plane security. further, a number of possible attacks against these protocols are investigated and, where applicable, mitigation measures are proposed.a comparative analysis of the security aspects of the multimedia key exchange protocols','Hardware-based security protocols'
'a comprehensive security system&#8212;the concepts, agents and protocols','Hardware-based security protocols'
'formal methods have proved useful in the analysis of security protocols. the paper proposes a generic model for the analysis of the security protocols (gspm for short) that supports message passing semantics and constructs for modelling the behavior of agents. gspm is simple, but it is expressive enough to express security protocols and properties in a precise and faithful manner. using gspm it is shown how security properties such as confidentiality, authentication, non-repudiation, fairness, and anonymity can be described. finally an example of formal verification is illustrated.a generic model for analyzing security protocols','Hardware-based security protocols'
'a hybrid analysis method for protocol is proposed which combines the power of both manual and automatic methods. at first, it defines a number of attack goals and then studies how to achieve these goals by misusing the messages. an automatic analysis tool is applied in order to reach completeness based on the manual analysis. some of the found attacks are excluded because of the low feasibility in the practical environment, and the other feasible attacks might be combined to launch more powerful compound attacks. some vulnerability is found when applying the hybrid method to check mobile ipv6 protocol. the result shows the hybrid method is a systematic method with completeness guarantee. it fits for analyzing non-security protocols and finding the subtle vulnerability.a hybrid vulnerability analysis method against non-security protocols','Hardware-based security protocols'
'a formal method for analyzing the security of cryptographic protocols is presented. this method is based on an original representation of the participants\' knowledge. the author proves the probabilistic properties of the cryptographic protocols and models the possible attacks on these protocols. this method is applied to well-known protocols like the kerberos authentication protocol and the x.509 standarda new method for analyzing the security of cryptographic protocols','Hardware-based security protocols'
'gorrieri and martinelli\'s tgndc is a general framework for the formal verification of security protocols in a concurrent scenario. we generalise their tgndc schema to verify wireless network security protocols. our generalisation relies on a simple timed broadcasting process calculus whose operational semantics is given in terms of a labelled transition system which is used to derive a standard simulation theory. we apply our tgndc schema to perform a security analysis of lisp, a well-known key management protocol for wireless sensor networks.a semantic analysis of wireless network security protocols','Hardware-based security protocols'
'security is becoming an everyday concern for a wide range of electronic systems that manipulate, communicate, and store sensitive data. an important and emerging category of such electronic systems are battery-powered mobile appliances, such as personal digital assistants (pdas) and cell phones, which are severely constrained in the resources they possess, namely, processor, battery, and memory. this work focuses on one important constraint of such devices&#8212;battery life&#8212;and examines how it is impacted by the use of various security mechanisms. in this paper, we first present a comprehensive analysis of the energy requirements of a wide range of cryptographic algorithms that form the building blocks of security mechanisms such as security protocols. we then study the energy consumption requirements of the most popular transport-layer security protocol: secure sockets layer (ssl). we investigate the impact of various parameters at the protocol level (such as cipher suites, authentication mechanisms, and transaction sizes, etc.) and the cryptographic algorithm level (cipher modes, strength) on the overall energy consumption for secure data transactions. to our knowledge, this is the first comprehensive analysis of the energy requirements of ssl. for our studies, we have developed a measurement-based experimental testbed that consists of an ipaq pda connected to a wireless local area network (lan) and running linux, a pc-based data acquisition system for real-time current measurement, the openssl implementation of the ssl protocol, and parameterizable ssl client and server test programs. based on our results, we also discuss various opportunities for realizing energy-efficient implementations of security protocols. we believe such investigations to be an important first step toward addressing the challenges of energy-efficient security for battery-constrained systems.a study of the energy consumption characteristics of cryptographic algorithms and security protocols','Hardware-based security protocols'
'wsns usually deployed in the targeted area to monitor or sense the environment and depending upon the application sensor node transmit the data to the base station. to relay the data intermediate nodes communicate together, select appropriate routing path and transmit data towards the base station. routing path selection depends on the routing protocol of the network. base station should receive unaltered and fresh data. to fulfill this requirement, routing protocol should beenergy-efficient and secure. hierarchical or cluster-base routing protocol for wsns is the most energy-efficient among other routing protocols. in this paper, we study different hierarchical routing technique for wsns. further we analyze and compare secure hierarchical routing protocols based on various criteria.a survey on secure hierarchical routing protocols in wireless sensor networks','Hardware-based security protocols'
'security protocol attacks are known to have various sources, from flawed implementations, to running parallel sessions of the same protocol. because of this attack diversity, it is quite difficult (or impossible) to create an abstract model that is suitable for analyzing a protocol against all possible attacks. however, if we categorize the attacks based on their characteristics we should be able to create multiple abstract models that simplify the analysis. therefore, in this paper we identify attacks based on message similarities, that we call \"structural attacks\", and create an abstract model, based on message component types (session keys, nonces, participants), that is powerful enough to capture the structure of security protocol messages.a typed specification for security protocols','Hardware-based security protocols'
'colored petri nets (cpn) are a well known formalism for modeling concurrency. we extend a version of cpn method for analyzing cryptographic protocols. by establishing modeling and analysis techniques of cpn, it can be verified whether any security threats exist when many instances of the protocol are executed concurrently. our method contributes greatly to understanding of the concurrent security of cryptographic protocols, along with constructioanalysis of concurrent security protocols using colored petri nets','Hardware-based security protocols'
'analyzing internet security protocols','Hardware-based security protocols'
'validating security protocols is a well-known hard problem even in a simple setting of a single global network. but a real network often consists of, besides the public-accessed part, several sub-networks and thereby forms a hierarchical structure. in this paper we first present a process calculus capturing the characteristics of hierarchical networks and describe the behavior of protocols on such networks. we then develop a static analysis to automate the validation. finally we demonstrate how the technique can benefit the protocol development and the design of network systems by presenting a series of experiments we have conducted.analyzing security protocols in hierarchical networks','Hardware-based security protocols'
'current studies to analyzing security protocols using formal methods require users to predefine authentication goals. besides, they are unable to discover potential correlations between secure messages. this research attempts to analyze security protocols using data mining. this is done by extending the idea of association rule mining and converting the verification of protocols into computing the frequency and confidence of inconsistent secure messages. it provides a novel and efficient way to analyze security protocols and find out potential correlations between secure messages. the conducted experiments demonstrate our approaches.analyzing security protocols using association rule mining','Hardware-based security protocols'
'we study and further develop two language-based techniques for analyzing security protocols. one is based on a typed process calculus; the other, on untyped logic programs. both focus on secrecy properties. we contribute to these two techniques, in particular by extending the former with a flexible, generic treatment of many cryptographic operations. we also establish an equivalence between the two techniques.analyzing security protocols with secrecy types and logic programs','Hardware-based security protocols'
'architectural considerations for lan security protocols','Hardware-based security protocols'
'masquerading and eavesdropping are major threats to the security of wireless communications. to provide proper protection for the communication of the wireless link, contents of the communication should be enciphered and mutual authentication should be conducted between the subscriber and the serving network. several protocols have been proposed by standards bodies and independent researchers in recent years to counteract these threats. however, the strength of these protocols is usually weakened in the roaming environment where the security breach of a visited network could lead to persistent damages to subscribers who visit. the subscriber\'s identity is not well protected in most protocols, and appropriate mechanisms solving disputes on roaming bills are not supported either. to solve these problems, new authentication protocols are proposed in this paper with new security features that have not been fully explored before.authentication protocols for personal communication systems','Hardware-based security protocols'
'abstract: we propose a new method to check authenticity properties of cryptographic protocols. first, code up the protocol in the spi-calculus of abadi and gordon. second, specify authenticity properties by annotating the code with correspondence assertions in the style of woo and lam. third, figure out types for the keys, nonces, and messages of the protocol. fourth, check that the spi-calculus code is well-typed according to a novel type and effect system presented in this paper. our main theorem guarantees that any well-typed protocol is robustly safe, that is, its correspondence assertions are true in the presence of any opponent expressible in spi.authenticity by typing for security protocols','Hardware-based security protocols'
'the design of security protocols is usually performed manually by pen and paper, by experts in security. assumptions are rarely specified explicitly. we present a new way to approach security specification: the protocol is refined fully automated into a specification that contains assumptions sufficient to execute the protocol. as a result, the protocol designer using our method does not have to be a security expert to design a protocol, and can learn immediately how the protocol should work in practice.automated refinement of security protocols','Hardware-based security protocols'
'we present a new model for automated verification of security protocols, permitting the use of an unbounded number of protocol runs. we prove its correctness, completeness and also that it terminates. it has been implemented and its efficiency is clearly shown by the number of protocols successfully studied. in particular, we present an attack previously unreported on the denning-sacco symmetric key protocol.automated unbounded verification of security protocols','Hardware-based security protocols'
'we present a new technique for verifying correspondences in security protocols. in particular, correspondences can be used to formalize authentication. our technique is fully automatic, it can handle an unbounded number of sessions of the protocol, and it is efficient in practice. it significantly extends a previous technique for the verification of secrecy. the protocol is represented in an extension of the pi calculus with fairly arbitrary cryptographic primitives. this protocol representation includes the specification of the correspondence to be verified, but no other annotation. this representation is then translated into an abstract representation by horn clauses, which is used to prove the desired correspondence. our technique has been proved correct and implemented. we have tested it on various protocols from the literature. the experimental results show that these protocols can be verified by our technique in less than 1 s.automatic verification of correspondences for security protocols','Hardware-based security protocols'
'this paper investigates the problem of the automatic verification of the computational indistinguishability of systems in the simulation-based security setting, which allows proving the composable security of cryptographic protocols whose security relies on computational hardness assumptions. we use task-structured probabilistic i/o automata (task-pioa) as our modeling framework. in this context, proofs of indistinguishability between real and ideal systems are typically divided into steps involving either proofs of perfect indistinguishability or proofs of computational indistinguishability. our method automates the proof of perfect indistinguishability for a class of simple protocols, which is, by far, the most error-prone and time-consuming part of those security proofs. we proceed by transforming the targeted real and ideal probabilistic systems into nondeterministic ones, and check the bisimulation between the obtained systems by a partition refinement algorithm. we prove the correctness of our transformation. our method has also been implemented in a symbolic way and we showed its usefulness by applying it to a practical protocol for oblivious transfer.automatic verification of simulatability in security protocols','Hardware-based security protocols'
'the verification of security protocols has been proven to be undecidable in general. different approaches use simplifying hypotheses in order to obtain decidability for interesting subclasses. amongst the most common is type abstraction, i.e. considering only well-typed runs, therefore bounding message length. in this paper, we show how to get message boundedness \"for free\" under a reasonable (syntactic) assumption on protocols, which we call well-formedness. this enables us to improve existing decidability results.bounding messages for free in security protocols','Hardware-based security protocols'
'we propose a direct and fully automated translation from standard security protocol descriptions to rewrite rules. this compilation defines non-ambiguous operational semantics for protocols and intruder behavior: they are rewrite systems executed by applying a variant of ac-narrowing. the rewrite rules are processed by the theorem-prover datac. multiple instances of a protocol can be run simultaneously as well as a model of the intruder (among several possible). the existence of flaws in the protocol is revealed by the derivation of an inconsistency. our implementation of the compiler casrul, together with the prover datac, permitted us to derive security flaws in many classical cryptographic protocols.compiling and verifying security protocols','Hardware-based security protocols'
'anonymity, as an instance of information hiding, is one of the security properties intensively studied nowadays due to its applications to various fields such as e-voting, e-commerce, e-mail, e -cash, and so on. in this paper we study the decidability and complexity status of the anonymity property in security protocols. we show that anonymity is undecidable for unrestricted security protocols, is nexptime-complete for bounded security protocols, and it is np-complete for 1-session bounded security protocols. in order to reach these objectives, an epistemic language and logic to reason about anonymity properties for security protocols under an active intruder, are provided. agent states are endowed with facts derived from actions performed by agents in protocol executions, and an inference system is provided. to define anonymity, an observational equivalence is used, which is shown to be decidable in deterministic polynomial time.complexity of anonymity for security protocols','Hardware-based security protocols'
'a truly secure protocol is one which never violates its security requirements, no matter how bizarre the circumstances, provided those circumstances are within its terms of reference. such cast-iron guarantees, as far as they are possible, require formal, rigorous techniques: proof or model-checking. informally, they are difficult or impossible to achieve. our rigorous technique is refinement, until recently not much applied to security. we argue its benefits by using refinement-based program algebra to develop several security case studies. that is one of our contributions here. the soundness of the technique follows from its compositional semantics, one which we defined (elsewhere) to support a specialisation of standard refinement by enriching standard semantics with information that tracks correlations between hidden state and visible behaviour. a further contribution is to extend the basic theory of secure refinement (morgan in mathematics of program construction, springer, berlin, vol. 4014, pp. 359&#x2013;378, 2006) with special features required by our case studies, namely agent-based systems with complementary security requirements, and looping programs.compositional refinement in agent-based security protocols','Hardware-based security protocols'
'the application of formal methods to security protocol analysis has been extensively researched during the last 25 years. several formalisms and (semi-)automatic tools for the verification of security protocols have been developed. however, their applicability is limited to relatively small protocols that run in isolation. many of the protocols that are in use today cannot be verified using these methods. one of the main reasons for this is that these protocols are composed of several sub-protocols. such a composition of protocols is not addressed in the majority of formalisms. in this paper we identify a number of issues that are relevant to applying formal methods to the problem of security protocol composition. additionally, we describe what research needs to be done to meet this challenge.compositionality of security protocols','Hardware-based security protocols'
'since the 1980s, two approaches have been developed for analyzing security protocols. one of the approaches relies on a computational model that considers issues of complexity and probability. this approach captures a strong notion of security, guaranteed against all probabilistic polynomial-time attacks. the other approach relies on a symbolic model of protocol executions in which cryptographic primitives are treated as black boxes. since the seminal work of dolev and yao, it has been realized that this latter approach enables significantly simpler and often automated proofs. however, the guarantees that it offers have been quite unclear. in this paper, we show that it is possible to obtain the best of both worlds: fully automated proofs and strong, clear security guarantees. specifically, for the case of protocols that use signatures and asymmetric encryption, we establish that symbolic integrity and secrecy proofs are sound with respect to the computational model. the main new challenges concern secrecy properties for which we obtain the first soundness result for the case of active adversaries. our proofs are carried out using casrul, a fully automated tool.computationally sound, automated proofs for security protocols','Hardware-based security protocols'
'this paper incorporates time constraints in the horn logic model, and this extended model can verify wide-mouthed-frog protocol quickly. it discusses relations between the constraint system and horn model, abstracts the constraint system, and gives the proofs of some propositions and theorems. we also give the algorithm about how to compute the abstract constraint, and analyze its complexity. as a case study we discuss the verification of wide-mouthed-frog protocol whose attack can be found quickly in new model. therefore, the method in this paper is very effective in verification of time sensitive security protocols. in the future, we will use this method to verify some complex protocols, such as kerberos protocol etc.constraint abstraction in verification of security protocols','Hardware-based security protocols'
'with the development of network and distributed systems, more and more security protocols rely heavily on time stamps, which are taken into account by a few formal methods. generally, these methods use constraints to describe the characteristic of time variables. however, few of them give a feasible solution to the corresponding constraints solving problem. an effective framework to model and verify time sensitive security protocols is introduced in [1], which doesn\'t give an automatic algorithm for constraints solution. in this paper, an effective method is presented to determine whether the constraints system has a solution, and then implemented in our verifying tool spvt. finally, denning-sacco protocol is taken as an example to show that security protocols with time constraints can be modeled naturally and verified automatically and efficiently in our models.constraints solution for time sensitive security protocols','Hardware-based security protocols'
'radio frequency identification rfid system is a low-cost contactless automatic identification technology; and barcode as a traditional technology is now broadly replaced by rfid systems to make objects more manageable in supply chains and other enterprises. this technology is operational in open wireless communication spaces whereby its transmission signals can be easily accessed resulting in security problems. consequently, it becomes an absolute necessity to develop efficient security protocols to protect the data against various attacks. this paper outlines a critical evaluation of the rfid systems, the security and privacy issues in the rfid security protocols, the epcglobal class-1 generation-2 standard as it is an international standard, its lower cost of implementation, and high speed data transmission and operation.critical evaluation of rfid security protocols','Hardware-based security protocols'
'cryptographic protocols and network security','Hardware-based security protocols'
'this paper presents an application of active learning methodologies to teach cryptography and security protocols for undergraduate it students. this course is offered to sophomore/junior students and is based upon the recently approved computing curricula cc 2001 by ieee computer society/acm taskforce. we illustrate the teaching methodology to cover the security related topics such as encryption/decryption methodologies, security and protection, cryptographic algorithms and standards, and computer crime. an example lesson covering the des is included.cryptography and security protocols course for undergraduate it students','Hardware-based security protocols'
'an important problem in the analysis of security protocols is that of checking whether a protocol preserves secrecy, i.e., no secret owned by the honest agents is unintentionally revealed to the intruder. this problem has been proved to be undecidable in several settings. in particular, durgin et al. prove the undecidability of the secrecy problem in the presence of an unbounded set of nonces, even when the message length is bounded. in this paper we prove that even in the presence of an unbounded set of nonces the secrecy problem is decidable for a reasonable subclass of protocols, which we call context-explicit protocols.decidability of context-explicit security protocols','Hardware-based security protocols'
'many recent results are concerned with interpreting proofs of security done in symbolic models in the more detailed models of computational cryptography. in the case of symmetric encryption, these results stringently demand that no key cycle (e.g. {k}k) can be produced during the execution of protocols. while security properties like secrecy or authentication have been proved decidable for many interesting classes of protocols, the automatic detection of key cycles has not been studied so far. in this paper, we prove that deciding the existence of key-cycles is np-complete for a bounded number of sessions. next, we observe that the techniques that we use are of more general interest and apply them to reprove the decidability of a significant existing fragment of protocols with timestamps.deciding key cycles for security protocols','Hardware-based security protocols'
'in formal approaches, messages sent over a network are usually modeled by terms together with an equational theory, axiomatizing the properties of the cryptographic functions (encryption, exclusive or, ...). the analysis of cryptographic protocols requires a precise understanding of the attacker knowledge. two standard notions are usually used: deducibility and indistinguishability. only few results have been obtained (in an ad-hoc way) for equational theories with associative and commutative properties, especially in the case of static equivalence. the main contribution of this paper is to propose a general setting for solving deducibility and indistinguishability for an important class (called monoidal) of these theories. our setting relies on the correspondence between a monoidal theory e and a semiring se which allows us to give an algebraic characterization of the deducibility and indistinguishability problems. as a consequence we recover easily existing decidability results and obtain several new ones.deciding knowledge in security protocols for monoidal equational theories','Hardware-based security protocols'
'logics for specifying properties of security protocols and reasoning about them have received increasing attention over the past few years. in this paper, we propose a propositional logic of knowledge, augmented with tense modalities, in which many important properties of security protocols can be naturally expressed. we also describe in some detail the protocol model, which helps provide a precise and general semantics for the logic. the main technical result is the decidability of the verification problem for the logic.deciding knowledge properties of security protocols','Hardware-based security protocols'
'security protocols aim at securing communications over public networks. their design is notoriously difficult and error-prone. formal methods have shown their usefulness for providing a careful security analysis in the case of standard authentication and confidentiality protocols. however, most current techniques do not apply to protocols that perform recursive computation e.g. on a list of messages received from the network. while considering general recursive input/output actions very quickly yields undecidability, we focus on protocols that perform recursive tests on received messages but output messages that depend on the inputs in a standard way. this is in particular the case of secured routing protocols, distributed right delegation or pki certification paths. we provide nptime decision procedures for protocols with recursive tests and for a bounded number of sessions. we also revisit constraint system solving, providing a complete symbolic representation of the attacker knowledge.deciding security for protocols with recursive tests','Hardware-based security protocols'
'research on the automatic analysis of cryptographic protocols has so far concentrated on reachability properties, such as secrecy and authentication. in this article, we prove that certain game-theoretic security properties, including balance for contract-signing protocols, can be decided in a dolev-yao style model with a bounded number of sessions. the decision algorithm that we develop is based on standard constraint-solving procedures, which, in the past, have successfully been employed in tools for reachability properties. our result thus paves the way for extending these tools to deal with game-theoretic security properties.deciding strategy properties of contract-signing protocols','Hardware-based security protocols'
'many cryptographic protocols and attacks on these protocols make use of the fact that the order in which encryption is performed does not affect the result of the encryption, i.e., encryption is commutative. however, most models for the automatic analysis of cryptographic protocols can not handle such encryption functions since in these models the message space is considered a free term algebra. in this paper, we present an np decision procedure for the insecurity of protocols that employ rsa encryption, which is one of the most important instances of commuting public key encryption.deciding the security of protocols with commuting public key encryption','Hardware-based security protocols'
'security analysis of communication protocols is a slippery business, many ``secure\'\' protocols later turn out to be insecure. among many, two complains are more frequent: inadequate definition of security and unstated assumptions in the security model. in our experience, one principal cause for such state of affairs is an apparent overlap of security and correctness, which may lead to many sloppy security definitions and security models. although there is no inherent need to separate security and correctness requirements, practically, such separation is significant. it makes security analysis easier and enables us to define security goals with a fine granularity. we present one such separation, by introducing the notion of {binding sequence} as a security primitive. a \\emph{binding sequence}, roughly speaking, is the only required security property of an authentication protocol. all other authentication goals, the correctness requirements, can be derived from the {binding sequence}.demarcation of security in authentication protocols','Hardware-based security protocols'
'two provably secure group identification schemes are presented in this report: 1) we extend de santis, crescenzo and persiano\'s (scp) anonymous group identification scheme to the discrete logarithm based case; then we provide a 3-move anonymous group identification scheme, which is more efficient than that presented in [scpm, cds], with the help of this basic scheme; 2) we also extend the original de santis, crescenzo and persiano anonymous group identification scheme to the general case where each user holds public key which is chosen by herself independently. the communication cost for one round execution of the protocolis 2mk, where k is bit length of public key n and m is the number of users in the group.design and security analysis of anonymous group identification protocols','Hardware-based security protocols'
'security protocols have been widely used to safeguard secure electronic transactions. we usually assume that principals are credible and shall not maliciously disclose their individual secrets to someone else. nevertheless, it is impractical to completely ignore the possibility that some principals may collude in private to achieve a fraudulent or illegal purpose. therefore, it is critical to address the possibility of collusion attacks in order to correctly analyse security protocols. this paper proposes a framework by which to detect collusion attacks in security protocols. the possibility of security threats from insiders is especially taken into account. the case study demonstrates that our methods are useful and promising in discovering and preventing collusion attacks.detecting collusion attacks in security protocols','Hardware-based security protocols'
'differential privacy is a confidentiality property for database queries which allows for the release of statistical information about the content of a database without disclosing personal data. the variety of database queries and enforcement mechanisms has recently sparked the development of a number of mechanized proof techniques for differential privacy.personal data, however, are often spread across multiple databases and queries have to be jointly computed by multiple, possibly malicious, parties. many cryptographic protocols have been proposed to protect the data in transit on the network and to achieve differential privacy in a distributed, adversarial setting. proving differential privacy for such protocols is hard and, unfortunately, out of the scope of the aforementioned mechanized proof techniques.in this work, we present the first framework for the mechanized verification of distributed differential privacy. we propose a symbolic definition of differential privacy for distributed databases, which takes into account dolev-yao intruders and can be used to reason about compromised parties. furthermore, we develop a linear, distance-aware type system to statically and automatically enforce distributed differential privacy in cryptographic protocol implementations (expressed in the rcf calculus). we also provide an algorithmic variant of our type system, which we prove sound and complete. finally, we tested our analysis technique on a recently proposed protocol for privacy-preserving web analytics: we discovered a new attack acknowledged by the authors, proposed a fix, and successfully type-checked the revised variant.differential privacy by typing in security protocols','Hardware-based security protocols'
'partial replication is an alluring technique to ensure the reliability of very large and geographically distributed databases while, at the same time, offering good performance. by correctly exploiting access locality most transactions become confined to a small subset of the database replicas thus reducing processing, storage access and communication overhead associated with replication. the advantages of partial replication have however to be weighted against the added complexity that is required to manage it. in fact, if the chosen replica configuration prevents the local execution of transactions or if the overhead of consistency protocols offsets the savings of locality, potential gains cannot be realized. these issues are heavily dependent on the application used for evaluation and render simplistic benchmarks useless. in this paper, we present a detailed analysis of partial database state machine (pdbsm) replication by comparing alternative partial replication protocols with full replication. this is done using a realistic scenario based on a detailed network simulator and access patterns from an industry standard database benchmark. the results obtained allow us to identify the best configuration for typical on-line transaction processing applications.evaluating certification protocols in the partial database state machine','Hardware-based security protocols'
'the notion of external consistency--that system state correctly reflects the real world--provides a basis for a denotational definition of integrity. we regard segregation of duties, well formed transactions, auditing, replication, macs, and so forth, as simply implementation techniques: they define how to achieve this notion of integrity in an operational sense. therefore, we argue that when a designer claims that a system is fault-tolerant, or that a protocol properly authenticates, or that a system is secure against fraud, then what the designer is actually claiming is that it is externally consistent. an advantage of taking this view is that it allows us to give a meaning to the \'security\' of a system that uses a combination of these implementation techniques.external consistency and the verification of security protocols (position paper)','Hardware-based security protocols'
'a fair e-commence protocol can help the value-added service provider(vasp) and users deal with the exchange safely and efficiently. in this paper, we firstly analyze a ppv(pay-per-view) protocol, and find some potential shortcomings and limitations. by multiplying the hash chains technology, we employ a couple of hash chains as billing cell, and improve the fairness problem which a cell&#8217;s loss existed when communication interrupted. consequently, we propose a new protocol. we assume that user, vasp and mobile network operators communicate each other with an agreed session key. this protocol is able to achieve the service application and payment function commendably. the protocol is specially fit for mobile communication network environment(ad-hoc) as the low computing and storing requirement in mobile terminals,. by systematic research, the protocol not only satisfies the fairness, non-repudiation and security, but also is simply and efficient, so it is a commendable mobile payment protocol.fair security protocols with off-line ttp','Hardware-based security protocols'
'an automatic security protocol generator is proposed that used logic-based synthesis rules to guide it in a backward search for suitable protocols from protocol goals. the approach taken is unlike existing automatic protocol generators which typically carry out a forward search for candidate protocols from the protocol assumptions. a prototype generator has been built that performs well in the automatic generation of authentication and key exchange protocols.fast automatic synthesis of security protocols using backward search','Hardware-based security protocols'
'finite-state analysis of security protocols','Hardware-based security protocols'
'focus on the asant&#233; friendlynet vr2004 series vpn security router','Hardware-based security protocols'
'formal correctness of security protocols by giampaolo bella, springer-verlag','Hardware-based security protocols'
'traditional security protocols are mainly concerned with authentication and key establishment and rely on predistributed keys and properties of cryptographic operators. in contrast, new application areas are emerging that establish and rely on properties of the physical world. examples include protocols for secure localization, distance bounding, and secure time synchronization. we present a formal model for modeling and reasoning about such physical security protocols. our model extends standard, inductive, trace-based, symbolic approaches with a formalization of physical properties of the environment, namely communication, location, and time. in particular, communication is subject to physical constraints, for example, message transmission takes time determined by the communication medium used and the distance between nodes. all agents, including intruders, are subject to these constraints and this results in a distributed intruder with restricted, but more realistic, communication capabilities than those of the standard dolev-yao intruder. we have formalized our model in isabelle/hol and have used it to verify protocols for authenticated ranging, distance bounding, broadcast authentication based on delayed key disclosure, and time synchronization.formal reasoning about physical properties of security protocols','Hardware-based security protocols'
'we present a new technique for verifying authenticity in cryptographic protocols. this technique is fully automatic, it can handle an unbounded number of sessions of the protocol, and it is efficient in practice. it significantly extends a previous technique for the verification of secrecy. the protocol is represented in an extension of the pi calculus with fairly arbitrary cryptographic primitives. this protocol representation includes the authentication specification to be verified, but no other annotation. our technique has been proved correct, implemented, and tested on various protocols from the literature. the experimental results show that we can verify these protocols in less than 1 s.from secrecy to authenticity in security protocols','Hardware-based security protocols'
'formal methods have been very successful in analyzing security protocols for reachability properties such as secrecy or authentication. in contrast, there are very few results for equivalence-based properties, crucial for studying e.g. privacy-like properties such as anonymity or vote secrecy. we study the problem of checking equivalence of security protocols for an unbounded number of sessions. since replication leads very quickly to undecidability (even in the simple case of secrecy), we focus on a limited fragment of protocols (standard primitives but pairs, one variable per protocol\'s rules) for which the secrecy preservation problem is known to be decidable. surprisingly, this fragment turns out to be undecidable for equivalence. then, restricting our attention to deterministic protocols, we propose the first decidability result for checking equivalence of protocols for an unbounded number of sessions. this result is obtained through a characterization of equivalence of protocols in terms of equality of languages of (generalized, real-time) deterministic pushdown automata.from security protocols to pushdown automata','Hardware-based security protocols'
'pekka nikander: do you have any feeling for how much of this system you can model? reply: it&#39;s a moveable feast: you can choose the boundary. but if you don&#39;t have any boundary at all then i don&#39;t think you&#39;ve got enough context...you need to talk about conditions that are established through combinations of sub-protocols, and they have to be in context. i think it&#39;s not just a simple matter of fact, of saying &#8220;ok, we&#39;ll just work with the protocol and hope everything else will go away.&#8221; it won&#39;t. and i don&#39;t yet know where to stop. bruce christianson: when you say &#8220;the protocol&#8221; i assume you&#39;re using that in a narrow sense of just meaning the messages and the state of the protocol itself, is that right? it&#39;s clear that you need to model some of the other aspects of those systems on which the protocols run. reply: yes, that&#39;s where i was coming from. i originally started doing this work on very constrained message sequence charts. when people asked if i could prove completeness, i said that there may be a key to prove completeness for this very narrow class of protocols, but what was the point, because the real problems were all around the side. you need to broaden out the model and then completeness questions probably disappear.from security protocols to systems security','Hardware-based security protocols'
'our aim is to present a game semantics model for the specification of security protocols. game semantics has been used to give an operational flavor to denotational semantics, thereby combining the best of both worlds by having an elegant mathematical structure and at the same time describing steps of execution. game semantics was successfully used to prove full abstraction of pcf and has since been used to describe the semantics of a variety of programming languages. it fits naturally in the framework of security protocols as the interactions between communicating parties can be described as moves in a game, where honest agents are the players and the intruder is the opponent. we propose a game-based calculus for the specification of security protocols. first, we define games that represent interactions in security protocols, these games are then used to ascribe denotational semantics to security protocols.game semantics model for security protocols','Hardware-based security protocols'
'we generalize the selective-id security model for hibe by introducing two new security models. both these models allow the adversary to commit to a set of identities and in the challenge phase choose any one of the previously committed identities. two constructions of hibe are presented which are secure in the two models. one of the hibe constructions supports an unbounded number of levels, i.e., the maximum number of levels does not need to be specified during the set-up. further, we show that this hibe can be modified to obtain a multiple receiver ibe which is secure in the selective-id model without the random oracle assumption.generalization of the selective-id security model for hibe protocols','Hardware-based security protocols'
'security protocols are notoriously difficult to debug. one approach to the automatic verification of security protocols with a bounded set of agents uses logic programming with analysis and synthesis rules to describe how the attacker gains information and constructs new messages. we propose a generic approach to verifying security protocols in spin. the dynamic process creation mechanism of spin is used to nondeterministically create different combinations of role instantiations. we incorporate the synthesis and analysis features of the logic programming approach to describe how the intruder learns information and replays it back into the system. we formulate a generic &#8220;loss of secrecy&#8221; property that is flagged whenever the intruder learns private information from an intercepted message. we also describe a simplification of the dolev-yao attacker model that suffices to analyze secrecy properties.generic verification of security protocols','Hardware-based security protocols'
'in this paper we extend the approximation based theoretical framework in which the security problem &#8211; secrecy preservation against an intruder &#8211; may be semi-decided through a reachability verification. we explain how to cope with algebraic properties for an automatic approximation-based analysis of security protocols. we prove that if the initial knowledge of the intruder is a regular tree language, then the security problem may by semi-decided for protocols using cryptographic primitives with algebraic properties. more precisely, an automatically generated approximation function enables us 1) an automatic normalization of transitions, and 2) an automatic completion procedure. the main advantage of our approach is that the approximation function makes it possible to verify security protocols with an arbitrary number of sessions. the concepts are illustrated on an example of the view-only protocol using a cryptographic primitive with the exclusive or algebraic property.handling algebraic properties in automatic analysis of security protocols','Hardware-based security protocols'
'the feasibility of security solution for rfid tags relies heavily on its hardware cost and performance. in the literature the term lightweight solution is used liberally and causes problems when selecting a solution for e.g. rfid environment. evaluating the actually feasibility of the solution requires electrical engineering skills that many security developers and decision makers may lack. in this paper we describe simple guidelines for approximating the feasibility of the security solution in terms of gates and clock cycles. these guidelines make it easier to evaluate the cryptographic solutions feasibility for targeted hardware and provide a basis for categorisation of lightweight security solutions.hardware cost measurement of lightweight security protocols','Hardware-based security protocols'
'we investigate the question of whether the security of protocols in the information-theoretic setting (where the adversary is computationally unbounded) implies the security of these protocols under concurrent composition. this question is motivated by the folklore that all known protocols that are secure in the information-theoretic setting are indeed secure under concurrent composition. we provide answers to this question for a number of different settings (i.e., considering perfect versus statistical security, and concurrent composition with adaptive versus fixed inputs). our results enhance the understanding of what is necessary for obtaining security under composition, as well as providing tools (i.e., composition theorems) that can be used for proving the security of protocols under composition while considering only the standard stand-alone definitions of security.information-theoretically secure protocols and security under composition','Hardware-based security protocols'
'there are two sources of threats to secure operation of routing protocols in networks. the first source of threats is subverted routers that legitimately participate in a routing protocol. the second source of threats is intruders which may illegally attempt to interfere in routing protocols by masquerading as routers. in this paper, we first analyse the security requirements of network routing protocols and then discuss the necessary measures which can be adopted to make the operation of these protocols secure.integration of security in network routing protocols','Hardware-based security protocols'
'it is often difficult to specify exactly what a security protocol is intended to achieve, and there are many example of attacks on protocol which have been proved to satisfy the \'wrong\', or too unreal a specification. contrary to the usual approach of attempting to capture what it is that protocol achieves in abstract terms, we propose a readily automatable style of specification which simply asserts that a node can only complete its part in a protocol run if the pattern of messages anticipated by the designer has occurred. while this intensional style of specification does not replace more abstract ones such as confidentiality, it does appear to preclude a wide range of the styles of attack that are hardest to exclude by other means.intensional specifications of security protocols','Hardware-based security protocols'
'this article describes various efforts to address security in three areas of the internet protocol suite: the internet protocol itself (ipsec), the domain between transport and application layer (the secure sockets layer and the transport layer security protocols) and security extensions for the hypertext transfer protocol (s-http). for each area the current technology, relevant standardization activities and likely future developments are discussed. in addition, a brief introduction to the internet standardization process is given.internet security protocols','Hardware-based security protocols'
'we explore the applicability of the programming method of feijen and van&#x00a0;gasteren to the domain of security protocols. this method addresses the derivation of concurrent programs from a formal specification, and it is based on common notions like invariants and pre- and post-conditions. we show that fundamental security concepts like secrecy and authentication can nicely be specified in this way. using some small extensions, the style of formal reasoning from this method can be applied to the security domain. to demonstrate our approach, we discuss an authentication protocol and a public-key distribution protocol, and we deal with their composition. by focussing on a general setting where agents run the protocols multiple times, the nonce concept turns out to pop-up naturally. although this work does not contain any new protocols, it does offer a new view on reasoning about security protocols.invariant-based reasoning about parameterized security protocols','Hardware-based security protocols'
'key agreement protocols and their security analysis','Hardware-based security protocols'
'a new knowledge-based security protocol verification approach is proposed in this paper. a number of predicates, functions, assumptions and rules are used to infer the knowledge of participating principals. these items are implemented with isabelle, which enables mechanical proving. this approach can prove protocols concerning interleaving protocol sessions and can prove the correctness of a mediumsized security protocol in a couple of seconds. the mechanical proofs of a number of important secure properties and then of the correctness of the needham-schroeder-lowe protocol are given as examples to show the effectiveness of this method.knowledge based approach for mechanically verifying security protocols','Hardware-based security protocols'
'languages for formal specification of security protocols','Hardware-based security protocols'
'security protocols are used to exchange information in a distributed system with the aim of providing security guarantees. we present an approach to modeling security protocols using lazy data types in a higher-order functional programming language. our approach supports the formalization of protocol models in a natural and high-level way, and the automated analysis of safety properties using infinite-state model checking, where the model is explicitly constructed in a demand-driven manner. we illustrate these ideas with an extended example: modeling and checking the needham-schroeder public-key authentication protocol.lazy infinite-state analysis of security protocols','Hardware-based security protocols'
'we present a formalism for the automatic verification of security protocols based on multi-agent systems semantics. we give the syntax and semantics of a temporal-epistemic securityspecialised logic and provide a lazy-intruder model for the protocol rules that we argue to be particularly suitable for verification purposes. we exemplify the technique by finding a (known) bug in the traditional nspk protocol.ldyis: a framework for model checking security protocols','Hardware-based security protocols'
'we introduce a new syntax-based security testing (sst) framework that uses a protocol specification to perform security testing on text-based communication protocols. a protocol specification of a particular text-based protocol under-tested represents its syntactic grammar and static constraints. the specification is used to generate test cases by mutating valid messages, breaking the syntactic and constraints of the protocol. the framework is demonstrated using a toy web application and the open source application korganizer.linguistic security testing for text communication protocols','Hardware-based security protocols'
'this thesis is about the application of automated reasoning techniques to the formal analysis of security protocols. more in detail, it proposes a general model-checking framework for security protocols based on a set-rewriting formalism that, coupled with the use of linear temporal logic, allows for the specification of assumptions on principals and communication channels as well as complex security properties that are normally not handled by state-of-the-art protocol analyzers. the approach successfully combines encoding techniques originally developed for planning with bounded model-checking techniques. the effectiveness of the approach proposed is assessed against the formal analysis of relevant security protocols, with the detection of a severe security flaw in google\'s saml-based sso for google apps and a previously unknown attack on a patched version of the asw contract-signing protocol.ltl model-checking for security protocols','Hardware-based security protocols'
'explicit model-checking (mc) is a classical solution to find flaws in a security protocol. but it is well-known that for non trivial protocols, mc may enumerate state-spaces of astronomical sizes--the famous state-space explosion problem. distributed model checking is a solution but complex and subject to bugs: a mc can validate a model but miss an invalid state. in this paper, we focus on using a verification condition generator that takes annotated distributed algorithms and ensures their termination and correctness. we study five algorithms (one sequential and four distributed where three of them are dedicated and optimised for security protocol) of state-space construction as a first step towards mechanised verification of distributed model-checkers.mechanised verification of distributed state-space algorithms for security protocols','Hardware-based security protocols'
'security protocols provide critical services for distributed communication infrastructures. however, it is a challenge to ensure the correct functioning of their implementations, particularly, in the presence of malicious parties. we study testing of message confidentiality &#8211; an essential security property. we formally model protocol systems with an intruder using dolev-yao model. we discuss both passive monitoring and active testing of message confidentiality. for adaptive testing, we apply a guided random walk that selects next input on-line based on transition coverage and intruder&#39;s knowledge acquisition. for mutation testing, we investigate a class of monotonic security flaws, for which only a small number of mutants need to be tested for a complete checking. the well-known needham-schroeder-lowe protocol is used to illustrate our approaches.message confidentiality testing of security protocols','Hardware-based security protocols'
'we introduce a version of distributed temporal logic for rigorously formalizing and proving metalevel properties of different protocol models, and establishing relationships between models. the resulting logic is quite expressive and provides a natural, intuitive language for formalizing both local (agent specific) and global properties of distributed communicating processes. through a sequence of examples, we show how this logic may be applied to formalize and establish the correctness of different modeling and simplification techniques, which play a role in building effective protocol tools.metareasoning about security protocols using distributed temporal logic','Hardware-based security protocols'
'we propose a middleware for automated implementation of security protocols for web services. the proposed middleware consists of two main layers: the communication layer and the service layer. the communication layer is built on the soap layer and ensures the implementation of security and service protocols. the service layer provides the discovery of services and the authorization of client applications. in order to provide automated access to the platform services we propose a novel specification of security protocols, consisting of a sequential component, implemented as a wsdl-s specification, and an ontology component, implemented as an owl specification. specifications are generated using a set of rules, where information related to the implementation of properties such as cryptographic algorithms or key sizes, are provided by the user. the applicability of the proposed middleware is validated by implementing a video surveillance system. middleware for automated implementation of security protocols','Hardware-based security protocols'
'we consider the problem of implementing a security protocol in such a manner that secrecy of sensitive data is not jeopardized. the implementation is assumed to take place in the context of a language named join voyagers used to program a peer-to-peer network using object oriented techniques and join methods. a mobile agent is an object that can migrate throughout a heterogeneous network of computers, under its own control, in order to perform tasks using resources of these nodes. we develop a basic architecture to express security protocols for communication and strong migration, or to enforce cryptographic primitives based on spi calculus. these protocols rely on cryptography and on communication channels with properties like authenticity and privacy. this article will present also a series of aspects related to mobile code security, namely the protection of hosts receiving a malicious code and the protection of a mobile code within a malicious host from our programmable peer-to-peer network.mobile objects security protocols','Hardware-based security protocols'
'offloading user management functions like authentication and authorization to identity providers is a key enabler for cloud computing based services. protocols used to provide identity as a service (idaas) are the foundation of security for many business transactions on the web and need to be thoroughly analyzed. while analysis of cryptographic protocols has been an active research area over the past three decades, the techniques have not been adapted to analyze security for complex web interactions. in this paper, we identify gaps in the area and propose means to address them. we extend an important belief logic (the so-called ban logic) used for analyzing security in authentication protocols to support new concepts that are specific to browser based idaas protocols. we also address the problem of automating belief based security analysis through a uml based model driven approach which can be easily integrated with existing software engineering tools. we demonstrate benefits of the extended logic and model driven approach by analyzing two of the most commonly used idaas protocols.model driven security analysis of idaas protocols','Hardware-based security protocols'
'in this paper we offer a novel methodology for verifying correctness of (timed) security protocols. the idea consists in computing the time of a correct execution of a session and finding out whether the intruder can change it to shorter or longer by an active attack. moreover, we generalize the correspondence property so that attacks can be also discovered when some time constraints are not satisfied. an implementation of our method is described. as case studies we verify generalized (timed) authentication of kerberos, tmn, neumann stubblebine protocol, andrew secure protocol,wide mouthed frog, and nspk.modelling and checking timed authentication of security protocols','Hardware-based security protocols'
'the security of key agreement protocols has traditionally been notoriously hard to establish. in this paper we present a modular approach to the construction of proofs of security for a large class of key agreement protocols. by following a modular approach to proof construction, we hope to enable simpler and less error-prone analysis and proof generation for such key agreement protocols. the technique is compatible with bellare-rogaway style models as well as the more recent models of bellare et al. and canetti and krawczyk. in particular, we show how the use of a decisional oracle can aid the construction of proofs of security for this class of protocols and how the security of these protocols commonly reduces to some form of gap assumption.modular security proofs for key agreement protocols','Hardware-based security protocols'
'in the recent years, important efforts have been made for offering a dedicated language for modelling and verifying security protocols. outcome of the european project avispa, the high-level security protocol language (hlpsl) aims at providing a means for verifying usual security properties (such as data secrecy) in message exchanges between agents. nevertheless, verifying the security protocol model does not guarantee that the actual implementation of the protocol will fulfil these properties. we propose in this paper a testing technique that makes it possible to validate an implementation of a security protocol, based on a hlpsl model. we introduce a set of mutation operators for hlpsl models that aim at introducing leaks in the security protocols. the mutated models are then analysed by the avispa tool set that will produce counter-example traces leading to the leaks, thus providing the test cases. we report an experiment of our mutation technique on a wide range of security protocols and discuss the relevance of the proposed mutation operators.mutation-based test generation from security protocols in hlpsl','Hardware-based security protocols'
'this paper reports on two case-studies of applying ban logic to industrial strength security protocols. these studies demonstrate the flexibility of the ban language, as it caters for the addition of appropriate constructs and rules. we argue that, although a semantical foundation of the formalism is lacking, ban logic provides an intuitive and powerful technique for security analysis.on ban logics for industrial security protocols','Hardware-based security protocols'
'mutual authentication and session key exchange protocols based on certificates for the wireless mobile communication/computing system are proposed. first, two improved versions for the conventional certificate-based systems are proposed, and an offline authentication mechanism based on the dynamic certificate is introduced. then, an end-to-end internetwork-authenticated session key exchange protocol, which preserves a private communication between two mobile users, is finally proposed. in designing the security protocols proposed, the low computational power of the mobile stations and the low bandwidth of the wireless networks are consideredon certificate-based security protocols for wireless mobile communication systems','Hardware-based security protocols'
'we use an enhanced operational semantics to infer quantitative measures on systems describing cryptographic protocols. system transitions carry enhanced labels. we assign rates to transitions by only looking at these labels. the rates reflect the distributed architecture running applications and the use of possibly different crypto-systems. we then map transition systems to markov chains and evaluate performance of systems, using standard tools.on evaluating the performance of security protocols','Hardware-based security protocols'
'group key exchange (gke) protocols can be used to guarantee confidentiality and authentication in group applications. the paradigm of provable security subsumes an abstract formalization (security model) that considers the protocol environment and identifies its security goals. the first security model for gke protocols was proposed by bresson, chevassut, pointcheval, and quisquater in 2001, and has been subsequently applied in many security proofs. their definitions of ake-security (authenticated key exchange; a.k.a. indistinguishability of the key) and ma-security (mutual authentication) became meanwhile standard. in this paper we analyze the bcpq model and some of its variants and identify several risks resulting from its technical core construction - the notion of partnering. consequently, we propose a revised model extending ake- and ma-security in order to capture attacks by malicious participants and strong corruptions. then, we turn to generic solutions (known as compilers) for ake- and ma-security in bcpq-like models. we describe a compiler c-ama which provides ake- and ma-security for any gke protocol, under standard cryptographic assumptions, that eliminates some identified limitations in existing compilers.on security models and compilers for group key exchange protocols','Hardware-based security protocols'
'invariably, new technologies introduce new vulnerabilities which often enable new attacks by increasingly potent adversaries. yet new systems are more adept at handling well-known attacks by old adversaries than anticipating new ones. our adversary models seem to be perpetually out of date: often they do not capture adversary attacks and sometimes they address attacks rendered impractical by new technologies.in this talk, i provide a brief overview of adversary models beginning with those required by program and data sharing technologies (\'60-\'70s), continuing with those required by computer communication and networking technologies (\'70s-\'90s), and ending with those required by and sensor network technologies (\'00s -&gt;). i argue that sensor, ad-hoc, and mesh networks require new models, different from those in common use, namely those of the dolev-yao and byzantine adversaries. i illustrate this with adversaries that attack perfectly sensible and otherwise correct protocols of sensor networks. these attacks cannot be countered with traditional security protocols using end-to-end design arguments and require emergent security properties as countermeasures.on the evolution of adversary models in security protocols','Hardware-based security protocols'
'xml and web services security specifications define elements to incorporate security tokens within a soap message. we propose a method for mapping such messages to an abstract syntax in the style of dolev-yao, and in particular casper notation. we show that this translation preserves flaws and attacks. therefore we provide a way for all the methods, and specifically casper and fdr, that have been developed in the last decade by the theoretical community for the analysis of cryptographic protocols to be used for analysing ws-security protocols. finally, we demonstrate how this technique can be used to prove properties and discover attacks upon a proposed microsoft ws-secureconversation protocol.on the relationship between web services security and traditional protocols','Hardware-based security protocols'
'we consider the problem of implementing a security protocol in such a manner that secrecy of sensitive data is not jeopardized. implementation is assumed to take place in the context of an api that provides standard cryptography and communication services. given a dependency specification, stating how api methods can produce and consume secret information, we propose an information flow property based on the idea of invariance under perturbation, relating observable changes in output to corresponding changes in input. besides the information flowcondition itself, the main contributions of the paper are results relating the admissibility property to a direct flow property in the special case of programs which branch on secrets only in cases permitted by the dependency rules. these results are used to derive an unwinding-like theorem, reducing a behavioral correctness check (strong bisimulation) to an invariant.on the secure implementation of security protocols','Hardware-based security protocols'
'on the security of public key protocols','Hardware-based security protocols'
'on the security of server-aided rsa protocols','Hardware-based security protocols'
'in an internet environment, such as unix, a remote user has to obtain the access right from a server before doing any job. the procedure of obtaining acess right is called a user authentication protocol. user authentication via user memorable password provides convenience without needing any auxiliary devices, such as smart card. a user authentication protocol via username and password should basically withstand the off-line password guessing attack, the stolen verifier attack, and the dos attack. recently, peyravian and zunic proposed one password transmission protocol and one password change protocol. later, tseng et al. (2001) pointed out that peyravian and zunic\'s protocols can not withstand the off-line password guessing attack, and therefore proposed an improved protocol to defeat the attack. independently, hwang and yeh also showed that peyravian and zunic\'s protocols suffer from some secury flaws, and an improved protocol was also presented. in this paper, we show that both peyravian and zunic\'s protocols and tseng et al.\'s improved protocol are insecure against the stolen verifier attack. moreover, we show that all peyravian and zunic\'s, tseng et al.\'s, and hwang and yeh\'s protocols are insecure against dos attack.on the security of some password authentication protocols','Hardware-based security protocols'
'based on a concise domain analysis we develop a formal semantics of security protocols. its main virtue is that it is a generic model, in the sense that it is parameterized over e.g. the intruder model. further characteristics of the model are a straightforward handling of parallel execution of multiple protocols, locality of security claims, the binding of local constants to role instances, and explicitly defined initial intruder knowledge. we validate our framework by analysing the needham-schroeder-lowe protocol.operational semantics of security protocols','Hardware-based security protocols'
'in this paper, we classify the rfid distance bounding protocols having bitwise fast phases and no final signature. we also give the theoretical security bounds for two specific classes, leaving the security bounds for the general case as an open problem. as for the classification, we introduce the notion of k-previous challenge dependent (k-pcd) protocols where each response bit depends on the current and k-previous challenges and there is no final signature. we treat the case k = 0, which means each response bit depends only on the current challenge, as a special case and define such protocols as current challenge dependent (ccd) protocols. in general, we construct a trade-off curve between the security levels of mafia and distance frauds by introducing two generic attack algorithms. this leads to the conclusion that ccd protocols cannot attain the ideal security against distance fraud, i.e. 1/2, for each challenge-response bit, without totally losing the security against mafia fraud. we extend the generic attacks to 1-pcd protocols and obtain a trade-off curve for 1-pcd protocols pointing out that 1-pcd protocols can provide better security than ccd protocols. thereby, we propose a natural extension of a ccd protocol to a 1-pcd protocol in order to improve its security. as a study case, we give two natural extensions of hancke and kuhn protocol to show how to enhance the security against either mafia fraud or distance fraud without extra cost.optimal security limits of rfid distance bounding protocols','Hardware-based security protocols'
'security and privacy are growing concerns in the internet community, due to the internet\'s rapid growth and the desire to conduct business over it safely. this desire has led to the advent of several proposals for security standards, such as secure ip, secure http, and the secure socket layer. all of these standards propose using cryptographic protocols such as des and rsa. thus, the need to use encryption protocols is increasing.shared-memory multiprocessors make attractive server platforms, for example as secure world-wide web servers. these machines are becoming more common, as shown by recent vendor introductions of platforms such as sgi\'s challenge, sun\'s sparccenter, and dec\'s alphaserver. the spread of these machines is due both to their relative ease of programming and their good price/performance.this paper is an experimental performance study that examines how encryption protocol performance can be improved by using parallelism. we show linear speedup for several different internet-based cryptographic protocol stacks running on a symmetric shared-memory multiprocessor using two different approaches to parallelism.parallelized network security protocols','Hardware-based security protocols'
'two extensions of the partial order reduction algorithm of clarke, jha and marrero are presented. the proposed algorithms are suitable for branching security protocols, e.g. optimistic fair contract signing schemes. the first extension is proved to generate a reduced state space which is branching bisimilar to the full state space, while the second extension generates a state space that is trace equivalent to the full state space. experimental results using an implementation of the algorithms in the toolset of the &#181;crl process algebra are reported.partial order reduction for branching security protocols','Hardware-based security protocols'
'we use a special operational semantics which drives us in inferring quantitative measures on system describing cryptographic protocols. the transitions of the system carry enhanced labels. we assign rates to transitions by only looking at these labels. the rates reflect the distributed architecture running applications and the use of possibly different cryptosystems. we then map transition systems to markov chains and evaluate performance of systems, using standard tools.performance evaluation of security protocols specified in lysa','Hardware-based security protocols'
'formal verification of security protocols has become a key issue in computer security. yet, it has proven to be a hard task often error prone and discouraging for non-experts in formal methods.in this paper we show how security protocols can be specified and verified efficiently and effectively by embedding reasoning about actions into a logic programming language.in a nutshell, we view a protocol trace as a plan to achieve a goal, so that protocol attacks are plans achieving goals that correspond to security violations. building on results from logic programming and planning, we map the existence of an attack to a protocol into the existence of a model for the protocol specification that satisfies the specification of an attack. to streamline such way of modeling security protocols, we use a description language alsp which makes it possible to describe protocols with declarative ease and to search for attacks by relying on efficient model finders (e.g. the smodels systems by niemela and his group). this paper shows how to use alsp for modeling two significant case studies in protocol verification: the classical needham-schroeder public-key protocol, and aziz-diffie key agreement protocol for mobile communication.planning attacks to security protocols','Hardware-based security protocols'
'in this paper we define a sequent calculus to formally specify, simulate, debug and verify security protocols. in our sequents we distinguish between the current knowledge of principals and the current global state of the session. hereby, we can describe the operational semantics of principals and of an intruder in a simple and modular way. furthermore, using proof theoretic tools like the analysis of permutability of rules, we are able to find efficient proof strategies that we prove complete for special classes of security protocols including needham-schroeder. based on the results of this preliminary analysis, we have implemented a prolog meta-interpreter which allows for rapid prototyping and for checking safety properties of security protocols, and we have applied it for finding error traces and proving correctness of practical examples.proof theory, transformations, and logic programming for debugging security protocols','Hardware-based security protocols'
'we consider the refinement-based process for the development of security protocols. our approach is based on the event b refinement, which makes proofs easier and which makes the design process faithfull to the structure of the protocol as the designer thinks of it. we introduce the notion of mechanism related to a given security property; a mechanism can be combined with another mechanism through the double refinement process ensuring the preservation of previous security properties of mechanisms. mechanisms and combination of mechanisms are based on event b models related to the security property of the current mechanism. analysing cryptographic protocols requires precise modelling of the attacker&#8217;s knowledge and the attacker&#8217;s behaviour conforms to the dolev-yao model.proof-based design of security protocols','Hardware-based security protocols'
'first page of the articleprotocols for data security','Hardware-based security protocols'
'we develop an approach to deriving concrete engineering advice for cryptographic protocols from provable-security-style proofs of security. the approach is illustrated with a simple, yet useful protocol. the proof is novel and is the first published proof that provides an exact relationship between a high-level protocol and multiple cryptographic primitives.provable security for cryptographic protocols','Hardware-based security protocols'
'informal justifications of security protocols involve arguing backwards that various events are impossible. inductive definitions can make such arguments rigorous. the resulting proofs are complicated, but can be generated reasonably quickly using the proof tool isabelle/hol. there is no restriction to finite-state systems and the approach is not based on belief logics. protocols are inductively defined as sets of traces, which may involve many interleaved protocol runs. protocol descriptions model accidental key losses as well as attacks. the model spy can send spoof messages made up of components decrypted from previous traffic.several key distribution protocols have been studied, including needham-schroeder, yahalom and otway-rees. the method applies to both symmetric-key and public-key protocols. a new attack has been discovered in a variant of otway-rees (already broken by mao and boyd). assertions concerning secrecy and authenticity have been proved.proving properties of security protocols by induction','Hardware-based security protocols'
'security protocols use cryptography to set up private communication channels on an insecure network. many protocols contain flaws, and because security goals are seldom specified in detail, we cannot be certain what constitutes a flaw. thanks to recent work by a number of researchers, security protocols can now be analyzed formally.the paper outlines the problem area, emphasizing the notion of freshness. it describes how a protocol can be specified using operational semantics and properties proved by rule induction, with machine support from the proof tool isabelle. the main example compares two versions of the yahalom protocol. unless the model of the environment is sufficiently detailed, it cannot distinguish the correct protocol from a flawed version.the paper attempts to draw some general lessons on the use of formalisms. compared with model checking, the inductive method performs a finer analysis, but the cost of using it is greater.proving security protocols correct','Hardware-based security protocols'
'in recent years, puf-based schemes have not only been suggested for the basic security tasks of tamper sensitive key storage or system identification, but also for more complex cryptographic protocols like oblivious transfer (ot), bit commitment (bc), or key exchange (ke). in these works, so-called \"strong pufs\" are regarded as a new, fundamental cryptographic primitive of their own, comparable to the bounded storage model, quantum cryptography, or noisebased cryptography. this paper continues this line of research, investigating the correct adversarial attack model and the actual security of such protocols. in its first part, we define and compare different attack models. they reach from a clean, first setting termed the \"stand-alone, good puf model\" to stronger scenarios like the \"bad puf model\" and the \"puf re-use model\". we argue why these attack models are realistic, and that existing protocols would be faced with them if used in practice. in the second part, we execute exemplary security analyses of existing schemes in the new attack models. the evaluated protocols include recent schemes from brzuska et al. published at crypto 2011 [1] and from ostrovsky et al. [18]. while a number of protocols are certainly secure in their own, original attack models, the security of none of the considered protocols for ot, bc, or ke is maintained in all of the new, realistic scenarios. one consequence of our work is that the design of advanced cryptographic puf protocols needs to be strongly reconsidered. furthermore, it suggests that strong pufs require additional hardware properties in order to be broadly usable in such protocols: firstly, they should ideally be \"erasable\", meaning that single puf-responses can be erased without affecting other responses. if the area efficient implementation of this feature turns out to be difficult, new forms of controlled pufs [8] (such as logically erasable and logically reconfigurable pufs [13]) may suffice in certain applications. secondly, pufs should be \"certifiable\", meaning that one can verify that the puf has been produced faithfully and has not been manipulated in any way afterwards. the combined implementation of these features represents a pressing and challenging problem, which we pose to the puf hardware community in this work.pufs in security protocols','Hardware-based security protocols'
'anonymity, as an instance of information hiding, is one of the security properties intensively studied nowadays due to its applications to various fields such as electronic voting, electronic commerce, electronic mail, and so on. this paper presents a comprehensive study on minimal anonymity properties in security protocols. in order to reach this objective, an epistemic language and logic to reason about anonymity properties in security protocols, are provided. agent states are endowed with facts derived from actions performed by agents in protocol executions, and an inference system is proposed. to define minimal anonymity, an observational equivalence is used, which is shown to be decidable in deterministic polynomial time. we distinguish between various forms of sender and receiver anonymity with respect to two types of observers: honest agents and the intruder. a large spectrum of relationships between these anonymity concepts is then derived. it is also shown that an anonymous action in a security protocol under a passive intruder might not be anonymous in the same security protocol if the intruder is active, and vice-versa. the decidability and complexity status of the anonymity concepts introduced in the paper is finally investigated. thus, it is shown that minimal anonymity is undecidable in unrestricted security protocols, is nexptime-complete in bounded security protocols, and is np-complete in 1-session bounded security protocols.reasoning about minimal anonymity in security protocols','Hardware-based security protocols'
'in recent years, there have been strong interests in the networking community in designing new internet architectures that provide strong security guarantees. however, none of these proposals back their security claims by formal analysis. in this paper, we use a reduction-based approach to prove the route authenticity property in secure routing protocols. these properties require routes announced by honest nodes in the network not to be tampered with by the adversary. we focus on protocols that rely on layered signatures to provide security: each route announcement is associated with a list of signatures attesting the authenticity of its subpaths. our approach combines manual proofs with automated analysis. we define several reduction steps to reduce proving route authenticity properties to simple conditions that can be automatically checked by the proverif tool. we show that our analysis is correct with respect to the trace semantics of the routing protocols.reduction-based security analysis of internet routing protocols','Hardware-based security protocols'
'a correct security protocol should satisfy four requirements: no-intrusion, authenticity, freshness, and secrecy. we divide the four requirements into two groups (one for no-intrusion, authenticity, and freshness and the other for secrecy). for the former we use the message-exchange forms and for the latter, we use the p-knows and n-knows information that is inferred from a latest topological orders of the trigger-graph. based on these methods, flaws and weaknesses hidden in an imperfect security protocol could be uncovered.requirements for security protocols','Hardware-based security protocols'
'current mainstream work on security protocols usually focuses on achieving one instance of some security property, even when composition of smaller components is used in the design. this paper, instead, advocates special attention to internal components and the environment of security protocols in order to implement extra instances of the same security property. these multiple instances would be able to maintain the security property even if one or more of them failed under the attack of an adversary, providing a degrading path for the protocol assurances instead of the usual catastrophic failure.resiliency aspects of security protocols','Hardware-based security protocols'
'security protocols are small programs that are executed in hostile environments. many results and tools have been developed to formally analyze the security of a protocol. however even when a protocol has been proved secure, there is absolutely no guarantee if the protocol is executed in an environment where other protocols, possibly sharing some common identities and keys like public keys or long-term symmetric keys, are executed. in this paper, we show that whenever a protocol is secure, it remains secure even in an environment where arbitrary protocols are executed, provided each encryption contains some tag identifying each protocol, like e.g. the name of the protocol.safely composing security protocols','Hardware-based security protocols'
'rfid technology continues to flourish as an inherent part of virtually every ubiquitous environment. however, it became clear that the public--implying the industry--seriously needs mechanisms emerging the security and privacy issues for increasing rfid applications. as the nodes of rfid systems mostly suffer from low computational power and small memory size, various attempts which propose to implement the existing security primitives and protocols, have ignored the realm of the cost limitations and failed. in this study, two recently proposed protocols--ssm and lrmap--claiming to meet the standard privacy and security requirements are analyzed. the design of both protocols based on defining states where the server authenticates the tag in constant time in a more frequent normal state and needs a linear search in a rare abnormal states. although both protocols claim to provide untraceability criteria in their design objectives, we outline a generic attack that both protocols failed to fulfill this claim. moreover, we showed that the ssm protocol is vulnerable to a desynchronization attack which prevents a server from authenticating a legitimate tag. resultantly, we conclude that defining computationally unbalanced tag states yields to a security/scalability conflict for rfid authentication protocols.scalability and security conflict for rfid authentication protocols','Hardware-based security protocols'
'we identify privacy, security and performance requirements for radio frequency identification (rfid) protocols, as well as additional functional requirements such as tag ownership transfer. many previously proposed protocols suffer from scalability issues because they require a linear search to identify or authenticate a tag. in support of scalability, some rfid protocols, however, only require constant time for tag identification, but, unfortunately, all previously proposed schemes of this type have serious shortcomings. we propose a novel scalable rfid authentication protocol based on the scheme presented in song and mitchell (2009) [1], that takes constant time to authenticate a tag. we also propose secret update protocols for tag ownership and authorisation transfer. the proposed protocols possess the identified privacy, security and performance properties and meet the requirements for secure ownership transfer identified here.scalable rfid security protocols supporting tag ownership transfer','Hardware-based security protocols'
'the complexity of the initial secrecy problem for protocols in restricted form, with bounded length messages, unbounded number of protocol sessions, bounded existentials, disequality tests and an intruder with existentials remained open.in this paper, we prove that the problem above is undecidable, using exactly the same setting and formalism as in the paper in that the problem was initially stated.to our knowledge, up to now there is no proof of undecidability of the open problem mentioned above that constructs a protocol (well-founded) in restricted form in multiset rewriting formalism.secrecy for bounded security protocols','Hardware-based security protocols'
'in the setting of secure multiparty computation, a set of mutually distrustful parties wish to securely compute some joint function of their private inputs. the computation should be carried out in a secure way, meaning that no coalition of corrupted parties should be able to learn more than specified or somehow cause the result to be &#x201c;incorrect.&#x201d; typically, corrupted parties are either assumed to be semi-honest (meaning that they follow the protocol specification) or malicious (meaning that they may deviate arbitrarily from the protocol). however, in many settings, the assumption regarding semi-honest behavior does not suffice and security in the presence of malicious adversaries is excessive and expensive to achieve. in this paper, we introduce the notion of covert adversaries, which we believe faithfully models the adversarial behavior in many commercial, political, and social settings. covert adversaries have the property that they may deviate arbitrarily from the protocol specification in an attempt to cheat, but do not wish to be &#x201c;caught&#x201d; doing so. we provide a definition of security for covert adversaries and show that it is possible to obtain highly efficient protocols that are secure against such adversaries. we stress that in our definition, we quantify over all (possibly malicious) adversaries and do not assume that the adversary behaves in any particular way. rather, we guarantee that if an adversary deviates from the protocol in a way that would enable it to &#x201c;cheat&#x201d; (meaning that it can achieve something that is impossible in an ideal model where a trusted party is used to compute the function), then the honest parties are guaranteed to detect this cheating with good probability. we argue that this level of security is sufficient in many settings.security against covert adversaries: efficient protocols for realistic adversaries','Hardware-based security protocols'
'an approach to protocol analysis using asynchronous product automata (apa) and the simple homomorphism verification tool (shvt) is demonstrated on several variants of the well known zhou&#x2013;gollmann fair non-repudiation protocol and on two more recent optimistic fair non-repudiation protocols. attacks on all these protocols are presented and an improved version of the zhou&#x2013;gollmann protocol is proposed.security analysis of efficient (un-) fair non-repudiation protocols','Hardware-based security protocols'
'security analysis of network protocols is a rich scientific area with two different foundations, one based on logic and symbolic computation, and one based on computational complexity theory. the symbolic approach has led to formal logics and automated tools that have been used successfully in a number of case studies. the computational approach yields more insight into the strength and vulnerabilities of protocols, but it involves explicit reasoning about probability and computational complexity. ideally, we would like to combine the advantages of both and develop a simple, automatable method that captures intuitive high-level reasoning principles, yet accurately reflects the subtleties of probabilistic polynomial-time computation. this talk will summarize some of the main lines of prior work and discuss ways to bridge the gap between symbolic and computational analysis. a significant portion of the talk will focus on a high-level protocol logic whose provable statements are correct when regarded as assertions about probabilistic polynomial-time protocol execution in the face of probabilistic polynomial-time attack.security analysis of network protocols','Hardware-based security protocols'
'the importance of the electronic health record (ehr), that stores all healthcare-related data belonging to a patient, has been recognised in recent years by governments, institutions and industry. initiatives like the integrating the healthcare enterprise (ihe) have been developed for the definition of standard methodologies for secure and interoperable ehr exchanges among clinics and hospitals. using the requisites specified by these initiatives, many large scale projects have been set up for enabling healthcare professionals to handle patients\' ehrs. the success of applications developed in these contexts crucially depends on ensuring such security properties as confidentiality, authentication, and authorization. in this paper, we first propose a communication protocol, based on the ihe specifications, for authenticating healthcare professionals and assuring patients\' safety. by means of a formal analysis carried out by using the specification language cows and the model checker cmc, we reveal a security flaw in the protocol thus demonstrating that to simply adopt the international standards does not guarantee the absence of such type of flaws. we then propose how to emend the ihe specifications and modify the protocol accordingly. finally, we show how to tailor our protocol for application to more critical scenarios with no assumptions on the communication channels. to demonstrate feasibility and effectiveness of our protocols we have fully implemented them.security analysis of standards-driven communication protocols for healthcare scenarios','Hardware-based security protocols'
'security analysis of the cliques protocols suites','Hardware-based security protocols'
'in this paper, we analyze the security of two recently proposed distance bounding protocols called the \"hitomi\" and the \"nus\" protocols. our results show that the claimed security of both protocols has been overestimated. namely, we show that the hitomi protocol is susceptible to a full secret key disclosure attack which not only results in violating the privacy of the protocol but also can be exploited for further attacks such as impersonation, mafia fraud and terrorist fraud attacks. our results also demonstrates that the probability of success in a distance fraud attack against the nus protocol can be increased up to &lt;inlineequation id=\"ieq1\">&lt;inlinemediaobject&gt;&lt;imageobject fileref=\"978-3-642-25286-0_7_chapter_tex2gif_ieq1.gif\" format=\"gif\" color=\"blackwhite\" type=\"linedraw\" rendition=\"html\"/>&lt;/inlinemediaobject&gt; &lt;equationsource format=\"tex\">$(\\frac{3}{4})^n$&lt;/equationsource&gt; &lt;/inlineequation&gt; and even slightly more, if the adversary is furnished with some computational capabilities.security analysis of two distance-bounding protocols','Hardware-based security protocols'
'in 2005, a. nenadic n. zhang and q. shi proposed a new cryptographic primitive, called verifiable and recoverable encryption of signature vres. based on rsa-based vres, they presented two variant protocols rsa-cemd1 and rsa-cemd2 for certified e-mail delivery with rsa receipts. they claimed that the protocols provided strong fairness to ensure that the recipient receives the e-mail if and only if the sender receives the receipt. later, n. zhang, q. shi, m. merabti, and r. askwith presented a practical and efficient fair document exchange protocol based on a verifiable and recoverable encryption of keys that is somewhat similar to the vres. in this paper, we find that the vres scheme is universal forgeable. anyone can generate the false vres for any message without the knowledge of any private key of the sender, the recipient and the ttp. it follows that the two variant protocols rsa-cemd1, rsa-cemd2 are all insecure. meanwhile, we show that the document exchange protocol is not fair since the verifiable and recoverable encryption of keys is not recoverable.security analysis of two rsa-based fair document exchange protocols','Hardware-based security protocols'
'authentication and key distribution (akd) protocols have become more important in the design of communication systems. the design criteria of the akd protocols include the scalability, the communication efficiency, the computational efficiency, and the robustness of security. in this paper, we first analyze the vulnerability of an akd protocol under the off-line guessing attack. then, we propose an enhanced akd protocol to overcome the vulnerability. security analysis and formal verification by using avispa toolkit show that the proposed protocol can keep all the previous properties and is secure against the off-line guessing attack. copyright &#x00a9; 2012 john wiley & sons, ltd.security enhancement of the communication-efficient authmac_dh protocols','Hardware-based security protocols'
'first page of the articlesecurity in high-level network protocols','Hardware-based security protocols'
'security mechanisms in high-level network protocols','Hardware-based security protocols'
'this paper describes few, popular security mechanisms used by network protocols. many of protocols were designed with assumption that there are no intruders, that every device in the network should trust to other devices and users. such assumption was true many years ago. the network situation has been changed for last ten years and we should realize that some properties of existing protocols may be abused. basic level of trust is necessary, but if this level is too high, standard protocols may be abused. this paper is also an attempt to create some basic rules and requirements which should be met by secure network protocols.security mechanisms in network protocols','Hardware-based security protocols'
'manets (mobile ad hoc networks) allow to deploy networks without any preexisting infrastructure. they are appealing approach for battlefield and emergency networks since they permit to easily interconnect the units that are engaged on the field. an unit is a predefined group of trusted users working together to accomplish a specific task. in this paper, we propose an approach that allows to deal with such predefined groups in manets. our approach consists in dividing the manet in groups of trusted nodes: the nodes of a given group belong to the same predefined group (i.e., the same unit), and communications between them only rely on nodes of this group. by using cryptographicfunctions, we extend the ad hoc routing protocol so as to ensure the security of the communications between the members of a predefined group even if they are scattered on the field.security of predefined groups in manets','Hardware-based security protocols'
'security of public key certificate based authentication protocols','Hardware-based security protocols'
'in the context of dolev-yao style analysis of security protocols, we investigate the security claims of a recently proposed rfid authentication protocol. we exhibit a flaw which has gone unnoticed in rfid protocol literature and present the resulting attacks on authentication, untraceability, and desynchronization resistance. we analyze and discuss the authors\' proofs of security. references to other vulnerable protocols are given.security of rfid protocols -- a case study','Hardware-based security protocols'
'the design of ultra-lightweight authentication protocols is imperative to the pervasive deployment of low-cost rfids. this paper examines the security of two well known ultra-lightweight authentication protocols (lmap and m2ap) and the improved scheme. we demonstrate our efficient attacks on the protocols, and highlight the key weaknesses of the designs.security of ultra-lightweight rfid authentication protocols and its improvements','Hardware-based security protocols'
'the rfid market in europe is growing. it is expected that in 2007 the market exceeds 2.5 billions usd. unfortunately, despite of benefits of the technology some problems remaining. one of them is security. many different attacks have been described in theory and tested practically, but it is difficult to find the perfect secure solution for a wireless unit that has limited computing abilities because of cost constraints. on the way to find the suitable solution, this paper describes different attack types and analyses the ability of two proposed authentication protocols to resist these attacks.security problems of rfid authentication protocols','Hardware-based security protocols'
'organizations need to develop formally analyzed systems in order to achieve well-known formal method benefits. in order to study the security of communication systems, we have developed a methodology for the application of the formal analysis techniques, commonly used in communication protocols, to the analysis of cryptographic protocols. in particular, we have extended the design and analysis phases with security properties. our proposal uses a specification notation based on one of the most used standard requirement languages, hmsc/msc, which can be automatically translated into a generic sdl specification. the sdl system obtained can then be used for the analysis of the addressed security properties, by using an observer process schema. besides our main goal to provide a notation for describing the formal specification of security systems, our proposal also brings additional benefits, such as the study of the possible attacks to the system, and the possibility of re-using the specifications produced to describe and analyze more complex systems.security protocols analysis','Hardware-based security protocols'
'specifications for security protocols range from informal narrations of message flows to formal assertions of protocol properties. this paper (intended to accompany a lecture at etaps \'99) discusses those specifications and suggests some gaps and some opportunities for further work. some of them pertain to the traditional core of the field; others appear when we examine the context in which protocols operate.security protocols and specifications','Hardware-based security protocols'
'wireless communications are being driven by the need for providing network access to mobile or nomadic computing devices. the need for wireless access to a network is evident in current work environments. a number of new protocols have been recently published with the goal of providing both privacy of data and authentication of users for mobile systems. such protocols can employ private-key and/or public key cryptographic algorithms. publickey algorithms hold the promise of simplifying the network infrastructure required to provide security services such as: privacy, authentication and non-repudiation, while symmetric algorithms require less processing power on the mobile device.in this paper a selection of protocols are reviewed and they are broadly divided into two categories: second generation and third generation protocols. a summary of the capabilities and services provided by each protocol is then provided.security protocols for 2g and 3g wireless communications','Hardware-based security protocols'
'security protocols for use with wireless sensor networks','Hardware-based security protocols'
'in this paper, a comprehensive quality of security service (qoss) model for addressing security within a service-oriented architecture (soa) is proposed. we define a detailed soa security model that supports and incorporates a number of networking security techniques and protocols. it utilizes symmetric keys, public keys and hash functions techniques, in order to provide different levels of qoss agreements to satisfy the requirements of both the services providers and requesters. these levels are based on core networking security requirements such as mutual authentication, session keys, anonymity, and perfect forward secrecy. in addition, the proposed model forms a strong line of defense against replay, man-in-the-middle, and denial-of-services attacks.security protocols in service-oriented architecture','Hardware-based security protocols'
'formal methods, theory, and supporting tools can aid the design, analysis, and verification of the security-related and cryptographic protocols used over open networks and distributed systems. the most commonly followed techniques for the application of formal methods for the ex-post analysis and verification of cryptographic protocols, as the analysis approach, are reviewed, followed by the examination of robustness principles and application limitations. modern high-level specification languages and tools can be used for automatically analysing cryptographic protocols. recent research work focuses on the ex-ante use of formal methods in the design state of new security protocols, as the synthesis approach. finally, an outline is presented on current trends for the utilisation of formal methods for the analysis and verification of modern complicated protocols and protocol suites for the real commercial world.security protocols over open networks and distributed systems','Hardware-based security protocols'
'in this paper we present by a case study an approach to the verification of security protocols based on abductive logic programming. we start from the perspective of open multi-agent systems, where the internal architecture of the individual system&#39;s components may not be completely specified, but it is important to infer and prove properties about the overall system behaviour. we take a formal approach based on computational logic, to address verification at two orthogonal levels: &#8216;static&#39; verification of protocol properties (which can guarantee, at design time, that some properties are a logical consequence of the protocol), and &#8216;dynamic&#39; verification of compliance of agent communication (which checks, at runtime, that the agents do actually follow the protocol). in order to explain the approach, we adopt as a running example the well-known needham-schroeder protocol. we first show how the protocol can be specified in our previously developed socs-si framework, and then demonstrate the two types of verification. we also demonstrate the use of the socs-si framework for the static verification of the netbill e-commerce protocol.security protocols verification in abductive logic programming','Hardware-based security protocols'
'when formally analyzing security protocols it is often important to express properties in terms of an adversary\'s inability to distinguish two protocols. it has been shown that this problem amounts to deciding the equivalence of two constraint systems, i.e., whether they have the same set of solutions. in this paper we study this equivalence problem when cryptographic primitives are modeled using a group equational theory, a special case of monoidal equational theories. the results strongly rely on the isomorphism between group theories and rings. this allows us to reduce the problem under study to the problem of solving systems of equations over rings. we provide several new decidability and complexity results, notably for equational theories which have applications in security protocols, such as exclusive or and abelian groups which may additionally admit a unary, homomorphic symbol.security protocols, constraint systems, and group theories','Hardware-based security protocols'
'this paper examines the suitability and use of runtime verification as means for monitoring security protocols and their properties. in particular, we employ the runtime verification framework introduced in [5] to monitor complex, history-based security-properties of the ssl-protocol. we give a detailed account of the methodology, compare its formal expressiveness to prior art, and describe its application to an open-source java-implementation of the sslprotocol. in particular, we show how one can make use of runtime verification to dynamically enforce that assumptions on the crypto-protocol implementations (that are commonly made when statically verifying crypto-protocol specifications against security requirements) are actually satisfied in a given protocol implementation at runtime. our analysis of these properties shows that some important runtime correctness properties of the ssl-protocol exceed the commonly used class of safety properties, and as such also the expressiveness of other monitoring frameworks.security protocols, properties, and their monitoring','Hardware-based security protocols'
'we relate two models of security protocols, namely the linear logic or multiset rewriting model, and the classical logic, horn clause representation of protocols. more specifically, we show that the latter model is an abstraction of the former, in which the number of repetitions of each fact is forgotten. this result formally characterizes the approximations made by the classical logic model.security protocols','Hardware-based security protocols'
'security verification of hardware-enabled attestation protocols','Hardware-based security protocols'
'in this paper, we analyze the protocols of tan, lim et al., chen et al. and five protocols of holbl et al. after the analysis, we found that tan et al.@?s, lim et al.@?s and two protocols of holbl et al. are insecure against the impersonation attack and the man-in-the-middle attack, chen et al.@?s protocol cannot withstand the key-compromise impersonation attack, one protocol of holbl et al. is vulnerable to the insider attack, one allows an adversary to compute the private key of any user and one protocol allows her to compute the shared secret key.security weaknesses of authenticated key agreement protocols','Hardware-based security protocols'
'this paper presents a sound ban-like logic for reasoning about security protocols with theorem prover support. the logic has formulas for sending and receiving messages (with nonces, public and private encryptions, etc.), and has both temporal and epistemic operators (describing the knowledge of participants). the logic\'s semantics is based on strand spaces. several (secrecy or authentication) formulas are proven in general and are applied to the needham-schroeder(-lowe), bilateral key exchange and the otway-rees protocols, as illustrations.semantics and logic for security protocols','Hardware-based security protocols'
'despite much work on sessions and session types in non-adversarial contexts, session-like behavior given an active adversary has not received an adequate definition and proof methods. we provide a syntactic property that guarantees that a protocol has session-respecting executions. any uncompromised subset of the participants are still guaranteed that their interaction will respect sessions. a protocol transformation turns any protocol into a session-respecting protocol. we do this via a general theory of separability. our main theorem applies to different separability requirements, and characterizes when we can separate protocol executions sufficiently to meet a particular requirement. this theorem also gives direct proofs of some old and new protocol composition results. thus, our theory of separability appears to cover protocol composition and session-like behavior within a uniform framework, and gives a general pattern for reasoning about independence.sessions and separability in security protocols','Hardware-based security protocols'
'in this paper we offer a methodology allowing for simulation of security protocols, implemented in the higher-level language estelle, using scenarios designed for external attacks. to this aim we apply a translation of specifications of security protocols from common syntax to estelle and an encoding of schemes of attacks into estelle scenarios. we show that such an intelligent simulation may efficiently serve for validating security protocols.simulation of security protocols based on scenarios of attacks','Hardware-based security protocols'
'security protocols stipulate how the remote principals of a computer network should interact in order to obtain specific security goals. the crucial goals of confidentiality and authentication may be achieved in various forms, each of different strength. using soft (rather than crisp) constraints, we develop a uniform formal notion for the two goals. they are no longer formalised as mere yes&#x002f;no properties as in the existing literature, but gain an extra parameter, the security level. for example, different messages can enjoy different levels of confidentiality, or a principal can achieve different levels of authentication with different principals. the goals are formalised within a general framework for protocol analysis that is amenable to mechanisation by model checking. following the application of the framework to analysing the asymmetric needham-schroeder protocol (bella and bistarelli 2001; bella and bistarelli 2002), we have recently discovered a new attack on that protocol as a form of retaliation by principals who have been attacked previously. having commented on that attack, we then demonstrate the framework on a bigger, largely deployed protocol consisting of three phases, kerberos.soft constraint programming to analysing security protocols','Hardware-based security protocols'
'this paper presents a tool called asm-spv (abstract state machines-security protocols verifier) for verifying security protocols by model checking. in asm-spv, a security protocol is modeled by coreasm language which is an executable asm (abstract state machines) language. then a modified coreasm engine takes the coreasm model of the protocol to build state space on-demand. furthermore, security properties of the protocol are described as ctl (computation tree logic) formulas and an adapted model checking algorithm is introduced to check whether the coreasm model satisfies a given ctl formula or not. in this paper, we show the effectiveness of asm-spv with regard to memory consumption and speed of generating states compared with another coreasm based model checker [mc]square.some improvements on model checking coreasm models of security protocols','Hardware-based security protocols'
'many security protocols have appeared in the literature, with aims such as agreeing upon a cryptographic key, or achieving authentication. however, many of these have been shown to be flawed. in this paper we present a number of new attacks upon security protocols, and discuss ways in which we may avoid designing incorrect protocols in the future.some new attacks upon security protocols','Hardware-based security protocols'
'the logical correctness of security protocols is important. so are efficiency and cost. this paper shows that meta-heuristic search techniques can be used to synthesise protocols that are both provably correct and satisfy various non-functional efficiency criteria. our work uses a subset of the svo logic, which we view as a specification language and proof system and also as a \'\'protocol programming language\'\'. our system starts from a set of initial security assumptions, carries out meta-heuristic search in the design space, and ends with a protocol (described at the logic level) that satisfies desired goals.synthesising efficient and effective security protocols','Hardware-based security protocols'
'in the context of security protocol parallel composition, where messages belonging to different protocols can intersect each other, we introduce a new paradigm: term-based composition (i.e. the composition of message components also known as terms). first, we create a protocol specification model by extending the original strand spaces. then, we provide a term composition algorithm based on which new terms can be constructed. to ensure that security properties are maintained, we introduce the concept of term connections to express the existing connections between terms and encryption contexts. we illustrate the proposed composition process by using two existing protocols.term-based composition of security protocols','Hardware-based security protocols'
'this paper presents an efficient protocol for securely computing the fundamental problem of pattern matching. this problem is defined in the two-party setting, where party p1 holds a pattern and party p2 holds a text. the goal of p1 is to learn where the pattern appears in the text, without revealing it to p2 or learning anything else about p2&#8217;s text. our protocol is the first to address this problem with full security in the face of malicious adversaries. the construction is based on a novel protocol for secure oblivious automata evaluation which is of independent interest. in this problem party p1 holds an automaton and party p2 holds an input string, and they need to decide if the automaton accepts the input, without learning anything else.text search protocols with simulation based security','Hardware-based security protocols'
'the systematic study of security protocols started, as far as the public literature is concerned, almost 20 years ago. a paper by m.d. schroeder and the present writer may be taken as a specimen; it was written in 1977 and published in 1978. it was, of course, written against the background of the technology of the time and made various assumptions about the organizational context in which its techniques would be used. the substantial research literature that has since appeared has, by and large, made similar assumptions about the technological organizational environments. those environments have in fact changed very considerably, and the purpose of this article is to consider whether the changes should affect our approach to security problems. it turns out that where confidentiality is concerned, as distinct from authenticity and integrity, there is indeed a new range of optionsthe changing environment for security protocols','Hardware-based security protocols'
'despite the importance of proofs in assuring protocol implementers about the security properties of key establishment protocols, many protocol designers fail to provide any proof of security. flaws detected long after the publication and/or implementation of protocols will erode the credibility of key establishment protocols. we revisit recent work of choo, boyd, hitchcock, maitland where they utilize the bellare, pointcheval, rogaway (authenticated key exchange secure against dictionary attacks, in: b. preneel (ed.), advances in cryptology - eurocrypt 2000, springer-verlag, lncs 1807/2000, pp. 139-155, 2000) computational complexity proof model in a machine specification and analysis (using an automated model checker - shvt) for provably secure key establishment protocol analysis. we then examine several key establishment protocols without proofs of security, namely: protocols due to j.-k. jan, y.-h. chen (a new efficient makep for wireless communications, in: 18th international conference on advanced information networking and applications - aina 2004, ieee computer society, pp. 347-350, 2004), w.-h. yang, j.-c. shen, s.-p. shieh (designing authentication protocols against guessing attacks. technical report 2(3), institute of information & computing machinery, taiwan, 1999. http://www.iicm.org.tw/communication/c2_3/page07.doc), y.-s. kim, e.-n. huh, j. hwang, b.-w. lee (an efficient key agreement protocol for secure authentication, in: a. lagana, m.l. gavrilova, v. kumar, y. mun, c.j.k. tan, o. gervasi (eds.), international conference on computational science and its applications - iccsa 2004, springer-verlag, lncs 3043/2004, pp. 746-754, 2004), c.-l. lin, h.-m. sun, t. hwang. (three-party encrypted key exchange: attacks and a solution, in: a cm sigops operating systems review, pp. 12-20, 2000), and h.-t. yeh, h.-m. sun (simple authenticated key agreement protocol resistant to password guessing attacks, in: a cm sigops operating systems review, 36(4), pp. 14-22, 2002). using these protocols as case studies, we demonstrate previously unpublished flaws in these protocols. we may speculate that such errors could have been found by protocol designers if proofs of security were to be constructed, and hope this work will encourage future protocol designers to provide proofs of security.the importance of proofs of security for key establishment protocols','Hardware-based security protocols'
'a new approach to verification of timed security protocols is given. the idea consists in modelling a finite number of users (including an intruder) of the computer network and their knowledge about secrets by timed automata. the runs of the product automaton of the above automata correspond to all the behaviours of the protocol for a fixed number of sessions. verification is performed using the module bmc of the tool verics.timed automata based model checking of timed security protocols','Hardware-based security protocols'
'we propose a method for engineering security protocols that are aware of timing aspects. we study a simplified version of the well-known needham schroeder protocol and the complete yahalom protocol. timing information allows us to study of different attack scenarios. we illustrate the attacks by model checking the protocol using uppaal.  we also present new challenges and threats that arise when considering time.timed model checking of security protocols','Hardware-based security protocols'
'computer and network security researchers usually focus on the security of computers and networks. although it might seem as if there is more than enough insecurity here to keep all of us fully occupied for the foreseeable future, this narrow view of our domain may actually be contributing to the very problems that we are trying to solve. we miss important insights from, and opportunities to make contributions to, a larger world that has been grappling with security since long before the computer was invented.toward a broader view of security protocols','Hardware-based security protocols'
'we present a model-theoretic approach for reasoning about security protocols, applying recent insights from dynamic epistemic logics. this enables us to describe exactly the subsequent epistemic states of the agents participating in the protocol, using kripke models and transitions between them based on updates of the agents\' beliefs associated with steps in the protocol. as a case study we will consider the sra three pass protocol.toward reasoning about security protocols','Hardware-based security protocols'
'we&#39;ve been looking at what we call the security protocol negotiation problem. people who use security protocols to authenticate, do key exchange, or whatever, are typically stuck with using whatever protocol is available. if the participants can&#39;t agree on a suitable protocol, then they don&#39;t get to use each other&#39;s services. we are interested in self-configuring security protocols where the participants have to say what their requirements are and synthesise or con.gure their own suitable protocol that would meet their respective goals. if we have this idea of self-configuring security protocols then, as a consequence, we also have selfhealing security protocols. if the requirements change, or if the assumptions about our principals and keys change, then we have this ability to recon.gure and produce a security protocol that will meet those new goals, based on those assumptions.towards a framework for autonomic security protocols','Hardware-based security protocols'
'security protocols form a central part of the trust infrastructure of the online world. they allow principals to make decisions that authorize or prohibit actions of other principals, and to make those decisions based on information gathered from other principals. in this invited talk at socio-technical aspects in security and trust 2012 (stast), i described a view of protocol design that can serve as a trust infrastructure.trust engineering via security protocols','Hardware-based security protocols'
'trust management and network layer security protocols','Hardware-based security protocols'
'radio frequency identification (rfid) systems are widely used in access control, transportation, real-time inventory and asset management, automated payment systems, etc. nevertheless, the use of this technology is almost unexplored in healthcare environments, where potential applications include patient monitoring, asset traceability and drug administration systems, to mention just a few. rfid technology can offer more intelligent systems and applications, but privacy and security issues have to be addressed before its adoption. this is even more dramatical in healthcare applications where very sensitive information is at stake and patient safety is paramount. in wu et al. (j. med. syst. 37:19, 43) recently proposed a new rfid authentication protocol for healthcare environments. in this paper we show that this protocol puts location privacy of tag holders at risk, which is a matter of gravest concern and ruins the security of this proposal. to facilitate theimplementation of secure rfid-based solutions in the medical sector, we suggest two new applications (authentication and secure messaging) and propose solutions that, in contrast to previous proposals in this field, are fully based on iso standards and nist security recommendations.two rfid standard-based security protocols for healthcare environments','Hardware-based security protocols'
'security protocols are indispensable in secure communication. we give an operational semantics of security protocols in terms of a prolog-like language. with this semantics, we can uncover attacks on a security protocol that are possible with no more than a given number of rounds. though our approach is exhaustive testing, the majority of fruitless search is cut off by selecting a small number of representative values that could be sent by an attacker. hence, the number of scenarios is relatively small and our method is quite practical. furthermore, our method not only reports possible attacks but also describes the attacks in great detail. this description would be very helpful to protocol designers and analyzers.uncovering attacks on security protocols','Hardware-based security protocols'
'user-friendly grid security architecture and protocols','Hardware-based security protocols'
'verification of network security protocols','Hardware-based security protocols'
'we explain how the formal language lotos can be used to specify security protocols and cryptographic operations. we describe how security properties can be modelled as safety properties and how a model-based verification method can be used to verify the robustness of a protocol against attacks of an intruder. we illustrate our technique on a concrete registration protocol. we find an attack, correct the protocol, propose a simpler yet secure protocol, and finally a more sophisticated protocol that allows a better discrimination between intruder\'s attacks and classical protocol errors.verification of security protocols using lotos-method and application','Hardware-based security protocols'
'we present a novel, simple technique for proving secrecy properties for security protocols that manipulate lists of unbounded length, for an unbounded number of sessions. more specifically, our technique relies on the horn clause approach used in the automatic verifier proverif: we show that if a protocol is proven secure by our technique with lists of length one, then it is secure for lists of unbounded length. interestingly, this theorem relies on approximations made by our verification technique: in general, secrecy for lists of length one does not imply secrecy for lists of unbounded length. our result can be used in particular to prove secrecy properties for group protocols with an unbounded number of participants and for some xml protocols (web services) with proverif.verification of security protocols with lists','Hardware-based security protocols'
'security protocols are short programs aiming at securing communications over a network. they are widely used in our everyday life. they may achieve various goals depending on the application: confidentiality, authenticity, privacy, anonymity, fairness, etc. their verification using symbolic models has shown its interest for detecting attacks and proving security properties. a famous example is the needham-schroeder protocol [23] on which g. lowe discovered a flaw 17 years after its publication [20]. secrecy preservation has been proved to be co-npcomplete for a bounded number of sessions [24], and decidable for an unbounded number of sessions under some additional restrictions (&lt;em&gt;e&lt;/em&gt; .&lt;em&gt;g&lt;/em&gt; . [3,12,13,25]). many tools have also been developed to automatically verify cryptographic protocols like [8,21]. verification of security protocols','Hardware-based security protocols'
'we present an architecture and tools for verifying implementations of security protocols. our implementations can run with both concrete and symbolic implementations of cryptographic algorithms. the concrete implementation is for production and interoperability testing. the symbolic implementation is for debugging and formal verification. we develop our approach for protocols written in f&num;, a dialect of ml, and verify them by compilation to proverif, a resolution-based theorem prover for cryptographic protocols. we establish the correctness of this compilation scheme, and we illustrate our approach with protocols for web services security.verified interoperable implementations of security protocols','Hardware-based security protocols'
'we describe a new reference implementation of the web services security specifications. the implementation is structured as a library in the functional programming language f#. applications written using this library can interoperate with other compliant web services, such as those written using microsoft wse and wcf frameworks. moreover, the security of such applications can be automatically verified by translating them to the applied pi calculus and using an automated theorem prover. we illustrate the use of our reference implementation through examples drawn from the sample applications included with wse and wcf. we formally verify their security properties. we also experimentally evaluate their interoperability and performance.verified reference implementations of ws-security protocols','Hardware-based security protocols'
'we propose a technique for verifying high-level security properties of cryptographic protocol implementations based on stepwise refinement. our refinement strategy supports reasoning about abstract protocol descriptions in the symbolic model of cryptography and gradually concretizing them towards executable code. we have implemented the technique within a general-purpose program verifier vcc and applied it to an extract from a draft reference implementation of trusted platform module, written in c.verifying implementations of security protocols by refinement','Hardware-based security protocols'
'in this paper we show a novel method for modelling behaviours of security protocols using networks of communicating automata in order to verify them with sat-based bounded model checking. these automata correspond to executions of the participants as well as to their knowledge about letters. given a bounded number of sessions, we can verify both correctness or incorrectness of a security protocol proving either reachability or unreachability of an undesired state. we exemplify all our notions on the needham schroeder public key authentication protocol (nspk) and show experimental results for checking authentication using the verification tool verics.verifying security protocols modelled by networks of automata','Hardware-based security protocols'
'due to the rapid growth of the &#8220;internet&#8221; and the &#8220;world wide web&#8221; security has become a very important concern in the design and implementation of software systems. since security has become an important issue, the number of protocols in this domain has become very large. these protocols are very diverse in nature. if a software architect wants to deploy some of these protocols in a system, they have to be sure that the protocol has the right properties as dictated by the requirements of the system. in this article we present brutus, a tool for verifying properties of security protocols. this tool can be viewed as a special-purpose model checker for security protocols. we also present reduction techniques that make the tool efficient. experimental results  are provided to demonstrate the efficiency of brutus.verifying security protocols with brutus','Hardware-based security protocols'
'the field of protocol analysis is one area in which csp has proven particularly successful, and several techniques have been proposed that use csp to reason about security properties such as confidentiality and authentication. in this paper we describe one such approach, based on theorem-proving, that uses the idea of a rank function to establish the correctness of protocols. this description is motivated by the consideration of a simple, but flawed, authentication protocol. we show how a rank function analysis can be used to locate this flaw and prove that a modified version of the protocol is correct.verifying security protocols','Hardware-based security protocols'
'jxta defines a set of six core protocols specifically suited for ad hoc, pervasive, multi&#45;hop, peer&#45;to&#45;peer &#40;p2p&#41; computing. these protocols allow peers to cooperate and form autonomous peer groups. this paper presents a satisfactory method that provides security services to the core protocols&#58; privacy, authenticity, integrity and non&#45;repudiation. the presented mechanisms are fully distributed and based on a pure peer&#45;to&#45;peer model, not requiring the arbitration of a trusted third party or a previously established trust relationship between peers, which is one of the main challenges under this kind of environments.xml&#45;based security for jxta core protocols','Hardware-based security protocols'
'blind signatures allow a signer to digitally sign a document without being able to glean any information about the document. in this paper, we investigate the symmetric analog of blind signatures, namely blind message authentication codes (blind macs). one may hope to get the same efficiency gain from blind mac constructions as is usually obtained when moving from asymmetric to symmetric cryptosystems. our main result is a negative one however: we show that the natural symmetric analogs of the unforgeability and blindness requirements cannot be simultaneously satisfied. faced with this impossibility, we show that blind macs do exist (under the one-more rsa assumption in the random oracle model) in a more restrictive setting where users can share common state information. our construction, however, is only meant to demonstrate the existence; it uses an underlying blind signature scheme, and hence does not achieve the desired performance benefits. the construction of an efficient blind mac scheme in this restrictive setting is left as an open problem.*a study of blind message authentication codes','Hash functions and message authentication codes'
'we propose and investigate the notion of aggregate message authentication codes (macs) which have the property that multiple mac tags, computed by (possibly) different senders on multiple (possibly different) messages, can be aggregated into a shorter tag that can still be verified by a recipient who shares a distinct key with each sender. we suggest aggregate macs as an appropriate tool for authenticated communication in mobile ad-hoc networks or other settings where resource-constrained devices share distinct keys with a single entity (such as a base station), and communication is an expensive resource.aggregate message authentication codes','Hash functions and message authentication codes'
'this paper introduces approximate image message authentication codes (imacs) for soft image authentication. the proposed approximate imac survives small to moderate image compression and it is capable of detecting and locating tampering. techniques such as block averaging and smoothing, parallel approximate message authentication code (amac) computation, and image histogram enhancement are used in the construction of the approximate imac. the performance of the approximate imac in three image modification scenarios, namely, jpeg compression, deliberate image tampering, and additive gaussian noise, is studied and compared. simulation results are presentedapproximate image message authentication codes','Hash functions and message authentication codes'
'approximate message authentication code (amac) is a recently introduced cryptographic primitive with several applications in the areas of cryptography and coding theory. briefly speaking, amacs represent a way to provide data authentication that is tolerant to acceptable modifications of the original message. although constructs had been proposed for this primitive, no security analysis or even modeling had been done. in this paper we propose a rigorous model for the design and security analysis of amacs. we then present two amac constructions with desirable efficiency and security properties. amac is a useful primitive with several applications of different nature. a major one, that we study in this paper, is that of entity authentication via biometric techniques or passwords over noisy channels. we present a formal model for the design and analysis of biometric entity authentication schemes and show simple and natural constructions of such schemes starting from any amac.approximate message authentication and biometric entity authentication','Hash functions and message authentication codes'
'cryptanalysis of message authentication codes','Hash functions and message authentication codes'
'differential attack on message authentication codes','Hash functions and message authentication codes'
'aggregate message authentication codes, as introduced by katz and lindell (ct-rsa 2008), combine several macs into a single value, which has roughly the same size as an ordinary mac. these schemes reduce the communication overhead significantly and are therefore a promising approach to achieve authenticated communication in mobile ad-hoc networks, where communication is prohibitively expensive. here we revisit the unforgeability notion for aggregate macs and discuss that the definition does not prevent \"mix-and-match\" attacks in which the adversary turns several aggregates into a \"fresh\" combination, i.e., into a valid aggregate on a sequence of messages which the attacker has not requested before. in particular, we show concrete attacks on the previous scheme. to capture the broader class of combination attacks, we provide a stronger security notion of aggregation unforgeability. while we can provide stateful transformations lifting (non-ordered) schemes to meet our stronger security notion, for the statefree case we switch to the new notion of history-free sequential aggregation. this notion is somewhat between non-ordered and sequential schemes and basically says that the aggregation algorithm is carried out in a sequential order but must not depend on the preceding messages in the sequence, but only on the shorter input aggregate and the local message. we finally show that we can build an aggregation-unforgeable, history-free sequential mac scheme based on general assumptions.history-free aggregate message authentication codes','Hash functions and message authentication codes'
'the use of cryptographic hash functions like md5 or sha-1 for message authentication has become a standard approach in many applications, particularly internet security protocols. though very easy to implement, these mechanisms are usually based on ad hoc techniques that lack a sound security analysis. we present new, simple, and practical constructions of message authentication schemes based on a cryptographic hash function. our schemes, nmac and hmac, are proven to be secure as long as the underlying hash function has some reasonable cryptographic strengths. moreover we show, in a quantitative way, that the schemes retain almost all the security of the underlying hash function. the performance of our schemes is essentially that of the underlying hash function. moreover they use the hash function (or its compression function) as a black box, so that widely available library code or hardwair can be used to implement them in a simple way, and replaceability of the underlying hash function is easily supported.keying hash functions for message authentication','Hash functions and message authentication codes'
'we design an efficient mode of operation on block ciphers, ss-nmac. our mode has the following properties, when instantiated with a block cipher f to yield a variable-length, keyed hash function h:    (1)  mac preservation.  h is a secure message authentication code (mac) with birthday security, as long as f is unpredictable.    (2)  prf preservation.  h is a secure pseudorandom function (prf) with birthday security, as long as f is pseudorandom.    (3)  security against side-channels. as long as the block cipher f does not leak side-channel information about its internals to the attacker, properties (1) and (2) hold even if the remaining implementation of h is completely leaky. in particular, if the attacker can learn the transcript of all block cipher calls and other auxiliary information needed to implement our mode of operation. our mode is the first to satisfy the mac preservation property (1) with birthday security, solving the main open problem of dodis et al. [7] from eurocrypt 2008. combined with the prf preservation (2), our mode provides a hedge against the case when the block cipher f is more secure as a mac than as a prf: if it is false, as we hope, we get a secure variable-length prf; however, even if true, we still \"salvage\" a secure mac, which might be enough for a given application.we also remark that no prior mode of operation offered birthday security against side channel attacks, even if the block cipher was assumed pseudorandom.although very efficient, our mode is three times slower than many of the prior modes, such as cbc, which do not enjoy properties (1) and (3). thus, our work motivates further research to understand the gap between unpredictability and pseudorandomness of the existing block ciphers, such as aes.message authentication codes from unpredictable block ciphers','Hash functions and message authentication codes'
'in this paper, we propose classes of message authentication codes (mac) based on error correcting-codes. we introduce a new notion of error tolerant forgery of hash messages. these macs allow full error recovery for all applications, while being error-tolerant for less information-sensitive applications. the classes of the keyed hash functions are highly secure, and provide the capabilities of correcting errors on transmission, including burst-errors, which is a typical phenomenon in wireless communications. these classes of hash functions are easily implementable in hardware by means of simple linear feedback shift register structures.message authentication codes with error correcting capabilities','Hash functions and message authentication codes'
'we show that toeplitz matrices generated by sequences drawn from small biased distributions provide hashing schemes applicable to secure message authentication. this work extends our previous results from crypto\'94 [4] where an authentication scheme based on toeplitz matrices generated by linear feedback shift registers was presented. our new results have as special case the lfsr-based construction but extend to a much wider and general family of sequences, including several simple and efficient constructions with close to optimal security. examples of the new constructions include toeplitz matrices generated by the legendre symbols of consecutive integers modulo a prime (of size significantly shorter than required by public-key modular arithmetic) as well as other algebraic constructions. the interest of these schemes extends beyond the proposed cryptographic applications to other uses of universal hashing (including other cryptographic applications).new hash functions for message authentication','Hash functions and message authentication codes'
'blind signatures allow a signer to digitally sign a document without being able to glean any information about the document. in this paper, we investigate the symmetric analog of blind signatures, namely blind message authentication codes (blind macs). one may hope to get the same efficiency gain from blind mac constructions as is usually obtained when moving from asymmetric to symmetric cryptosystems. our main result is a negative one however: we show that the natural symmetric analogs of the unforgeability and blindness requirements cannot be simultaneously satisfied. faced with this impossibility, we show that blind macs do exist (under the one-more rsa assumption in the random oracle model) in a more restrictive setting where users can share common state information. our construction, however, is only meant to demonstrate the existence; it uses an underlying blind signature scheme, and hence does not achieve the desired performance benefits. the construction of an efficient blind mac scheme in this restrictive setting is left as an open problem.on the (im)possibility of blind message authentication codes','Hash functions and message authentication codes'
'the security of iterated message authentication code (mac) algorithms is considered, and in particular, those constructed from unkeyed hash functions. a new mac forgery attack applicable to all deterministic iterated mac algorithms is presented, which requires on the order of 2n/2 known text-mac pairs for algorithms with n bits of internal memory, as compared to the best previous general attack which required exhaustive key search. a related key-recovery attack is also given which applies to a large class of mac algorithms including a strengthened version of cbc-mac found in ansi x9.19 and iso/iec 9797, and envelope mac techniques such as &amp;ldquo;keyed md5&amp;rdquo;. the security of several related existing macs based directly on unkeyed hash functions, including the secret prefix and secret suffix methods, is also examinedon the security of iterated message authentication codes','Hash functions and message authentication codes'
'in this paper, we present two classes of optimal authentication codes without secrecy from difference balanced functions. the new codes are as good as or have more flexible parameters than the optimal codes from perfect nonlinear functions.optimal authentication codes from difference balanced functions','Hash functions and message authentication codes'
'it is known that message authentication codes are extremely sensitive to any change of the message they are appended to. even one or more bits of the changed message invert about 50\% of bits of message authentication codes, making in such a way the message useless. the hard condition for the successful verification of message authentication codes is that all bits of the received message authentication code and that one recalculated of the received message have to be equal. this condition for the successful verification of messages protected by message authentication codes is not suitable in many applications. therefore an introduction of a softer condition for the successful verification would enable the correction and improvement of the verification rate of messages corrupted by transmission over a noisy channel. the following paper is based on an algorithm for soft verification which introduces robustness into the verification of messages protected by message authentication codes together with a correction of messages corrupted due to the noisy channel. a soft output of the algorithm gives information about the reduction of the security level of the successful authentication to the source decoder. this information can be compared to reliability values as soft output of siso channel decoder.reliability of soft verification of message authentication codes','Hash functions and message authentication codes'
'side channel attacks are a serious menace to embedded devices with cryptographic applications which are utilized in sensor and ad hoc networks. in this paper we show that side channel attacks can be applied to message authentication codes, even if the countermeasure is applied to the underlying block cipher. in particular, we show that emac, omac, and pmac are vulnerable to our attack. based on simple power analysis, we show that several key bits can be extracted, and based on differential power analysis, we present selective forgery against these macs. our results suggest that protecting block ciphers against side channel attacks is not sufficient, and countermeasures are needed for macs as well.side channel attacks on message authentication codes','Hash functions and message authentication codes'
'stream processing architectures have been proposed as efficient and flexible platforms for network packet processing. as part of an investigation into stream-based network processors, we have implementedmmh, a family of almostuniversal hash functions for message authentication, on a simd stream processor (imagine). the hash computation over an entire packet is a good fit for the stream programming model, with an abundance of producer-consumer locality: hash values are computed and stored in the stream register file (srf), then used for calculating new hash values repeatedly. by using eight vliw clusters, the construction is performed in a multi-simdfashion, achieving multi- gigabit-per-second throughput with a collision probability on the order of 2^-120 .stream-based implementation of hash functions for multi-gigabit message authentication codes','Hash functions and message authentication codes'
'chaos functions are mainly used to develop mathematical models of non-linear systems. they have attracted the attention of many mathematicians owing to their extremely sensitive nature to initial conditions and their immense applicability to problems in daily life. in this paper, two widely used chaos functions the logistic equation and the lorenz equation are taken and analyzed. we found that these equations exhibit characteristics, which satisfy the expected properties of message authentication code (mac). we also provide a novel approach of generating mac with higher security but with smaller key size. this is achieved by the design of the algorithm using variable initialization vectors (iv) instead of the constant ivs. variable iv adds strength to the security of the message. the experimental results show that it satisfies the expected characteristics of a message authentication code generation algorithm.study of chaos functions for their suitability in generating message authentication codes','Hash functions and message authentication codes'
'recently, highly nonlinear functions have been successfully employed to construct authentication codes with and without secrecy. in this paper, we construct four classes of systematic authentication codes from perfect nonlinear functions and almost-perfect nonlinear functions. the systematic authentication codes presented in this paper are either better than existing codes or as good as the best codes known.systematic authentication codes from highly nonlinear functions','Hash functions and message authentication codes'
'in this paper, we study the application of universal hashing to the construction of unconditionally secure authentication codes without secrecy. this idea is most useful when the number of authenticators is exponentially small compared to the number of possible source states (plaintext messages). we formally define some new classes of hash functions and then prove some new bounds and give some general constructions for these classes of hash functions. then we discuss the implications to authentication codes.universal hashing and authentication codes','Hash functions and message authentication codes'
'10 ways to control dialog alert costs','Information accountability and usage control'
'accountability and fairness are very important properties of electronic commerce security protocols. some protocols are found that cannot achieve their desired security goals. in order to remedy the flaws of the cmp1 protocol, the paper proposes a new fair accountability protocol by adopting cryptographic hash function operation and introducing ftp transmission method. the proposed protocol is verified to satisfy accountability and fairness by utilizing the improved kailar logic. the performance analysis of the proposed protocol shows that it is better in confidentiality and usability than the cmp1 protocol. it can be widely used in electronic commerce applications, such as e-mail and digital contract signing, and so on.a fair accountability protocol and its security properties analysis','Information accountability and usage control'
'a project accountability chart (pac)','Information accountability and usage control'
'accountability and automation bias','Information accountability and usage control'
'are designers responsible for all of the uses of the systems they create?accountability and computer decision systems','Information accountability and usage control'
'the standard technical approach to privacy and security in online life is preventive: before someone can access confidential data or take any other action that implicates privacy or security, he should be required to prove that he is authorized to do so. as the scale and complexity of online activity has grown, it has become apparent that the preventive approach is inadequate; thus, a growing set of information-security researchers has embraced greater reliance on accountability mechanisms to complement preventive measures. despite widespread agreement that \"accountability\" is important in online life, the term has no standard definition. we make three contributions to the study of accountability: (1) we flesh out with realistic examples our claim that a purely preventive approach to security is inadequate; (2) we present, compare, and contrast some existing formal frameworks for accountability; (3) we explore the question of whether \"deterrence\" may be a better general term in this context than \"accountability.\"accountability and deterrence in online life','Information accountability and usage control'
'accountability and discipline','Information accountability and usage control'
'citizens across the world are constantly innovating new mechanisms of holding their governments accountable for their actions. one such mechanism that is gaining popularity in developing countries is e-governance. using two case studies of online public grievance redressal mechanism (opgrm), namely online complaint management system (ocms) in the city of mumbai and egovernments foundation\'s public grievance redressal mechanism (egov-pgr) operational in the city of bangalore, this paper seeks answer to the questions -- can accountability be mainstreamed in e-governance initiatives? can citizens use e-governance initiatives to hold government officials accountable? an extensive review of literature including evaluation reports, official documents, webpages, and news reports were undertaken in order to situate each of these cases in relation to the theoretical framework. interview with government officials using this system were also interviewed. this paper concludes that there are definite improvements in the answerability element of accountability such as time-bound redressal of complaints, better internal accountability through automatic escalation, clear performance standards and easier access to information. however, enforcement aspect of accountability is very much lacking due to the absence of a performance or incentive based system of these bureaucracies and insufficient monitoring by other accounting agencies. for policy makers and systems architects, this paper recommends the strengthening of horizontal accountability mechanisms, increase in transparency of complaint related information to general public and aligning e-governance with larger public management reforms as key factors that could improve the use of e-governance to strengthen accountability relationship between citizens and the public service providers.accountability and the new media','Information accountability and usage control'
'accountability and the public trust','Information accountability and usage control'
'classroom response devices, such as clickers, have proved effective in improving student engagement during class time. we performed a study to investigate how much of this improvement was due to heightened accountability, either because students were required to take and pass a pre-quiz over the lecture material, or because students were given credit for each answer submitted. we found that the presence of a pre-quiz was associated with a much higher response rate, 38.5\% vs. 29.3\%. giving credit for answering questions also boosted the response rate, from 30.3\% to 43.2\%. we also found that asking more questions during class tended was associated with a lower response rate. when only one question was asked, the response rate was above 60\%, but if more than five questions were asked, the response rate was barely 30\%. these findings suggest that accountability is important in making effective use of classroom response devices.accountability and the use of classroom response devices','Information accountability and usage control'
'an overview of the new workshop on accountability and traceability in global software engineering (atgse2007) will be introduced here.accountability and traceability in global software engineering (atgse2007)','Information accountability and usage control'
'cooperation and trust play an increasingly important role in today\'s information systems. for instance, peer-to-peer systems like bittorrent, sopcast and skype are powered by resource contributions from participating users, federated systems like the internet have to respect the interests, policies and laws of participating organizations and countries; in the cloud, users entrust their data and computation to third-part infrastructure. in this talk, we consider accountability as a way to facilitate transparency and trust in cooperative systems. we look at practical techniques to account for the integrity of distributed, cooperative computations, and look at some of the difficulties and open problems in accountability.accountability and trust in cooperative information systems','Information accountability and usage control'
'cloud and it service providers should act as responsible stewards for the data of their customers and users. however, the current absence of accountability frameworks for distributed it services makes it difficult for users to understand, influence and determine how their service providers honour their obligations. the a4cloud project will create solutions to support users in deciding and tracking how their data is used by cloud service providers. by combining methods of risk analysis, policy enforcement, monitoring and compliance auditing with tailored it mechanisms for security, assurance and redress, a4cloud aims to extend accountability across entire cloud service value chains, covering personal and business sensitive information in the cloud.accountability for cloud and other future internet services','Information accountability and usage control'
'in most commercial and legal transactions, the ability to hold individuals or organizations accountable for transactions is important. hence, electronic protocols that implement commercial transactions must be designed to provide adequate accountability assurances for transacting parties. without such assurances, electronic transactions can be susceptible to disputes. currently, protocol design for electronic commerce is done in an ad-hoc manner, a technique which has been shown to be error-prone by past experience with key distribution protocols [4]. despite the importance of accountability in electronic commerce, and the subtlety of designing error-free protocols, currently, there are no analysis methods to examine whether a protocol design conforms to the accountability goals of the transaction that it implements. since most current protocol analysis methods have been developed to analyze key management protocols, they focus on properties such as message replay detection, and key origin authentication (e.g., [20], [4], [11], [14], [18]).in this paper, a new framework is proposed for the analysis of communication protocols that require accountability, such as those for electronic commerce. this framework can be used to analyze protocol designs to detect accountability (or lack thereof). arguments are presented to show that a heretofore unexplored property \"provability\" is pertinent to examining the potential use of communication protocols in the context of litigation, and in the context of audit. a set of postulates which are applicable to the analysis of proofs in general and the proofs of accountability in particular, are proposed. the proposed approach is more natural for the analysis of accountability than the existing belief logics (e.g., [4]) that have been used in the past for the analysis of key distribution protocols. some recently proposed protocols for electronic commerce and public-key delegation are analyzed to illustrate the use of the new analysis framework in detecting (and suggesting remedies for eliminating) their lack of accountability, and in detecting and eliminating redundancies.accountability in electronic commerce protocols','Information accountability and usage control'
'many cryptographic tasks and protocols, such as non-repudiation, contract-signing, voting, auction, identity-based encryption, and certain forms of secure multi-party computation, involve the use of (semi-)trusted parties, such as notaries and authorities. it is crucial that such parties can be held accountable in case they misbehave as this is a strong incentive for such parties to follow the protocol. unfortunately, there does not exist a general and convincing definition of accountability that would allow to assess the level of accountability a protocol provides. in this paper, we therefore propose a new, widely applicable definition of accountability, with interpretations both in symbolic and computational models. our definition reveals that accountability is closely related to verifiability, for which we also propose a new definition. we prove that verifiability can be interpreted as a weak form of accountability. our findings on verifiability are of independent interest. as a proof of concept, we apply our definitions to the analysis of protocols for three different tasks: contract-signing, voting, and auctions. our analysis unveils some subtleties and unexpected weaknesses, showing in one case that the protocol is unusable in practice. however, for this protocol we propose a fix to establish a reasonable level of accountability.accountability','Information accountability and usage control'
'accounting and accountability for distributed and grid systems','Information accountability and usage control'
'the use of icts in the public sector has long been touted for its potential to transform the institutions that govern and provide social services. the focus, however, has largely been on systems that are used within particular scales of the public sector, such as at the scale of state or national government, the scale of regional or municipal entity, or at the scale of local service providers. the work presented here takes aim at examining ict use that crosses these scales of influence and accountability. we report on a year long ethnographic investigation conducted at a variety of social service outlets to understand how a shared information system crosses the boundaries of these very distinct organizations. we put forward that such systems are central to the work done in the public sector and represent a class of collaborative work that has gone understudied.across boundaries of influence and accountability','Information accountability and usage control'
'user identification and session identification are two major steps in preprocessing web log data for web usage mining. this paper introduces a fast active user-based user identification algorithm with time complexity o(n). the algorithm uses both an ip address and a finite users\' inactive time to identify different users in the web log. website ontology is useful for identifying website structure and break points for browsing behavior. for session identification, we present an ontology-based method that utilizes the website structure and functionalities to identify different sessions.active user-based and ontology-based web log data preprocessing for web usage mining','Information accountability and usage control'
'governmental service provisioning is increasingly organized around networks of agencies. these agencies have various objectives and a variety of heterogeneous types of information systems. each agency can independently design (a set of) subsystems that should function together to provide a service. often the collaborating organizations have various degrees of technology-readiness and different objectives and interests. the service executing depends on the weakest subsystem and total network performance need to be accounted for. furthermore, network arrangements are likely to change and evolve over time and need to adapt to changing circumstances, whereas, the entire network\'s performance is dependent on the performance of the individual departments. in this paper we investigate adaptability and accountability aspects of information systems in networks of organizations. adaptability is the ability to deal with new environmental conditions. accountability is the answerability for one\'s actions or inactions and to be responsible for their consequences. adaptability requires a loosely coupled structure, which can seriously hamper accountability. accountability requires tight integration and clear processes. we analyzed the adaptability and accountability aspects in two case studies. the case studies demonstrate that both requirements are not necessarily conflicting and can be complimentary. more research is necessary on designing architectures which are able to meet the adaptability and accountability requirements at the same time.adaptability and accountability of information architectures in interorganizational networks','Information accountability and usage control'
'the secure interaction between two or more administrative domains is a major concern. irbac2000 is a model that quickly establishes a flexible policy for dynamic role translation from foreign domains to local. a-irbac2000 mode utilizes rbac to manage dynamic role translation between foreign and local domains. we will see that these mechanisms have significant shortcomings. we propose an improved administrative usage control model named aucon to overcome the weakness of previous models. aucon provides administrates userrole assignment for local and foreign domain with unified method. it provides flexible enough mechanism to distinguish users of foreign and local domain and can enforce more strict control for foreign user. while retaining the advantage of traditional rbac model, aucon model is being implemented in experiment system.administrative usage control model for secure interoperability','Information accountability and usage control'
'recent work has studied real-life social and usage datasets from educational applications, highlighting the opportunity to combine or merge them. it is expected that being able to put together different datasets from various applications will make it possible to support learning analytics of a much larger scale and across different contexts. we examine how this can be achieved from a practical perspective by carrying out a study that focuses on three real datasets. more specifically, we combine social data that has been collected from the users of three learning portals and reflect on how they should be handled. we start by studying the data types and formats that these portals use to represent and store social and usage data. then we develop crosswalks between the different schemas, so that merged versions of the source datasets may be created. the results of this bottom-up, hands-on investigation reveal several interesting issues that need to be overcome before aggregated sets of social and usage data can be actually used to support learning analytics research or services.aggregating social and usage datasets for learning analytics','Information accountability and usage control'
'ai and accountability','Information accountability and usage control'
'allocating software components and messages to computing devices and their interconnecting networks is a great challenge in vehicle control system development. the allocation decisions are usually made at an early development stage without the implementation details of the computing platform. to produce low-cost products with correct vehicle control, the software allocation must facilitate the final system scheduling with better resource utilization and the presence of platform implementation uncertainties. in this paper, we present an allocation method to improve the resource utilization and scheduling while keeping the decisions independent of delayed platform implementation. our method determines the allocation using a genetic algorithm with an optimization function combining the peak usage request and the usage request density to minimize the potential contentions on shared resources. the preliminary evaluation using the representative synthetic systems has shown the effectiveness and applicability of the method.allocating software with improved resource usage and scheduling for vehicle control system development','Information accountability and usage control'
'it is recommendable that interactive digital tv application development should be oriented by user interface structure and interaction design. better interactivity experiences can be obtained with the return channel usage, contributing to the offering of more convergent applications and addressing their integration to remote businesses logics and services. many solutions enable construction of interactive applications with textual/visual programming support, but a few of them considers the disposal between the services operated over a return channel, the underlying data and the application widgets. usually, these solutions require the direct manipulation of intermediary models whose their structures and notations demand an additional learning effort and, in some cases, they only focus on the achievement of a specific domain application template, instead of the final application instance itself. an alternative for application authoring will be presented aiming to abstract both the visual programming interface structure domain and its behavior/responsiveness when it requires the usage of the return channel based on services over ip and meeting the expectances for agility and quality in digital interactive television conception.an alternative to align interactive interfaces construction and return channel usage','Information accountability and usage control'
'we describe a policy language and implement its associated proof checking system. in our system, agents can distribute data along with usage policies in a decentralized architecture. our language supports the specification of conditions and obligations, and also the possibility to refine policies. in our framework, the compliance with usage policies is not actively enforced. however, agents are accountable for their actions, and may be audited by an authority requiring justifications.an audit logic for accountability','Information accountability and usage control'
'an examination of microcomputer usage in taiwan','Information accountability and usage control'
'given the ubiquity of data on the web, and the lack of usage restriction enforcement mechanisms, stories of personal, creative and other kinds of data misuses are on the rise. there should be both sociological and technological mechanisms that facilitate accountability on the web that would prevent such data misuses. sociological mechanisms appeal to the data consumer\'s self-interest in adhering to the data provider\'s desires. this involves a system of rewards such as recognition and financial incentives, and deterrents such as prohibitions by laws for any violations and social pressure. bur there is no well-defined technological mechanism for the discovery of accountability or the lack of it on the web. as part of my phd thesis i propose a solution to this problem by designing a web protocol called httpa (accountable http). this protocol will enable data consumers and data producers to agree to specific usage restrictions, preserve the provenance of data transferred from a web server to a client and back to another web server, and more importantly provide a mechanism to derive an `audit trail\' for the data reuse with the help of a trusted intermediary called a `provenance tracker network\'.augmenting the web with accountability','Information accountability and usage control'
'in an electronic cash (e-cash) system, a user can withdraw coins from the bank, and then spend each coin anonymously and unlinkably. for some applications, it is desirable to set a limit on the dollar amounts of anonymous transactions. for example, governments require that large transactions be reported for tax purposes. in this work, we present the first e-cash system that makes this possible without a trusted party. in our system, a user&#39;s anonymity is guaranteed so long as she does not: (1) double-spend a coin, or (2) exceed the publicly-known spending limit with any merchant. the spending limit may vary with the merchant. violation of either condition can be detected, and can (optionally) lead to identification of the user and discovery of her other activities. while it is possible to balance accountability and privacy this way using e-cash, this is impossible to do using regular cash. our scheme is based on our recent compact e-cash system. it is secure under the same complexity assumptions in the random-oracle model. we inherit its efficiency: 2&#8467; coins can be stored in o(&#8467;+k) bits and the complexity of the withdrawal and spend protocols is o(&#8467;+k), where k is the security parameter.balancing accountability and privacy using e-cash (extended abstract)','Information accountability and usage control'
'cibbarelli\'s surveys','Information accountability and usage control'
'concurrency control in multidatabase systems','Information accountability and usage control'
'consolidate accountability for data and records','Information accountability and usage control'
'this paper presents two fast and robust digitalwatermarking schemes for compressed music format andmidi format, which are suitable for on-line distributionthrough internet. for compressed music, the watermark isembedded in partially uncompressed domain and theembedding scheme is high related to music content. formidi, the watermark is embedded in virtual notes that aregenerated and randomly hide among the midi notes. thispaper also presents a method for controlling usage andprotecting against copying of digital music contents andtheir associated players. the digital music contents andtheir players installed in one computer cannot be used ifthey are copied to another computers. even in the samecomputer, the duplicated digital music contents are notusable. the usage associated to each music content willrestrict the running time and times the music can be playedand it will be automatically updated after each playing ofthe music. the digital music may not be accessible if itspermitted usage has expired.content protection and usage control for digital music','Information accountability and usage control'
'creating business value through business-it alignment, shared knowledge, commitment and accountability','Information accountability and usage control'
'current usage of case software','Information accountability and usage control'
'although the use of data base management systems is rapidly growing, there is little hard data on how these systems are being used. however, this information is critical to defining the functional requirements for a data base processor which can effectively support current applications. this paper reports on a survey conducted to show how data base management systems are currently being used. it describes the survey taken, the respondees to the questionnaire, and the questionnaire itself&#8212;what items were included and the reasons for including them. a summary of the results of this survey and the implications of these results on the different architecture features and alternatives are given. conclusions are drawn as to the survey/questionnaire approach. additional survey work that may be needed and new areas for investigation as well as areas in which more depth is required are included.data usage and the data base processor','Information accountability and usage control'
'distributed usage control is concerned with how data may or may not be used in distributed system environments after initial access has been granted. if data flows through a distributed system, there exist multiple copies of the data on different client machines. usage constraints then have to be enforced for all these clients. we extend a generic model for intra-system data flow tracking---that has been designed and used to track the existence of copies of data on single clients---to the cross-system case. when transferring, i.e., copying, data from one machine to another, our model makes it possible to (1) transfer usage control policies along with the data to the end of local enforcement at the receiving end, and (2) to be aware of the existence of copies of the data in the distributed system. as one example, we concretize \"transfer of data\" to the transmission control protocol (tcp). based on this concretized model, we develop a distributed usage control enforcement infrastructure that generically and application-independently extends the scope of usage control enforcement to any system receiving usage-controlled data. we instantiate and implement our work for openbsd and evaluate its security and performance.data usage control enforcement in distributed systems','Information accountability and usage control'
'db connections','Information accountability and usage control'
'in this paper we present a framework to provide dependability through accountability. our proposal exploits the asymmetry present in the majority of sensor data processing to cheaply validate events and processing which occurs at various points in a distributed middleware system. we exemplify our framework with reference to two real-world distributed location middlewares. we adapt one of these to evaluate our framework and present performance results.dependability and accountability for context-aware middleware systems','Information accountability and usage control'
'usage control is concerned with how data is used after access to it has been granted. as such, it is particularly relevant to end users who own the data. system implementations of access and usage control enforcement mechanisms, however, do not always adequately reflect end user requirements. this is due to several reasons, one of which is the problem of mapping concepts in the end user\'s domain to technical events and artifacts. for instance, semantics of basic operators such as \"copy\" or \"delete\", which are fundamental for specifying privacy policies, tend to vary according to context. for this reason they can be mapped to different sets of system events. the behaviour users expect from the system, therefore, may differ from the actual behaviour. in this paper we present a translation of specification-level usage control policies into implementation-level policies which takes into account the precise semantics of domain-specific abstractions. a tool for automating the translation has also been implemented.deriving implementation-level policies for usage control enforcement','Information accountability and usage control'
'with an increasing emphasis on pervasive healthcare services, providing a high degree of privacy to patients is becoming a major challenge due to&#58; &#40;a&#41; an increased number of avenues, such as device, access points, switches and database; &#40;b&#41; more threats to privacy then ever before and &#40;c&#41; a higher expectation from society due to the sensitive and personal nature of medical and health information. in this paper, we present a framework for privacy in pervasive healthcare. various applications and scenarios have been identified, details of the framework are presented and its applications to representative pervasive healthcare applications.design and application of a health insurance portability and accountability act&#45;compliant privacy framework for pervasive healthcare','Information accountability and usage control'
' software frameworks provide sets of generic functionalities that can be later customized for a specific task. when developers invoke api methods in a framework, they often encounter obstacles in finding the correct usage of the api, let alone to employ best practices. previous research addresses this line of questions by mining api usage patterns to induce api usage templates, by conducting and compiling interviews of developers, and by inferring correlations among apis. in this paper, we analyze api-related posts regarding ios and android development from a q&a website, (stackoverflow.com). assuming that api-related posts are primarily about api usage obstacles, we find several ios and android api classes that appear to be particularly likely to challenge developers, even after we factor out api usage hotspots, inferred by modelling api usage of open source ios and android applications. for each api with usage obstacles, we further apply a topic mining tool to posts that are tagged with the api, and we discover several repetitive scenarios in which api usage obstacles occur. we consider our work as a stepping stone towards understanding api usage challenges based on forum-based input from a multitude of developers, input that is prohibitively expensive to collect through interviews. our method helps to motivate future research in api usage, and can allow designers of platforms --- such as ios and android --- to better understand the problems developers have in using their platforms, and to make corresponding improvements. detecting api usage obstacles: a study of ios and android developer questions','Information accountability and usage control'
'discretionary access control by means of usage conditions','Information accountability and usage control'
'usage control is concerned with how data is used after access to it has been granted. respective enforcement mechanisms need to be implemented at different layers of abstraction in order to monitor or control data at and across all these layers. we present a usage control enforcement mechanism at the application layer. it is implemented for a common web browser and, as an example, is used to control data in a social network application. with the help of the mechanism, a data owner can, on the grounds of assigned trust values, prevent data from being printed, saved, copied&#38;pasted, etc., after this data has been downloaded by other users.distributed data usage control for web applications','Information accountability and usage control'
'digital ecosystems are distributed software environments through which organisations can seamlessly access customised, potentially disposable, services to aid them carry out a myriad of tasks. peer to peer networks are often cited as a suitable platform for digital ecosystem deployment. a typical feature of such systems is the lack of a point of control. in this regard these are untrusted environments. this lack of trust acts as a barrier to commercial applications emerging on these platforms. suitable mechanisms for identity, authentication and trust evolution are required to overcome this. this paper provides a model for distributed accountability in digital ecosystems which can strengthen the trust in the system both from a external viewpoint (i.e. the system as a whole) and between individuals within the system.distributed support for public and private accountability in digital ecosystems','Information accountability and usage control'
'whereas access control describes the conditions that have to be fulfilled before data is released, usage control describes how the data has to be treated after it is released. usage control can be applied to digital rights management, where the data are usually copyright-protected media, as well as in privacy, in which case the data are privacy-sensitive personal information. an important aspect of usage control for privacy, especially in light of the current trend towards composed web services (so-called mash-ups), is downstream usage, i.e., with whom and under which usage control restrictions data can be shared. in this work, we present a two-sided xml-based policy language: on the one hand, it allows users to express in their preferences in a fine-grained way the exact paths that their data is allowed to follow, and the usage restrictions that apply at each hop in the path. on the other hand, it allows data consumers to express in their policies how they intend to treat the data, with whom they intend to share it, and how the downstream consumers intend to treat the data.downstream usage control','Information accountability and usage control'
'end-user computing environments&#8212;finding a balance between productivity and control','Information accountability and usage control'
'electronic health record (ehr) and personal health record (phr) systems could allow patients to better manage their health information and share it to enhance the quality and efficiency of their healthcare. unfortunately, misuse of information stored in ehr and phr systems will create new risks for patients, and we need to empower them to safeguard their health information to avoid problems such as medical identity theft. in this paper, we introduce the notion of accountable use and update of electronic health records and design a patient-centric monitoring system based on it. we develop a system architecture and associated protocols that enable either explicit or implicit patient control over when and how health information is accessed. our approach provides a reasonable solution rather than addressing the more general information flow control problem in distributed systems. we also implement and evaluate a prototype system motivated by a health record sharing scenario based on nhin direct to demonstrate that enhanced accountability can be supported with acceptable performance and integration overheads.enhancing accountability of electronic health record usage via patient-centric monitoring','Information accountability and usage control'
'conformance with ethical behavior consists of adherence to the standards of conduct for any given group. when standards are not formalized, there can exist ethical disparity from which many diverse problems can result. these problems are especially evident in the cyberspace. within the cyberspace, the &#8220;given group&#8221; is culturally and ethnically diverse. as such, it is difficult to hold the individuals to a nonformalized set of standards. several important issues need to be addressed in order to balance each individual\'s dual needs for freedom of expression and protection in the cyberspace. these issues include development of formalized standards, a general protocol for cross-cultural interaction, and ethical accountability.ethical accountability in the cyberspace','Information accountability and usage control'
'ethics, professionalism and accountability in testing','Information accountability and usage control'
'evaluating productivity','Information accountability and usage control'
'what motivates use of an expert system? recent studies have found that the anticipated performance benefits of using an expert system -- such as increases in decision quality, consistency, and speed of decision making -- can lead to increases in expected usage. but is motivation limited to performance benefits? findings in job design theory suggest that other factors -- such as increasing a user\'s sense of control over a task or making a task less routine -- might also have an impact. if so, understanding these factors could be extremely valuable to managers seeking to build expert systems that will be readily accepted by users. this paper synthesizes findings from expert systems, information systems, and job design research to model how the task change experienced by an expert systems user during adoption can affect that user\'s motivation to continue using the system. using existing task constructs from the job design literature, a simplified version of the model is operationalized and tested on a data set of expert systems (all constructed in the early and mid-1980s) for which extensive quantitative and qualitative task change data was available, as well as data on systems usage. the findings suggest significant relationships between the nature of the task changes associated with adoption and long-term usage of the systems, all consistent with the predictions of the job design literature. the study, therefore, concludes that a job design perspective of expert systems adoption can be a valuable tool in predicting user acceptance and, ultimately, systems usage.expert systems usage','Information accountability and usage control'
'monitoring and controlling electrical loads is crucial for demand-side energy management in smart grids. home automation (ha) protocols, such as x10 and insteon, have provided programmatic load control for many years, and are being widely deployed in early smart grid field trials. while ha protocols include basic monitoring functions, extreme bandwidth limitations (&lt;180bps) have prevented their use in load monitoring. in this paper, we highlight challenges in designing autometer, a system for exploiting ha for accurate load monitoring at scale. we quantify insteon\'s limitations to query device status---once every 10 seconds to achieve less than 5\% loss rate---and then evaluate techniques to disaggregate coarse ha data from fine-grained building-wide power data. in particular, our techniques learn switched load power using on-off-dim events, and tag fine-grained building-wide power data using readings from plug meters every 5 minutes.exploiting home automation protocols for load monitoring in smart buildings','Information accountability and usage control'
'federated identity management and usage control technologies have received considerable attention from the research community during the past decade. we have investigated the views of, and attitudes towards, adopting federated identity management and usage control technologies in the oil and gas industry in norway through two case studies. although the industry combines extensive inter-organisational collaboration and information sharing with high demands for security, the adoption is thus far low. in this paper we review the results of the case studies jointly and attempt to give an industry-view on the obstacles to adoption. further, we propose a set of strategies to overcome the obstacles and improve the rate of adoption. more empirical research should be carried out to complement the views put forth in this paper, and to supplement the suggested strategies to facilitate technology adoption.federated identity management and usage control - obstacles to industry adoption','Information accountability and usage control'
'the filenet integrated document management (idm) products consists of a family of client applications and imaging and electronic document management (edm) services. these services provide robust facilities for document creation, update, and deletion along with document search capabilities. document properties are stored in an underlying relational database (rdbms); document content is stored in files or in a specialized optical disk hierarchical storage manager. filenet corporation\'s visual workflo&#174; and ensemble&#174; workflow products can be utilized in conjunction with filenet\'s idm technologies to automate production and ad hoc business processes respectively.this talk will discuss how integrated document management requirements affect an idm system\'s usage of a rdbms. some of the areas to be discussed include:filenet integrated document management database usage and issues','Information accountability and usage control'
'the recent usage control model (ucon) is a foundation for next-generation access control models with distinguishing properties of decision continuity and attribute mutability. a usage control decision is determined by combining authorizations, obligations, and conditions, presented as uconabc core models by park and sandhu. based on these core aspects, we develop a formal model and logical specification of ucon with an extension of lamport\'s temporal logic of actions (tla). the building blocks of this model include: (1) a set of sequences of system states based on the attributes of subjects, objects, and the system, (2) authorization predicates based on subject and object attributes, (3) usage control actions to update attributes and accessing status of a usage process, (4) obligation actions, and (5) condition predicates based on system attributes. a usage control policy is defined as a set of temporal logic formulas that are satisfied as the system state changes. a fixed set of scheme rules is defined to specify general ucon policies with the properties of soundness and completeness. we show the flexibility and expressive capability of this formal model by specifying the core models of ucon and some applications.formal model and policy specification of usage control','Information accountability and usage control'
'this paper explores the question which consequences the use of geographical information systems (giss) may have for public accountability. empirical data from a delphi-survey were used for this exploration. respondents were asked to reflect on the consequences of giss for the availability of data for accountability. based on their answers four hypotheses were formulated. the use of a gis leads to an increased focus on up-to-date data at the expense of historical data. important data for accountability will sometimes not be retained. the use of a gis leads to an increased need to maintain about data how, when and by whom data have been generated. this need may not be sufficiently met. in a process of accountability, data from a gis may then be difficult to interpret or cannot be trusted. a gis increases the possibility to look at the data in different ways. in processes of accountability for a may ask governmental organizations to substantiate the choice fora certain perspective. the use of a gis leads to better access to the data. therefore, data from a gis may play a more important role in a process of accountability. these hypotheses require further attention to gain a better understanding of long-term public accountability in an information age.geographical information systems and public accountability','Information accountability and usage control'
'standards, assessment, and accountability movements are changing practices in public education. these changes require extensive data gathering and analysis. teachers and administrators are overwhelmed by the demands, yet handheld computers offer a means to collect data unobtrusively and in ways that may provide better accountability. within this article, uses of the handheld computer in the standards charged environment are explored.handheld computers and the standards, assessment, accountability movement','Information accountability and usage control'
'these days, sensitive and personal information is used within a wide range of applications. the exchange of this information is increasingly faster and more and more unpredictable. hence, the person concerned cannot determine what happens with his personal data after it has been released. it is highly intransparent who is accountable for data misuse. usage control and provenance tracking are two different approaches to tackle this problem. this work compares the two concepts from a data protection perspective. the support and fulfillment of data protection requirements are analysed. models and architectures are investigated for commonalities. combining the two technologies can increase flexibility and effectiveness of provenance tracking and thereby enhance information accountability in practice, if resulting linkability drawbacks are properly handled. a joint architecture is proposed to support this insight.how usage control and provenance tracking get together - a data protection perspective','Information accountability and usage control'
'whether to solve specific business problems or comply with trading partner mandates, many companies are now engaged in pilot projects using radio frequency identification (rfid). rfid dramatically increases the potential for organizations to collect data about any tagable entity, which has implications for supply chain logistics, customer relationship management, health care systems, human resource management, and privacy/intellectual property policies. however, in many cases the adoption issues and the business case are not well understood for this emerging technology. this mini-track includes three papers on the implementation and usage of radio frequency identification (rfid) technology in today\'s organization and emphasizes the building of an on-going research tradition in this area.implementation and usage of radio frequency identification (rfid)','Information accountability and usage control'
'a willing victim of its own success, the f-16 approaches midlife with customer demand for new and modified capabilities exceeding its already repeatedly expanded avionics computer capacity. affordability and agility, the imperatives of the post-cold war military, drove the us air force and f-16 contractor lockheed martin tactical aircraft systems to embrace a bold, far-sighted solution: development of a modular mission computer designed to increase computing capacity by an order of magnitude and reduce the cost and schedule of future software changes.the participants thus faced a dual challenge: developing state-of-the-art avionics hardware and software, as well as developing team culture and modes of operation. this article presents a case study of the resulting management systems and underlying mental models, written from my partisan viewpoint as their chief advocate and designer.implementing accountability','Information accountability and usage control'
'the lte (long-term evolution) is a high-data rate open multimedia mobile system. in this paper, we consider the lte qos model framework and try to optimize the operator policy with regard to the radio use efficiency. our objective is to propose a link between the pcrf (policy charging and rules function) decisions and the radio situation as an evolution for the lte standard. we define a central database for the access network radio information that can be used for the operator policy choices. in this paper, we concentrate on the voice service, as a first step. simple computations are provided. we show that our contribution can lead to a greater user average quality without loss of capacity. other important openings are highlighted, promising interesting radio efficiency gains.improving radio resource usage with suitable policy and charging control in lte','Information accountability and usage control'
'email used to be the \"number one killer application\" of the internet. however, misuse and abuse such as spam, phishing, and malware attacks have plagued the email systems. considering deterrence as important as prevention and protection in countering misuse and abuse, we aim to improve the accountability in the email system beyond identification and non-repudiability. full accountability should be an intrinsic condition for trust, and it constitutes the basis of deterrence against email misuse and abuse. therefore, we propose to use a layered trust management framework to help email receivers eliminate their unwitting trust and provide them with accountability support. this helps systems to deter misuses and address wrongdoings. by describing and analyzing how our trust management facilitates email accountability, we also show that it can be used to improve the trustworthiness of the internet services as a whole.incorporating accountability into internet email','Information accountability and usage control'
'a critical assumption of the technology acceptance model (tam) is that its belief constructs - perceived ease of use (peou) and perceived usefulness (pu) - fully mediate the influence of external variables on it usage behavior. if this assumption is true, researchers can effectively \"assume away\" the effects of broad categories of external variables, those relating to the specific task, the technology, and user differences. one recent study did indeed find that belief constructs fully mediated individual differences, and its authors suggest that further studies with similar results could pave the way for simpler acceptance models that ignore such differences. to test the validity of these authors\' results, we conducted a similar study to determine the effect of staff seniority, age, and education level on usage behavior. our study involved 106 professional and administrative staff in the it division of a large manufacturing company who voluntarily use email and word processing. we found that these individual user differences have significant direct effects on both the frequency and volume of usage. these effects are beyond the indirect effects as mediated through the tam belief constructs. thus, rather than corroborating the recent study, our findings underscore the importance of users\' individual differences and suggest that tam\'s belief constructs are accurate but incomplete predictors of usage behavior.individual differences and usage behavior','Information accountability and usage control'
'information technology and social accountability','Information accountability and usage control'
'while e-governance is acclaimed as a means to decentralisation, and an efficiency and accountability enhancing mechanism, it can be implemented in different ways. in a strong centralized state like the indian state, decentralization is often pursued in a centralized manner through top-down interventions. this paper, traces the implementation of two centrally driven e-governance interventions in the state of karnataka, india i.e. helpline and aasthi to argue that while \'centralized decentralization\' may be justified on grounds of standardization, it can have divergent outcomes, many of which are often contrary to the objectives of decentralization. the experience of helpline and aasthi belies the claim of e-governance being an efficiency and accountability enhancing mechanism. on the contrary, the centralized approach to decentralization in implementing helpline and aasthi has weakened the accountability of the state and limited the efficiency gains of urban decentralization.instituting credibility, accountability and transparency in local service delivery?','Information accountability and usage control'
'this essay considers the problem of defining the context that context-aware systems should pay attention to from a human perspective. in particular, we argue that there are human aspects of context that cannot be sensed or even inferred by technological means, so context-aware systems cannot be designed simply to act on our behalf. rather, they will have to be able to defer to users in an efficient and nonobtrusive fashion. our point is particularly relevant for systems that are constructed such that applications are architecturally isolated from the sensing and inferencing that governs their behavior. we propose a design framework that is intended to guide thinking about accommodating human aspects of context. this framework presents four design principles that support intelligibility of system behavior and accountability of human users and a number of human-salient details of context that must be accounted for in context-aware system design.intelligibility and accountability','Information accountability and usage control'
'internet addiction, usage, gratification, and pleasure experience','Information accountability and usage control'
'introduction to the minitrack on information technology and social accountability','Information accountability and usage control'
'many countries are moving towards egovernment for ensuring higher efficiency, transparency and accountability in the public administration. accountability, the obligation to justify one\'s conduct, has been widely discussed in the social, public and political spheres. however, it is not so well understood how and to what extent government\'s accountability is affected with the transformation to egovernment. this paper focuses on this aspect by means of literature review and analyzing indexes related to egovernment and the factors associated with accountability available during the period 2005-2010. the findings show that the relationship between accountability and egovernment among the countries as whole is not straightforward as it depends on the characteristics of the individual governance. however, evidence in general shows that strong accountability, which is measured here through a level of integrity, influences the government to adopt egovernment as it helps to open up the government and its policy process to its citizens and other stakeholders.investigating the relationships between accountability and governments\' transformation to egovernment','Information accountability and usage control'
'making db2 run faster','Information accountability and usage control'
'material matters','Information accountability and usage control'
'usage control is a generalization of access control that also addresses how data is used after it is released. we present a formal model for different mechanisms that can enforce usage control policies on the consumer side.mechanisms for usage control','Information accountability and usage control'
' during software development, a developer often needs to discover specific usage patterns of application programming interface (api) methods. however, these usage patterns are often not well documented. to help developers to get such usage patterns, there are approaches proposed to mine client code of the api methods. however, they lack metrics to measure the quality of the mined usage patterns, and the api usage patterns mined by the existing approaches tend to be many and redundant, posing significant barriers for being practical adoption. to address these issues, in this paper, we propose two quality metrics (succinctness and coverage) for mined usage patterns, and further propose a novel approach called usage pattern miner (up-miner) that mines succinct and high-coverage usage patterns of api methods from source code. we have evaluated our approach on a large-scale microsoft codebase. the results show that our approach is effective and outperforms an existing representative approach mapo. the user studies conducted with microsoft developers confirm the usefulness of the proposed approach in practice. mining succinct and high-coverage api usage patterns from source code','Information accountability and usage control'
'digital systems underlie a wide range of teaching and learning activities in higher education today. the scope and reach of digital systems now increasingly extend to activities as they occur even inside lecture halls, classrooms, and informal study areas. learning management systems, interactive student response systems, lecture capture systems, and digitally controlled smart classrooms are examples of technology trends that bring along with them an unprecedented amount of instrumentation quietly collecting lots of data about teacher and learner activities in and across these various spaces. in snapshots, these usage streams offer data that can be helpful for understanding and supporting a particular service. if combined across time and location, the varied data sources potentially open windows onto even more interesting activity patterns and relations. these mosaics, however, can be somewhat difficult to analyze due to the dimensionality of the combined data. matrix techniques can ease the difficulties of exploring and discovering user activity patterns in such situations. this paper surveys commonly implemented matrix techniques that can be used to enable data mining of user activity information when temporal and spatial data sets are mixed and matched from varied sources.mixing and matching usage data','Information accountability and usage control'
'the term &#8220;negotiation&#8221; suggests that multi-step bidirectional communication takes place. in this position paper, we play the devil&#8217;s advocate and argue that (automated) policy negotiation essentially is one of the following, at least in the area of usage control. it can come down to a three-phase protocol that consists of a client request, a set of offers by the server, and the client&#8217;s choice of an offer or to abort. policy negotiation can also consist of a client request together with acceptable conditions plus the server&#8217;s choice of one condition or to abort. in other words, negotiation of policies is a mere choice among alternatives; there is no negotiation in the intuitive sense of the word. &#8212; the goal of this position paper is to stimulate the discussion on what(automated) &#8220;policy negotiation&#8221; really is or can be.negotiation of usage control policies - simply the best?','Information accountability and usage control'
'network control and usage-based charging','Information accountability and usage control'
'in this paper we propose an online lightweight anomaly symptom detection and process&#8217;s resource usage control mechanism. our system collects fine-grain resource information that can reflect the subtle changes of the application&#8217;sbehavior. then it creates models with a learning-based algorithm without manual configurations. if an anomaly symptom is detected, the automatic procedure will start. the system will control the suspected application&#8217;s resource use by limiting the upper bound resource of the process. the method will make the application yield its cpu to the administrative inspection. in this paper, we described whole architecture of the system and evaluate it with the non deterministic and deterministic failure. our experimental results indicate that our prototype system is able to detect non deterministic failure with high precision in anomaly training and control it&#8217;s resource use with an overhead of about 1\%.online anomaly symptom detection and process\'s resource usage control','Information accountability and usage control'
'originator control is an access control policy thatrequires recipients to gain originator\'s approval for re-disseminationof disseminated digital object. originatorcontrol policies are one of the generic and key concerns ofusage control. usage control is an emerging concept whichencompasses traditional access control and digital rightsmanagement solutions. however, current commercialdigital rights management (drm) solutions lackenforcement of access control policies such as role-basedaccess control (rbac), mandatory access control (mac),discretionary access control (dac) and originator controlbecause their control of access to digital object is mainlybased on payment.in this paper, we attempt to combine originator controlpolicies and usage control. then we show how this canextend traditional originator control solutions to enforceaccess control policies even outside of a local controlenvironment where a central control authority is notavailable. license and ticket concepts are proposed andused for originator control in usage control. also, wedefine seven different solution approaches to deal withvarious dissemination situations. in addition, we discusssome published drm solutions and relate these to oursolution approaches.originator control in usage control','Information accountability and usage control'
'the paper provides an account of the likely consequences that performance monitoring systems have on public service accountability. the research draws upon an in-depth empirical study on citizens service centres, one of the biggest projects of the greek e-government strategy. specifically, we outline the rationale for introducing performance monitoring technology in citizens service centres, the use the central government ministry made of the system and the ways in which citizens service centre staff responded to such performance monitoring. drawing upon studies on e-government and the critical literature on performance monitoring systems, we argue that performance monitoring technology is a limited tool for ensuring accountability. this is due to the effects of the monitoring and performance standards, which increase staff\'s concerns and are likely to encourage irresponsible and unaccountable practices. keywords: performance monitoring technology, accountability, e-government, discretion.performance monitoring and accountability through technology','Information accountability and usage control'
'performing resource usage analysis for a notis system','Information accountability and usage control'
'we show how users\' activity on facebook relates to their personality, as measured by the standard five factor model. our dataset consists of the personality profiles and facebook profile data of 180,000 users. we examine correlations between users\' personality and the properties of their facebook profiles such as the size and density of their friendship network, number uploaded photos, number of events attended, number of group memberships, and number of times user has been tagged in photos. our results show significant relationships between personality traits and various features of facebook profiles. we then show how multivariate regression allows prediction of the personality traits of an individual user given their facebook profile. the best accuracy of such predictions is achieved for extraversion and neuroticism, the lowest accuracy is obtained for agreeableness, with openness and conscientiousness lying in the middle.personality and patterns of facebook usage','Information accountability and usage control'
'a significant and growing class of location-based mobile applications aggregate position data from individual devices at a server and compute aggregate statistics over these position streams. because these devices can be linked to the movement of individuals, there is significant danger that the aggregate computation will violate the location privacy of individuals. this paper develops and evaluates privstats, a system for computing aggregate statistics over location data that simultaneously achieves two properties: first, provable guarantees on location privacy even in the face of any side information about users known to the server, and second, privacy-preserving accountability (i.e., protection against abusive clients uploading large amounts of spurious data). privstats achieves these properties using a new protocol for uploading and aggregating data anonymously as well as an efficient zero-knowledge proof of knowledge protocol we developed from scratch for accountability. we implemented our system on nexus one smartphones and commodity servers. our experimental results demonstrate that privstats is a practical system: computing a common aggregate (e.g., count) over the data of 10,000 clients takes less than 0.46 s at the server and the protocol has modest latency (0.6 s) to upload data from a nexus phone. we also validated our protocols on real driver traces from the cartel project.privacy and accountability for location-based aggregate statistics','Information accountability and usage control'
'in this talk, i will address three aspects of user privacy in advertiser-supported, online services. first, i present the design of a novel browser plug-in that enables anonymous search. next, i consider economic aspects of user privacy from the point of view of the operator of an advertiser-supported website. finally, i present recent work on \"accountability\" in online activity, where the goal is to hold website operators responsible for appropriate handling of users\' sensitive information rather than to prevent users from ever providing information that might be misused.privacy, anonymity, and accountability in ad-supported services','Information accountability and usage control'
'profiles and correlates of computer usage','Information accountability and usage control'
'usage control is concerned with what happens to data after access has been granted. in the literature, usage control models have been defined on the grounds of events that, somehow, are related to data. in order to better cater to the dimension of data, we extend a usage control model by the explicit distinction between data and representation of data. a data flow model is used to track the flow of data in-between different representations. the usage control model is then extended so that usage control policies can address not just one single representation (e.g., delete file1.txt after thirty days) but rather all representations of the data (e.g., if file1.txt is a copy of file2.txt, also delete file2.txt). we present three proof-of-concept implementations of the model, at the operating system level, at the browser level, and at the x11 level, and also provide an ad-hoc implementation for multi-layer enforcement.representation-independent data usage control','Information accountability and usage control'
'the presence of ubiquitous connectivity provided by wireless communications and mobile computing has changed the way humans interact with information. at the same time, it has made communication security and privacy a hot-button issue. in this article we address the security and privacy concerns in wireless access networks. we first discuss the general cryptographic means to design privacy-preserving security protocols, where the dilemma of attaining both security and privacy goals, especially user accountability vs. user privacy, is highlighted. we then present a novel authentication framework that integrates a new key management scheme based on the principle of separation of powers and an adapted construction of boneh and shacham\'s group signature scheme, as an enhanced resort to simultaneously achieve security, privacy, and accountability in wireless access networks.security, privacy, and accountability in wireless access networks','Information accountability and usage control'
'the author describes how deciding when a software product is ready for release is being made easier by a program package that automatically collects data from beta testers so developers can know exactly who has tested which features on what platformssoftware usage metrics for real-world software testing','Information accountability and usage control'
'separation-of-duty (sod) policy is a fundamental security principle for prevention of fraud and errors in computer security. the research of static sod (ssod) policy in recently presented usage control (ucon) model has not been explored. consequently, this paper attempts to address two important issues: the specification and enforcement of ssod in ucon. we give a set-based specification scheme, which is simpler and more general than existing approaches. as for the enforcement, we study the problem of determining whether an ssod policy is enforceable, and show that directly enforcing an ssod policy is a conp-complete problem. in indirect enforcement, we generate the least restrictive static mutually exclusive attribute (smea) constraints to enforce ssod policies, by using the attribute level ssod requirement as an intermediate step. the results are fundamental to understanding the effectiveness of using constraints to enforce ssod policies in ucon. specification and enforcement of static separation-of-duty policies in usage control','Information accountability and usage control'
'systems performance evaluation','Information accountability and usage control'
'technologies associated with telecommunications and computing are rapidly converging. it is therefore becoming difficult for is practitioners to ignore telecommunications technologies and important for is researchers to begin investigating them. this paper identifies a set of representative telecommunications technologies, and reports the results of a study undertaken to assess their usage in american industry. the data identifies technologies that are prominent and innovative as perceived by the executives surveyed.telecommunications technologies','Information accountability and usage control'
'the increasing number of elderly patients in the world has lead to various new appliances and technologies in the modem tele-healthcare platform. one such application is the medical sensor network (msn). in this application, patients are deployed with certain medical sensors and wearable devices and are remotely monitored by professionals. thus, seeing a doctor in person is no longer the only option for those in need of medical care. since it is also an economical way to reduce healthcare costs and save medical resources, we expect a robust, reliable, and scalable msn in the near future. however, the time signal and temporal history in the current msn are vulnerable due to unsecured infrastructure and transmission strategies. meanwhile, the msn may leak patients\' identifications or other sensitive information that violates personal privacy. to make sure that the critical time signal is accountable, we propose a new architecture for the msn that is capable of temporal accountability. in addition, it also preserves privacy ability via a crowds anonymous system. the analysis results clearly indicate the advantages of being our proposed methods in terms of low-cost and reliable and having scalable features.temporal accountability and anonymity in medical sensor networks','Information accountability and usage control'
'the health insurance portability and accountability act of 1996 (hipaa) was passed by congress with two formidable tasks: (1) reform the insurance market; and (2) simplify healthcare administrative processes. from these ambitious beginnings, hipaa has since taken on major healthcare issues including administrative simplification, security, privacy, and patient confidentiality.in november 1997, the secretary of health and human services (dhhs) introduced the proposed \"standards\" for what is referred to as \"administrative simplification\". but, establishing the standards and guidelines necessary for administrative compliance was not an easy undertaking. dhhs estimated that 400 different formats were used in the us for healthcare claims processing.three years later (august 2000), the final rule for standards for electronic transactions and code sets was published by dhhs. the rule provided for national standardization of the most common healthcare transactions and several code sets. the compliance deadline for implementation of the new transactions and code sets was set for october, 2002. in january 2001, congress enacted the administrative simplification compliance act to provide the industry with additional time to reach compliance. yet, recent survey results by phoenix health systems and the health information and management society (himss) indicate many providers and payers are not fully compliant with the security and privacy regulations mandated by hipaa. this issue threatens the effectiveness of administrative simplification and the integrity of healthcare information.overall, hipaa does offer the promise of a more efficient and effective means of sharing and disseminating vital healthcare information. but, many experts have concerns that the implementation and maintenance of hipaa standards and policies will become a financial burden for many in the healthcare industry. although dhhs has established mandates for the healthcare industry, many deadlines have been extended and the standards of administrative simplification continue to evolve over time. others question whether the department of health and human services is capable of spearheading an information technology revolution in the healthcare industry. unfortunately, it may take several years before the industry is able to realize any true cost savings from administrative simplification. yet, few can question how hipaa has revolutionized information technology in the healthcare industry.the health insurance portability and accountability act','Information accountability and usage control'
'the relationship between user satisfaction and systems usage','Information accountability and usage control'
'the subject is objects','Information accountability and usage control'
'the sustainable-cell-rate usage parameter control with adjustable window for high-speed multimedia communications','Information accountability and usage control'
'developments in e-government are resulting infundamental reorganizations of the ways in whichdemocratic governments operate as well as in the waysin which citizens relate to their own and othergovernments and to each other. of special relevancehere are the manners in which institutions and citizensare becoming interconnected into a complex web ofgovernance\' via largely uncoordinated informationnetworks.this paper examines how this web of governance issimultaneously producing changes in individual citizen\'ssenses of identity and challenges to conventional notionsof accountability in liberal democratic systems.together, it is argued, these suggest moving focus frome-government (the institutions of government) to e-governance(the larger web of formal and informalinstitutions, organizations, norms, traditions, authoritystructures, groups and behaviors within whichindividuals and groups live their lives). such arefocusing holds the promise of developing citizencapacity and identity in balance with formalgovernmental transformations. specific illustrativeexamples are provided including seoul metropolitangovernment\'s open system.the web of governance and democratic accountability','Information accountability and usage control'
'the scalable vector graphics (svg) language allows in its version 1.2 the description of multimedia scenes including audio, video, vector graphics, interactivity and animations. this standard has been selected by the mobile industry as the format for vector graphics and rich media content. for this purpose, additional tools were introduced in the language to solve the problem of the playback of long-running svg sequences on memory-constrained devices like mobile phones. however, the proposed tools are not entirely sufficient and solutions outside the scope of svg are needed. this paper proposes a method, complementary to the svg tools, to control the memory consumption while playing back long running svg sequences. this method relies on the use of an auxiliary xml document to describe the timed-fragmentation of the svg document and the storage and streaming properties of each svg fragment. using this method, this paper shows that some svg documents can be stored, delivered and played as streams, and that their playback as streams brings an important memory consumption reduction while using a standard svg 1.2 tiny player.timed-fragmentation of svg documents to control the playback memory usage','Information accountability and usage control'
'some modern information systems require temporal and privilege consuming usage of digital objects, but usage control model (ucon) can&#8217;t resolve it most conveniently. so in order to meet these requirements, the author introduces a new access control model&#8211;times-based usage control (tucon). first, the paper introduces the tucon model and then gives the working flow of the tucon. finally the implementation of tucon is explored.times-based usage control model and its implication','Information accountability and usage control'
'distributed usage control is concerned with how data may or may not be used after initial access to it has been granted and is therefore particularly important in distributed system environments. we present an application- and application-protocol-independent infrastructure that allows for the enforcement of usage control policies in a distributed environment. we instantiate the infrastructure for transferring files using ftp and for a scenario where smart meters are connected to a facebook application.towards a policy enforcement infrastructure for distributed usage control','Information accountability and usage control'
'accountability mechanisms, which rely on after-the-fact verification, are an attractive means to enforce authorization policies. in this paper, we describe an operational model of accountability-based distributed systems. we describe analyses which support both the design of accountability systems and the validation of auditors for finitary accountability systems. our study provides formal foundations to explore the tradeoffs underlying the design of accountability systems including: the power of the auditor, the efficiency of the audit protocol, the requirements placed on the agents, and the requirements placed on the communication infrastructure.towards a theory of accountability and audit','Information accountability and usage control'
'an important aspect of trust in cloud computing consists in preventing the cloud provider from misusing the user\'s data. in this work-in-progress paper, we propose the approach of data anonymization to solve this problem. as this directly leads to problems of cloud usage accounting, we also propose a solution for anonymous yet reliable access control and accountability based on ring and group signatures.towards an anonymous access control and accountability scheme for cloud computing','Information accountability and usage control'
'towards integration of use case modelling and usage-based testing','Information accountability and usage control'
'the secure multi-party computation (smc) model provides means for balancing the use and confidentiality of distributed data. this is especially important in the field of privacy-preserving data mining (ppdm). increasing security concerns have led to a surge in work on practical secure multi-party computation protocols. however, most are only proven secure under the semi-honest model, and security under this adversary model is insufficient for many ppdm applications. smc protocols under the malicious adversary model generally have impractically high complexities for ppdm. we propose an accountable computing (ac) framework that enables liability for privacy compromise to be assigned to the responsible party without the complexity and cost of an smc-protocol under the malicious model. we show how to transform a circuit-based semi-honest two-party protocol into a protocol satisfying the ac-framework. the transformations are simple and efficient. at the same time, the verification phase of the transformed protocol is capable of detecting any malicious behaviors that can be prevented under the malicious model.transforming semi-honest protocols to ensure accountability','Information accountability and usage control'
'there are several indications that the (long-term) memory of electronic government is in danger and that this decline in organizational memory may have important negative effects on accountability. case study research in the netherlands, however, highlights the positive effects of the use of information and communication technologies for parliamentary and legal accountability. although technological safeguards for authenticity may be lacking and data may not always be preserved in a durable way, parliamentary and legal fact-finding is generally facilitated. the use of icts leads to more informational and analytical transparency of government organizations: more data is recorded and there are also more ways to retrieve this data. this increased transparency is an unintentional effect of efforts to improve the support and management of business processes.transparent government: parliamentary and legal accountability in an information age','Information accountability and usage control'
'web cache consistency is a popular problem in literature. there are a number of web cache consistency schemes, of which the invalidation-based approach is known to be most promising for improving the freshness of cached content. yu et al. [27] proposed a scalable architecture for invalidation-based web cache consistency but it does not address the issue of trust and accountability for propagation of invalidations across several administrative domains. to address that issue, this article proposes an architecture called fiat. in designing fiat, we are concerned with trust and accountability issues that are likely to be encountered in designing a global event notification service. since invalidation-based web cache consistency is a relatively well-studied problem, the discussion becomes easier if we use web cache consistency as the running theme of this paper.trust and accountability issues in scalable invalidation-based web cache consistency','Information accountability and usage control'
'trust and accountability','Information accountability and usage control'
'ubiquitous computing aims to enhance computer use by utilizing many computer resources available through physical environments, but also making them invisible to users. the purpose of ubiquitous computing is anywhere and anytime access to information within computing infrastructures that is blended into a background and no longer be reminded. this ubiquitous computing poses new security challenges while the information can be accessed at anywhere and anytime because it may be applied by criminal users. the information may contain private information that cannot be shared by all user communities. several approaches are designed to protect information for pervasive environments. however, ad-hoc mechanisms or protocols are typically added in the approaches by compromising disorganized policies or additional components to protect from unauthorized access.usage control has been considered as the next generation access control model with distinguishing properties of decision continuity. in this paper, we present a usage control model to protect services and devices in ubiquitous computing environments, which allows the access restrictions directly on services and object documents. the model not only supports complex constraints for pervasive computing, such as services, devices and data types but also provides a mechanism to build rich reuse relationships between models and objects. finally, comparisons with related works are analysed.ubiquitous computing environments and its usage access control','Information accountability and usage control'
'it is becoming harder to find an app on one\'s smart phone due to the increasing number of apps available and installed on smart phones today. we collect sensory data including app use from smart phones, to perform a comprehensive analysis of the context related to mobile app use, and build prediction models that calculate the probability of an app in the current context. based on these models, we developed a dynamic home screen application that presents icons for the most probable apps on the main screen of the phone and highlights the most probable one. our models outperformed other strategies, and, in particular, improved prediction accuracy by 8\% over most frequently used from 79.8\% to 87.8\% (for 9 candidate apps). also, we found that the dynamic home screen improved accessibility to apps on the phone, compared to the conventional static home screen in terms of accuracy, required touch input and app selection time.understanding and prediction of mobile application usage for smart phones','Information accountability and usage control'
'the workshop on usage analysis and the web of data (usewod2011) was the first workshop in the field to investigate combinations of usage data with semantics and the web of data. questions the workshop aims to address are for example: how can semantics help in understanding usage data, how can semantic information be derived from usage data, and how can we learn about usage of and on the emerging web of data, and what can we learn from it? we report on the findings and results of this workshop, held on march 28, 2011 in conjunction with 20th international world wide web conference, in hyderabad, india.usage analysis and the web of data','Information accountability and usage control'
'sharing information allows businesses to take advantage of hidden knowledge, improve work processes and cooperation both within and across organisations. thus there is a need for improved information protection capable of restricting how information is used, as opposed to only accessed. usage control has been proposed to achieve this by combining and extending traditional access control, digital rights management and various encryption schemes. advances in usage control enforcement has received considerable attention from the research community and we therefore believe there is a need to synthesise these efforts to minimise the potential for overlap. this paper surveys the previous efforts on providing usage control enforcement and analyses the general strengths and weaknesses of these approaches. in this paper we demonstrate that there are several promising mechanisms for enforcing usage control, but that reliable empirical evidence is required in order to ensure the appropriateness and usability of the enforcement mechanisms.usage control enforcement - a survey','Information accountability and usage control'
'continuous access control after an object is released into a distributed environment has been regarded as the usage control problem and has been investigated by different researchers in various papers. however, the enabling technology for usage control is a challenging problem and the space has not been fully explored yet. in this paper we identify the general requirements of a trusted usage control enforcement in heterogeneous computing environments, and also propose a general platform architecture to meet these requirements.usage control platformization via trustworthy selinux','Information accountability and usage control'
'usage control is concerned with control over data after its release to third parties, and includes requirements such as \"this data must be deleted after 30 days\" and \"this data may be used for statistical purposes only\". with the ultimate goal of respective policy languages that go beyond current technology for drm of intellectual property, we provide (1) a catalog of requirements that are specific to the domains of mobile and ubiquitous computing applications, and (2) a classification of mobile and ubiquitous computing application scenarios that involve usage control, including a description of different kinds of involved data.usage control requirements in mobile and ubiquitous computing applications','Information accountability and usage control'
'in this paper we describe our general framework for usage control (ucon) enforcement on grid systems. it allows both grid services level enforcement of ucon as well as fine-grained one at the level of local grid node resources. in addition, next to the classical checks for usage control: checks of conditions, authorizations, and obligations, the framework also includes trust and risk management functionalities. indeed, we show how trust and risk issues naturally arise when considering usage control in grid systems and services and how our architecture is flexible enough to accommodate both notions in a pretty uniform way.usage control, risk and trust','Information accountability and usage control'
'this paper proposes a atm traffic management scheme that utilizes a deterministic source traffic descriptor, a deterministic usage parameter control (upc) algorithm and a conservative statistical bandwidth allocation method. the scheme not only guarantees the qos of all connections but also allows for large statistical multiplexing gain. the proposed method, therefore, creates an effective b-isdn that offers cost-effective broadband variable bit-rate services.usage parameter control and bandwidth allocation methods for atm-based b-isdn','Information accountability and usage control'
'usage statistics: about counter and sushi','Information accountability and usage control'
'scalability and input domain explosion make it impossible to exhaustively test simulation systems. improved methods such as statistical usage testing are needed to provide quantitative support for test planning and test management. this paper describes the challenges and the state of the practice of testing simulation systems. a brief introduction to statistical usage testing is provided. an approach to developing an abstract usage model structure appropriate for testing military simulation systems is suggested and illustrated. this approach supports the creation and analysis of test scenarios that are flexible enough to handle a wide range of uses in military simulations.usage testing of military simulation systems','Information accountability and usage control'
'preventing the misuse of personally identifiable information and preserving user privacy are key issues in the management of it services, especially when organizational borders are crossed. in this paper, we first present an analysis of the differences between grid environments and previous models of inter-organizational collaboration. based on requirements derived thereof, we demonstrate how existing policy-based privacy management architectures can be extended to provide grid-specific functionality and can be integrated into existing infrastructures. special emphasis is put on privacy policies which can be configured by users themselves, and distinguishing between the initial data access and the later data usage control phases. we also discuss the application of this approach to a xacml-based privacy management system.using policy-based management for privacy-enhancing data access and usage control in grid environments','Information accountability and usage control'
'in this paper, we propose integrating watermarking technology and access control model in order to enhance the protection of medical images in distributed healthcare infrastructures. for that purpose, we consider the well known organization based access control model (orbac) to express policy due to the intrinsic dynamic nature of the healthcare environment. orbac advantage is that it can specify contextual authorizations and obligations using \"context\", a native notion of this model, which may be viewed as a set of conditions to be satisfied when activating a given authorization or obligation. we suggest the embedding of some useful information into one image, information that can be used for tracing or controlling the access/usage of this image. in a more general way, we discuss how loss less or reversible watermarking can be instrumented by the orbac model. the loss less property preserves the image diagnostic value but imposes some other constraints. we illustrate the potential of such a watermarking orbac access and usage rules based on a new reversible watermarking scheme we proposed. based on its performances regarding different image modalities, we analyze how this one can be used to convey such access control elements.watermarking to enforce medical image access and usage control policy','Information accountability and usage control'
'the advent of digital technologies and the computerization of traditional media present major challenges to traditional schools and colleges. the analysis of the multiple challenges facing education force educators within academic institutions to evaluate the possibilities of alternative forms of course content delivery. web based educational systems (wbes) offer interesting delivery mechanisms to teachers and learners. stakeholders have to take ownership in the development of these wbes and have to be accountable for the performance results that justify their existence. governance and accountability are key criteria to consider during the deployment of these wbes. assessment of wbes also needs to be done to determine its effectiveness. theoretical concepts discussed in this paper will encourage researchers to do empirical studies to determine whether web base educational settings apply key principles such as governance, accountability, and effectiveness while offering curriculum over the web.web based educational systems-governance, accountability, and assessment','Information accountability and usage control'
'as part of a complete wireless networking education, students must have an in-depth understanding of basic concepts such as signal propagation, environmental effects on rf signals, fcc regulations and limits, power levels, antenna construction and antenna operation. lecture based curricula can only go so far in preparing a wireless professional to succeed in industry. to be complete, the student must have practical experience.our wireless coursework is comprised of three courses, the first of which is a wireless concepts course. this course has a significant hands-on component that requires students to understand the tools while applying what they are learning about the physical layer and basic network operation. the students engage in two very large projects; a wireless building survey and signal propagation testing using specialized equipment.as part of the projects, students create a series of experiments with a variety of equipment and provide useable test data. examples of the tests include interference, fresnel zone effects, throughput, range and the effects of multi-path on signals. however, in the presence of an increasing number of wireless networks, obtaining real world, reliable data illustrating the various physical layer phenomena is difficult. our solution was to build several wireless carts outfitted with various antennas, transmission equipment from different portions of the spectrum and that used different encoding or modulation techniques. in addition, a major requirement was that the carts be able to operate away from infrastructure support, including ac power.the carts have enabled students to isolate themselves from other wireless signals and have provided an extremely adaptive platform for experiments and projects. this paper will describe the coursework, projects, functions, costs, lessons learned and the data gathered as a result of their deployment.wireless carts','Information accountability and usage control'
'while preventative policy enforcement mechanisms can provide theoretical guarantees that policy is correctly enforced, they have limitations in practice. they are inflexible when unanticipated circumstances arise, and most are either inflexible with respect to the policies they can enforce or incapable of continuing to enforce policies on data objects as they move from one system to another. in this paper we propose an approach to enforcing policies not by preventing unauthorized use, but rather by deterring it. we believe this approach is complementary to preventative policy enforcement. we call our approach apple for a-posteriori policy enforcement. we introduce apple core, a logical framework for using logs to verify that actions taken by the system were authorized. a trust management system is used to ensure that data objects are provided only to users operating on auditable systems who are subject to penalty should they be found in violation. this combination of audit and accountability provides a deterence that strongly encourages trustworthy behavior, thereby allowing a high level of assurance of end-to-end policy enforcement.a posteriori compliance control','Information flow control'
'based on the layered video streaming technique, we proposed a proxy-based traffic flow control scheme that adjusts the presentation quality according to the currently available bandwidth, i.e., dropping/adding some video layers when the network situation gets congested/smooth. additionally, the proposed traffic flow control provides layered video stream buffering control to smoothen video playout. a re-transmission technique is also adopted to reduce the lost possibility of some important video frames. with the proposed traffic flow control scheme, the coming server-proxy-client multimedia presentation systems can have a better bandwidth utilization and presentation quality.a proxy-based adaptive flow control scheme for media streaming','Information flow control'
'the ability to enforce usage policies attached to data in a fine grained manner requires that the system be able to trace and control the flow of information within it. this paper presents the design and implementation of such an information flow control system, named trishul, as a java virtual machine. in particular we address the problem of tracing implicit information flow, which had not been resolved by previous run-time systems and the additional intricacies added on by the java architecture. we argue that the security benefits offered by trishul are substantial enough to counter-weigh the performance overhead of the system as shown by our experiments.a virtual machine based information flow control system for policy enforcement','Information flow control'
'this paper advocates a novel approach to the construction of secure software: controlling information flow and maintaining integrity via monadic encapsulation of effects. this approach is constructive, relying on properties of monads and monad transformers to build, verify, and extend secure software systems. we illustrate this approach by construction of abstract operating systems called separation kernels. starting from a mathematicalmodel of shared-state concurrency based on monads of resumptions and state, we outline the development by stepwise refinements of separation kernels supporting unix-like system calls, interdomain communication, and a formally verified security policy (domain separation). because monads may be easily and safely represented within any pure, higher-order, typed functional language, the resulting system models may be directly realized within a language such as haskell.achieving information flow security through precise control of effects','Information flow control'
'unresolved indirect branch instructions are a major obstacle for statically reconstructing a control flow graph (cfg) from machine code. if static analysis cannot compute a precise set of possible targets for a branch, the necessary conservative over-approximation introduces a large amount of spurious edges, leading to even more imprecision and a degenerate cfg. in this paper, we propose to leverage under-approximation to handle this problem. we provide an abstract interpretation framework for control flow reconstruction that alternates between over- and under-approximation. effectively, the framework imposes additional preconditions on the program on demand, allowing to avoid conservative over-approximation of indirect branches. we give an example instantiation of our framework using dynamically observed execution traces and constant propagation. we report preliminary experimental results confirming that our alternating analysis yields cfgs closer to the concrete cfg than pure over- or under-approximation.alternating control flow reconstruction','Information flow control'
'in this paper a meta-model for information flow control is defined using the foundation of barker\'s access control meta-model. the purposes for defining this meta-model is to achieve a more principled understanding of information flow control, to compare information flow control and access control at an abstract level, and to explore how information flow control and access control might be composed to yield a rich new set of ideas and systems for controlling the dissemination of sensitive information. it is shown that it is possible to define a meta-model for information flow control, that such a model is more complex compared to the access control meta-model, and that the meta-models for information flow control and access control can be composed in a conceptually straightforward way.an information flow control meta-model','Information flow control'
'an investigation of the frequency and power flow transients in a regional power system control','Information flow control'
'empirical observations of developers editing code revealed that difficulties following control flow relationships led to poor changes, wasted time, and bugs. i am designing a static analysis to compute interprocedural path-sensitive control flow to help developers more quickly and accurately visually answer these common questions about code.answering control flow questions about code','Information flow control'
'auto-placement of semi-graphic flow diagrams','Information flow control'
'composition of web services is of great interest to support business-to-business collaboration and provide value added services with desired properties or capabilities. nevertheless, the standard languages used to create business processes from composite web services lack of formal definition of their semantics and tools to support the analysis of a business process. in this paper we provide a practical approach to formal verification of bpel4ws executable processes. a syntax-driven operational semantics for bpel4ws is introduced and an automatic verifier is presented in order to perform a semantic analysis of the flow constructs used in the definition of bpel4ws processes.automatic analysis of control flow inweb services composition processes','Information flow control'
'a configurable process model is an integrated representation of multiple variants of a business process. it is designed to be individualized to meet a particular set of requirements. as such, configurable process models promote systematic reuse of proven or common practices. existing notations for configurable process modeling focus on capturing tasks and control-flow dependencies, neglecting equally important aspects of business processes such as data flow, material flow and resource management. this paper fills this gap by proposing an integrated meta-model for configurable processes with advanced features for capturing resources involved in the performance of tasks (through task-role associations) as well as flow of data and physical artifacts (through task-object associations). although embodied as an extension of a popular process modeling notation, namely epc, the meta-model is defined in an abstract and formal manner to make it applicable to other notations.beyond control-flow','Information flow control'
'the purpose of this paper is to present cost and error data collected during the development cycle of a large-scale software effort, to analyze this data in comparison with other available data from similar projects, and to evaluate the effectiveness of the techniques utilized on the project. the project being reported on is computer sciences corporation\'s development of the central flow control software system for the federal aviation administration\'s air traffic control system command center. analysis of the cost data provides insight not only into the added development costs associated with severely limiting module sizes, but also into the effectiveness of various cost estimation techniques. the error data analysis supports the usefulness of the software engineering techniques which were used on the project in conjunction with definitive module-level test requirements. the paper provides a foundation upon which to establish the development and data collection environment for future software systems.central flow control software development','Information flow control'
'any static, global analysis of the expression and data relationships in a program requires a knowledge of the control flow of the program. since one of the primary reasons for doing such a global analysis in a compiler is to produce optimized programs, control flow analysis has been embedded in many compilers and has been described in several papers. an early paper by prosser [5] described the use of boolean matrices (or, more particularly, connectivity matrices) in flow analysis. the use of &#8220;dominance&#8221; relationships in flow analysis was first introduced by prosser and much expanded by lowry and medlock [6]. references [6,8,9] describe compilers which use various forms of control flow analysis for optimization. some recent developments in the area are reported in [4] and in [7]. the underlying motivation in all the different types of control flow analysis is the need to codify the flow relationships in the program. the codification may be in connectivity matrices, in predecessor-successor tables, in dominance lists, etc. whatever the form, the purpose is to facilitate determining what the flow relationships are; in other words to facilitate answering such questions as: is this an inner loop?, if an expression is removed from the loop where can it be correctly and profitably placed?, which variable definitions can affect this use? in this paper the basic control flow relationships are expressed in a directed graph. various graph constructs are then found and shown to codify interesting global relationships.control flow analysis','Information flow control'
'a software obfuscator is a program o to transform a source program p for protection against malicious reverse engineering. o should be correct (o(p) has same functionality with p), resilient (o(p) is resilient against attacks), and effective (o(p) is not too much slower than p). in this paper we describe the design of an obfuscator which consists of two parts. the first part extracts the control flow information from the program and saves it in another process named monitor-process. the second part protects monitor-process converting it into an aucsmith like self-modifying version. we prove the correctness of the obfuscation scheme. we assess its resilience and efficiency to show that both are this supports the claim that our approach is practical.control flow based obfuscation','Information flow control'
'recent micro-architectural research has proposed various schemes to enhance processors with additional tags to track various properties of a program. such a technique, which is usually referred to as information flow tracking, has been widely applied to secure software execution (e.g., taint tracking), protect software privacy and improve performance (e.g., control speculation). in this paper, we propose a novel use of information flow tracking to obfuscate the whole control flow of a program with only modest performance degradation, to defeat malicious code injection, discourage software piracy and impede malware analysis. specifically, we exploit two common features in information flow tracking: the architectural support for automatic propagation of tags and violation handling of tag misuses. unlike other schemes that use tags as oracles to catch attacks (e.g., taint tracking) or speculation failures, we use the tags as flow-sensitive predicates to hide normal control flow transfers: the tags are used as predicates for control flow transfers to the violation handler, where the real control flow transfer happens. we have implemented a working prototype based on itanium processors, by leveraging the hardware support for control speculation. experimental results show that bosh can obfuscate the whole control flow with only a mean of 26.7\% (ranging from 4\% to 59\%) overhead on specint2006. the increase in code size and compilation time is also modest.control flow obfuscation with information flow tracking','Information flow control'
'control flow versus logic','Information flow control'
'control flow, data flow & data independence','Information flow control'
'although workflow languages are widely used for composing discrete services, these are not suitable for stream based interactions. in this paper we address the problem of how to extend a conventional web service composition language with the ability to deal with data streaming services. the paper discusses several modeling alternatives and presents a marker based semantics for safely dealing with pipelined processing in service compositions. the paper also presents application examples that illustrate the advantages of the proposed approach.control the flow','Information flow control'
'mobile and pc/server class processor companies continue to roll out flagship core micro architectures that are faster than their predecessors. meanwhile placing more cores on a chip coupled with constant supply voltage puts per-core energy consumption at a premium. hence, the challenge is to find future micro architecture optimizations that not only increase performance but also conserve energy. eliminating branch mispredictions--which waste both time and energy--is valuable in this respect. we first explore the control-flow landscape by characterizing mispredictions in four benchmark suites. we find that a third of mispredictions-per-1k-instructions (mpki) come from what we call separable branches: branches with large control-dependent regions (not suitable for if-conversion), whose backward slices do not depend on their control-dependent instructions or have only a short dependence. we propose control-flow decoupling (cfd) to eradicate mispredictions of separable branches. the idea is to separate the loop containing the branch into two loops: the first contains only the branch\'s predicate computation and the second contains the branch and its control-dependent instructions. the first loop communicates branch outcomes to the second loop through an architectural queue. micro architecturally, the queue resides in the fetch unit to drive timely, non-speculative fetching or skipping of successive dynamic instances of the control-dependent region. either the programmer or compiler can transform a loop for cfd, and we evaluate both. on a micro architecture configured similar to intel\'s sandy bridge core, cfd increases performance by up to 43\%, and reduces energy consumption by up to 41\%. moreover, for some applications, cfd is a necessary catalyst for future complexity-effective large-window architectures to tolerate memory latency.control-flow decoupling','Information flow control'
'runtime attacks that exploit software vulnerabilities are still an important concern nowadays. even smartphone operating systems such as apple\'s ios are affected by such attacks since the system is implemented in objective-c, a programming language that enables attacks such as buffer overflows. as a generic protection technique against a whole class of attacks, control-flow integrity (cfi) offers some interesting properties. recent work demonstrated that cfi can be implemented on ios by patching the binary during the loading process and adding an instrumentation layer that enforces cfi during runtime. however, this approach is of little practical value since it requires a jailbroken device, which hinders wide employment. furthermore, binary patching has a certain performance impact. in this paper, we show how cfi can be implemented directly within a compiler, making the approach widely deployable on all kinds of ios devices. we extend the llvm compiler and add our cfi enforcement approach during the compilation phase of a given app. an empirical evaluation shows that the size and performance overhead is reasonable.control-flow restrictor','Information flow control'
'as part of a larger project, we have built a declarative assembly language that enables us to specify multiple code paths to compute particular quantities, giving the instruction scheduler more flexibility in balancing execution resources for superscalar execution. since the key design points for this language are to only describe data flow, have built-in facilities for redundancies, and still have code that looks like assembler, by virtue of consisting mainly of assembly instructions, we are basing the theoretical foundations on data-flow graph theory, and have to accommodate also relational aspects. using functorial semantics into a kleene category of &#8220;hyper-paths&#8221;, we formally capture the data-flow-with-choice aspects of this language and its implementation, providing also the framework for the necessary correctness proofs.control-flow semantics for assembly-level data-flow graphs','Information flow control'
'control flow is often key problem in current web applications. for example, using the back button gives a postdata error, using multiple windows books the wrong hotel, and sending a link to a friend does not work. previous solutions used continuations as a model for user interaction. however continuations are insufficient as a model of all web interactions. we believe the protocol and browsers themselves are insufficiently powerful to represent the control flow desired in a web application. our solution is to extend the protocol and browser sufficiently that these problems can be avoided. we seek to be agnostic about how web applications are written and instead recognise that many of the problems stem from underlying weaknesses in the protocol. as an example, the application ought to be able to inform the browser that pressing back on a payment confirmation page is not allowed. instead, the cached page can be displayed in a read-only, archive fashion to the user, or a new page can be shown instead which is consistent with the global state. we discuss how some of these ideas may be implemented within the existing http/1.1 protocol; and what modest extensions to the protocol would enable full implementation. we also discuss the interaction with web 2.0 and the security and privacy implications of our extensions.controlling control flow in web applications','Information flow control'
'data flow driven computer for embedded control systems','Information flow control'
'a specialized data flow graph, database flow graph (dbfg) is introduced. dbfgs may be used for scheduling database operations, particularly in an mimd database machine environment. a dbfg explicitly maintains intertransaction and intratransaction dependencies, and is constructed from the transaction flow graphs (tfg) of active transactions. a tfg, in turn, is the generalization of a query tree used, for example, in direct [15].all dbfg schedules are serializable and deadlock free. operations needed to create and maintain the dbfg structure as transactions are added or removed from the system are discussed. simulation results show that dbfg scheduling performs as well as two-phase locking.database concurrency control using data flow graphs','Information flow control'
'many tasks that involve the dynamic manipulation of middleware and large-scale distributed applications, such as debugging and testing, require the monitoring of intricate relationships of execution events that trigger modifications to the executing system. furthermore, events often are of interest only if they occur as part of specific execution traces and not all possible non-deterministic interleavings of events in these traces. current techniques and tools for the definition of such manipulations provide only very limited support for such event relationships and do not allow to concisely define restrictions on the interleaving of events. in this paper, we argue for the use of aspect-based high-level programming abstractions for the definition of relationships between execution events of distributed systems and the control of non-deterministic interleavings of events. concretely, we provide the following contributions: we (i) motivate that such abstractions improve on current debugging and testing methods for middleware, (ii) introduce corresponding language support for pointcuts and advice defined in terms of causal event sequences by extending an existing aspect-oriented system for the dynamic manipulation of distributed systems, and (iii) evaluate our approach in the context of the debugging and testing of java-based middlewares, in particular, jboss cache for replicated caching.debugging and testing middleware with aspect-based control-flow and causal patterns','Information flow control'
'modern source-level debuggers support dynamic breakpoints that are guarded by conditions based on program state. such breakpoints address situations where a static breakpoint is not sufficiently precise to characterise a point of interest in program execution. however, we believe that current ide support for dynamic breakpoints are cumbersome to use. firstly, guard conditions formulated in (non-aspect-oriented) source-languages cannot directly express control-flow conditions, forcing developers to seek alternative formulations. secondly, guard-conditions can be complex expressions and manually typing them is cumbersome.we present the control-flow breakpoint debugger (cbd). cbd uses a dynamic pointcut language to characterise control-flow breakpoints---dynamic breakpoints which are conditional on the control-flow through which they were reached. cbd provides a \"point-and-click\" gui to specify and incrementally refine control-flow breakpoints, thereby avoiding the burden of manually editing the potentially complex expressions that define them.we performed 20 case studies debugging and fixing documented bugs in 3 existing applications. our results show that dynamic breakpoints in general are useful in practice, and that cbd\'s gui allows specifying them adequately in the majority of cases.debugging with control-flow breakpoints','Information flow control'
'in the computing science community there is a growing belief that the traditional von neumann programming model will be superceded in the 1990\'s by a new decentralised programming model. various &#8220;revolutionary&#8221; approaches are being promoted: data flow, reduction, actor and logic models. we propose an alternative &#8220;evolutionary&#8221; approach, namely a decentralised control flow model. this model, a generalisation of the von neumann model, can already be recognised as providing the basis of modern operating systems such as unix. in this paper we discuss initially the programming of future decentralised computing systems that represent a convergence of computer networks and parallel architectures. next we describe the decentralised control flow model, and finally we present a simple programming language called basix that embodies the model.decentralised control flow - based on unix','Information flow control'
'the article describes design and realization of an airflow structure for measurement. there are structure and electronic circuits design in the paper are described the principle of a flowmeter sensor operation is based on the mechanism of a fluidic oscillator. it uses fluidic paths in feedback. oscillator samples have been made using different technologies and materials. micromechanical shaping, etching and sticking as basic operations have been used. further there is described design, construction and measured results of the developed flowmeter with a fluidic oscillator for measurement of fluid flow. measured fluid flow is displayed on a digital or analog measuring device calibrated in values of immediate flow amount or flow velocity. a coventorware software was used for design of mechanical behaviour and cadence software was used for electronic circuits design.design and realization of sensor system for flow measurement','Information flow control'
'security in embedded systems such as smartphones requires protection of confidential data and applications. many of security mechanisms use dynamic taint analysis techniques for tracking information flow in software. but these techniques cannot detect control flows that use conditionals to implicitly transfer information from objects to other objects. in particular, malicious applications can bypass android system and get privacy sensitive information through control flows. we propose an enhancement of dynamic taint analysis that propagates taint along control dependencies by using the static analysis in embedded system such as google android operating system. by using this new approach, it becomes possible to protect sensitive information and detect most types of software exploits without reporting too many false positives.detecting control flow in smarphones','Information flow control'
'we investigate the integration of two approaches to information security: information flow analysis, in which the dependence between secret inputs and public outputs is tracked through a program, and differential privacy, in which a weak dependence between input and output is permitted but provided only through a relatively small set of known differentially private primitives. we find that information flow for differentially private observations is no harder than dependency tracking. differential privacy\'s strong guarantees allow for efficient and accurate dynamic tracking of information flow, allowing the use of existing technology to extend and improve the state of the art for the analysis of differentially private computations.differential privacy with information flow control','Information flow control'
'this paper presents a language in which information flow is securely controlled by a type system, yet the security class of data can vary dynamically. information flow policies provide the means to express strong security requirements for data confidentiality and integrity. recent work on security-typed programming languages has shown that information flow can be analyzed statically, ensuring that programs will respect the restrictions placed on data. however, real computing systems have security policies that cannot be determined at the time of program analysis. for example, a file has associated access permissions that cannot be known with certainty until it is opened. although one security-typed programming language has included support for dynamic security labels, there has been no demonstration that a general mechanism for dynamic labels can securely control information flow. in this paper, we present an expressive language-based mechanism for reasoning about dynamic security labels. the mechanism is formally presented in a core language based on the typed lambda calculus; any well-typed program in this language is secure because it satisfies noninterference.dynamic security labels and static information flow control','Information flow control'
'aspect-oriented programming (aop) is increasingly gaining in popularity. however, the focus of aspect-oriented language research has been mostly on language design issues; efficient implementation techniques have been less popular. as a result, the performance of certain aop constructs is still poor. this is in particular true for constructs that rely on dynamic properties of the execution (e.g., the cflow construct).in this paper, we present efficient implementation techniques for cflow that exploit direct access to internal structures of the virtual machine running an application, such as the call stack, as well as the integration of these techniques into the just-in-time compiler code generation process.our results show that aop has the potential to make programs that need to define control flow-dependent behavior not only more modular but also more efficient. by making means for control flow-dependent behavior part of the language, aop opens the possibility of applying sophisticated compiler optimizations that are out of reach for application programmers.efficient control flow quantification','Information flow control'
'the model of decentralized information flow control (difc) is effective at improving application security and can support rich confidentiality and integrity policies. we describe the design and implementation of dupro, an efficient user-space information flow control framework. dupro adopts software-based fault isolation (sfi) to isolate protection domains within the same process. it controls the end-to-end information flow at the granularity of sfi domains. being a user-space framework, dupro does not require any os changes. since sfi is more lightweight than hardware-based isolation (e.g., os processes), the inter-domain communication and scheduling in dupro are more efficient than process-level difc systems. finally, dupro supports a novel checkpointing-restoration mechanism for efficiently reusing protection domains. experiments demonstrate applications can be ported to dupro with negligible overhead, enhanced security, and with tight control over information flow.efficient user-space information flow control','Information flow control'
'data-centric scientific workflows are often modeled as dataflow process networks. the simplicity of the dataflow framework facilitates workflow design, analysis, and optimization. however, modeling \"control-flow intensive\" tasks using dataflow constructs often leads to overly complicated workflows that are hard to comprehend, reuse, and maintain. we describe a generic framework, based on scientific workflow templates and frames, for embedding control-flow intensive subtasks within dataflow process networks. this approach can seamlessly handle complex control-flow without sacrificing the benefits of dataflow. we illustrate our approach with a real-world scientific workflow from the astrophysics domain, requiring remote execution and file transfer in a semi-reliable environment. for such workflows, we also describe a 3-layered architecture based on frames and templates where the top-layer consists of an overall dataflow process network, the second layer consists of a tranducer template for modeling the desired control-flow behavior, and the bottom layer consists of frames inside the template that are specialized by embedding the desired component implementation. our approach can enable scientific workflows that are more robust (faulttolerance strategies can be defined by control-flow driven transducer templates) and at the same time more reusable, since the embedding of frames and templates yields more structured and modular workflow designs.enabling scientificworkflow reuse through structured composition of dataflow and control-flow','Information flow control'
'we present an approach to control information flow in object-oriented systems. the decision of whether an information flow is permitted or denied depends on both the authorizations specified on the objects and the process by which information is obtained and transmitted. depending on the specific computations, a process accessing sensitive information could still be allowed to release information to users who are not allowed to directly access it. exceptions to the permissions and restrictions stated by the authorizations are specified by means of exceptions associated with methods. two kinds of exceptions are considered: invoke exceptions, applicable during a mehtod execution and reply exceptions applicable to the information returned by a method.   information flowing from one object into another or returned to the user is subject to the different exceptions specified for the methods enforcing the transmission. we formally characterize information transmission and flow in a transaction and define the conditions for safe information flow. we define security specifications and characterize safe information flows. we propose an approach to control unsafe flows and present an algorithm to enforce it. we also illustrate an efficient implementation of our controls and present some experimental results evaluating its performance.exception-based information flow control in object-oriented systems','Information flow control'
'this paper is concerned with the issues we encountered when attempting to achieve enterprise level knowledge reuse. we present 3 pilot studies where new visualization techniques were used to allow manufacturing and service operations take advantage of engineering knowledge embodied in 3d models. though all these studies showed dramatic productivity increases, only one business unit from the studies is currently working to achieve the reuse. there are a number of reasons why this is so, but the key underlying theme is a lack of enterprise level commitment to knowledge sharing and a lack of an adequate knowledge architecture for sharing knowledge across organizational boundaries. we conclude with an approach for facilitating knowledge flow across functional units.facilitating knowledge flow through the enterprise','Information flow control'
'one of the main features of information flow control is to ensure the enforcement of privacy and regulated accessibility. however, most information flow control models that have been proposed do not provide substantial assurance to enforce end-to-end confidentiality policies or they are too restrictive, overprotected, and inflexible. we present a model for discretionary access controls that is in harmony with the object oriented paradigm. the model uses access rights applied to object attributes and methods, thus allowing considerable flexibility without compromising system security by leaking sensitive information. models based on message filtering intercept every message exchanged among objects to control the flow of information. we present an algorithm which enforces message filtering based on the defined access rights.fine granularity access rights for information flow control in object oriented systems','Information flow control'
'flow control and discretized learning algorithms for routing in computer networks','Information flow control'
'this paper investigates the performance implications of several end-to-end flow-control schemes based on the servernetr systemarea network. the static window (sw), packet pair (pp), and the simplified packet pair (spp) flow control schemes are studied. additionally, the alternating static window (asw) flow control is defined and evaluated. previously, it has been proven that the packet-pair scheme is stable for store-and-forward networks based on rate allocation servers. the applicability of a pp flow control to wormhole-routing networks is studied and evaluated through simulation. it is shown that if high throughput is desired, asw is the best method for controlling the average latency. on the other hand, if low throughput is acceptable, spp can be applied to maintain low latencies.flow control in servernetr clusters','Information flow control'
'according to the theory of network calculus based on the (min,+) algebra, analysis and measure of worst-case performance in communication networks can be made easily. in this context, this paper deals with traffic regulation and performance guarantee of a network i.e. with flow control. at first, assuming that a minimum service provided by a network is known, we aim at finding the constraint over the input flow in order to respect a maximal delay or backlog. then, we deal with the window flow control problem in the following manner: the data stream (from the source to the destination) and the acknowledgments stream (from the destination to the source) are assumed to be different and the service provided by the network is assumed to be known in an uncertain way, more precisely it is assumed to be in an interval. the results are obtained by considering the residuation theory which allows functions defined over idempotent semiring to be inverted.flow control with (min,+) algebra','Information flow control'
'anonymity features of electronic payment systems are important for protecting privacy in an electronic world. however, complete anonymity prevents monitoring financial transactions and following the money trail, which are important tools for fighting serious crimes. to solve these type of problems several \"escrowed cash\" systems, that allow a \"trustee\" to trace electronic money, were suggested. in this paper we suggest a completely different approach to anonymity control based on the fact that law enforcement is mainly concerned with large anonymous electronic payments. we describe a payment system that effectively limits the amount of money a user can spend anonymously in a given time frame. to achieve this we describe a technique to make electronic money strongly non-transferable. our payment system protects the privacy of the honest user who plays by the rules, while introducing significant hurdles for several criminal abuses of the system.flow control','Information flow control'
'flow measurement solved with venturi-cone meter','Information flow control'
'web service composition refers to the creation of new (web) services by combining functionalities provided by existing ones. a number of domain-specific languages for service composition have been proposed, with consensus being formed around a process-oriented language known as ws-bpel (or bpel). the kernel of bpel consists of simple communication primitives that may be combined using control-flow constructs expressing sequence, branching, parallelism, synchronization, etc. we present a comprehensive and rigorously defined mapping of bpel constructs onto petri net structures, and use this for the analysis of various dynamic properties related to unreachable activities, conflicting messages, garbage collection, conformance checking, and deadlocks and lifelocks in interaction processes. we use a mapping onto petri nets because this allows us to use existing theoretical results and analysis tools. unlike approaches based on finite state machines, we do not need to construct the state space, and can use structural analysis (e.g., transition invariants) instead. we have implemented a tool that translates bpel processes into petri nets and then applies petri-net-based analysis techniques. this tool has been tested on different examples, and has been used to answer a variety of questions.formal semantics and analysis of control flow in ws-bpel','Information flow control'
'formulation and evaluation of scheduling techniques for control flow graphs','Information flow control'
'decentralized information flow control (difc) is an approach to security that allows application writers to control how data flows between the pieces of an application and the outside world. as applied to privacy, difc allows untrusted software to compute with private data while trusted security code controls the release of that data. as applied to integrity, difc allows trusted code to protect untrusted software from unexpected malicious inputs. in either case, only bugs in the trusted code, which tends to be small and isolated, can lead to security violations. we present flume, a new difc model that applies at the granularity of operating system processes and standard os abstractions (e.g., pipes and file descriptors). flume was designed for simplicity of mechanism, to ease difc\'s use in existing applications, and to allow safe interaction between conventional and difc-aware processes. flume runs as a user-level reference monitor onlinux. a process confined by flume cannot perform most system calls directly; instead, an interposition layer replaces system calls with ipcto the reference monitor, which enforces data flowpolicies and performs safe operations on the process\'s behalf. we ported a complex web application (moinmoin wiki) to flume, changingonly 2\% of the original code. performance measurements show a 43\% slowdown on read workloadsand a 34\% slowdown on write workloads, which aremostly due to flume\'s user-level implementation.information flow control for standard os abstractions','Information flow control'
'instruction scheduling algorithms are used in compilers to reduce run-time delays for the compiled code by the reordering or transformation of program statements, usually at the intermediate language or assembly code level. considerable research has been carried out on scheduling code within the scope of basic blocks, i.e., straight line sections of code, and very effective basic block schedulers are now included in most modern compilers and especially for pipeline processors. in previous work golumbic and rainis: ibm j. res. dev., vol. 34, pp.93-97, 1990, we presented code replication techniques for scheduling beyond the scope of basic blocks that provide reasonable improvements of running time of the compiled code, but which still leaves room for further improvement. in this article we present a new method for scheduling beyond basic blocks called shacoof. this new technique takes advantage of a conventional, high quality basic block scheduler by first suppressing selected subsequences of instructions and then scheduling the modified sequence of instructions using the basic block scheduler. a candidate subsequence for suppression can be found by identifying a region of a program control flow graph, called an s-region, which has a unique entry and a unique exit and meets predetermined criteria. this enables scheduling of a sequence of instructions beyond basic block boundaries, with only minimal changes to an existing compiler, by identifying beneficial opportunities to cover delays that would otherwise have been beyond its scope.instruction scheduling across control flow','Information flow control'
'autonomic communication and computing is a new paradigm for dynamic service integration over a network. an autonomic network crosses organizational and management boundaries and is provided by entities that see each other just as partners. for many services no autonomic partner may guess a priori what will be sent by clients nor clients know a priori what credentials are required to access a service. to address this problem we propose a new interactive access control: servers should interact with clients, asking for missing credentials necessary to grant access, whereas clients may supply or decline the requested credentials. servers evaluate their policies and interact with clients until a decision of grant or deny is taken. this proposal is grounded in a formal model on policy-based access control. it identifies the formal reasoning services of deduction, abduction and consistency. based on them, the work proposes a comprehensive access control framework for autonomic systems. an implementation of the interactive model is given followed by system performance evaluation.interactive access control for autonomic systems','Information flow control'
'control flow analysis is a widely used approach for analysing functional and object oriented programs. once the applications become more demanding also the analysis needs to be more precise in its ability to deal with mutable state (or side-effects) and to perform polyvariant (or context-sensitive) analysis. several insights in data flow analysis and abstract interpretation show how to do so for imperative programs but the techniques have not had much impact on control flow analysis. we show how to incorporate a number of key insights from data flow analysis (involving such advanced interprocedural techniques as call strings and assumption sets) into control flow analysis (using abstract interpretation to induce the analyses from a collecting semantics).interprocedural control flow analysis','Information flow control'
'in this paper we provide an interprocedural algorithm for reconstructing the control flow of assembly code in presence of indirect jumps, call instructions and returns. in case that the underlying assembly code is the output of a compiler, indirect jumps primarily originate from high-level switch statements. for these, our methods succeed in resolving indirect jumps with high accuracy. we show that by explicitly handling procedure calls, additional precision is gained at calls to procedures exiting the program as well as through the analysis of side-effects of procedures onto the local state of the caller. our prototypical implementation applied to real-world examples shows that this approach yields reliable and meaningful results with decent efficiency.interprocedural control flow reconstruction','Information flow control'
'we introduce the use, monitoring, and enforcement of integrity constraints in trust management-style authorization systems. we consider what portions of the policy state must be monitored to detect violations of integrity constraints. then, we address the fact that not all participants in a trust-management system can be trusted to assist in such monitoring, and show how many integrity constraints can be monitored in a conservative manner so that trusted participants detect and report if the system enters a policy state from which evolution in unmonitored portions of the policy could lead to a constraint violation.maintaining control while delegating trust','Information flow control'
'an information flow control model prevents information leakage during the execution of an application. quite a few information flow control models have been developed. we also developed information flow control models based on role-based access control (rbac). our research revealed that user relationships might result in role relationships when users play roles in an application. moreover, role relationships may cause role permission change. we also identified that role relationship change may invalidate the results of previous join operations (which prevent indirect information leakage). according to our survey, we cannot identify a model that manages role relationships well. moreover, we cannot identify a model that corrects the invalidated join results. this paper presents an information flow control model that manages role relationships and corrects the invalidated join results. it is an extension of our previous work oorbac (object-oriented rbac). the model is named eoorbac (extended oorbac). we evaluate eoorbac against oorbac. the evaluation result is also shown in this paper.managing role relationships in an information flow control model','Information flow control'
'this paper describes the application of discrete event simulation to study continuous material flow. logistics is an integrated part of most manufacturing companies. the purpose of this study is to determine the required logistics operations to allow continuous operations of a chemical manufacturing plant. the application has been used to provide critical decision support.material flow and inventory control applications','Information flow control'
'current anomaly detection schemes focus on control flow monitoring. recently, chen et al. [2] discovered that a large category of attacks tamper program data but do not alter control flows. these attacks are not only realistic, but are also as important as classical attacks tampering control flows. detecting these attacks is a critical issue but has received little attention so far. in this work, we propose an intrusion detection scheme with both compiler and micro-architecture support detecting data tampering directly. the compiler first identifies program regions in which the data should not be modified as per program semantics. then the compiler performs an analysis to determine the conditions for modification of variables in different program regions and conveys this information to the hardware and the hardware checks the data accesses based on the information. if the compiler asserts that the data should not be modified but there is an attempt to do so at runtime, an attack is detected. the compiler starts with a basic scheme achieving maximum data protection but such a scheme also suffers from high performance overhead. we then attempt to reduce the performance overhead through different optimization techniques. our experiments show that our scheme achieves strong memory protection with tight control over the performance degradation. thus, our major contribution is to provide an efficient scheme to detect data tampering while minimizing the overhead.memory protection through dynamic access control','Information flow control'
'aspect mining tries to identify crosscutting concerns in existing systems and thus supports the adaption to an aspect-oriented design. this paper describes an automatic static aspect mining approach, where the control flow graphs of a program are investigated for recurring execution patterns based on different constraints, such as the requirement that the patterns have to exist in different calling contexts. a case study done with the implemented tool shows that most discovered crosscutting candidates are instances of crosscutting delegation and should not be refactored into aspects.mining control flow graphs for crosscutting concerns','Information flow control'
'trust and assurance of mobile platforms is a prime objective when considering their use in security-critical scenarios like in healthcare or e-government. currently, several complementary approaches can be found in literature, ranging from purely hardware based to operating system level and application level solutions. together, they build a \"trusted and secured\" technology stack. however, the complex policy configuration mechanisms at every single layer also represent the biggest stumbling block for a rapid adoption. we propose a practicable and efficient solution for leveraging operating system level and application level security mechanisms to realize security-critical applications and services.model-driven configuration of os-level mandatory access control','Information flow control'
'modelling flow control in estelle','Information flow control'
'the architectural design decides the quality and the longevity of the software. gross decomposition of a system into interacting components using proper abstractions for component interaction defines the modularity of the system which in turn decides the values of quality attributes such as performance, reliability, security and modifiability as well as the percentage of design reuse. the decisions of modularization are supported by metrics like cohesion and coupling. in this paper we will focus on the quality attributes, modifiability and evolvability of a system which are overlapping to a large extent and which mainly get affected by the modularization of the system. the principle of encapsulation in object oriented (oo) design overcame the flaws present in structured methodology due to separate data and process components and their interdependencies. but problems in the evolution of oo systems due to crosscutting concerns were resolved using aspect oriented paradigm. the externalization of business logic using rule-based systems also was taken as solution to the evolution of complex software systems. here we focus on the different existing modularization solutions which support the evolvability of a software and the framework cffes (control flow framework for evolving systems) proposed by us.modularization with externalization of control flow','Information flow control'
'monotonicity of optimal flow control for failure-prone production systems','Information flow control'
'we proposed earlier an optimization approach to flow control where the objective of the control is to maximize the aggregate utility of all sources over their transmission rates. the control mechanism is derived as a gradient projection algorithm to solve the dual problem. the algorithm assumes that a single path serves each source. in this paper, we extend the algorithm to the case where there are multiple paths between sources {destination pair. this allows flow control and routing to be jointly optimized.multipath optimization flow control','Information flow control'
'in this paper, we present a data-flow system which supports comparative analysis of time-dependent data and interactive simulation steering. the system creates data on-the-fly to allow for the exploration of different parameters and the investigation of multiple scenarios. existing data-flow architectures provide no generic approach to handle modules that perform complex temporal processing such as particle tracing or statistical analysis over time. moreover, there is no solution to create and manage module data, which is associated with alternative scenarios. our solution is based on generic data-flow algorithms to automate this process, enabling elaborate data-flow procedures, such as simulation, temporal integration or data aggregation over many time steps in many worlds. to hide the complexity from the user, we extend the world lines interaction techniques to control the novel data-flow architecture. the concept of multiple, special-purpose cursors is introduced to let users intuitively navigate through time and alternative scenarios. users specify only what they want to see, the decision which data are required is handled automatically. the concepts are explained by taking the example of the simulation and analysis of material transport in levee-breach scenarios. to strengthen the general applicability, we demonstrate the investigation of vortices in an offline-simulated dam-break data set.multiverse data-flow control','Information flow control'
'in this paper we present a concept for controlling the way information flows in a network by labeling packets and controlling the way they flow inside the network. we introduce the security model which is a simple distributed information flow control (difc) model enabling the definition of security classes for the labels and the security policy. we provide a proof of concept of the proposed network information flow control using an implementation based on labeling mechanisms that are readily available for quality of service (qos) of vlan network management devices.network information flow control','Information flow control'
'the clear advantages inherent in dynamic sharing of resources has accelerated the development of distributed processing. the number of networks which provide resource sharing have increased steadily from the early years of arpa network. as the number of users sharing the resources increases, there is a potential danger for degradation of services, if the demand for the resources is greater than the available resources. the maximum traffic that can be carried by a network depends on the finite network resources such as link bandwidth and storage buffers. an uncontrolled flow of traffic results in service degradation and deadlock if the traffic allowed into the network exceeds the capacity of the network.networks and flow control','Information flow control'
'optical control of flow-induced vibration of pipeline','Information flow control'
'this paper presents a method to improve the performance of the storage network. there are two parameters which are very important to the performance of the storage network. they are congestion window in the tcp layer and maxburstlength in the iscsi layer. they both have the similar function that controls the flow of the network. when the network is busy, they both become smaller and vise versa. but because the tcp layer is lower than the network storage layer, it can detect the network state earlier. the time to change these parameters is different and the max burst length is always changed later. so there is a period in which the max burst length is not match the network state perfectly which can diminish the utilization of network bandwidth. the method which changes these two parameters simultaneously is presented in this paper and its results are proved by the experiments. this optimization can be applied in most of the network storage system in the market.optimization of flow control mechanism in iscsi','Information flow control'
'this paper proposes a new sensitivity factor to solve the real-time line flow calculation problem, which is called jacobian based distribution factor(jbdf). it is derived from the jacobian matrix of the base case newton-raphson load flow solution, and kept constant during real-time line flow calculation. different from the well-known distribution factors, such as generation shift distribution factor (gsdf), generalized generation shift distribution factor (ggdf) and z-bus distribution factor (zbd), it not only reflects the change of each bus injection active power, but also the reactive power. when the loading conditions change from the base case loads, either conforming or non-confirming changes in real and reactive power, it can fast and precisely compute the active line flows without the need of iterations. the proposed jbdf method is tested by ieee 14-bus system. the numerical simulation results demonstrate that the proposed method is very accurate and fast in computation. it is very suitable for real-time applications.power system fast line flow calculation for security control by sensitivity factor','Information flow control'
'organizations are increasingly faced with the challenge of managing business processes, workflows, and, recently, web processes. one important aspect of processes that has been overlooked is their complexity. high complexity in processes may result in bad understandability, errors, defects, and exceptions leading processes to need more time to develop, test, and maintain. therefore, excessive complexity should be avoided. this paper describes an experiment designed to validate the control-flow complexity (cfc) metric that we have proposed in our previous work. in order to demonstrate that our cfc metric serves the purpose it was defined for, we have carried out an empirical validation by means of a controlled experiment. the explanation of the steps followed to do the experiment, the results, and the conclusions obtained are the main objectives of this paper.process control-flow complexity metric','Information flow control'
'protecting privacy within an application is essential. many information flow control models have been developed for that protection. we developed an information flow control model based on role-based access control (rbac) for object-oriented systems, which is called oorbac (object-oriented role-based access control). according to the experiences of using oorbac, we found that a model allowing every secure information flow and blocking every non-secure flow is too restricted. we propose that the following flexible access control features should be offered: (a) non-secure but harmless information flows should be allowed and (b) secure but harmful information flows should be blocked. according to our survey, no existing model offers the above control. we thus revised oorbac to offer the control. the revised oorbac have been implemented and evaluated. this paper presents flexible access control in the revised oorbac and the evaluation result.providing flexible access control to an information flow control model','Information flow control'
'model or specification based intrusion detection systems have been effective in detecting known and unknown host based attacks with few false alarms [12, 15] in this approach, a model of program behavior is developed either manually, by using a high level specification language, or automatically, by static or dynamic analysis of the program the actual program execution is then monitored using the modeled behavior; deviations from the modeled behavior are flagged as attacks in this paper we discuss a novel model generated using static analysis of executables (binary code) our key contribution is a model which is precise and runtime efficient specifically, we extend the efficient control flow graph (cfg) based program behavioral model, with context sensitive information, thus, providing the precision afforded by the more expensive push down systems (pds) executables are instrumented with operations on auxiliary variables, referred to as proxi variables these annotated variables allow the resulting context sensitive control flow graphs obtained by statically analyzing the executables to be deterministic at runtime we prove that the resultant model, called proxi-annotated control flow graph, is as precise as previous approaches which use context sensitive push-down models and in-fact, enhances the runtime efficiency of such models we show the flexibility of our technique to handle different variations of recursion in a program efficiently this results in better treatment of monitoring programs where the recursion depth is not pre-determined.proxi-annotated control flow graphs','Information flow control'
'in this paper, we propose a dynamic optimization approach to end-to-end flow control in data networks. the objective is to maximize the aggregate utilities of the data sources over soft transmission rate bounds and delay constraints. the network links and data sources are considered as processors of a distributed computational system that has a global objective function. the presented model works with different shapes of utility curves under the proposition of elastic data traffic. the approach relies on real-time observations of the delay as a measure of the data network congestion at the routers (network nodes). a primal-dual algorithm carried out by the data sources is used to solve the optimization problem in a decentralized manner. the calculated transmission rates are bounded and the sources are subjected to a maximum number of data packets that can be queued downstream of each transmission session. the algorithm solves for the rates without the access to any network global information while each source calculates its transmission rate that should maximize the global objective function. the calculated optimal rates conform to rate-to-queue proportionality. finally, we present an extensive simulation results to demonstrate the reliability of the algorithm.real-time optimization flow control','Information flow control'
'we propose and experimentally evaluate a technique of authenticating the execution of a program through the continuous run-time validation of control flow. control flow authentication is useful in detecting security violations that alter the normal flow of control at run time through techniques such as call stack smashing, return and jump-oriented programming. our technique relies on the use of existing support for branch tracing in contemporary processors, typified by the branch trace store (bts) mechanism of contemporary intel x86 server platforms. in contrast to existing techniques that require code modification, either statically or at run-time, our technique requires no modifications to the binaries, thus preserving binary compatibility. in this paper, we demonstrate how the existing hardware support for branch tracing can be used to perform control flow validation covering each and every executed control flow instruction in an application or the kernel as they run. although the performance overhead for full and continuous control flow authentication for an entire application is significant, we show how the technique can be used judiciously to selectively perform full and continuous control flow validation of critical functions with a tolerable overhead. as an example of this selective approach, we show how our technique detects security compromises introduced by kernel rootkits with a tolerable overhead.run-time control flow authentication','Information flow control'
'simulation helps tire manufacturer change from push to pull system in controlling material flow','Information flow control'
'taking use of intelligent group control technique, a customers-flow model and its based-on elevators group control technique and its simulation program are built and realized in this paper, which has the characteristics of strong adaptability, low cost and reliability. it is proofed that it can be used to promote the efficiency of elevator system in practices.simulation of customers-flow model based-on elevators group control technique','Information flow control'
'to allow better network utilization, systems network architecture (sna), the ibm data communications architecture, includes flow control procedures to guard against data overrun to devices and to prevent network congestion. the measurement of \"congestion\" used by sna to regulate traffic flow is performed by various sna products. this paper describes the flow control protocols in sna and the implementation of these protocols in the network control program (acf/ncp/vs release 3).sna flow control','Information flow control'
'embedded control systems consist of multiple components with different criticality levels interacting with each other. for example, in a passenger jet, the navigation system interacts with the passenger entertainment system in providing passengers the distance-to-destination information. it is imperative that failures in the non-critical subsystem should not compromise critical functionality. this architectural principle for robustness can, however, be easily compromised by implementation-level errors. we describe safe- flow, which statically analyzes core components in the system to ensure that they use non-core values communicated through shared memory only if they are run-time monitored for safety or recoverability. using simple, local annotations and semantic restrictions on shared memory usage in the core component, safeflow precisely identifies accesses to unmonitored non-core values. with a few false positives, it identifies erroneous dependencies of critical data on noncore values that can arise due to programming errors, inadvertent accesses, or wrong assumptions regarding the absence of difficult-to-detect implementation errors such as data races and synchronization. we demonstrate the utility of safeflow by applying it to discover critical value flow dependencies in three prototype systems.static analysis to enforce safe value flow in embedded control systems','Information flow control'
'recent advances in the mathematical analysis of flow control have prompted the creation of the scalable tcp (stcp) and exponential red (e-red) algorithms. these are designed to be scalable under the popular deterministic delay stability modeling framework. in this article, we analyze stochastic models of stcp and stcp combined with e-red link behavior. we find that under certain plausible network conditions, these probabilistic models also exhibit scalable behavior. in particular, we derive parameter choice schemes for which the equilibrium coefficients of variation of flow rates are bounded, however large, fast, or complex the network. our model is shown to exhibit behavior similar to the mean field convergence that has recently been observed in tcp.stochastically scalable flow control','Information flow control'
'in this paper, we describe how to extend the concept of superword-level parallelization (slp), used for multimedia extension architectures, so that it can be applied in the presence of control flow constructs. superword-level parallelization involves identifying scalar instructions in a large basic block that perform the same operation, and, if dependences do not prevent it, combining them into a superword operation on a multi-word object. a key insight is that we can use techniques related to optimizations for architectures supporting predicated execution, even for multimedia isas that do not provide hardware predication. we derive large basic blocks with predicated instructions to which slp can be applied. we describe how to minimize overheads for superword predicates and re-introduce control flow for scalar operations. we discuss other extensions to slp to address common features of real multimedia codes. we present automatically-generated performance results on 8 multimedia codes to demonstrate the power of this approach. we observe speedups ranging from 1.97x to 15.07x as compared to both sequential execution and slp alone.superword-level parallelism in the presence of control flow','Information flow control'
'decentralized workflow enactment -- the process of evaluating control flow in a distributed manner -- is a key aspect of the implementation of a decentralized workflow management system (wfms). a major challenge in this field is the support for join operations to synchronize concurrent threads of control flow. the original linda model however does not provide operations for matching more than one tuple in a single operation -- complex logic needs to be implemented on the client side, having severe impact on performance and breaking the concept of coordination languages by mixing coordination and application logic. in this paper, we stress the need for an extended tuplespace model that natively supports the sync operation realizing ws-bpel synchronizing joins directly on the tuplespace level. we pay special attention to the description of its semantics and propose an algorithm for efficient implementation on a single tuplespace. for the common case in distributed workflow enactment, where control flow is distributed over multiple tuplespaces, we present an optimization of the aforementioned algorithm in form of the sync pattern.synchronizing control flow in a tuplespace-based, distributed workflow management system','Information flow control'
'as companies move towards many-core chips, an efficient on-chip communication fabric to connect these cores assumes critical importance. to address limitations to wire delay scalability and increasing bandwidth demands, state-of-the-art on-chip networks use a modular packet-switched design with routers at every hop which allow sharing of network channels over multiple packet flows. this, however, leads to packets going through a complex router pipeline at every hop, resulting in the overall communication energy/delay being dominated by the router overhead, as opposed to just wire energy/delay.token flow control','Information flow control'
'this is a new applied development of trace theory to compilation. trace theory allows to commute independent program instructions, but overlooks the differences between control and data dependencies. control(c)-dependences, unlike data-dependences, are determined by the control flow graph, modelled as a local dfa. to ensure semantic equivalence, partial commutation must preserve c-dependences. new properties are proved for c-dependences and corresponding traces. any local language is star-connected with respect to c-dependences, hence this trace language family is recognizable. local languages unambiguously represent traces. within the family of local languages with the same c-dependences, we construct the language such that instructions are maximally anticipated. this language differs from the foata-cartier normal form. future directions for application of trace theory to program optimization are outlined.traces of control-flow graphs','Information flow control'
'the focus of this paper is on a facts device known as the unified power flow controller (upfc), which can provide simultaneous control of basic power system parameters like voltage, impedance and phase angle. in this research work, two simulation models of single machine infinite bus (smib) system, i.e. with & without upfc, have been developed. these simulation models have been incorporated into matlab based power system toolbox (pst) for their transient stability analysis. these models were analysed for three phase fault at different locations, i.e. at the middle and receiving end of the transmission line keeping the location of upfc fixed at the receiving end of the line. transient stability was studied with the help of curves of fault current, active & reactive power at receiving end, shunt injected voltage & its angle, series injected voltage & its angle and excitation voltage. with the addition of upfc, the magnitude of fault current reduces and oscillations of excitation voltage also reduce. series and shunt parts of upfc provide series and shunt injected voltage at certain different angles. therefore, it can be concluded that transient stability of smib is improved with the addition of unified power flow controller.transient stability improvement of smib with unified power flow controller','Information flow control'
'the current trend toward heterogeneous architectures motivates us to reconsider current software and hardware paradigms. the focus is centered around new parallel programming models, compiler design, and runtime resource management techniques to exploit the features of many-core processor architectures. graphics processing units (gpu) have become the platform of choice in this area for accelerating a large range of data-parallel and task-parallel applications. the rapid adoption of gpu computing has been greatly aided by the introduction of high-level programming environments such as cuda c and opencl. however, each vendor implements these programming models differently and we must analyze the internals in order to get a better understanding of the performance results. one of the main differences across implementations is the handling of program control flow by the compiler and the hardware. some implementations can support unstructured control flow based on branches and labels; others are based on structured control flow relying solely on if-then and while constructs. in this paper we describe a tool that can be used to analyze the difference between these two approaches. we created a dynamic compiler called caracal that translates applications with unstructured control flow so they can run on hardware that requires structured programs. in order to accomplish this, caracal builds a control tree of the program and creates single-entry, single-exit regions called hammock graphs. we used this tool to analyze the performance differences between nvidia\'s implementation of cuda c and amd\'simplementation of opencl. we found that the requirement for structured control flow can increase the number of registers allocated by 20 registers and impact performance as much as2x.unstructured control flow in gpgpu','Information flow control'
'the representation and execution of business processes have generated some important challenges in computer science. an important related concern is the choosing of the best formal foundation to specify processes behavior, mainly representing control-flow patterns in cooperative environments. the first contribution of this research is the complete definition of the navigation plan definition language (npdl) as an alternative for business process managing in cooperative environments. the second contribution is a complete implementation of control-flow patterns using npdl. these control-flow patterns have been proposed by aalst\'s group. our experience in applying suggestion of aalst\'s group to use control-flow patterns as a basis for comparison among control-flow specification languages shows that this comparison method is feasible and the results are useful. the simplicity of npdl representations shows the advantages of npdl as a process specification language. npdl uses a declarative specification (similar to process algebra) to describe the workflow and adds new operators to compensate for the limitations of process algebra and petri nets. npdl also increases the modeling flexibility by allowing the reuse of process expressions in relational data-base systems.using control-flow patterns for specifying business processes in cooperative environments','Information flow control'
'an important issue in computer network design is end-to-end control, a term covering error control and flow control. the validity of schemes for these purposes may be demonstrated using simple formal models, in which assertions about central program variables are proven. s. krogdahl [2] and d. e. knuth [1] have shown the validity of error control for a class of data link protocols using such methods. in this paper their method is extended to cover flow control. the method is illustrated by proving assertions for a simple case, and it is shown how complex systems may be seen as compositions of the simple case.verification of flow control protocols','Information flow control'
'network throughput can be increased by dividing the buffer storage associated with each network channel into several virtual channels. each physical channel is associated with several small queues, virtual channels, rather than a single deep queue. the virtual channels associated with one physical channel are allocated independently but compete with each other for physical bandwidth. virtual channels decouple buffer resources from transmission resources. this decoupling allows active messages to pass blocked messages using network bandwidth that would otherwise be left idle. the paper studies the performance of networks using virtual channels using both analysis and simulation. these studies show that virtual channels increase network throughput, by a factor of four for 10-stage networks, and reduce the dependence of throughput on the depth of the network.virtual-channel flow control','Information flow control'
'the difficulties associated with visualising control flow are well-known to visual language designers. it becomes even more problematical for low level code, where recognisable control structures are often the exception rather than the norm. conventional control flow graphs are incomprehensible for such code, even when viewed in terms of basic blocks.in our our work on visualising low level operations, we have designed a system that allows the programmer graphically to specify the modularity of the code as it is written, and to use the visual environment to interconnect, manipulate and view these modules. for code that has been created outside the system, we adopt a technique devised by the software engineering community to depict lcsaj(linear code sequence and jump) spans as the control flow nodes. finally, we introduce the notion of \'focus\' to allow a programmer to concentrate not only on individual nodes, but also on the program context in which those nodes are set.visualising complex control flow','Information flow control'
'work the shell: conditional statements and flow control','Information flow control'
'deadlock-free flow control should be designed with minimal cost, particularly for on-chip designs where area and power resources are greatly constrained. while bubble flow control, proposed a decade ago, can avoid deadlock in vct-switched tori with only one virtual channel (vc), there has been no working solution for wormhole switching that achieves the similar objective. wormhole switching allows the channel buffer size to be smaller than the packet size, thus is preferred by on-chip networks. however, wormhole packets can span multiple routers, thereby creating additional channel dependences and adding complexities in both deadlock and starvation avoidance. in this paper, we propose worm-bubble flow control (wbfc), a new flow control scheme that can avoid deadlock in wormhole-switched tori using minimally 1-flit-sized buffers per vc and one vc in total. moreover, any wormhole-switched topology with embedded rings can use wbfc to avoid deadlock within each ring. simulation results from synthetic traffic and parsec benchmarks show that the proposed approach can achieve significant throughput improvement and also area and energy savings compared to an optimized dateline routing approach.worm-bubble flow control','Information flow control'
'a comparative study of conceptual data modeling techniques','Information-theoretic techniques'
'software change impact analysis (cia) is an essential technique to identify the unpredicted and potential effects caused by software changes. a rich body of different cia techniques, especially static cia techniques, have continuously emerged in recent years. however, it is difficult for researchers or practitioners to decide which technique is most appropriate for their needs, or which cia technique is more effective. unfortunately, there was only a few work on the comparison of the cia techniques. this paper presents a comparison study of different types of popular static cia approaches, i.e., structural static analysis, textual analysis, and historical analysis. for each kind of static cia approach, we introduce a representative technique, that is fca -- cia, rose, and irc2m, respectively. finally, some empirical studies are conducted on three real-world programs to compare the accuracy of these cia techniques based on the precision and recall metrics. the results show that the accuracy of these three cia techniques is different, and fca - cia has the best precision while the irc2m has the best recall.a comparative study of static cia techniques','Information-theoretic techniques'
'this paper compares six general file organization techniques on four characteristics. then drawing on the implications of a set-theoretic approach, it sketches a seventh technique offering apparently a superior combination of characteristics for data base organization. the six techniques compared are the positional (sequential, indexed sequential, and random), the complex ring, the hierarchial (including partitioned), the multiple double-linked (muble) chain, the attributed string, and the inverted list. the four characteristics are naturalness, access, maintenance, and support.a comparison of file organization techniques','Information-theoretic techniques'
'a framework for the comparative evaluation of knowledge acquisition tools and techniques','Information-theoretic techniques'
'the proposal outlined here is based on three premises: (1) that the digital computing field needs, and will continue to need, not only more people who are capable of designing and programming digital computers, but more people who understand the basic limitations and potential uses of digital computers; (2) that the computer industry should take an active interest in providing a basic computer training to the largest number of people, in addition to more extensive training to those who show an interest in designing and programming computers; and (3) that the typical 12-year-old youngster has the interest, skill and basic knowledge necessary to build and understand simple working models of practically anything. consider a typical 12-year old: you would not be surprised to learn that he\'s been flying gliders for years, has probably built at least one rubber-powered stick model airplane, and is wondering whether he should spend his allowance on a more fancy model. at this age, he can take his bike apart and put it together again, diagnose and often correct a short circuit on his friend\'s electric train, build a bridge with his erector set, and can develop and print pictures taken with his own box camera. he knows the difference between a rocket and a jet, alternating current and direct current, telescopes and microscopes, and between a gasoline engine and a diesel. for a small sum of money - often less than $20 - he can buy himself a working model of any of these things, plus a subscription to a magazine which will keep him informed of progress in the real devices, in the models, and in ways of using and improving the models. it is not surprising to find such youngsters entering the aeronautics, automotive, and electronics industries.a proposal for training youngsters in digital computing techniques','Information-theoretic techniques'
'b-trees have been ubiquitous in database management systems for several decades, and they are used in other storage systems as well. their basic structure and basic operations are well and widely understood including search, insertion, and deletion. concurrency control of operations in b-trees, however, is perceived as a difficult subject with many subtleties and special cases. the purpose of this survey is to clarify, simplify, and structure the topic of concurrency control in b-trees by dividing it into two subtopics and exploring each of them in depth.a survey of b-tree locking techniques','Information-theoretic techniques'
'as one of the most successful approaches to building recommender systems, collaborative filtering (cf) uses the known preferences of a group of users to make recommendations or predictions of the unknown preferences for other users. in this paper, we first introduce cf tasks and their main challenges, such as data sparsity, scalability, synonymy, gray sheep, shilling attacks, privacy protection, etc., and their possible solutions. we then present three main categories of cf techniques: memory-based, modelbased, and hybrid cf algorithms (that combine cf with other recommendation techniques), with examples for representative algorithms of each category, and analysis of their predictive performance and their ability to address the challenges. from basic techniques to the state-of-the-art, we attempt to present a comprehensive survey for cf techniques, which can be served as a roadmap for research and practice in this area.a survey of collaborative filtering techniques','Information-theoretic techniques'
'a survey of information requirements analysis techniques','Information-theoretic techniques'
'a survey of knowledge acquisition techniques and tools','Information-theoretic techniques'
'this demo presents a desktop vr system for evaluating human performance in 3d pointing tasks. the system supports different input devices (e.g., mouse and 6dof remote pointer), pointing techniques (e.g., screen-plane and depth cursors), and cursor visualization styles (e.g., one-eyed and stereo 3d cursors). the objective is to comprehensively compare all combinations of these conditions. we especially focus in on fair and direct comparisons between 2d and 3d pointing tasks. finally, our system includes a new pointing technique that outperforms standard ray pointing.a system for evaluating 3d pointing techniques','Information-theoretic techniques'
'distributed question answering services, like yahoo answer and aardvark, are known to be useful for end users and have also opened up numerous topics ranging in many research fields. in this paper, we propose a user-support tool for composing questions in such services. our system incrementally recommends similar questions while users are typing their question in a sentence, which gives the users opportunities to know that there are similar questions that have already been solved. a question database is semantically analyzed and searched in the semantic space by boosting the performance of similarity searches with database techniques such as server/client caching and lsh (locality sensitive hashing). the more text the user enters, the more similar the recommendations will become to the ultimately desired question. this unconscious editing-as-a-sequence-of-searches approach helps users to form their question incrementally through interactive supplementary information. not only askers nor repliers, but also service providers have advantages such as that the knowledge of the service will be autonomously refined by avoiding for novice users to repeat questions which have been already solved.accelerating instant question search with database techniques','Information-theoretic techniques'
'adaptable quad tree techniques','Information-theoretic techniques'
'adaptive learning defuzzification techniques and applications','Information-theoretic techniques'
'an increasing number of companies rely on distributed data storage and processing over large clusters of commodity machines for critical business decisions. although plain mapreduce systems provide several benefits, they carry certain limitations that impact developer productivity and optimization opportunities. higher level programming languages plus conceptual data models have recently emerged to address such limitations. these languages offer a single machine programming abstraction and are able to perform sophisticated query optimization and apply efficient execution strategies. in massively distributed computation, data shuffling is typically the most expensive operation and can lead to serious performance bottlenecks if not done properly. an important optimization opportunity in this environment is that of judicious placement of repartitioning operators and choice of alternative implementations. in this paper we discuss advanced partitioning strategies, their implementation, and how they are integrated in the microsoft scope system. we show experimentally that our approach significantly improves performance for a large class of real-world jobs.advanced partitioning techniques for massively distributed computation','Information-theoretic techniques'
'due to rapid changing business requirements the complexity in developing applications which deliver business solutions is continually growing. to manage this complexity, environments providing flexible metamodelling capabilities instead of fixed metamodels has shown to be helpful. the main characteristic of such environments is that the formalism of modelling -the metamodel -can be freely defined and therefore be adapted to the problem under consideration. this paper gives an introduction into metamodelling concepts and presents a generic architecture for metamodelling platforms. three best practice examples from industry projects applying metamodelling concepts in the area of business process modelling for e-business, e-learning, and knowledge management are presented. finally, an outlook to future developments and research directions in the area of metamodelling is given.advanced techniques for metamodelling','Information-theoretic techniques'
'advanced tips and techniques','Information-theoretic techniques'
'this paper evaluates five supervised learning methods in the context of statistical spam filtering. we study the impact of different feature pruning methods and feature set sizes on each learner\'s performance using cost-sensitive measures. it is observed that the significance of feature selection varies greatly from classifier to classifier. in particular, we found support vector machine, adaboost, and maximum entropy model are top performers in this evaluation, sharing similar characteristics: not sensitive to feature selection strategy, easily scalable to very high feature dimension, and good performances across different datasets. in contrast, naive bayes, a commonly used classifier in spam filtering, is found to be sensitive to feature selection methods on small feature set, and fails to function well in scenarios where false positives are penalized heavily. the experiments also suggest that aggressive feature pruning should be avoided when building filters to be used in applications where legitimate mails are assigned a cost much higher than spams (such as &#955; = 999), so as to maintain a better-than-baseline performance. an interesting finding is the effect of mail headers on spam filtering, which is often ignored in previous studies. experiments show that classifiers using features from message header alone can achieve comparable or better performance than filters utilizing body features only. this implies that message headers can be reliable and powerfully discriminative feature sources for spam filtering.an evaluation of statistical spam filtering techniques','Information-theoretic techniques'
'we propose the application of a dimensionality reduction algorithm that could provide a breakthrough in the efforts to retrieve and present mobile personal information to the user in context.application of dimensionality reduction techniques for mobile social context','Information-theoretic techniques'
'in the emerging area of sensor-based systems, a significantchallenge is to develop scalable, fault-tolerantmethods to extract useful information from the data thesensors collect.an approach to this data managementproblem is the use of sensor database systems, exemplifiedby tinydb and cougar, which allow users to performaggregation queries such as min, count andavg on a sensor network.due to power and range constraints,centralized approaches are generally impractical,so most systems use in-network aggregation to reducenetwork traffic.however, these aggregation strategiesbecome bandwidth-intensive when combined with thefault-tolerant, multi-path routing methods often used inthese environments.for example, duplicate-sensitive aggregatessuch as sum cannot be computed exactly usingsubstantially less bandwidth than explicit enumeration.to avoid this expense, we investigate the use of approximatein-network aggregation using small sketches.our contributions are as follows: 1) we generalize wellknown duplicate-insensitive sketches for approximatingcount to handle sum, 2) we present and analyze methodsfor using sketches to produce accurate results withlow communication and computation overhead, and 3)we present an extensive experimental validation of ourmethods.approximate aggregation techniques for sensor databases','Information-theoretic techniques'
'in this paper we present arches, a novel rule-based system implemented as a web tool for formulating and testing hypothetical archaeological social scenarios. scenarios are organized by archaeologists and are based on rules and facts that derive from findings in excavation areas. arches analyses different variables that encode social factors affecting areas and findings, in the past as well as the present. the presented tool was implemented using java web technologies and jess as the rule engine. arches is part of the seearchweb project that aims to develop a new instructional approach for the domain of archaeology.arches','Information-theoretic techniques'
'in this paper, we present art of defense (aod), a cooperative handheld augmented reality (ar) game. aod is an example of what we call an ar board game, a class of tabletop games that combine handheld computers (such as camera phones) with physical game pieces to create a merged physical/virtual game on the table-top. this paper discusses the technical aspects of the game, the design rationale and process we followed, and the resulting player experience. the goal of this research is to explore the affordances and constraints of handheld ar interfaces for collaborative social games, and to create a game that leverages them as fully as possible. the results from the user study show that the game is fun to play, and that by tightly registering the virtual content with the tangible game pieces, tabletop ar games enable a kind of social play experience unlike non-ar computer games. we hope this research will inspire the creation of other handheld augmented reality games in the future, both on and off the tabletop.art of defense','Information-theoretic techniques'
'associative document retrieval techniques using bibliographic information','Information-theoretic techniques'
'battle of the modeling techniques','Information-theoretic techniques'
'biblio-techniques,inc.: the promise that was blis','Information-theoretic techniques'
'this work investigates clustering techniques for relation extraction (re). relation extraction is the task of extracting relationships among named entities (e.g., people, organizations and geo-political entities) from natural language text. we are particularly interested in the open re scenario, where the number of target relations is too large or even unknown. our contributions are in two aspects of the clustering process: (1) extraction and weighting of features and (2) scalability. in order to evaluate our techniques in large scale, we propose an automatic evaluation method based on pointwise mutual information. our preliminary results show that our clustering techniques as well as our evaluation method are promising.clustering techniques for open relation extraction','Information-theoretic techniques'
'clustering techniques in object bases','Information-theoretic techniques'
'clustering techniques','Information-theoretic techniques'
'in this paper we present a comparison between different approaches to cfa (colour filter array) images encoding. we show different performances offered by a new algorithm based on a vector quantization technique, jpeg-ls a low complexity encoding standard and classical jpeg. we also show the effects of cfa image encoding on the colour reconstructed images by a typical image generation pipeline. a discussion about different computational complexity and memory requirement of the different encoding approaches is also presented.coding techniques for cfa data images','Information-theoretic techniques'
'the idea of a universal class of hash functions is due to carter and wegman. the goal is to define a collection of hash functions in such a way that a random choice of a function in the class yields a low probability that any two distinct inputs will collide. in this paper, we present some characterizations of universal classes of hash functions in terms of combinatorial designs such as resolvable balanced incomplete block designs and orthogonal arrays. the two classes of hash functions that we study are called optimally universal and strongly universal. we show that optimally universal classes of hash functions are equivalent to resolvable balanced incomplete block designs and strongly universal classes are equivalent to orthogonal arrays. consequently, known classes of combinatorial designs yield new, small, and efficient classes of universal hash functions.combinatorial techniques for universal hashing','Information-theoretic techniques'
'comments on practical constraints of software validation techniques','Information-theoretic techniques'
'we compare various document clustering techniques including k-means, svd-based method and a graph-based approach and their performance on short text data collected from twitter. we define a measure for evaluating the cluster error with these techniques. observations show that graph-based approach using affinity propagation performs best in clustering short text data with minimal cluster error.comparative study of clustering techniques for short text documents','Information-theoretic techniques'
'virt-uam (virtual worlds at universidad aut&#243;noma de madrid) platform allows to design and implement virtual spaces where a set of avatars can be intensively monitored using a set of tools which can be managed by an administrator. in a virtual world, the users can move and interact between them with a high degree of freedom. the movements, interactions and any other information related to the avatars conversations can be stored. hence this data is available for processing and analysing to obtain the user behavioural patterns. document clustering techniques have been intensively applied to automatically organize a document corpus into clusters or similar groups. the topic detection problem can be considered as a special case of document clustering, therefore, these techniques can be used over textual chat to detect clusters from the data, and then extract the conversation topics. mahout(tm) machine learning library is an apache(tm) project whose main goal is to build scalable machine learning libraries. this library provides a set of algorithms for data mining and for information retrieval ready to use. this paper shows a practical application of some of these available clustering mahout algorithms, in a virtual world-based scenario. these algorithms have been applied to extract the topics based on clusters obtained from the text messages. finally, a comparative study of these document clustering algorithms used is presented.comparative study of text clustering techniques in virtual worlds','Information-theoretic techniques'
'constituting users in requirements techniques','Information-theoretic techniques'
'contextual techniques starter kit','Information-theoretic techniques'
'innovation and novelty are seen as important elements in game design but systematic tools and methods for producing creative ideas may be little known or poorly available, and creativity itself can be seen as something mystical [2] that cannot be methodologically enhanced. however, modern creativity research claims that creativity is in the scope of learning and techniques for generating ideas are argued to give competitive advantage [1,2,6]. this may be an important message to designers, but also to the creative leaders. even the most creative mind can commit the crime of repetition. this is because it is natural for the mind to create patterns [1]. usually these patterns are helpful, but seeking new and innovative solutions as in product design, one should be able \"to think outside\" the common practices. designers are required to be creative on demand, yet the procedures and methods for breaking the common approaches are often based on intuitive belief systems rather than on empirically validated theory [9]. one of the solutions to enhance creativity in game design is to use idea generation techniques that help designers to be creative on demand. studies from other industries suggest that there is a strong relationship between the number of idea generation techniques and the number of successful products [8,10]. however, brainstorming, the best known technique, does not necessarily lead to innovation [5], which is also acknowledged in game design [3]. even though brainstorming is useful in some cases, no single creativity technique can provide the ultimate solution for innovation in general: different techniques are needed [10]. idea generation may seem a relatively easy task. however, while anybody can come up with some ideas, applicable and novel ideas do not come easily [9]. this is well established in those studies showing that one of the characteristics of companies successful in development is their ability to generate ideas [3]. in a successful ideating session, the generation of ideas is separated from idea evaluation and early criticism may be seen as harmful to the overall process[6]. whereas vertical thinking targets the one and only solution, lateral thinking targets quantity [1] as a tool for quality [6]. additionally, since idea generation is not a random process governed solely by an individual\'s personal traits, but a relatively structured process that can be explained [8], a methodological approach is indeed possible. since we believe that game ideas have their special characteristics, and that and general idea generation techniques may not be so very supportive of the nature of game design processes, we designed several experimental game-specific techniques in the gamespace project (http://gamelab.uta.fi/gamespace). these techniques are based on game-related stimuli and structural modules for ideating casual, multiplayer and mobile games. during the project, computer programs and other tools were created to help documenting, game analysis, randomization of stimuli and communicative aspects. these techniques are easy to approach from their functional aspects: the activity of idea generation is based on playing specific board games, card games, using small computer applications or other tools and toys. in light of our workshop experiences with finnish game professionals in 2006 and 2007, these idea generation techniques can be successfully utilized and help designers to create applicable and novel game ideas that they would not otherwise come up with. hence these techniques can be seen as a successful way to help \"creativity on demand\" in game design practices. some of these techniques have already been fruitfully adopted by the finnish mobile game industry. while we have already documented several positive user experiences and know that our techniques work, we are conducting a more extensive user study in autumn 2007 and spring 2008 to gain a systematic understanding of game specific idea generation techniques and game idea generation processes.creativity techniques in game design','Information-theoretic techniques'
'we describe an interactive multimedia application called crossing colorful communication (c.c.c), players of which can enjoy collaborative and interactive media control by speaking or making sound. players of c.c.c. fire visual bullets, the direction and the size of which are determined by the pitch and the intensity of each player\'s voice. when bullets collide on the screen, visual and sound effects are created. players of c.c.c. can enjoy a new style of communication powered by mediatechnologies.crossing colorful communication','Information-theoretic techniques'
'predictive models developed by applying data mining techniques are used to improve forecasting accuracy in the airline business. in order to maximize the revenue on a flight, the number of seats available for sale is typically higher than the physical seat capacity (overbooking). to optimize the overbooking rate, an accurate estimation of the number of no-show passengers (passengers who hold a valid booking but do not appear at the gate to board for the flight) is essential. currently, no-shows on future flights are estimated from the number of no-shows on historical flights averaged on booking class level. in this work, classification trees and logistic regression models are applied to estimate the probability that an individual passenger turns out to be a no-show. passenger information stored in the reservation system of the airline is either directly used as explanatory variable or used to create attributes that have an impact on the probability of a passenger to be a no-show. the total number of no-shows in each booking class or on the total flight is then obtained by accumulating the individual no-show probabilities over the entity of interest. we show that this forecasting approach is more accurate than the currently used method. in addition, the selected models lead to a deepened insight into passenger behavior.data mining techniques to improve forecast accuracy in airline business','Information-theoretic techniques'
'data mining, or knowledge discovery in databases, has been popularly recognized as an important research issue with broad applications. we provide a comprehensive survey, in database perspective, on the data mining techniques developed recently. several major kinds of data mining methods, including generalization, characterization, classification, clustering, association, evolution, pattern matching, data visualization, and meta-rule guided mining, will be reviewed. techniques for mining knowledge in different kinds of databases, including relational, transaction, object-oriented, spatial, and active databases, as well as global information systems, will be examined. potential data mining applications and some research issues will also be discussed.data mining techniques','Information-theoretic techniques'
'an internet transaction is a transaction that involves communication over the internet using standard internet protocols such as https. such transactions are widely used in internet-based applications such as e-commerce. with the growth of the internet, the volume and complexity of internet transactions are rapidly increasing. we present data versioning techniques that can reduce the complexity of managing internet transactions and improve their scalability and reliability. these techniques have been implemented using standard database technology, without any change in database kernel. our initial empirical results argue for the effectiveness of these techniques in practice.data versioning techniques for internet transaction management','Information-theoretic techniques'
'in this paper, we report on the implementation of secsi, an expert system for database design written in prolog. starting from an application description given with either a subset of the natural language, or a formal language, or a graphical interface, the system generates a specific semantic network portraying the application. then, using a set of design rules, it completes and simplifies the semantic network up to reach flat normalized relations. all the design is interactively done with the end-user. the system is evolutive in the sense that it also offers an interactive interface which allows the database design expert to modify or add design rules.database design tools','Information-theoretic techniques'
'this paper introduces database lexicography, a metadata analysis discipline that applies lexical graph theory to data design. database lexicography proposes a formal design criterion for data dependencies, and it provides metrics to evaluate the conformance of designs to this criterion. it treats the data dictionary as a first class object encoding design concepts, and its benefits include identification of database dependency architecture; quantification of interdependent data elements\' sensitivity to change; categorization of core and peripheral data elements; model integration; and figures of merit by which to fortify data architectures to withstand design fossilization and guide their evolution amidst changing requirements.database lexicography','Information-theoretic techniques'
'database replication','Information-theoretic techniques'
'dbase tricks and techniques','Information-theoretic techniques'
'proper analysis of production systems becomes quite complex when considering large, multiple-product systems. production becomes limited when products compete for resources that are shared among products and production lines. these limitations can cause collisions, backlogging, and underutilization of these resources. within the scope of industrial planning, several goals should be met as a result of the application of production analysis techniques. these include the determination of: production levels for each product, utilization of production resources, and costs of production. discussion will be directed towards the problem situation, the problem needs, and applicable solution approaches.decision-supportive modeling techniques for production planning (abstract only)','Information-theoretic techniques'
'due to resource constraints, web archiving systems and search engines usually have difficulties keeping the entire local repository synchronized with the web. we advance the state-of-art of the sampling-based synchronization techniques by answering a challenging question: given a sampled webpage and its change status, which other webpages are also likely to change? we present a study of various downloading granularities and policies, and propose an adaptive model based on the update history and the popularity of the webpages. we run extensive experiments on a large dataset of approximately 300,000 webpages to demonstrate that it is most likely to find more updated webpages in the current or upper directories of the changed samples. moreover, the adaptive strategies outperform the non-adaptive one in terms of detecting important changes.designing efficient sampling techniques to detect webpage updates','Information-theoretic techniques'
'visualization techniques are of increasing importance in exploring and analyzing large amounts of multidimensional information. one important class of visualization techniques which is particularly interesting for visualizing very large multidimensional data sets is the class of pixel-oriented techniques. the basic idea of pixel-oriented visualization techniques is to represent as many data objects as possible on the screen at the same time by mapping each data value to a pixel of the screen and arranging the pixels adequately. a number of different pixel-oriented visualization techniques have been proposed in recent years and it has been shown that the techniques are useful for visual data exploration in a number of different application contexts. in this paper, we discuss a number of issues which are of high importance in developing pixel-oriented visualization techniques. the major goal of this article is to provide a formal basis of pixel-oriented visualization techniques and show that the design decisions in developing them can be seen as solutions of well-defined optimization problems. this is true for the mapping of the data values to colors, the arrangement of pixels inside the subwindows, the shape of the subwindows, and the ordering of the dimension subwindows. the paper also discusses the design issues of special variants of pixel-oriented techniques for visualizing large spatial data sets. the optimization functions for the mentioned design decisions are important for the effectiveness of the resulting visualizations. we show this by evaluating the optimization functions and comparing the results to the visualizations obtained in a number of different application.designing pixel-oriented visualization techniques','Information-theoretic techniques'
'disaster recovery techniques for database systems','Information-theoretic techniques'
'we study the problem of disinformation. we assume that an ``agent\'\' has some sensitive information that the ``adversary\'\' is trying to obtain. for example, a camera company (the agent) may secretly be developing its new camera model, and a user (the adversary) may want to know in advance the detailed specs of the model. the agent\'s goal is to disseminate false information to ``dilute\'\' what is known by the adversary. we model the adversary as an entity resolution (er) process that pieces together available information. we formalize the problem of finding the disinformation with the highest benefit given a limited budget for creating the disinformation and propose efficient algorithms for solving the problem. we then evaluate our disinformation planning algorithms on real and synthetic data and compare the robustness of existing er algorithms. in general, our disinformation techniques can be used as a framework for testing er robustness.disinformation techniques for entity resolution','Information-theoretic techniques'
'the presence of software bloat in large flexible software systems can hurt energy efficiency. however, identifying and mitigating bloat is fairly effort intensive. to enable such efforts to be directed where there is a substantial potential for energy savings, we investigate the impact of bloat on power consumption under different situations. we conduct the first systematic experimental study of the joint power-performance implications of bloat across a range of hardware and software configurations on modern server platforms. the study employs controlled experiments to expose different effects of a common type of java runtime bloat, excess temporary objects, in the context of the specpower_ssj2008 workload. we introduce the notion of equi-performance power reduction to characterize the impact, in addition to peak power comparisons. the results show a wide variation in energy savings from bloat reduction across these configurations. energy efficiency benefits at peak performance tend to be most pronounced when bloat affects a performance bottleneck and non-bloated resources have low energy-proportionality. equi-performance power savings are highest when bloated resources have a high degree of energy proportionality. we develop an analytical model that establishes a general relation between resource pressure caused by bloat and its energy efficiency impact under different conditions of resource bottlenecks and energy proportionality. applying the model to different \"what-if\" scenarios, we predict the impact of bloat reduction and corroborate these predictions with empirical observations. our work shows that the prevalent software-only view of bloat is inadequate for assessing its power-performance impact and instead provides a full systems approach for reasoning about its implications.does lean imply green?','Information-theoretic techniques'
'efficient recompression techniques for dynamic full-text retrieval systems','Information-theoretic techniques'
'several studies have recently concentrated on the generation of wrappers for extracting data from web data sources. the roadrunner system aims at automating the tedious and expensive process of writing wrappers in an unsupervised, domain-independent, and scalable manner. the system is based on a grammar inference algorithm, called match, which has been designed in a sound theoretical framework. however, in its original definition match lacks in expressivity; that is, in many cases when match runs over real-life web pages, it is not able to produce a solution. in this paper we address the challenging issue of developing techniques that allow us to build upon match an effective and efficient system, without renouncing to the original formal background. first, we analyze the main limitations of match; then we illustrate the techniques we have developed to overcome such limitations. finally we report on the results of some experiments, that show the efficacy of the introduced techniques and demonstrate the improvements of the overall system.efficient techniques for effective wrapper induction','Information-theoretic techniques'
'encryption-decryption techniques','Information-theoretic techniques'
'expert or knowledge-based systems are the most common type of aim (artificial intelligence in medicine) system in routine clinical use. they contain medical knowledge, usually about a very specifically defined task, and are able to reason with data from individual patients to come up with reasoned conclusions. although there are many variations, the knowledge within an expert system is typically represented in the form of a set of rules. the thyroid gland is one of the most important organs in the body as thyroid hormones are responsible for controlling metabolism. as a result, thyroid function impacts on every essential organ in the body. when the thyroid produces too much hormone, the body uses energy faster than it should. this condition is called hyperthyroidism. when the thyroid does not produce enough hormone, the body uses energy slower than it should. this condition is called hypothyroidism. thyroid disease can be difficult to diagnose because symptoms are easily confused with other conditions. when thyroid disease is caught early, treatment can control the disorder even before the onset of symptoms. this study aims at diagnosing thyroid diseases with a expert system that we called as a estdd (expert system for thyroid disease diagnosis). we found fuzzy rules by using neuro fuzzy method, which will be emplaced in estdd system. estdd could diagnose with 95.33\% accuracy thyroid diseases. beside it can be benefited from this system for training of students in medicine.estdd','Information-theoretic techniques'
'this paper presents the design and comparison of a mouse-based interaction technique (hereafter it) and two advanced it, used in public spaces to support navigation in a 3d space. the comparison is based on a composite evaluation, including performance and satisfaction aspects. these preliminary results demonstrate that the use of mixed it in a public space do not result in more differences among user than a mouse-based it. it also highlights the fact that performance and satisfaction have to be considered simultaneously since they appear to be two complementary aspects of an evaluation, especially in public space environment, where the performance is no longer the only dimension to consider.evaluating advanced interaction techniques for navigating google earth','Information-theoretic techniques'
'aspect mining aims at identifying cross-cutting concerns in existing systems and therefore advocates the adaption to an aspect-oriented design. this paper presents a case study examining three existing aspect mining techniques from the literature by applying them to four different open source java applications. we compare and evaluate the individual technique and confirm the findings of a previous study of combining different aspect mining techniques in order to get better results with less manual intervention.evaluating aspect mining techniques','Information-theoretic techniques'
'evaluating database selection techniques','Information-theoretic techniques'
'evolution of business system analysis techniques','Information-theoretic techniques'
'web caching has been proposed as an effective solution to the problems of network traffic and congestion, web objects access and web load balancing. this paper presents a model for optimizing web cache content by applying either a genetic algorithm or an evolutionary programming scheme for web cache content replacement. three policies are proposed for each of the genetic algorithm and the evolutionary programming techniques, in relation to objects staleness factors and retrieval rates. a simulation model is developed and long term trace-driven simulation is used to experiment on the proposed techniques. the results indicate that all evolutionary techniques are beneficial to the cache replacement, compared to the conventional replacement applied in most web cache server. under an appropriate objective function the genetic algorithm has been proven to be the best of all approaches with respect to cache hit and byte hit ratios.evolutionary techniques for web caching','Information-theoretic techniques'
'experimental techniques for information requirements analysis','Information-theoretic techniques'
'expert systems','Information-theoretic techniques'
'external it environment','Information-theoretic techniques'
'performance model interchange formats provide a mechanism for automatically moving performance models among modelling tools. the experiment schema extension (ex-se) specifies performance studies to be run on the model and the xml performance metrics desired. moreover, the results schema extension (results-se) specifies how to automatically transform the output metrics into useful results. this paper presents the tool forge (friendly output to results generator engine), a gui based transformation tool from output to results which simplifies the specification of common tables for the output files the user provides. a case study demonstrates the use of the tool.forge','Information-theoretic techniques'
'formal techniques for object oriented software development','Information-theoretic techniques'
'formalization techniques','Information-theoretic techniques'
'framework for development of conceptual data modelling techniques','Information-theoretic techniques'
'a crucial step towards understanding the properties of cellular systems in organisms is to map their network of protein-protein interactions (ppis) on a proteomic-wide scale completely and as accurately as possible. uncovering the diverse function of proteins and their interactions within the cell may improve our understanding of disease and provide a basis for the development of novel therapeutic approaches. the development of large-scale high-throughput experiments has resulted in the production of a large volume of data which has aided in the uncovering of ppis. however, these data are often erroneous and limited in interactome coverage. therefore, additional experimental and computational methods are required to accelerate the discovery of ppis. this paper provides a review on the prediction of ppis addressing key prediction principles and highlighting the common experimental and computational techniques currently employed to infer ppi networks along with relevant studies in the area.from experimental approaches to computational techniques','Information-theoretic techniques'
'fuzzy decisions','Information-theoretic techniques'
'in this paper, we address the problem of user personalization and recommendation of news streams. this involves \'learning\' from past user behaviour, such as the articles she read or did not read and accurately predicting new articles which she would be most likely to read. our contribution in this paper is the development of a new algorithm for news personalization using an adaptation of the classical nearest neighbour algorithm coupled with a knowledge graph which we create. this algorithm provides a powerful tool for user behaviour analysis as we demonstrate in subsequent sections. using implicit user data like the articles that were read as well as the articles that weren\'t along with their position and distance in the graph, we rank new articles on the basis of the predicted interest of the user in the content of that article.graph based techniques for user personalization of news streams','Information-theoretic techniques'
'in this work, we present hardware decompression accelerators for widening the bottleneck between slow nonvolatile memories on the one side and high-speed fpga configuration interfaces and fast softcore cpus on the other side. we discuss different compression algorithms suitable for a hardware accelerated decompression on fpgas as well as on cplds. the algorithms will be investigated with respect to the achievable compression ratio, throughput, and hardware overhead. this leads to various decompressor implementations with one capable to decompress at high data rates of up to 400 megabytes per second under optimal conditions while only requiring slightly more than a hundred lookup tables. we will evaluate how these decompressors perform on configuration bitstreams for different fpgas as well as for softcore cpu binaries.hardware decompression techniques for fpga-based embedded systems','Information-theoretic techniques'
iconkat,'Information-theoretic techniques'
'with the tremendous growth of ict (information and communication technology), we are able to generate, store, share and transfer enormous amount of information. world wide web have further made is easy to access the information anytime, anywhere in the world. with the advent of high capacity communication links and storage devices even most of the information generated is of multimedia in nature. images have major share in this information and the number of image achieves are growing with the jet speed just having the tremendous amount of information is not useful unless we do not have the methodologies to effectively search the related data from it in minimum possible duration. the relativity of the image data is application specific. here to search and retrieve the expected images from the database we need content based image retrieval (cbir) system. cbir extracts the features of query image and try to match them with the extracted features of images in the database. then based on the similarity measures and threshold the best possible candidate matches are given as result. there have been many approaches to decide and extract the features of images in the database. binary truncation coding based features is one of the cbir methods proposed using color features of image. the approach basically considers red, green and blue planes of image together to compute feature vector. here we have augmented this btc based cbir as btc-rgb and spatial btc-rgb. in btc-rgb feature vector is computed by considering red, green and blue planes of the image independently. while in spatial btc-rgb, the feature vector is composed of four parts. each part is representing the features extracted from one of the four non overlapping quadrants of the image. the new proposed methods are tested on the 1000 images database and the results show that the precession is improved in btc-rgb and is even better in spatial btc-rgb.image retrieval using augmented block truncation coding techniques','Information-theoretic techniques'
'the technique of latent semantic indexing (lsi) has wide applicability in information retrieval and data mining tasks. to date, however, most applications of lsi have addressed relatively small collections of data. this has been due partly to hardware and software limitations and partly to overly pessimistic estimates of the processing requirements of the singular value decomposition (svd) process. in recent years, advances in hardware capabilities and software implementations have enabled much larger lsi applications. moreover, experience with large lsi indexes has shown that the svd is not the limitation on scalability that it was long thought to be. this paper describes techniques applicable to creating large-scale (multi-million document) lsi indexes. detailed data regarding the lsi index creation process is presented for collections of up to 100 million documents. four key factors are shown to contribute to the scalability of lsi. first, in most situations, the time required for calculation of the singular value decomposition (svd) of the term-document matrix is not the dominant factor determining the overall time required to build an lsi index. second, the time required to calculate the svd in lsi is linear in the number of objects indexed. third, incremental index creation greatly facilitates use of lsi in dynamic environments. fourth, distributed query processing can be employed to support large numbers of users. it is shown that lsi is well-suited for implementation in modern distributed computing environments. this paper provides the first measurements of the execution time for large-scale lsi build processes in a cloud environment.implementation techniques for large-scale latent semantic indexing applications','Information-theoretic techniques'
improv,'Information-theoretic techniques'
'improve your database design techniques','Information-theoretic techniques'
'current information retrieval systems use inverted index structures for efficient query processing. due to the extremely large size of many data sets, these index structures are usually kept in compressed form, and many techniques for optimizing compressed size and query processing speed have been proposed. in this paper, we focus on versioned document collections, that is, collections where each document is modified over time, resulting in multiple versions of the document. consecutive versions of the same document are often similar, and several researchers have explored ideas for exploiting this similarity to decrease index size. we propose new index compression techniques for versioned document collections that achieve reductions in index size over previous methods. in particular, we first propose several bitwise compression techniques that achieve a compact index structure but that are too slow for most applications. based on the lessons learned, we then propose additional techniques that come close to the sizes of the bitwise technique while also improving on the speed of the best previous methods.improved index compression techniques for versioned document collections','Information-theoretic techniques'
'query processing is a major cost factor in operating large web search engines. in this paper, we study query result caching, one of the main techniques used to optimize query processing performance. our first contribution is a study of result caching as a weighted caching problem. most previous work has focused on optimizing cache hit ratios, but given that processing costs of queries can vary very significantly we argue that total cost savings also need to be considered. we describe and evaluate several algorithms for weighted result caching, and study the impact of zipf-based query distributions on result caching. our second and main contribution is a new set of feature-based cache eviction policies that achieve significant improvements over all previous methods, substantially narrowing the existing performance gap to the theoretically optimal (clairvoyant) method. finally, using the same approach, we also obtain performance gains for the related problem of inverted list caching.improved techniques for result caching in web search engines','Information-theoretic techniques'
'improving data quality','Information-theoretic techniques'
'existing undecidability proofs of checking secrecy of cryptographic protocols have the limitations of not considering protocols common in literature, which are in the form of communication sequences, since only protocols as non-matching roles are considered, and not considering an attacker who is an insider since only an outsider attacker is considered. therefore the complexity of checking the realistic attacks, such as the attack to the public key needham-schroeder protocol, is unknown.the limitations have been observed independently and described similarly by froschle in a recently published paper, where two open problems are posted. this paper investigates these limitations, and we present a generally applicable approach by reductions with novel features from the reachability problem of 2-counter machines, and we solve the two open problems. we also prove the undecidability of checking authentication which is the first detailed proof to our best knowledge. a unique feature of the proof is to directly address the secrecy and authentication goals as defined for the public key needham-schroeder protocol, whose attack has motivated many researches of formal verification of security protocols.improving techniques for proving undecidability of checking cryptographic protocols','Information-theoretic techniques'
'a brief report on several teaching techniques that have rewards that far outweigh their costs. experiences with four techniques are discussed: a \"pause\" during lectures, student submission of exam questions, group projects, and a technique for learning students\' names. these experiences are largely in the context of a data structures class, but are applicable to most lecture-oriented classes.inexpensive teaching techniques with rich rewards','Information-theoretic techniques'
infocrystal,'Information-theoretic techniques'
'we study the structured records of web pages and the relevant problems associated with the extraction and alignment of these structured records. current automatic wrappers are complicated because they take into consideration the problems of locating relevant data region using visual cues and the use of complicated algorithms to check the similarity of data records. in this paper, we develop a non-visual automatic wrapper which questions the need for complex visual based wrappers in data extraction. the novel techniques for our wrapper are (1) filtering rules to detect and filter out irrelevant data records, (2) a tree matching algorithm using frequency measures to increase the speed of data extraction, (3) an algorithm to calculate the number and size of the components of data records to detect the correct data region, (4) a data alignment algorithm which is able to align iterative (repetitive html command tags) and disjunctive (optional) data items and (5) a data merging and partitioning method to solve the imperfect segmentation problem (the problem of correctly identifying the atomic entities in data items). results show that our wrapper is as robust and in many cases outperforms the state of the art wrappers such as vint and depta. this wrapper could have significant speed advantages when processing large volumes of web sites data, which could be helpful in meta search engine development.information extraction for search engines using fast heuristic techniques','Information-theoretic techniques'
'integration of problem-solving techniques in agriculture','Information-theoretic techniques'
'in this paper, we present the intelligent multimedia data hiding techniques and their possible applications. an introduction on intelligent multimedia data hiding is described which covers backgrounds, recent advances, methodologies, and implementations. the histogram-based reversible data hiding technique is then presented with simulation results and also illustrated by using actual implementations.intelligent multimedia data hiding techniques and applications','Information-theoretic techniques'
'falling hardware prices and ever more displays being connected to the internet will lead to large public display networks, potentially forming a novel communication medium. we envision that such networks are not restricted to display owners and advertisers anymore, but allow also passersby (e.g., customers) to exchange content, similar to traditional public notice areas, such as bulletin boards. in this context it is crucial to understand emerging practices and provide easy and straight forward interaction techniques to be used for creating and exchanging content. in this paper, we present digifieds, a digital public notice area we built to investigate and compare possible interaction techniques. based on a lab study we show that using direct touch at the display as well as using the mobile phone as a complementing interaction technology are most suitable. direct touch at the display closely resembles the interaction known from classic bulletin boards and provides the highest usability. mobile phones preserve the users\' privacy as they exchange (sensitive) data with the display and at the same time allow content to be created on-the-go or to be retrieved.interaction techniques for creating and exchanging content with public displays','Information-theoretic techniques'
'interactive data visualization in qualitative research','Information-theoretic techniques'
'internet search techniques and strategies','Information-theoretic techniques'
'&#8212;a profile of john hancock during the past year, john hancock has been experimenting with productivity techniques. the purpose of this paper is to report our results. john hancock mutual life insurance company, one of the country\'s largest mutual life insurance companies, has its home office in boston, mass. the data processing function is provided, on a centralized basis, by a department within the company composed of six hundred personnel. edp areas include machine operations, data entry and keypunch, systems and programming, education, and software support. the equipment installed currently includes three ibm 370/168\'s; the software environment includes os/vs with mvs in the planning stage, tso and ims. most of the programming is done in cobol.john hancock\'s experience with productivity techniques','Information-theoretic techniques'
'knowledge systems','Information-theoretic techniques'
'this paper looks at a collection of especially simple conventional cryptosystems that use a very large blocksize. one variation uses a single xor randomization followed by a single bit permutation. tight upper and lower bounds are obtained on the number of bits of matching plaintext/ciphertext needed to break the systems. these results follow from two interesting combinatorial theorems. the cryptosystems are not practical because the number of bits above is about the same as the keysize. we can make the systems practical by introducing key-dependent pseudo-random numbers, though we then lose any proofs of the difficulty of cryptanalysis.large-scale randomization techniques','Information-theoretic techniques'
'as the mobile technologies advance, location-based services (lbs) become promising for improving people\'s daily lives. these services not only can identify users\' activities at a location, but also can attain users\' social activity networks. the context-aware information including location and social information of mobile users can help to analyze their common interests and current commercial intentions. this is especially helpful in determining the marketing strategy in a special region on behalf of the users. upon the rapid development and deployment of mobile location-based services, various kinds of location-based and social data will be enormous. how to learn uses\' interests and commercial intentions from the ubiquitously available data come to be essential for the innovation of location-based services. in this talk, we will explore related learning techniques to tackle the data mining problems associated with social and location-based service recommendation. our talk includes the following topics: what kinds of learning techniques can be adopted and developed to solve the location-based and social data learning problems? what are the pros and cons for content-based learning techniques and collaborative filtering techniques? how to include location-based or social information into content-based learning techniques for effective data mining? how to incorporate location-based or social information into collaborative filtering techniques for effective data mining? how to perform recommendation based on the learning results? what are the future research issues and promising applications in this emerging area?learning techniques in social and location-based service recommendation','Information-theoretic techniques'
'legal problems of information filtering techniques','Information-theoretic techniques'
'we have developed methods which can deal with the users\' interaction without the conventional conscious searching manner. when a user generally performs map operations with certain information retrieval intentions (less-conscious), a system using our method can detect the specific operation sequences. for example, if the user performs zooming-in and centering operations, the user is narrowing down the search area to a certain location. we define such operation sequences as chunks. the system detects the chunks and uses them to analyze the user\'s operations and thereby detect the user\'s intentions. we have developed several prototype systems based on the proposed methods.less-conscious information retrieval techniques for location based services','Information-theoretic techniques'
'in the vein of recent algorithmic advances in polynomial factorization based on lifting and recombination techniques, we present new faster algorithms for computing the absolute factorization of a bivariate polynomial. the running time of our probabilistic algorithm is less than quadratic in the dense size of the polynomial to be factored.lifting and recombination techniques for absolute factorization','Information-theoretic techniques'
'this paper describes an integrated architecture called live designed for learning from the environment. the task that live faces is to solve problems in a new environment using its actions and senses. in order to be able to adapt to different environments, live\'s actions and senses are defined as \"innate\" properties and their linkage (i.e., actions and their consequences) is unknown before entering a new environment. live must coordinate a variety of intelligent activities in order to learn from the environment. these include problem solving, exploration, knowledge creation, knowledge revision, experimentation, and discovery. the paper gives an overview of the system and discusses its strengths and weaknesses along several dimensions in comparison with other similar architectures.live','Information-theoretic techniques'
'making multimedia work','Information-theoretic techniques'
'traditional software project management theory often focuses on desk-based development of software and algorithms, much in line with the traditions of the classical project management and software engineering. this can be described as a tools and techniques perspective, which assumes that software project management success is dependent on having the right instruments available, rather than on the individual qualities of the project manager or the cumulative qualities and skills of the software organisation. surprisingly, little is known about how (or whether) these tools techniques are used in practice. this study, in contrast, uses a qualitative grounded theory approach to develop the basis for an alternative theoretical perspective: that of competence. a competence approach to understanding software project management places the responsibility for success firmly on the shoulders of the people involved, project members, project leaders, managers. the competence approach is developed through an investigation of the experiences of project managers in a medium sized software development company (wm-data) in denmark. starting with a simple model relating project conditions, project management competences and desired project outcomes, we collected data through interviews, focus groups and one large plenary meeting with most of the company\'s project managers. data analysis employed content analysis for concept (variable) development and causal mapping to trace relationships between variables. in this way we were able to build up a picture of the competences project managers use in their daily work at wm-data, which we argue is also partly generalisable to theory. the discrepancy between the two perspectives is discussed, particularly in regard to the current orientation of the software engineering field. the study provides many methodological and theoretical starting points for researchers wishing to develop a more detailed competence perspective of software project managers\' work.management competences, not tools and techniques','Information-theoretic techniques'
'managing disparity','Information-theoretic techniques'
'managing information technology','Information-theoretic techniques'
'mapping techniques for document processing','Information-theoretic techniques'
'context aware recommender systems (cars) adapt the recommendations to the specific situation in which the items will be consumed. in this paper we present a novel context-aware recommendation algorithm that extends matrix factorization. we model the interaction of the contextual factors with item ratings introducing additional model parameters. the performed experiments show that the proposed solution provides comparable results to the best, state of the art, and more complex approaches. the proposed solution has the advantage of smaller computational cost and provides the possibility to represent at different granularities the interaction between context and items. we have exploited the proposed model in two recommendation applications: places of interest and music.matrix factorization techniques for context aware recommendation','Information-theoretic techniques'
'with the growth in digital representations of music, and of music stored in these representations, it is increasingly attractive to search collections of music. one mode of search is by similarity, but, for music, similarity search presents several difficulties: in particular, for melodic query support, deciding what part of the music is likely to be perceived as the theme by a listener, and deciding whether two pieces of music with different sequences of notes represent the same theme. in this paper we propose a three-stage framework for matching pieces of music. we use the framework to compare a range of techniques for determining whether two pieces of music are similar, by experimentally testing their ability to retrieve different transcriptions of the same piece of music from a large collection of midi files. these experiments show that different comparison techniques differ widely in their effectiveness; and that, by instantiating the framework with appropriate music manipulation and comparison techniques, pieces of music that match a query can be identified in a large collection.melodic matching techniques for large music databases','Information-theoretic techniques'
'microcomputer security: data protection techniques','Information-theoretic techniques'
modcad,'Information-theoretic techniques'
'the use of business intelligence systems has been increased over the last fifteen years. small and medium enterprises have seen the importance of this type of systems in supporting decision-making process. however, there is no consensus about standards that supports activities associated to the development process of this type of systems, namely, etl design, datawarehouse design, olap cubes design, and others. hence, this paper focuses on presenting a literature review about the proposed techniques for modeling the etl process, in order to identify trends, to provide to the practitioners an overview of the field enabling them to make decisions to choose an alternative, and finally, to show to the researchers in the field of business intelligence in particular, and software engineering researchers in general, the need to work on the construction of standards to help reduce the blur found in the design activities.modeling techniques for extraction transformation and load processes','Information-theoretic techniques'
'modelling offices through discourse analysis','Information-theoretic techniques'
'oodbmss need more than declarative query languages and programming languages as their interfaces since they are designed and implemented for complex applications requiring more advanced and easy to use visual interfaces. we have developed a complete programming environment for this purpose, called moodview. moodview translates all the user actions performed through its graphical interface to sql statements and therefore it can be ported onto any object-oriented database systems using sql.moodview provides the database programmer with tools and functionalities for every phase of object oriented database application development. current version of moodview allows a database user to design, browse, and modify database schema interactively and to display class inheritance hierarchy as a directed acyclic graph. moodview can automatically generate graphical displays for complex and multimedia database objects which can be updated through the object browser. furthermore, a database administration tool, a full screen text-editor, a sql based query manager, and a graphical indexing tool for the spatial data, i.e., r trees are also implemented.moodview','Information-theoretic techniques'
'more multi-line form techniques','Information-theoretic techniques'
'multimedia synchronization techniques','Information-theoretic techniques'
'this paper proposes a cache hierarchy that enables web search engines to efficiently process user queries. the different caches in the hierarchy are used to store pieces of data which are useful to solve frequent queries. cached items range from specific data such as query answers to generic data such as segments of index retrieved from secondary memory. the paper also presents a comparative study based on discrete-event simulation and bulk-synchronous parallelism. the studied performance metrics include overall query throughput, single-user query latency and power consumption. in all cases, the results show that the proposed cache hierarchy leads to better performance than a baseline approach built on state of the art caching techniques.new caching techniques for web search engines','Information-theoretic techniques'
'a system for private stream searching, introduced by ostrovsky and skeith, allows a client to provide an untrusted server with an encrypted search query. the server uses the query on a stream of documents and returns the matching documents to the client while learning nothing about the nature of the query. we present a new scheme for conducting private keyword search on streaming data which requires o(m) server to client communication complexity to return the content of the matching documents, where m is an upper bound on the size of the documents. the required storage on the server conducting the search is also o(m). the previous best scheme for private stream searching was shown to have o(m logm) communication and storage complexity. our solution employs a novel construction in which the user reconstructs the matching files by solving a system of linear equations. this allows the matching documents to be stored in a compact buffer rather than relying on redundancies to avoid collisions in the storage buffer as in previous work. this technique requires a small amount of metadata to be returned in addition to the documents; for this the original scheme of ostrovsky and skeith may be employed with o(m logm) communication and storage complexity. we also present an alternative method for returning the necessary metadata based on a unique encrypted bloom filter construction. this method requires o(m log(t/m)) communication and storage complexity, where t is the number of documents in the stream. in this article we describe our scheme, prove it secure, analyze its asymptotic performance, and describe a number of extensions. we also provide an experimental analysis of its scalability in practice. specifically, we consider its performance in the demanding scenario of providing a privacy preserving version of the google news alerts service.new techniques for private stream searching','Information-theoretic techniques'
'e-governance is the use of information and communication technologies for improving the functioning of government organizations to enable more citizen-centric services and relationships. it involves the engagement of citizen in the decision making processes for providing more effective services and policies. in this paper, we shall be discussing the application of natural language processing (nlp) in e-governance. nlp has the strong potentials in supporting e-governance objectives particularly in the area of localization of contents and analysis of contributions by citizens in discussion forum. in this paper we present some ideas on how nlp technologies can support localization in e-governance.nlp techniques in enriching e-governance','Information-theoretic techniques'
'recently, parallel search engines have been implemented based on scalable distributed file systems such as google file system. however, we claim that building a massively-parallel search engine using a parallel dbms can be an attractive alternative since it supports a higher-level (i.e., sql-level) interface than that of a distributed file system for easy and less error-prone application development while providing scalability. regarding higher-level functionality, we can draw a parallel with the traditional o/s file system vs. dbms. in this paper, we propose a new approach of building a massively-parallel search engine using a db-ir tightly-integrated parallel dbms. to estimate the performance, we propose a hybrid (i.e., analytic and experimental) performance model for the parallel search engine. we argue that the model can accurately estimate the performance of a massively-parallel (e.g., 300-node) search engine using the experimental results obtained from a small-scale (e.g., 5-node) one. we show that the estimation error between the model and the actual experiment is less than 2.13\% by observing that the bulk of the query processing time is spent at the slave (vs. at the master and network) and by estimating the time spent at the slave based on actual measurement. using our model, we demonstrate a commercial-level scalability and performance of our architecture. our proposed system odys is capable of handling 1 billion queries per day (81 queries/sec) for 30 billion web pages by using only 43,472 nodes with an average query response time of 194 ms. by using twice as many (86,944) nodes, odys can provide an average query response time of 148 ms. these results show that building a massively-parallel search engine using a parallel dbms is a viable approach with advantages of supporting the high-level (i.e., dbms-level), sql-like programming interface.odys','Information-theoretic techniques'
'this paper addresses the desktop search problem by considering varioustechniques for ranking results of a search query over thefile system. first, basic ranking techniques, which are based ona single file feature (e.g., file name, file content, access date, etc.)are considered. next, two learning-based ranking schemes are presented, and are shown to be significantly more effective than the basic ranking methods. finally, a novel ranking technique, based on query selectiveness is considered,for use during the cold-start period of the system. this method isalso shown to be empirically effective, even though it does notinvolve any learning.on ranking techniques for desktop search','Information-theoretic techniques'
'we investigate the performance of some of the best-known object clustering algorithms on four different workloads based upon the tektronix benchmark. for all four workloads, stochastic clustering gave the best performance for a variety of performance metrics. since stochastic clustering is computationally expensive, it is interesting that for every workload there was at least one cheaper clustering algorithm that matched or almost matched stochastic clustering. unfortunately, for each workload, the algorithm that approximated stochastic clustering was different. our experiments also demonstrated that even when the workload and object graph are fixed, the choice of the clustering algorithm depends upon the goals of the system. for example, if the goal is to perform well on traversals of  small portions of the database starting with a cold cache, the important metric is the per-traversal expansion factor, and a well-chosen placement tree will be nearly optimal; if the goal is to achieve a high steady-state performance with a reasonably large cache, the appropriate metric is the number of pages to which the clustering algorithm maps the active portion of the database. for this metric, the prp clustering algorithm, which only uses access probabilities achieves nearly optimal performance.on the performance of object clustering techniques','Information-theoretic techniques'
'oo techniques in the classroom','Information-theoretic techniques'
'object-relational database management systems allow knowledgeable users to define new data types as well as new methods (operators) for the types. this flexibility produces an attendant complexity, which must be handled in new ways for an object-relational database management system to be efficient. in this article we study techniques for optimizing queries that contain time-consuming methods. the focus of traditional query optimizers has been on the choice of join methods and orders; selections have been handled by &#8220;pushdown&#8221; rules. these rules apply selections in an arbitrary order before as many joins as possible, using th e assumption that selection takes no time. however, users of object-relational systems can embed complex methods in selections. thus selections may take significant amounts of time, and the query optimization model must be enhanced. in this  article we carefully define a query cost framework that incorporates both selectivity and cost estimates for selections. we develop an algorithm called predicate migration, and prove that it produces optimal plans for queries with expensive methods. we then describe our implementation of predicate migration in the commercial object-relational database management system illustra, and discuss practical issues that affect our earlier assumptions. we compare predicate migration to a variety of simplier optimization techniques, and demonstrate that predicate migration is the best general solution to date. the alternative techniques we present may be useful for constrained workloads.optimization techniques for queries with expensive methods','Information-theoretic techniques'
'scientific visualization is a computer-based field concerned with techniques that allow scientists to create graphical representations from datasets generated by computational simulations or acquisition instruments. to address the computational cost of visualization tasks, specially for large datasets, researchers have explored grid environments as a platform for their parallel evaluation. it is however not trivial to adapt each different visualization technique to run in grid environments. a desirable alternative would separate the specificities of data and process distribution in grids from visualization computation logic. in this work we claim that the qef (query evaluation framework) leverages scientific visualization computation with the above mentioned characteristics. visualization computation techniques are modeled as operators in an algebra and integrated with a set of control operators that manage data distribution leading to a parallel qep (query execution plan). we show the benefits of parallelization for two of those techniques: particle tracing and volume rendering. for these techniques, our experiments demonstrate many positive aspects of the solution presented, as well as opportunities for future work.optimizing the pre-processing of scientific visualization techniques using qef','Information-theoretic techniques'
'consistency is always sacrificed to achieve the key features of a cloud platform: scalability, availability and reliability. these key features require different forms of consistent states to be introduced. different techniques are also introduced to maintain consistency. our goal is to compare the performance and throughput of different data consistency techniques. an experimental set up is planned for the comparison among these consistency techniques.performance comparison of consistency maintenance techniques for cloud database','Information-theoretic techniques'
'physical database design techniques','Information-theoretic techniques'
pmi,'Information-theoretic techniques'
'performance model interchange formats are common representations for data that can be used to move models among modeling tools. in order to manage the research scope, the initial version of pmif is limited to qnm that can be solved by efficient, exact solution algorithms. the overall model interoperability approach has now been demonstrated to be viable. this paper is a first step to broaden the scope of pmif to represent models that can be solved with additional methods.pmif extensions','Information-theoretic techniques'
'professional boundaries','Information-theoretic techniques'
'information extraction technologies meet the market need for automatic tools for extracting semi-structured information from web pages. however, pages may change over time due to different reasons, ranging from restyling pages to on-purpose modifications brought about into pages in order to puzzle web wrappers. in this paper we deal with this latter scenario, by studying the issue of on-purpose wrapper spoiling and its relationship to wrapping. we present an architecture and a tool implementing a wrapper spoiling system, and discuss some practical spoiling techniques which are also experimentally tested.protection techniques from information extraction','Information-theoretic techniques'
'table partitioning splits a table into smaller parts that can be accessed, stored, and maintained independent of one another. from their traditional use in improving query performance, partitioning strategies have evolved into a powerful mechanism to improve the overall manageability of database systems. table partitioning simplifies administrative tasks like data loading, removal, backup, statistics maintenance, and storage provisioning. query language extensions now enable applications and user queries to specify how their results should be partitioned for further use. however, query optimization techniques have not kept pace with the rapid advances in usage and user control of table partitioning. we address this gap by developing new techniques to generate efficient plans for sql queries involving multiway joins over partitioned tables. our techniques are designed for easy incorporation into bottom-up query optimizers that are in wide use today. we have prototyped these techniques in the postgresql optimizer. an extensive evaluation shows that our partition-aware optimization techniques, with low optimization overhead, generate plans that can be an order of magnitude better than plans produced by current optimizers.query optimization techniques for partitioned tables','Information-theoretic techniques'
'radical change','Information-theoretic techniques'
'real-time state machine implementation programming techniques','Information-theoretic techniques'
'while in recent years some effort has been put into helping users manage their personal information in their computers, little has been done to provide meaningful ways to organize and retrieve a user\'s personal physical objects. nowadays, technologies such as rfid tags can help bridge the gap between the real and electronic worlds. we propose that a tool that keeps track of the users\' objects and seamlessly inter-relates information about them with other relevant autobiographical and contextual data, about the users and their activities, can help manage and retrieve both physical and electronic items in meaningful ways. we describe a prototype tool, realfind that allows this to take place in a synergistic and effective way. objects can be searched for based on their properties, but also by relating them to a wide range of contextual information stored on their computers.realfind','Information-theoretic techniques'
'the use of relations as a model of information stored in large data bases has several significant advantages over other methods of data description. it is often, however, criticized as difficult to implement efficiently. in this paper, some of the pros and cons of the relational model are discussed. the concepts of modeling information structures with data is illustrated with the codasyl dbtg and the relational models. then the main concepts of a relational data management system are presented based on current implementations. the advantages and disadvantages of the relational model is discussed in the light of these implementations. by presenting the theories of relational data management in a context of specific system implementations, the advantages and disadvantages of this data organization will be more clearly understood.relational data management implementation techniques','Information-theoretic techniques'
'this article has been retracted at the request of the editor-in-chief. please see elsevier policy on article withdrawal (http://www.elsevier.com/locate/withdrawalpolicy). reason: it has come to the attention of the editor-in-chief of expert systems with applications that this article closely resembles \'\'a real-time vehicle license plate recognition (lpr) system\'\', b.-h. ron, supervised by j. erez, available: http://visl.technion.ac.il/projects/2003w24/. the publisher, technion, has asked elsevier to retract the youssef and abdelrahman article for using parts of its article without permission. for example: (i) fig. 2 in the article is identical to the block diagram of the technion work and includes the algorithm developed by the author; (ii) fig. 3 in the article is identical to fig. 2 in the technion work; (iii) fig. 20 in the article is identical to fig. 20 in the technion work.retracted','Information-theoretic techniques'
'robots and management techniques (abstract only)','Information-theoretic techniques'
'text detection and recognition in natural images using a single mobile device is becoming relevant due to the increasing interest in optical character recognition (ocr) applications. ocr application on mobile devices is no longer a dream due to the advancement in mobile technology. there are many ongoing researches in this field. most of the researches focus on the ocr engine of the application and there is not much focus on the processing stage of the ocr application. pre-processing plays a major role in optimizing the image for character recognition. thus, in this paper, two pre-processing techniques are proposed for the mobile ocr application. the first technique helps to locate a sign in a natural image efficiently, while the second technique implements otsu\'s threshold algorithm to convert images into binary image. these techniques are implemented in an ocr mobile application which is developed using desktop open sources library. after implementation, the ocr application is tested with 50 sign images to verify the accuracy. experimental results have demonstrated that these techniques can significantly improve the ocr accuracy and decrease the overall computation time.robust pre-processing techniques for ocr applications on mobile devices','Information-theoretic techniques'
'web search engines depend on the full-text inverted index data structure. because the query processing performance is so dependent on the size of the inverted index, a plethora of research has focused on fast end effective techniques for compressing this structure. recently, several authors have proposed techniques for improving index compression by optimizing the assignment of document identifiers to the documents in the collection, leading to significant reduction in overall index size. in this paper, we propose improved techniques for document identifier assignment. previous work includes simple and fast heuristics such as sorting by url, as well as more involved approaches based on the traveling salesman problem or on graph partitioning. these techniques achieve good compression but do not scale to larger document collections. we propose a new framework based on performing a traveling salesman computation on a reduced sparse graph obtained through locality sensitive hashing. this technique achieves improved compression while scaling to tens of millions of documents. based on this framework, we describe a number of new algorithms, and perform a detailed evaluation on three large data sets showing improvements in index size.scalable techniques for document identifier assignment in inverted indexes','Information-theoretic techniques'
'mining for association rules in market basket data has proved a fruitful area of research. measures such as conditional probability (confidence) and correlation have been used to infer rules of the form &#8220;the existence of item a implies the existence of item b.&#8221; however, such rules indicate only a statistical relationship between a and b. they do not specify the nature of the relationship: whether the presence of a causes the presence of b, or the converse, or some other attribute or phenomenon causes both to appear together. in applications, knowing such causal relationships is extremely useful for enhancing understanding and effecting change. while distinguishing causality from correlation is a truly difficult problem, recent work in statistics and bayesian learning provide some avenues of attack. in these fields, the goal has generally been to learn complete causal models, which are essentially impossible to learn in large-scale data mining applications with a large number of variables.in this paper, we consider the problem of determining casual relationships, instead of mere associations, when mining market basket data. we identify some problems with the direct application of bayesian learning ideas to mining large databases, concerning both the scalability of algorithms and the appropriateness of the statistical techniques, and introduce some initial ideas for dealing with these problems. we present experimental results from applying our algorithms on several large, real-world data sets. the results indicate that the approach proposed here is both computationally feasible and successful in identifying interesting causal structures. an interesting outcome is that it is perhaps easier to infer the lack of causality than to infer causality, information that is useful in preventing erroneous decision making.scalable techniques for mining causal structures','Information-theoretic techniques'
'comparative evaluations of information retrieval systems are based on a number of key premises, including that representative topic sets can be created, that suitable relevance judgements can be generated, and that systems can be sensibly compared based on their aggregate performance over the selected topic set. this paper considers the role of the third of these assumptions -- that the performance of a system on a set of topics can be represented by a single overall performance score such as the average, or some other central statistic. in particular, we experiment with score aggregation techniques including the arithmetic mean, the geometric mean, the harmonic mean, and the median. using past trec runs we show that an adjusted geometric mean provides more consistent system rankings than the arithmetic mean when a significant fraction of the individual topic scores are close to zero, and that score standardization (webber et al., sigir 2008) achieves the same outcome in a more consistent manner.score aggregation techniques in retrieval experimentation','Information-theoretic techniques'
'screen erasing techniques','Information-theoretic techniques'
'due to the popularity of web applications and their heavy usage, it is important to obtain a good understanding of their workloads in order to improve performance of search services. existing works have typically focused on generic web workloads without putting emphasis on specific domains. in this paper, we analyze the usage logs of citeseer, a scientific literature digital library and search engine, to characterize workloads for both robots and users. essential ingredients that contribute to workloads are proposed. among them we find the access intervals show high variance, and thus cannot be predicted well with time-series models. on the other hand, client visiting path and semantics can be well captured with probabilistic models and zipf-law. based on the findings, we propose searchgen, a synthetic workload generator to output traces for scientific literature digital libraries and search engines. a comparison between synthetic workloads and actual logged traces suggests that the synthetic workload fits well.searchgen','Information-theoretic techniques'
'at work and at play, people need access to the right information, and they frequently need to share that information with others. while current tools such as electronic mail and usb flash drives provide powerful mechanisms for managing and sharing information, they too often require that users anticipate what information they might share (e.g. so they have it on their flash drive) and when they might share it (e.g. so they bring the flash drive with them). these tools thus provide excellent support for planned sharing but inadequate support for serendipitously sharing digital information with others. in this paper we discuss eight design goals that a successful system for serendipitous sharing should meet, and we present serefe, a new architecture for serendipitous file exchange that we designed to meet those goals. serefe extends an instant messaging architecture to allow users to use any of their devices, including a cell phone, to share information stored on any of their devices with other users or to copy it to another of their devices. we describe the serefe user experience, implementation, and initial evaluation results.serefe','Information-theoretic techniques'
shelley,'Information-theoretic techniques'
'we are interested in the problem of tracking broad topics such as \"baseball\" and \"fashion\" in continuous streams of short texts, exemplified by tweets from the microblogging service twitter. the task is conceived as a language modeling problem where per-topic models are trained using hashtags in the tweet stream, which serve as proxies for topic labels. simple perplexity-based classifiers are then applied to filter the tweet stream for topics of interest. within this framework, we evaluate, both intrinsically and extrinsically, smoothing techniques for integrating \"foreground\" models (to capture recency) and \"background\" models (to combat sparsity), as well as different techniques for retaining history. experiments show that unigram language models smoothed using a normalized extension of stupid backoff and a simple queue for history retention performs well on the task.smoothing techniques for adaptive online language models','Information-theoretic techniques'
'some spatial decision support techniques in gis','Information-theoretic techniques'
'spam message is one of the major problems in today\'s internet, which brings financial damage to companies and annoying individual users. spam filtering is able to control the problem in a variety of ways. many researches in spam filtering have been centered on the classifier-related issues. currently, machine learning for spam classification is an important research issue at present. this paper explores and identifies the use of different machine learning algorithms for classifying spam messages from e-mail. finally, a comparative analysis among the algorithms has also been presented with spam classification.spam classification using supervised learning techniques','Information-theoretic techniques'
'the internal organization of string processing systems is discussed. six techniques for data structures are presented and evaluated on the basis of: (1) creation of strings; (2) examination of strings; and (3) alteration of strings. speed of operation, storage requirements, effect on paging, and programmer convenience are also considered. one of the techniques, single-word linked blocks, is used in an example demonstrating an implementation of a snobol string processing language on an ibm system/360.string processing techniques','Information-theoretic techniques'
'structured techniques for the development of expert systems','Information-theoretic techniques'
'structures task group','Information-theoretic techniques'
'surface imaging techniques','Information-theoretic techniques'
'survival techniques of a conversion','Information-theoretic techniques'
'tape searching techniques','Information-theoretic techniques'
'techniques for application software maintenance','Information-theoretic techniques'
'techniques for data hiding','Information-theoretic techniques'
'the user service group within a university computing center faces a growing training challenge as computing becomes available to much larger numbers of students. in many institutions, larger class sizes for computer training are becoming a necessity. at washington state university (wsu), our class sizes have grown from a maximum of 12 students to a maximum of 250 students in four years. the larger classes require new and enhanced training techniques. this paper presents some techniques which have been successful at washington state university for classes up to 250 students. also presented are some speculations about the impact on training techniques of even larger classes (750 to 1000 students).techniques for mass training in basic computer skills','Information-theoretic techniques'
'techniques for selecting pose algorithms','Information-theoretic techniques'
'techniques for structuring database records','Information-theoretic techniques'
'techniques for trusted software engineering','Information-theoretic techniques'
'what you are about to read will probably sound familiar. indeed, it has been said many times before. however, i believe this formulation is original and may help you better apply it in your marketing communication. i immodestly call it yaffes law.techniques of persuasive communication','Information-theoretic techniques'
'the problem of capacity sharing in a demand-assignment multiple-access system is considered. the well established sharing strategies, all possessing the \'convexity property\', lead to queueing models with various levels of computational complexity. a conjecture on the global validity of an optimal sharing strategy is made, and an optimization procedure is developed. the procedure is based on multidimensional pattern search. a sensitivity analysis of the optimal parameters obtained is also given.techniques','Information-theoretic techniques'
'telephone consulting becomes very important when academic computing moves into a distributed environment. this is especially true in network consulting where end users are often hundreds of miles from the main site. in order to maintain a good consulting service in this type of situation, consultants need to develop and use effective telephone techniques. the goal is to provide efficient professional service with as few misunderstandings as possible. a wealth of knowledge can be locked behind poor telephone communication techniques. besides presenting some general rules and guidelines of telephone techniques this paper presents methods for probing the caller for the right question, getting and holding attention, terminating a call graciously, handling complaints, &#8220;attending behavior&#8221; in telephone listening, and others. since most user services personnel are already familiar with the problems of telephone consulting, this presentation is intended to sharpen skills and to provide a forum in which ideas can be discussed.telephone techniques for computer consulting','Information-theoretic techniques'
'the continuous zoom','Information-theoretic techniques'
'this article reports on work conducted at nottingham university to investigate the utility and efficacy of selected knowledge elicitation (ke) techniques. the experimental approach taken to this research is advanced as a necessary and essential part of the process of coming to understand how to make acquisition more tractable. we discuss the results of our work and possible implications for programmes of acquisition. we also describe the assumptions implicit in our work and the attendant caveats that have to be borne in mind.the empirical study of knowledge elicitation techniques','Information-theoretic techniques'
'this paper describes an information retrieval system which began in 1960 as an experiment in normal english text processing, using the ibm 650. the experiment has developed into a practical operating system on ibm 7090 and ibm 1401 computers serving approximately 5000 ibm technical and engineering professionals at ibm locations throughout the united states and europe. the principal operation is at the thomas j. watson research center at yorktown heights, new york. the ibm technical information retrieval center (itirc) does retrospective searching and current information selection for the u.s. installations, with a satellite installation at ibm san jose, california, servicing the west coast on retrospective searching. a satellite operation at ibm france serves the ibm european laboratories with both retrospective searching and cis from la gaude, france. the data bases are prepared and controlled at itirc headquarters at yorktown heights. another satellite operation at endicott, n. y. handles ibm\'s suggestion plan.the ibm information retrieval center-(itirc) system techniques and applications','Information-theoretic techniques'
'data is reported regarding the impact of personal computer use in the naval military personnel command. the emphasis is on furthering knowledge of variables which may be affected by the introduction of office automation, and of how those variables might be measured. particular attention was paid to the problem of abstracting generalizable measures from descriptions of what people did with their computers.analysis of results revealed six general categories of impact, each with several specific, measurable indicators. the general categories include: budgeting, information for planning or administration, amount of time saved, uses of time saved, \"information creation\", and communication. examples of measurable indicators of each category are: budgeting - percent of allocated budget spent within a fiscal year; planning and administration- currency of available data; use of time saved accomplishment of new tasks; information creation - development of \"private\" data bases; and communication - development of \"private\" data bases; and communication - expectations concerning data that should be available.the intent is to demonstrate that an effort to measure the impact of office automation can be successful, and to urge others to undertake similar analyses with other data sets. together, such efforts would lead to much improved understanding of how office automation affects activity within organizations.the organizational consequences of office automation: refining measurement techniques','Information-theoretic techniques'
theme,'Information-theoretic techniques'
'the recall all your senses (rays) project envisions a social networking system that empowers its users to store, retrieve, and share data produced by streaming devices. an example device is the popular apple iphone that produces continuous media, audio and video clips. this paper focuses on the stream manager of rays, rays-sm, and its peer-to-peer overlay network. for a request that streams data from a device, rays-sm initiates more than one stream in order to minimize loss of data when nodes in its network fail. we present the design of 3 data availability techniques, quantifying their throughput and mean time to data loss (mttdl). these two metrics highlight the tradeoff between the resource usage of each technique during normal mode of operation in order to minimize loss of data in the presence of node failures.three highly available data streaming techniques and their tradeoffs','Information-theoretic techniques'
'tips, techniques and words of wisdom','Information-theoretic techniques'
'tips, techniques, and words of wisdom','Information-theoretic techniques'
'performance measurement of large distributed multiagent systems (mas) offers challenges that must be addressed explicitly in the agent infrastructure. performance data is widely distributed and voluminous, and poor data collection can impact the operation of the system itself. however, performance metrics are essential to internal system function, e.g., autonomous adaptation to dynamic environments, as well as to external assessment. in this paper we describe the tools, techniques, and results of performance characterization of the cougaar distributed agent architecture. these techniques include infrastructure instrumentation, plugin-based instrumentation of agents, and dynamic control of metric collection. we introduce multiple redundant \"channels\" for metric delivery, each serving separate quality of service requirements. we present our techniques for instrumenting the agent society, justify the metrics chosen, and describe the tools developed for collecting these metrics. we also present results from distributed agent societies comprising hundreds of agents.tools and techniques for performance measurement of large distributed multiagent systems','Information-theoretic techniques'
'the problems involved in using a smaller alphabet than that naturally called for are investigated, with attention focussed on the use of 12 digit keyboards such as found on touch-tone (trademark reg.) telephones. the feasibility of avoiding the use of codebooks or user encodings is examined for some medical information systems, and a technique to minimize redundant inputs is described.touch-tone input techniques','Information-theoretic techniques'
'in this paper we present a number of advanced concepts with respect to the transformation of residual blocks in video coders, which go beyond what is incorporated in today\'s video coding standards. some of these concepts will undoubtedly be adopted as part of future standards. we discuss directional transforms for extrapolation-based prediction schemes, shape-adaptive transformation for object-based coding, and large transform sizes. for the latter we provide in-depth coding efficiency results which clearly illustrate the potential benefit, especially for high definition source material, which will dominate the requirements of tomorrow\'s video coding standards. the compression efficiency gains that can be achieved with large block transforms range from 6 to 25\%. finally we also comment on the paradigm of coupling partition and transform areas, a principle which can be applied to block transforms as well as to shape-adaptive transforms for object-based coding.transformation techniques for future video coding','Information-theoretic techniques'
'uncertainty techniques in expert system software','Information-theoretic techniques'
'we present a set of novel user interaction techniques for fast and efficient multimedia querying and retrieval. three new types of visual queries are proposed for retrieving different types of multimedia. iconic queries facilitate editorial image retrieval and social multimedia retrieval. simple sketches on a map form spatial queries for retrieving location-based multimedia. sketches on an interactive calendar form queries for temporal multimedia retrieval. we also propose an interactive querying strategy that combines multiple search dimensions, to facilitate easier refinement of searches. we outline two applications that use the proposed interaction techniques and querying strategy. the users found these techniques and applications easy to learn and use.user interaction techniques for multimedia retrieval','Information-theoretic techniques'
'privacy is an important issue in data mining and knowledge discovery. in this paper, we propose to use the randomized response techniques to conduct the data mining computation. specially, we present a method to build decision tree classifiers from the disguised data. we conduct experiments to compare the accuracy of our decision tree with the one built from the original undisguised data. our results show that although the data are disguised, our method can still achieve fairly high accuracy. we also show how the parameter used in the randomized response techniques affects the accuracy of the results.using randomized response techniques for privacy-preserving data mining','Information-theoretic techniques'
'using three minimally biasing elicitation techniques for knowledge acquisition','Information-theoretic techniques'
'one important challenge in the field of recommender systems is the sparsity of available data. this problem limits the ability of recommender systems to provide accurate predictions of user ratings. we overcome this problem by using the publicly available user generated information contained in wikipedia. we identify similarities between items by mapping them to wikipedia pages and finding similarities in the text and commonalities in the links and categories of each page. these similarities can be used in the recommendation process and improve ranking predictions. we find that this method is most effective in cases where ratings are extremely sparse or nonexistent. preliminary experimental results on the movielens dataset are encouraging.using wikipedia to boost collaborative filtering techniques','Information-theoretic techniques'
'very large projects','Information-theoretic techniques'
'an effective way for software development organizations to survive in competitive markets is to make competition irrelevant through requirements that create new value. value-innovative requirements engineering is a novel requirements-engineering process to support this market approach. vire is based on the blue-ocean strategy for creating an uncontested new market space that satisfies new customer desires and needs. the vire process guides the creation of new value for potential customers, using blue ocean&#237;s errc (eliminate, reduce, raise, and create) requirements analysis process as well as quantitative requirements analyses. a case study shows how vire created significant new market value.vire','Information-theoretic techniques'
wasa,'Information-theoretic techniques'
'the importance of the web service technology for business, government, among other sectors, is growing. its use in these sectors demands security concern. the web services security standard is a step towards satisfying this demand. however, in the current security approach, the mechanism used for describing security properties of web services restricts security policy specification and intersection. in environments that include loosely-coupled components, a rich description of components is needed to determine whether they can interact in a secure manner. the goal of this paper is to propose a security approach for web services, which combines web services policy framework policies and a web ontology language ontology to overcome the limitation of the current syntactic approach. the main contribution of this paper is an extended approach based on semantics-enriched security policies.web service security management using semantic web techniques','Information-theoretic techniques'
'protecting individual privacy is an important problem in microdata distribution and publishing. anonymization algorithms typically aim to satisfy certain privacy definitions with minimal impact on the quality of the resulting data. while much of the previous literature has measured quality through simple one-size-fits-all measures, we argue that quality is best judged with respect to the workload for which the data will ultimately be used. this article provides a suite of anonymization algorithms that incorporate a target class of workloads, consisting of one or more data mining tasks as well as selection predicates. an extensive empirical evaluation indicates that this approach is often more effective than previous techniques. in addition, we consider the problem of scalability. the article describes two extensions that allow us to scale the anonymization algorithms to datasets much larger than main memory. the first extension is based on ideas from scalable decision trees, and the second is based on sampling. a thorough performance evaluation indicates that these techniques are viable in practice.workload-aware anonymization techniques for large-scale datasets','Information-theoretic techniques'
'classification accuracy in intrusion detection systems (idss) deals with such fundamental problems as how to compare two or more idss, how to evaluate the performance of an ids, and how to determine the best configuration of the ids. in an effort to analyze and solve these related problems, evaluation metrics such as the bayesian detection rate, the expected cost, the sensitivity and the intrusion detection capability have been introduced. in this paper, we study the advantages and disadvantages of each of these performance metrics and analyze them in a unified framework. additionally, we introduce the intrusion detection operating characteristic (idoc) curves as a new ids performance tradeoff which combines in an intuitive way the variables that are more relevant to the intrusion detection evaluation problem. we also introduce a formal framework for reasoning about the performance of an ids and the proposed metrics against adaptive adversaries. we provide simulations and experimental results to illustrate the benefits of the proposed framework.a framework for the evaluation of intrusion detection systems','Intrusion detection systems'
'despite the proliferation of detection and containment techniques in the worm defense literature, simple threshold-based methods remain the most widely deployed and most popular approach among practitioners. this popularity arises out of the simplistic appeal, ease of use, and independence from attack-specific properties such as scanning strategies and signatures. however, such approaches have known limitations: they either fail to detect low-rate attacks or incur very high false positive rates. we propose a multi-resolution approach to enhance the power of threshold-based detection and rate-limiting techniques. using such an approach we can not only detect fast attacks with low latency, but also discover low-rate attacks - several orders of magnitude less aggressive than today\'s fast propagating attacks with low false positive rates. we also outline a multi-resolution rate limiting mechanism for throttling the number of new connections a host can make, to contain the spread of worms. our trace analysis and simulation experiments demonstrate the benefits of a multiresolution approach for worm defense.a multi-resolution approach forworm detection and containment','Intrusion detection systems'
'many techniques have been applied to anomaly detection to detect novel attacks, such as statistical analysis, clustering, support vector machines, neural networks and etc. although the results are promising, there\'s still a serious problem, high false positive rates, which make anomaly detection systems practically unusable. we observe that most network intrusion detection systems (idss) work on information that is only available on lower layers of the network or on higher layers, but not on both. we argue that by correlating the information on different layers, we can have a more efficient anomaly detection system. we introduce an anomaly detection system based on the layer correlation. bayesian networks and statistical analysis are used to build normal system models for the anomaly detection engine. the prototype system is tested on tcpdump traces including normal and anomalous email activities. our experimental results show that our proposed solution is capable of reducing false alarm rates.a multilayer approach of anomaly detection for email systems','Intrusion detection systems'
'network intrusion detection system serves as a second line of defense to intrusion prevention. anomaly detection approach is important in order to detect new attacks. this paper adopted connectivity-based outlier detection scheme from statistical field to detect anomaly behavior in network intrusion detection. we also evaluated the capability of this new approach with the data set from kdd cup 1999 data mining competition. the results indicated that the connectivity-based scheme outperform current anomaly detection approach in the capability to detect attacks and low false alarm rate.a novel outlier detection scheme for network intrusion detection systems','Intrusion detection systems'
'to detect any (unknown) virus, automatic signature schemes are proposed to be embedded in honestly-made compilers. but compiling load is centralized on the compiler makers. to distribute compiling load with the help of distributed servers, proxy automatic signature schemes are proposed for the distributed compilers. however, lin and jan&#253;s proxy automatic signature scheme is insecure and has length restriction of source programs. to remove these flaws, hwang and li also proposed their scheme. however, two signatures are used for the agreement of compiler makers and servers, respectively. but only the signature for the proxy agreement of compiler makers can be validated by anyone. to remove this inefficient flaw, a new efficient proxy automatic signature scheme is proposed. except the efficient advantage, the proxy agreement being researched both by the compiler maker and servers can be validated by anyone at the same time. only one signature is used to show the agreement. the correctness of compilers and executable programs can be validated without releasing source codes. moreover the moderator can easily find out infection sources.a proxy automatic signature scheme using a compiler in distributed systems for (unknown) virus detection','Intrusion detection systems'
'in the search for a robust and efficient algorithm to be used for computer virus detection, we have developed an artificial immune system genetic algorithm (realgo) based on the human immune system\'s use of reverse transcription ribonucleic acid (rna). the realgo algorithm provides memory such that during a complex search the algorithm can revert back to and attempt to mutate in a different \"direction\" in order to escape local minima. in lieu of non-existing virus generic templates, validation is addressed by using an appropriate variety of function optimizations with landscapes believed to be similar to that of virus detection. it is empirically shown that the realgo algorithm finds \"better\" solutions than other evolutionary strategies in four out of eight test functions and finds equally \"good\" solutions in the remaining four optimization problems.a retrovirus inspired algorithm for virus detection & optimization','Intrusion detection systems'
'recently, the number of malware incidents has been rapidly increasing but we are too short of experts to handle the situation. fortunately, most malware instances are a mutation of an existing one, so it might be effective to use an auto-analysis system to observe a specific malware. the auto-analysis system for malware uses both the dynamic and static methods, but the latter still has some limitations and accordingly requires further research. however, this study uses the static method, which calculates the similarity between two files to be executed through by comparing character strings to identify and classify malware. this method, whose performance depends on the number and type of character strings to be compared, requires a process to refine the character strings. in addition, it provides an advanced comparison mechanism that reflects the characteristics of the character strings held by a specific malware, in the calculation of similarity.a study of malware detection and classification by comparing extracted strings','Intrusion detection systems'
'the current intrusion detection system (ids) technologyis a major investment for a firm and its evaluation is desiredprior to a commitment. a testbed compares differentidss on a common platform. a major challenge in evaluatingidss stems from the fact that they are generally testedin specific environments. a real-world environment couldbe different from the environment designed for a testbed.the results obtained, from such testbeds, may not be accurateand reliable. hence, a quantitative and metrics basedevaluation of idss is desired.we propose testbed for evaluating intrusion detectionsystems (tides), that allows a user to select the best idsfor a specific customized environment. a quantitative analysisis provided by tides, using fuzzy logic, under varyingnetwork loads. we also propose robust metrics to evaluatean ids. we follow up with recommendations, based on ourexperience, on the general practices in the field of idss.a testbed for quantitative assessment of intrusion detection systems using fuzzy logic','Intrusion detection systems'
'a virus detection framework based on spmos','Intrusion detection systems'
'abstraction is an important issue in intrusion detection, since it not only hides the difference between heterogeneous systems, but also allows generic intrusion-detection models. however, abstraction is an error-prone process and is not well supported in current intrusion-detection systems (idss). this article presents a hierarchical model to support attack specification and event abstraction in distributed intrusion detection. the model involves three concepts: system view, signature, and view definition. a system view provides an abstract interface of a particular type of information; defined on the instances of system views, a signature specifies certain distributed attacks or events to be monitored; a view definition is then used to derive information from the matches of a signature and presents it through a system view. with the three elements, the model provides a hierarchical framework for maintaining signatures, system views, as well as event abstraction. as a benefit, the model allows generic signatures that can accommodate unknown variants of known attacks. moreover, abstraction represented by a system view can be updated without changing either its specification or the signatures specified on its basis. this article then presents a decentralized method for autonomous but cooperative component systems to detect distributed attacks specified by signatures. specifically, a signature is decomposed into finer units, called detection tasks, each of which represents the activity to be monitored on a component system. the component systems (involved in a signature) then perform the detection tasks cooperatively according to the \"dependency\" relationships among these tasks. an experimental system called cards has been implemented to test the feasibility of the proposed approach.abstraction-based intrusion detection in distributed environments','Intrusion detection systems'
'checksums, long used for random error detection in communications, is now being employed to detect changes for integrity purposes. for example, checksums are being used for the detection of computer viruses [poz86]. the checksum algorithms for detecting random errors are not sufficient against an entity that wishes to \"fool\" the checksum mechanism. this entity wants to be able to insert a forgery in place of the original data such that an unsuspecting user does not realize the forgery has occurred. this paper describes checksum algorithms and features of checksum algorithms to deter this type of forgery.adequacy of checksum algorithms for computer virus detection','Intrusion detection systems'
'recent years have seen a growing interest in computational methods based upon natural phenomena with biologically inspired techniques, such as cellular automata, immune human systems, neural networks, dna and molecular computing. some of these techniques are classified under the realm of a general paradigm, called bio-computing. in this paper, we propose a security system for fraud detection of intruders and improper use of both computer system and mobile telecommunication operations. our technique is based upon data analysis inspired by the natural immune human system. we show how immune metaphors can be used efficiently to tackle this challenging problem. we also describe how our scheme extracts salient features of the immune human system and maps them within a software package designed to identify security violations of a computer system and anusual activities according to the usage log files. our results indicate that our system shows a significant size reduction of the logs file (i.e., registration of each log activity), and thereby the size of the report maintained by the computer system manager. this might help the system manager to monitor and observe unusual activities on the machine hosts more efficiently, as they happen, and can act accordingly before it is too late. last but not least, we propose an intrusion and fraud detection model based upon immune human analogy for mobile phone operations. we discuss our model and present its specification using the z language.an artificial immune based intrusion detection model for computer and telecommunication systems','Intrusion detection systems'
'signature-based intrusion detection systems use a set ofattack descriptions to analyze event streams, looking forevidence of malicious behavior. if the signatures are expressedin a well-defined language, it is possible to analyzethe attack signatures and automatically generate eventsor series of events that conform to the attack descriptions.this approach has been used in tools whose goal is to forceintrusion detection systems to generate a large number ofdetection alerts. the resulting \"alert storm\" is used to desensitizeintrusion detection system administrators and hideattacks in the event stream. we apply a similar technique toperform testing of intrusion detection systems. signaturesfrom one intrusion detection system are used as input toan event stream generator that produces randomized syntheticevents that match the input signatures. the resultingevent stream is then fed to a number of different intrusiondetection systems and the results are analyzed. this paperpresents the general testing approach and describes thefirst prototype of a tool, called mucus, that automaticallygenerates network traffic using the signatures of the snortnetwork-based intrusion detection system. the paper describespreliminary cross-testing experiments with both anopen-source and a commercial tool and reports the results.an evasion attack that was discovered as a result of analyzingthe test results is also presented.an experience developing an ids stimulator for the black-box testing of network intrusion detection systems','Intrusion detection systems'
'automatic deception detection (add) becomes more and more important. add can be facilitated with the development of data mining techniques. in the paper we focus on decision tree to automatic classify deceptions. the major question is how to select experiment data (input data for training in decision tree) so that it maximally benefits the decision tree performance. we investigate promising level of the cues of experiment data, and then adjust the applications in decision tree accordingly. five comparative decision tree experiments demonstrate that tree performance, such as accurate rate and complexity, is dramatically improved by statistically and semantically selecting cues.an exploratory study on promising cues in deception detection and application of decision tree','Intrusion detection systems'
'the propagation speed of fast scanning worms and the stealthy nature of slow scanning worms present unique challenges to intrusion detection. typically, techniques optimized for detection of fast scanning worms fail to detect slow scanning worms, and vice versa. in practice, there is interest in developing an integrated approach to detecting both classes of worms. in this paper, we propose and analyze a unique integrated detection approach capable of detecting and identifying traffic flow(s) responsible for simultaneous fast and slow scanning malicious worm attacks. the approach uses a combination of evidence from distributed host-based anomaly detectors, a self-adapting profiler and bayesian inference from network heuristics to detect intrusion activity due to both fast and slow scanning worms. we assume that the extreme nature of fast scanning worm epidemics make them well suited for extreme value theory and use sample mean excess function to determine appropriate thresholds for detection of such worms. random scanning worm behavior is considered in analyzing the stochastic time intervals that affect behavior of the detection technique. based on the analysis, a probability model for worm detection interval using the detection scheme was developed. simulations are used to validate our assumptions and analysis.an integrated approach to detection of fast and slow scanning worms','Intrusion detection systems'
'the task of detecting, repulsing and preventing attacks in wireless environments is becoming more and more difficult as a result of several factors including defective design and improper requirements specification of wireless intrusion detection systems. systems specifications provide the basics and framework to design an effective system, and for wireless intrusion detection systems, this assumes greater importance due to the complexity of wireless environments, and the architectural and functional constraints created by such environments. in the light of this, this paper discusses the architectural and functional issues involved in intrusion systems design for mobile and adhoc networks (wireless environments); and also proposes a new way of making systems requirements specification more effective and responsive to the needs of wireless implementations.architectural and functional issues in systems requirements specifications for wireless intrusion detection systems implementation','Intrusion detection systems'
'many small devices are capable of connecting to the internet now. the trend of pervasive computing and networking is going strong. at the same time, security threats on the internet continue to increase. existing defense mechanisms against network-based attacks can hardly apply to small network devices due to limitations of their computing resources. this paper presents a method to mitigate this problem. the approach embeds the attack detection mechanism inside the device&#8217;s network module. it focuses on the operational behavior of the module to identify abnormal events. the mechanism does not compete for resources with the main function of the device. it has little impact on the system performance. furthermore, the method may detect unknown attacks.attack detection for resource-constrained network devices','Intrusion detection systems'
'the aim of this paper is to propose a new formal technique for modeling system vulnerabilities and automatic generation of attack scenarios exploiting these vulnerabilities for intrusion detection systems. our technique is generic and can be applied to various domains including web services, web servers, distributed applications using the internet, etc.automatic generation of attack scenarios for intrusion detection systems','Intrusion detection systems'
'in this paper, we present the design and implementation ofa collaborative intrusion detection system (cids) foraccurate and efficient intrusion detection in a distributedsystem. cids employs multiple specialized detectors at thedifferent layers - network, kernel and application - and amanager based framework for aggregating the alarms fromthe different detectors to provide a combined alarm for anintrusion. the premise is that a carefully designed andconfigured cids can increase the accuracy of detectioncompared to individual detectors, without a substantialdegradation in performance. in order to validate the premise,we present the design and implementation of a cids whichemploys snort, libsafe, and a new kernel level ids calledsysmon. the manager has a graph-based and a bayesiannetwork based aggregation method for combining the alarmsto finally come up with a decision about the intrusion. thesystem is evaluated using a web-based electronic store frontapplication and under three different classes of attacks -buffer overflow, flooding and script-based attacks. the resultsshow performance degradations compared to no detection of3.9\% and 6.3\% under normal workload and a buffer overflowattack respectively. the experiments to evaluate the accuracyof the system show that the normal workload generates falsealarms for snort and the elementary detectors produce missedalarms. cids does not flag the false alarm and reduces theincidence of missed alarms to 1 of the 7 cases. cids can alsobe used to measure the propagation time of an intrusion whichis useful in choosing an appropriate response strategy.collaborative intrusion detection system (cids)','Intrusion detection systems'
'this paper discusses the financial benefit of intrusion detection systems (ids) deployment techniques and addresses the problems of bridging the gap between technical security solutions and the business need for it. this is an area of interest to both the research and the business community; most idses balance host and network monitoring, but the decision about how to adjust usage of each technique tends to be made in a rather ad-hoc way, or based upon effectiveness of detection only without regard to cost of technique. in practice, selections based on how well a strategy helps a company to perform are preferable and methodologies supporting a selection process of this type will assist an information technology officer to explain security mechanism selections more effectively to ceos. in this context, the approach we propose could be applied when choosing one intrusion detection system over another based on which has a better or higher return on investment for the company.through a case study, we illustrate the benefits of a better ids management that leads to a positive return on investment (roi) for ids deployment. we conceive strategies and approaches to support effective decision-making about which techniques are appropriate for the cost effective management of the ids in a given environment. it is our intent that this research will serve as a foundation for the formal description of cost structures, analysis, and selection of effective implementation approaches to support the management of ids deployments.cost effective management frameworks for intrusion detection systems','Intrusion detection systems'
'the steady evolution of the web has paved the way for miscreants to take advantage of vulnerabilities to embed malicious content into web pages. up on a visit, malicious web pages steal sensitive data, redirect victims to other malicious targets, or cease control of victim\'s system to mount future attacks. approaches to detect malicious web pages have been reactively effective at special classes of attacks like drive-by-downloads. however, the prevalence and complexity of attacks by malicious web pages is still worrisome. the main challenges in this problem domain are (1) fine-grained capturing and characterization of attack payloads (2) evolution of web page artifacts and (3) exibility and scalability of detection techniques with a fast-changing threat landscape. to this end, we proposed a holistic approach that leverages static analysis, dynamic analysis, machine learning, and evolutionary searching and optimization to effectively analyze and detect malicious web pages. we do so by: introducing novel features to capture fine-grained snapshot of malicious web pages, holistic characterization of malicious web pages, and application of evolutionary techniques to fine-tune learning-based detection models pertinent to evolution of attack payloads. in this paper, we present key intuition and details of our approach, results obtained so far, and future work.effective analysis, characterization, and detection of malicious web pages','Intrusion detection systems'
'we investigate the feasibility of a simple, traffic volumebased intrusion detection for an ieee 802.15.4 compliant sensor cluster operating in beacon-enabled, slotted csmaca mode. we have used simple exponential averaging to filter out some of the inherent variability in individual device arrival rate, and introduced a small hysteresis in the decision process in order to avoid false alarms due to dithering. initial results demonstrate that the intrusion detection implemented in this manner may indeed operate quickly and efficiently.evaluating the feasibility of traffic-based intrusion detection in an 802.15.4 sensor cluster','Intrusion detection systems'
'the speed of today\'s worms demands automated detection, but the risk of false positives poses a difficult problem. in prior work, we proposed a host-based intrusion-detection system for worms that leveraged collaboration among peers to lower its risk of false positives, and we simulated this approach for a system with two peers. in this paper, we build upon that work and evaluate our ideas ``in the wild.\'\' we implement wormboy 2.0, a prototype of our vision that allows us to quantify and compare worms\' and non-worms\' temporal consistency, similarity over time in worms\' and non-worms\' invocations of system calls. we deploy our prototype to a network of 30 hosts running windows xp with service pack 2 to monitor and analyze 10,776 processes, inclusive of 511 unique non-worms (873 if we consider unique versions to be unique non-worms). we identify properties with which we can distinguish non-worms from worms 99\% of the time. we find that our collaborative architecture, using patterns of system calls and simple heuristics, can detect worms running on multiple peers. and we find that collaboration among peers significantly reduces our probability of false positives because of the unlikely appearance on many peers simultaneously of non-worm processes with worm-like properties.exploiting temporal consistency to reduce false positives in host-based, collaborative detection of worms','Intrusion detection systems'
'several alert correlation methods have been proposed over the past several years to construct high-level attack scenarios from low-level intrusion alerts reported by intrusion detection systems (idss). however, all of these methods depend heavily on the underlying idss, and cannot deal with attacks missed by idss. in order to improve the performance of intrusion alert correlation and reduce the impact of missed attacks, this paper presents a series of techniques to hypothesize and reason about attacks possibly missed by the idss. in addition, this paper also discusses techniques to infer attribute values for hypothesized attacks, to validate hypothesized attacks through raw audit data, and to consolidate hypothesized attacks to generate concise attack scenarios. the experimental results in this paper demonstrate the potential of these techniques in building high-level attack scenarios.hypothesizing and reasoning about attacks missed by intrusion detection systems','Intrusion detection systems'
'in this paper, we study the impact of today\'s it policies, defined based upon a monoculture approach, on the performance of endhost anomaly detectors. this approach leads to the uniform configuration of host intrusion detection systems (hids) across all hosts in an enterprise networks. we assess the performance impact this policy has from the individual\'s point of view by analyzing network traces collected from 350 enterprise users. we uncover a great deal of diversity in the user population in terms of the \"tail\" behavior, i.e., the component which matters for anomaly detection systems. we demonstrate that the monoculture approach to hids configuration results in users that experience wildly different false positive and false negatives rates. we then introduce new policies, based upon leveraging this diversity and show that not only do they dramatically improve performance for the vast majority of users, but they also reduce the number of false positives arriving in centralized it operation centers, and can reduce attack strength.impact of it monoculture on behavioral end host intrusion detection','Intrusion detection systems'
'computer-mediated communication is becoming ubiquitous in our networked society. yet one aspect of human communication that is rarely explored in the context of computer-mediated communication is deception. deception is common, yet communication research has shown that people perform little better than chance at successfully detecting it. two different laboratory studies of deception and its detection in face-to-face groups, communicating either with electronic support or without it, are reported on here. in both studies, subjects were very poor at detecting deception, across media. yet third parties who had not taken part in the group meetings were able to detect about half or more of the deceptive statements that appeared in the transcripts of the group meetings. a possible explanation explored here is the role of social facilitation and the inhibiting influence it may have on deception and on its detection.inhibiting deception and its detection','Intrusion detection systems'
'currently, the most significant line of defense against malware is anti-virus products which focus on authenticating valid software from a white list, blocking invalid software from a black list, and running any unknown software (i.e., the gray list) in a controlled manner. the gray list, containing unknown software programs which could be either normal or malicious, is usually authenticated or rejected manually by virus analysts. unfortunately, along with the development of the malware writing techniques, the number of file samples in the gray list that need to be analyzed by virus analysts on a daily basis is constantly increasing. in this paper, we develop an intelligent file scoring system (ifss for short) for malware detection from the gray list by an ensemble of heterogeneous base-level classifiers derived by different learning methods, using different feature representations on dynamic training sets. to the best of our knowledge, this is the first work of applying such ensemble methods for malware detection. ifss makes it practical for virus analysts to identify malware samples from the huge gray list and improves the detection ability of anti-virus software. it has already been incorporated into the scanning tool of kingsoft\'s anti-virus software. the case studies on large and real daily collection of the gray list illustrate that the detection ability and efficiency of our ifss system outperforms other popular scanning tools such as nod32 and kaspersky.intelligent file scoring system for malware detection from the gray list','Intrusion detection systems'
'in this paper, we describe a new solution for detecting mobile phone viruses. the solution is based on bayesian decision theory using heuristic rules derived from common functionalities among different virus samples. specifically, we detect viruses according to the dll usage of a program, which is directly linked to the functionality of this program. our solution is able to detect unknown viruses, especially the variants of existing ones. we evaluate our solution on the symbian platform, where most viruses are present in the wild. we constructed a virus detector based on dll functions from a small set of virus samples. it detects 95\% of mobile viruses and yields no false alarm.intelligent virus detection on mobile devices','Intrusion detection systems'
'a virtual machine is a software replica of an underlying real machine. multiple virtual machines can operate on the same host machine concurrently, without interfere each other. such concept is becoming valuable in production computing systems, due to its benefits in terms of costs and portability. as they provide a strong isolation between the virtual environment and the underlying real system, virtual machines can also be used to improve the security of a computer system in face of attacks to its network services. this paper presents a new approach to achieve this goal, by applying intrusion detection techniques to virtual machine based systems, thus keeping the intrusion detection system out of reach from intruders. the results obtained from a prototype implementation confirm the usefulness of this approach.intrusion detection in virtual machine environments','Intrusion detection systems'
'intrusion detection systems','Intrusion detection systems'
'distributed intrusion detection system (dids) is one of important devices for information security. in this field, how to improve detection rate is one of key issues. in this paper, the importance of event sequence in time is presented. then, we discuss three factors, i.e. timestamp precision, time synchronization and network delay, which effect detection rate on the view of event sequence in time. on the three aspects, timestamp precision is the key to keep internal event sequence, time synchronization is the base of correcting event sequence among computers, and that network delay makes time-series analysis not true. accordingly, we address some methods, i.e. raising timestamp precision, active selfadapting time synchronization algorithm and state turnabout mechanism. experiments indicate that anyone of three measures can elevate detection performance to a certain extent. if they all are adopted, better detection results are revealed.issue of event sequence in time of distributed intrusion detection system','Intrusion detection systems'
'this article presents results in two mutually complementary areas: distributed immunological information assurance and a new signature-matching technique based on kolmogorov complexity. this article introduces a distributed model for security based on biological paradigms of epidemiology and immunology. in this model each node in the network has an immune system that identifies and destroys pathogens in the incoming network traffic as well as files resident on the node. the network nodes present a collective defense to the pathogens by working symbiotically and sharing pathogen information with each other. each node compiles a list of pathogens that are perceived as threats by using information provided from all the nodes in the network. the signatures for these pathogens are incorporated into the detector population of the immune systems to increase the probability of detection. critical to the success of this system is the detection scheme, which should not only be accurate but also efficient. three separate schemes for detecting pathogens are examined, namely, contiguous string matching, hamming distance, and kolmogorov complexity. this work provides a model of the system and examines the efficiency of different detection schemes. a simulation model is built to study the sensitivity of model parameters, such as signature length, sampling rate, and network topology, to detection of pathogens.kolmogorov complexity estimates for detection of viruses in biologically inspired security systems','Intrusion detection systems'
'we have designed and implemented an intrusion detection system (ids) prototype based on mobile agents. our agents travel between monitored systems in a network of distributed systems, obtain information from data cleaning agents, classify and correlate information, and report the information to a user interface and database via mediators.agent systems with lightweight agent support allow runtime addition of new capabilities to agents. we describe the design of our multi-agent ids and show how lightweight agent capabilities allowed us to add communication and collaboration capabilities to the mobile agents in our ids.lightweight agents for intrusion detection','Intrusion detection systems'
'financial loss due to malware nearly doubles every two years. for instance in 2006, malware caused near 33.5 million gbp direct financial losses only to member organizations of banks in uk. recent malware cannot be detected by traditional signature based anti-malware tools due to their polymorphic and/or metamorphic nature. malware detection based on its immutable characteristics has been a recent industrial practice. the datasets are not public. thus the results are not reproducible and conducting research in academic setting is difficult. in this work, we not only have improved a recent method of malware detection based on mining application programming interface (api) calls significantly, but also have created the first public dataset to promote malware research. our technique first reads api call sets used in a collection of portable executable (pe) files, then generates a set of discriminative and domain interpretable features. these features are then used to train a classifier to detect unseen malware. we have achieved detection rate of 99.7\% while keeping accuracy as high as 98.3\%. our method improved state of the art technology in several aspects: accuracy by 5.24\%, detection rate by 2.51\% and false alarm rate was decreased from 19.86\% to 1.51\%. this project\'s data and source code can be found at http://home.shirazu.ac.ir/~sami/malware.malware detection based on mining api calls','Intrusion detection systems'
'inspired by the potential interesting ideas of the danger theory (dt), the research into artificial immune system (ais) has been developing faster then everthe basic role of dt embedded in ais is to provide what t-cells should respond through the professional antigen-presenting cells when there are some cells undergoing injury, or stress or `bad cell death\'however, in the context of ais-based network security systems, how to minimize damage by mitigating virus infection is a key step to solve real-world problems. in this paper we present the blueprint of a dt inspired multi-agents ais for anomaly detection, it is called maais, which can dynamically adapt to various types of immune responses by configuring parameters during the course of evaluating the degree of danger signals receivedadditionally, a suitable mechanism of communication between agents is employed to stipulate the better performance of the novel systemmulti-agents artificial immune system (maais) inspired by danger theory for anomaly detection','Intrusion detection systems'
'mobility can be exploited to spread malware among wireless nodes moving across network domains. because such mobile worms spread across domains by exploiting the physical movement of mobile nodes, they cannot be contained by existing defenses. in this paper we address this new challenge using techniques for detecting the existence of stealthy mobile worms in the early stages of their infection and identifying the origins of such infections. the proposed mechanisms are based on random moonwalks which were originally used in post mortem analysis of internet worms. however as we demonstrate, the original technique fails against mobile worms which are inherently stealthier than existing malware. in this paper, we extend the moonwalk algorithm by considering new heuristics and show that the proposed mechanism can reliably detect mobile worms during the early stages of infection. our simulation results, based on network traces collected from a university-wide wireless network, show that a mobile infection can be reliably detected before it infects 10\% of the vulnerable population. furthermore, the proposed mechanism identifies the origin of the infection, by limiting the search for the initial victims to within 2\% of the mobile node populationon the detection and origin identification of mobile worms','Intrusion detection systems'
'the rapid increasing of security incidents imposes a great burden on internet users and system administrators. in this paper we discuss a parallel analysis for lightweight network incident detection using nonlinear adaptive systems. we run aid (anomaly intrusion detection) and mid (misuse intrusion detection) systems in parallel. two detectors generate binary output misuse = {yes/no} and anomaly = {yes/no}. then, we can determine whether we need to perform network or security operation. we apply clustering algorithm for aid and classification algorithm for mid. the nonlinear adaptive system is trained for running mid and aid in parallel. proposed parallel system is more lightweight and simple to operate even if the number of incident patterns is increased. experimental results in the case where false positive is frequently caused show that our method is functional with a recognition rate of attacks less than 10\%, while finding the anomaly status. also, performance evaluation show that proposed system can work with reasonable cpu utilization compared with conventional serial search based system.parallel analysis for lightweight network incident detection using nonlinear adaptive systems','Intrusion detection systems'
'a worm is a self-replicating computer program which does not need neither to attach itself to an existing program nor require user intervention unlike viruses. worms exploit operating system and application software vulnerabilities to infect the systems. polymorphic code itself is the art of developing code that mutates at each copy while keeping the original algorithm unchanged. by the way, a polymorphic worm changes its pattern each time it sends a copy to another system. thereby this avoids detection by simple signature matching techniques. on the other hand, there is still some part of code that remains unchanged. in this work, we propose token-pair conjunction and token-pair subsequence signatures for detecting polymorphic worm threats. experiments of the proposed model were performed using two real polymorphic worms. experiment results show that the proposed signature schema have low false negatives and false positives.polymorphic worm detection using token-pair signatures','Intrusion detection systems'
'this paper presents a feasibility study of novel attack detection mechanisms in wireless sensor networks (wsn) based on detecting anomalies and changes in sensor signals and data values. typical wsn attacks are considered in the empirical study of various attack detection techniques utilizing features based on sensor signal strength and other wsn technological parameters and using machine learning classification techniques such as clustering, rule learners, and neural networks. for the attack detection implementation the study employed wsn built from sun kits available on the market and extended sensor network anomaly detection system (snads) framework of methods and tools.poster: signal anomaly based attack detection in wireless sensor networks','Intrusion detection systems'
'electric utilities are currently installing wide area monitoring systems which include phasor measurement units (pmu) and phasor data concentrators (pdc). the ieee c37.118 protocol is used for network communication between these devices. protocol mutation attacks against this protocol can lead to denial of service and a loss of visibility of the state of the power system. this paper is a preview of a set rules developed to proactively monitor for protocol mutation attacks against the ieee c37.118 protocol. eleven rules were described in this paper. a total of 36 rules have been developed. this paper also describes validation of the rules and proposes and architecture for implementing the rules in a utility network.protocol mutation intrusion detection for synchrophasor communications','Intrusion detection systems'
'the android permission system informs users about the privileges demanded by applications (apps), and in principle allows users to assess potential risks of apps. unfortunately, recent studies showed that the installation-time permission verification procedure is often ignored, due to users\' lack of attention or insufficient understanding of the privileges or the android permission system. as a consequence, malicious apps are likely granted with security- and privacy-critical permissions, and launch various kinds of attacks without being noticed by the users. in this paper, we present the design, analysis, and implementation of droidpad, a novel solution that aims to leverage system-wide state information to detect and block in real-time possible abuses of android permissions. especially, with a focus on sms-related permissions, we have implemented a proof-of-concept prototype. our evaluation based on 48 representative benign and malicious apps shows that droidpad successfully detected sms permissions-abusing activities with low false-positive rates, and zero false-negative rates.real-time detection and prevention of android sms permission abuses','Intrusion detection systems'
'we suggest an efficient framework to detect malware in intrusion detection system (ids). the framework generates signatures from malware families and generates corresponding detection rules. the generated signatures are not influenced by small changes of malware while they can be used to detect malware that has similar behaviors with normal programs. our signatures are stored as an aho-corasick tree form to improve signature matching performance in ids.real-time malware detection framework in intrusion detection systems','Intrusion detection systems'
'this paper presents two schemes for detecting bufferoverflow attacks at run-time. one is sensor embedding,which hides sensor data objects inside code pointers, andthe other, stack frame inversion checking, which detects attacksby inspecting processor registers. our methods makeit difficult for attackers to guess the locations of sensors sothat they cannot easily bypass sensors when they attemptto access code pointers. we have implemented the schemesby extending the gcc toolchain. experimental data showsthat our schemes provide programs with powerful detectionand protection capabilities at the reasonable sacrificeof execution efficiency. operating systems would improveon the defense against buffer overflow attacks by using ourtoolchain when they are built.run-time detection of buffer overflow attacks without explicit sensor data objects','Intrusion detection systems'
'since current computer infrastructures are increasingly vulnerable to malicious activities, intrusion detection is necessary but unfortunately not sufficient.we need to design effective response techniques to circumvent intrusions when they are detected. our approach is based on a library that implements differenttypes of counter-measures. the idea is to design a decision support tool to help the administrator to choose, in this library, the appropriate counter-measure whena given intrusion occurs. for this purpose, we formally define the notion of anti-correlation which is used to determine the counter-measures that are effective to stop the intrusion. finally, we present a platform of intrusion detection, called diams, that implements the response mechanisms presented in this paper.selecting appropriate counter-measures in an intrusion detection framework','Intrusion detection systems'
'testing and evaluating computer intrusion detection systems','Intrusion detection systems'
'in 1998 and again in 1999, the lincoln laboratory of mit conducted a comparative evaluation of intrusion detection systems (idss) developed under darpa funding. while this evaluation represents a significant and monumental undertaking, there are a number of issues associated with its design and execution that remain unsettled. some methodologies used in the evaluation are questionable and may have biased its results. one problem is that the evaluators have published relatively little concerning some of the more critical aspects of their work, such as validation of their test data. the appropriateness of the evaluation techniques used needs further investigation. the purpose of this article is to attempt to identify the shortcomings of the lincoln lab effort in the hope that future  efforts of this kind will be placed on a sounder footing. some of the problems that the article points out might well be resolved if the evaluators were to publish a detailed description of their procedures and the rationale that led to their adoption, but other problems would clearly remain./par>testing intrusion detection systems','Intrusion detection systems'
'in recent years, organizations ranging from defense and other government institutions to commercial enterprises, research labs, etc., have witnessed an increasing amount of sophisticated insider attacks that manage to bypass existing security controls. insider threats are staged by either disgruntled employees, or employees engaged in malicious activities such as industrial espionage. the objectives of such threats range from sabotage, e.g., in order to disrupt the completion of a project, to exfiltration of sensitive data such as trade secrets, patents, etc. insiders are often skilled and motivated individuals with good knowledge of internal security measures in the organization. they devise effective and carefully planned attacks, prepared over long periods of time and customized to inflict maximum damage. such attacks are difficult to detect and protect against, because insiders have the proper credentials to access services and systems within the organization, and possess knowledge that may allow them to deceive network defense controls. as a result, a large number of hosts may be taken over, allowing malicious insiders to maintain control over the network even after leaving the organization. the objective of this study is to identify a high-level architecture and mechanisms for early detection and protection against insider threats. one of the main aspects we focus on is preventing data exfiltration, which is known to cost billions of dollars in losses annually. the goal is to either (i) detect attacks as they occur and prevent insiders from gaining control over the network, or (ii) detect early hosts and services that are compromised such that malware is prevented from spreading/morphing, hence insiders are no longer able to control the network or to exfiltrate sensitive data. we envision a data-intensive approach that leverages large amounts of events collected from a diverse set of sources such as network sensors, intrusion detection systems, service logs, as well as known attack databases (e.g., virus signature collections, digital artifacts), security and service logs, etc. the proposed approach aims to study and understand the relationships and correlations between events, with the purpose of detecting anomalous and/or malicious behavior.the optimization of situational awareness for insider threat detection','Intrusion detection systems'
'the increasing significance of information technology (it) security to firms is evident from their growing it security budgets. firms rely on security technologies such as firewalls and intrusion detection systems (idss) to manage it security risks. although the literature on the technical aspects of it security is proliferating, a debate exists in the it security community about the value of these technologies. in this paper, we seek to assess the value of idss in a firm\'s it security architecture. we find that the ids configuration, represented by detection (true positive) and false alarm (false positive) rates, determines whether a firm realizes a positive or negative value from the ids. specifically, we show that a firm realizes a positive value from an ids only when the detection rate is higher than a critical value, which is determined by the hacker\'s benefit and cost parameters. when the firm realizes a positive (negative) value, the ids deters (sustains) hackers. however, irrespective of whether the firm realizes a positive or negative value from the ids, the ids enables the firm to better target its investigation of users, while keeping the detection rate the same. our results suggest that the positive value of an ids results not from improved detection per se, but from an increased deterrence enabled by improved detection. finally, we show that the firm realizes a strictly nonnegative value if the firm configures the ids optimally based on the hacking environment.the value of intrusion detection systems in information technology security architecture','Intrusion detection systems'
'in this paper we present a tool for network anomaly detection and network intelligence which we named ulisse. it uses a two tier architecture with unsupervised learning algorithms to perform network intrusion and anomaly detection. ulisse uses a combination of clustering of packet payloads and correlation of anomalies in the packet stream. we show the experiments we conducted on such architecture, we give performance results, and we compare our achievements with other comparable existing systems.ulisse, a network intrusion detection system','Intrusion detection systems'
'systems proposed in academic research have so far failed to make a significant impact on real-world vulnerability detection. most software bugs are still found by methods with little input from static-analysis and verification research. these research areas could have a significant impact on software security, but first we need a shift in research goals and approaches. we need systems that incorporate human code auditors\' knowledge and abilities, and we need evaluation methods that actually test proposed systems\' usability in real situations. without changes, academic research will continue to be ignored by the security community, and opportunities to build better tools for finding bugs and understanding software will be missed.vulnerability detection systems','Intrusion detection systems'
'   Bud1  @      @     \t     0                                       G         e f f i c                                                                                                                                                                                                                                                                                                                                                                                                                                            k A n   e f f i c i e n t   k e y - m a n a g e m e n t   s c h e m e   f o r   h i e r a r c h i c a l   a c c e s s   c o n t r o l   b a s e d   o n   e l l i p t i c   c u r v e   c r y p t o s y s t e m . t x tIlocblob      ?  ???????         K T D C K M - S D C . t x tIlocblob     R  ??????   x tIlocblob     ?  ???????     : C o m b i n a t o r i a l   O p t i m i z a t i o n   o f   M u l t i c a s t   K e y   M a n a g e m e n t . t x tIlocblob     ^  ???????      C r y                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          @      ?                                        @      ?                                          @      ?                                          @                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 o M a n a g e m e n t   o f   a   p u b l i c   k e y   c e r t i f i c a t i o n   i n f r a s t r u c t u r e & # 8 2 1 2 ; e x p e r i e n c e s   f r o m   t h e   D e T e B e r k o m   p r o j e c t   B M S e c . t x tIlocblob     ?  ??????     v M i n i m i z i n g   c e n t e r   k e y   s t o r a g e   i n   h y b r i d   o n e - w a y   f u n c t i o n   b a s e d   g r o u p   k e y   m a n a g e m e n t   w i t h   c o m m u n i c a t i o n   c o n s t r a i n t s . t x tIlocblob     ^  ??????     # M u l t i c h a n n e l   S e c u r i t y   P r o t o c o l s . t x tIlocblob      F  ???????     > O n - d e m a n d   p u b l i c - k e y   m a n a g e m e n t   f o r   m o b i l e   a d   h o c   n e t w o r k s . t x tIlocblob      ?  ???????     + O p t i m a l   D i s p e r s a l   o f   C e r t i f i c a t e   C h a i n s . t x tIlocblob     R  ???????     @ O p t i m i z i n g   d e l e t i o n   c o s t   f o r   s e c u r e   m u l t i c a s t   k e y   m a n a g e m e n t . t x tIlocblob     ?  ???????     3 P e r f o r m a n c e   A n a l y s i s   G r o u p - B a s e d   K e y   M a n a g e m e n t . t x tIlocblob     ^  ???????     > P e r f o r m a n c e   O p t i m i z a t i o n s   f o r   G r o u p   K e y   M a n a g e m e n t   S c h e m e s . t x tIlocblob      F  ???????     \t P o r K I . t x tIlocblob      ?  ???????     P P o w e r   p r o x i m i t y   b a s e d   k e y   m a n a g e m e n t   f o r   s e c u r e   m u l t i c a s t   i n   a d   h o c   n e t w o r k s . t x tIlocblob     R  ???????     9 P r o x y   E n c r y p t i o n s   f o r   S e c u r e   M u l t i c a s t   K e y   M a n a g e m e n t . t x tIlocblob     ?  ???????     C P s e u d o n y m   m a n a g e m e n t   u s i n g   m e d i a t e d   i d e n t i t y - b a s e d   c r y p t o g r a p h y . t x tIlocblob     ^  ???????     + P u b l i c   K e y   d i s t r i b u t i o n   w i t h   s e c u r e   D N S . t x tIlocblob      F  h??????     3 P u b l i c   k e y   m a n a g e m e n t   f o r   X . 2 5   n e t w o r k   s e c u r i t y . t x tIlocblob      ?  h??????     j Q o S - a w a r e   a n d   c o m p r o m i s e - r e s i l i e n t   k e y   m a n a g e m e n t   s c h e m e   f o r   h e t e r o g e n e o u s   w i r e l e s s   i n t e r n e t   o f   t h i n g s . t x tIlocblob     R  h??????     K S c a l a b l e   a n d   e f f i c i e n t   k e y   m a n a g e m e n t   f o r   h e t e r o g e n e o u s   s e n s o r   n e t w o r k s . t x tIlocblob     ?  h??????     E S c a l a b l e   C r y p t o g r a p h i c   K e y   M a n a g e m e n t   i n   W i r e l e s s   S e n s o r   N e t w o r k s . t x tIlocblob     ^  h??????     R S e c C O S E n   -   A   K e y   M a n a g e m e n t   S c h e m e   f o r   S e c u r i n g   C h a i n   O r i e n t e d   S e n s o r   N e t w o r k s . t x tIlocblob      F  ???????     A S e c u r e   a n d   e f f i c i e n t   k e y   m a n a g e m e n t   i n   m o b i l e   a d   h o c   n e t w o r k s . t x tIlocblob      ?  ???????     Q S e c u r e   a n d   e f f i c i e n t   p u b l i c   k e y   m a n a g e m e n t   i n   n e x t   g e n e r a t i o n   m o b i l e   n e t w o r k s . t x tIlocblob     R  ???????     C S e l f - O r g a n i z e d   P u b l i c - K e y   M a n a g e m e n t   f o r   M o b i l e   A d   H o c   N e t w o r k s . t x tIlocblob     ?  ???????     \% T h e   & # 9 3 7 ;   k e y   m a n a g e m e n t   s e r v i c e . t x tIlocblob     ^  ???????     G T h e   E f f i c i e n c y   o f   P e r i o d i c   R e k e y i n g   i n   D y n a m i c   G r o u p   K e y   M a n a g e m e n t . t x tIlocblob      F  H??????                                                                                                                                                                                                                                                                                                                                                                d A   C o m p l e t e   S e t   o f   P r o t o c o l s   f o r   D i s t r i b u t e d   K e y   M a n a g e m e n t   i n   C l u s t e r e d   W i r e l e s s   S e n s o r   N e t w o r k s . t x tIlocblob      F   (??????     T A   f o r w a r d   a u t h e n t i c a t i o n   k e y   m a n a g e m e n t   s c h e m e   f o r   h e t e r o g e n e o u s   s e n s o r   n e t w o r k s . t x tIlocblob      ?   (??????     Y A   h e t e r o g e n e o u s - n e t w o r k   a i d e d   p u b l i c - k e y   m a n a g e m e n t   s c h e m e   f o r   m o b i l e   a d   h o c   n e t w o r k s . t x tIlocblob     R   (??????     H A   H y b r i d   G r o u p   K e y   M a n a g e m e n t   S c h e m e   f o r   T w o - L a y e r e d   A d   H o c   N e t w o r k s . t x tIlocblob     ?   (??????     X A   H y b r i d   S c a l a b l e   G r o u p   K e y   M a n a g e m e n t   A p p r o a c h   f o r   L a r g e   D y n a m i c   M u l t i c a s t   N e t w o r k s . t x tIlocblob     ^   (??????     h A   K e y   M a n a g e m e n t   P r o t o c o l   f o r   S e c u r i n g   S t a b i l i t y   o f   a n   I n t e r m e d i a t e   N o d e   i n   W i r e l e s s   S e n s o r   N e t w o r k s . t x tIlocblob      F   ???????     4 A   k e y - m a n a g e m e n t   s c h e m e   b a s e d   o n   c o n t r o l   v e c t o r s . t x tIlocblob      ?   ???????     J A   L i g h t w e i g h t   K e y   M a n a g e m e n t   P r o t o c o l   f o r   H i e r a r c h i c a l   S e n s o r   N e t w o r k s . t x tIlocblob     R   ???????     b A   L o c a l l y   G r o u p   K e y   M a n a g e m e n t   w i t h   R e v o c a t i o n   a n d   S e l f - h e a l i n g   C a p a b i l i t y   f o r   S e n s o r   N e t w o r k s . t x tIlocblob     ?   ???????     K A   N e w   A u t h e n t i c a t i o n   M e t h o d   w i t h   D i s t r i b u t e d   H a s h   T a b l e   f o r   P 2 P   N e t w o r k . t x tIlocblob     ^   ???????     9 A   n o v e l   a p p r o a c h   t o   c e r t i f i c a t e   r e v o c a t i o n   m a n a g e m e n t . t x tIlocblob      F  ??????     ` A   s c a l a b l e   k e y - m a n a g e m e n t   s c h e m e   w i t h   m i n i m i z i n g   k e y   s t o r a g e   f o r   s e c u r e   g r o u p   c o m m u n i c a t i o n s . t x tIlocblob      ?  ??????     R A   s c a l a b l e   m u l t i c a s t   k e y   m a n a g e m e n t   s c h e m e   f o r   h e t e r o g e n e o u s   w i r e l e s s   n e t w o r k s . t x tIlocblob     R  ??????     Q A   S t u d y   o n   a n   E f f e c t i v e   G r o u p   M a n a g e m e n t   S c h e m e   f o r   S e c u r e   M u l t i c a s t   i n   M I P v 6 . t x tIlocblob     ?  ??????     _ A   u n i f i e d   s e c u r i t y   f r a m e w o r k   w i t h   t h r e e   k e y   m a n a g e m e n t   s c h e m e s   f o r   w i r e l e s s   s e n s o r   n e t w o r k s . t x tIlocblob     ^  ??????     ] A d d i n g   C o n f i d e n t i a l i t y   t o   A p p l i c a t i o n - L e v e l   M u l t i c a s t   b y   L e v e r a g i n g   t h e   M u l t i c a s t   O v e r l a y . t x tIlocblob      F  x??????     G A n   e f f i c i e n t   g r o u p   k e y   m a n a g e m e n t   s c h e m e   f o r   m o b i l e   a d   h o c   n e t w o r k s . t x tIlocblob      ?  x??????     , A n   E f f i c i e n t   G r o u p   K e y   M a n a g e m e n t   S c h e m e . t x tIlocblob     R  x??????     8 A n   E f f i c i e n t   K e y   M a n a g e m e n t   f o r   L a r g e   D y n a m i c   G r o u p s . t x tIlocblob     ?  x??????     > A n   E f f i c i e n t   K e y   M a n a g e m e n t   S c h e m e   f o r   P e r v a s i v e   C o m p u t i n g . t x tIlocblob     ^  x??????     A A n   E f f i c i e n t   K e y   M a n a g e m e n t   S c h e m e   f o r   S e c u r e   S e n s o r   N e t w o r k s . t x tIlocblob      F  ???????                                                                                                                                                        K A p p r o x i m a t e l y   o p t i m a l   t r e e s   f o r   g r o u p   k e y   m a n a g e m e n t   w i t h   b a t c h   u p d a t e s . t x tIlocblob     R  ???????     B C e n t r a l i z e d   d i r e c t o r y   s e r v i c e s   a n d   a c c o u n t s   m a n a g e m e n t   p r o j e c t . t x tIlocblob     ?  ???????     : C o m b i n a t o r i a l   O p t i m i z a t i o n   o f   M u l t i c a s t   K e y   M a n a g e m e n t . t x tIlocblob     ^  ???????      C r y p t o   k e y   m a n a g e m e n t . t x tIlocblob      F  X??????     J C r y p t o g r a p h i c   K e y   M a n a g e m e n t   f o r   S C A D A   S y s t e m s - I s s u e s   a n d   P e r s p e c t i v e s . t x tIlocblob      ?  X??????       C r y p t o g r a p h i c   k e y   m a n a g e m e n t . t x tIlocblob     R  X??????     ; D i s t r i b u t e d   k e y   m a n a g e m e n t   f o r   d y n a m i c   g r o u p s   i n   M A N E T s . t x tIlocblob     ?  X??????     : D i s t r i b u t i v e   K e y   M a n a g e m e n t   f o r   M o b i l e   A d   H o c   N e t w o r k s . t x tIlocblob     ^  X??????     ^ D u a l - L e v e l   K e y   M a n a g e m e n t   f o r   s e c u r e   g r i d   c o m m u n i c a t i o n   i n   d y n a m i c   a n d   h i e r a r c h i c a l   g r o u p s . t x tIlocblob      F  ???????      E D D K . t x tIlocblob      ?  ???????     X E f f e c t   o f   k e y   g e n e r a t o r s   o n   t h e   a u t o m a t i c   s e a r c h   f o r   f l a w s   i n   k e y   m a n a g e m e n t   s c h e m e s . t x tIlocblob     R  ???????     J E f f i c i e n t   p u b l i c   k e y   c e r t i f i c a t e   m a n a g e m e n t   f o r   m o b i l e   a d   h o c   n e t w o r k s . t x tIlocblob     ?  ???????     W E l i m i n a t i n g   c o u n t e r e v i d e n c e   w i t h   a p p l i c a t i o n s   t o   a c c o u n t a b l e   c e r t i f i c a t e   m a n a g e m e n t . t x tIlocblob     ^  ???????     S E v a l u a t i o n   o f   D u a l - S t r u c t u r e   K e y - m a n a g e m e n t   S c h e m e   S u i t a b l e   f o r   M o b i l e   S e r v i c e s . t x tIlocblob      F  8??????     E E x p e r t s   s y s t e m s   a p p l i e d   t o   t h e   a n a l y s i s   o f   k e y   m a n a g e m e n t   s c h e m e s . t x tIlocblob      ?  8??????     ? E x p l o i t i n g   X . 5 0 9   C e r t i f i c a t e   a n d   M u l t i - a g e n t   S y s t e m   A r c h i t e c t u r e   f o r   R o l e - B a s e d   A c c e s s   C o n t r o l   a n d   A u t h e n t i c a t i o n   M a n a g e m e n t . t x tIlocblob     R  8??????     @ F o r m a l i z i n g   G D O I   g r o u p   k e y   m a n a g e m e n t   r e q u i r e m e n t s   i n   N P A T R L . t x tIlocblob     ?  8??????      H G K M . t x tIlocblob     ^  8??????     E H i e r a r c h i c a l   k e y   m a n a g e m e n t   s c h e m e   u s i n g   p o l y n o m i a l   i n t e r p o l a t i o n . t x tIlocblob      F  ???????      J E T . t x tIlocblob      ?  ???????     D K e y   M a n a g e m e n t   f o r   E n c r y p t e d   D a t a   S t o r a g e   i n   D i s t r i b u t e d   S y s t e m s . t x tIlocblob     R  ???????     < K e y   m a n a g e m e n t   f o r   r o l e   h i e r a r c h y   i n   d i s t r i b u t e d   s y s t e m s . t x tIlocblob     ?  ???????     / K e y   M a n a g e m e n t   S t a n d a r d s   H i t   t h e   F a s t   T r a c k . t x tIlocblob     ^  ???????      K e y   M a n a g e m e n t . t x tIlocblob      F  ??????     < K e y - M a n a g e m e n t   I n f r a s t r u c t u r e   f o r   P r o t e c t i n g   S t o r e d   D a t a . t x tIlocblob      ?  ??????  Ilocblob     ^  x??????     A A n   E f f i c i e n t   K e y   M a n a g e m e n t   S c h e m e   f o r   S e c u r e   S e n s o r   N e t w o r k s . t x tIlocblob      F  ???????                                                                                                                                                       @   E    \t     0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           DSDB                                 `          ?                        H      P      `          ?                                          @      ?                                          @         i n   d y n a m i c   a n d   h i e r a r c h i c a l   g r o u p s . t x tIlocblob      F  ???????      E D D K . t x tIlocblob      ?  ???????     X E f f e c t   o f   k e y   g e n e r a t o r s   o n   t h e   a u t o m a t i c   s e a r c h   f o r   f l a w s   i n   k e y   m a n a g e m e n t   s c h e m e s . t x tIlocblob     R  ???????     J E f f i c i e n t   p u b l i c   k e y   c e r t i f i c a t e   m a n a g e m e n t   f o r   m o b i l e   a d   h o c   n e t w o r k s . t x tIlocblob     ?  ???????     W E l i m i n a t i n g   c o u n t e r e v i d e n c e   w i t h   a p p l i c a t i o n s   t o   a c c o u n t a b l e   c e r t i f i c a t e   m a n a g e m e n t . t x tIlocblob     ^  ???????     S E v a l u a t i o n   o                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ','Key management'
'in wireless sensor networks, security has to be mainly provided to two types of communications: one-toall and one-to-one. this paper proposes a complete set of low complexity protocols to generate and distribute two types of secret keys. the protocols are designed for a clustered hierarchical network. these protocols include the initial key management, the periodic key management and the key management upon cluster dynamics. all the communications in the protocols are properly authenticated using elliptic curve digital signature scheme. the protocols are implemented for tinyos using nesc, simulated under tossim and are viable for implementation in resource-constrained platforms like mica.a complete set of protocols for distributed key management in clustered wireless sensor networks','Key management'
'key encryption technology is a basic technique for protecting the secrecy of transmitted data among sensor nodes in wireless sensor networks. however, sensor nodes are inherently limited by insufficient hardware resources such as memory capacity and battery lifetime. as a result, few current key management schemes are appropriate for wireless sensor networks. this paper proposes a new key management method that uses dynamic key management schemes for heterogeneous sensor networks. the proposed scheme loads a hash function into the base station, cluster heads, and sensor nodes. the cluster heads and sensor nodes then generate their own keychains to provide forward authentication in case of key changes, security breaches, key changes due to security breaches. the cluster heads and sensor nodes establish pairwise keys to ensure transmission secrecy. the proposed scheme decreases the number of keys required for sensor nodes and cluster heads and is robust to the following attacks: guessing attacks, replay attacks, man-in-the-middle attacks, node capture attacks, and denial-of-service attacks.a forward authentication key management scheme for heterogeneous sensor networks','Key management'
'a mobile ad hoc network does not require fixed infrastructure to construct connections among nodes. due to the particular characteristics of mobile ad hoc networks, most existing secure protocols in wired networks do not meet the security requirements for mobile ad hoc networks. most secure protocols in mobile ad hoc networks, such as secure routing, key agreement and secure group communication protocols, assume that all nodes must have pre-shared a secret, or pre-obtained public-key certificates before joining the network. however, this assumption has a practical weakness for some emergency applications, because some nodes without pre-obtained certificates will be unable to join the network. in this paper, a heterogeneous-network aided public-key management scheme for mobile ad hoc networks is proposed to remedy this weakness. several heterogeneous networks (such as satellite, unmanned aerial vehicle, or cellular networks) provide wider service areas and ubiquitous connectivity. we adopt these wide-covered heterogeneous networks to design a secure certificate distribution scheme that allows a mobile node without a pre-obtained certificate to instantly get a certificate using the communication channel constructed by these wide-covered heterogeneous networks. therefore, this scheme enhances the security infrastructure of public key management for mobile ad hoc networks.a heterogeneous-network aided public-key management scheme for mobile ad hoc networks','Key management'
'a hybrid group key management scheme is proposed in ad hoc networks. the nodes in network are divided into two parties: cell group and control group. a centralized key management scheme is employed in cell group and a decentralized scheme is employed in control group. an implicitly certified public key approach is used, which reduces the certificate overhead.a hybrid group key management scheme for two-layered ad hoc networks','Key management'
'in this paper we propose a minimal distribution of re-key messages strategy called \"hybrid scalable group key management approach (hybrid sgkm) for dynamic multicast networks\". the need for network applications requiring group communications will accelerate in future. the keys possessed should be changed for every user join/leave the group so that a new user joins the group is prevented from accessing past communications and when an existing user leaves the group is prevented from accessing future communications. in our approach we try to generate and distribute keys to the group members during leave or join of members by using key graph based boolean minimization technique in order to improve scalability. experimental results illustrate that hybrid sgkm brings out better performance than other existing key management approaches.a hybrid scalable group key management approach for large dynamic multicast networks','Key management'
'recent development of sensor and computer technology enabled wsns(wireless sensor networks) to be expanded. to conserve power from each sensor, intermediate nodes should aggregate results from individual sensors. however, it can make it that a single compromised sensor can render the network useless, or worse, mislead the operator into trusting a false reading. in this paper, we propose a protocol to give us a key aggregation mechanism that intermediate nodes could aggregate data more safely. the proposed protocol is more helpful at multi-tier network architecture in secure sessions established between sensor nodes and gateways. in simulation result, since the proposed scheme distributed key management role to an internal node to avoid parent node\'s concentrated management, it has 3.5\% lower energy consumption load caused by key management; 0.3\% and 0.6\% more improved key transmission time and process time, respectively, compared with lha-sp.a key management protocol for securing stability of an intermediate node in wireless sensor networks','Key management'
'a key-management scheme based on control vectors','Key management'
'key management is a fundamental security service in distributed sensor networks. in this paper, the focus is to design a lightweight group key management scheme to safeguard the data packet passing on the sensor networks under different types of attacks. we propose an energy efficient level-based hierarchical system and build the secure route from the source node to sink node .we compromise between the energy consumption and the shortest path by utilizing the number of neighbors (nbr) of a sensor when renewing a cluster head or choosing the next hop in the hierarchical clusters. in addition, our protocol contains group communication policies, group membership requirements and an algorithm for generating a distributed group key for secure communication.a lightweight key management protocol for hierarchical sensor networks','Key management'
'in this paper, we present an efficient key management scheme for resource limited sensor networks. motivated by the fact that group key distribution from remote base station is costly in term of communication, we introduce the scheme of locally group key management scheme with revocation and self-healing capability for sensor network. the idea behind the scheme is use secret sharing to distribute group key and manage group member as well as group header.a locally group key management with revocation and self-healing capability for sensor networks','Key management'
'in recent years, p2p networks have been evolving at a rapid pace. authentication of nodes in p2p networks, however, remains a difficult task to conduct efficiently. in this paper, we therefore propose a new authentication method called hash-based distributed authentication method (hdam). hdam realizes a decentralized efficient mutual authentication mechanism for each pair of nodes in the p2p network. it performs a distributed management of public keys by using web of trust and distributed hash table. our proposal significantly reduces both the memory size requirement and the overhead of communication data sent by the nodes.a new authentication method with distributed hash table for p2p network','Key management'
'with the ever-increasing growth in electronic messaging and electronic commerce, the need for an infrastructure to provide confidentiality, security, and confidence for such exchanges to take place is quite evident [2]. here, public keys and certificates are issued to users for authorization purposes. one of the primary concerns in these systems is the handling of certificate revocation prior to the expiration date. in this paper, we propose a new approach for managing certificate revocation. all existing schemes require that the information about revoked certificates be sent only periodically to the directories used for verification. this gives rise to the problem of obsolescence. to overcome this problem, we have introduced a new layer in the traditional architecture. using a preliminary analysis, we show the impact of the new scheme on the up-to-datedness, robustness, load distribution, and response time of the system. similarly, we show the additional costs incurred in terms of communication cost, processing cost, and hardware costs.a novel approach to certificate revocation management','Key management'
'recently, many group communication services have become the focus for future developments in the internet and wireless network applications, such as video-conferencing, collaborative work, networking games or online videos. in particular, these applications require data delivery from one sender to a large number of authorized receivers. therefore, secure multicast communication will become an important networking issue in the future. using a common encryption key only known by authorized members to encrypt transmitted data is a practical approach. but, whenever a group member joins or leaves the group, the common encryption key must be updated to ensure both past and future secrecy. as a result, minimizing key update communication cost and the key storage requirement of a group controller is a critical issue in a scalable and dynamically changing large group. a new key-management scheme is proposed to reduce the key storage requirement of a group controller to a constant size, which is far better than that of the previously proposed schemes, while retaining the same key update communication cost. in addition, the correlation between the key storage requirement of each group member and key update communication cost are also presented.a scalable key-management scheme with minimizing key storage for secure group communications','Key management'
'secure multicast applications require key management that provides access control. in wireless networks, where the error rate is high and the bandwidth is limited, the design of key management schemes should place emphasis on reducing the communication burden associated with key updating. a communication-efficient class of key management schemes is those that employ a tree hierarchy. however, these tree-based key management schemes do not exploit issues related to the delivery of keying information that provide opportunities to further reduce the communication burden of rekeying. in this paper, we propose a method for designing multicast key management trees that match the network topology. the proposed key management scheme localizes the transmission of keying information and significantly reduces the communication burden of rekeying. further, in mobile wireless applications, the issue of user handoff between base stations may cause user relocation on the key management tree. we address the problem of user handoff by proposing an efficient handoff scheme for our topology-matching key management trees. the proposed scheme also addresses the heterogeneity of the network. for multicast applications containing several thousands of users, simulations indicate a 55\%-80\% reduction in the communication cost compared to key trees that are independent of the network topology. analysis and simulations also show that the communication cost of the proposed topology-matching key management tree scales better than topology-independent trees as the size of multicast group grows.a scalable multicast key management scheme for heterogeneous wireless networks','Key management'
'while a lot of important information is being sent and received on the internet, the information could be exposed to many threats, and the more the multicast service is various and generalized, the more the service range is widened. when a new member joins in or leaves from the multicast group, the group key, which the existing member used, should be newly updated. the existing method had a problem that the performance was depreciated by the key exchanging. this paper proposes the effective group management mechanism for a secure transmission of the multicast data on the multicast group.a study on an effective group management scheme for secure multicast in mipv6','Key management'
'pervasive computing environments find their practical manifestations through wireless sensor networks, which sense a relationship amongst themselves and the environment. currently the proposed keying schemes for ensuring security, in wireless sensor networks, may be classified into public and private keying schemes, or their hybrid. however, an investigation in peer work underpins the fact that neither of these works relates the key management schemes with the granularity of key generation, distribution, renewal, and revocation. in this paper, we propose a unified security framework with three key management schemes, sack, sack-p, and sack-h that incorporate symmetric key cryptography, asymmetric key cryptography and the hybrid, respectively. we have evaluated the key management schemes against a broad range of metrics such as energy, resource utilization, scalability and resilience to node compromises. our evaluation comprises both analytical investigation and experimental validation. the results show that though sack-p is heavy on resources, it provides maximal security and offers the best resilience to node compromises. on the contrary, sack is very efficient in terms of storage and communication. our results substantiate a relationship between the level of security and resource utilization and form a design benchmark for security frameworks.a unified security framework with three key management schemes for wireless sensor networks','Key management'
'while scalability, routing and performance are core issues for application-level multicast (alm) protocols, an important but less studied problem is security. in particular, confidentiality (i.e. data secrecy, achieved through data encryption) in alm protocols is needed. key management schemes must be simple, scalable, and must not degrade the performance of the alm protocol. we explore three key management schemes that leverage the underlying overlay to distribute the key(s) and secure alm.we evaluate their impact on three well-known alm protocols: esm, almi and nice. through analysis and simulations, we show that utilizing the alm overlay to distribute key(s) is feasible. for a given alm protocol, choice of the best key management scheme depends on the application needs: minimizing rekeying latency or minimizing data multicasting latency.adding confidentiality to application-level multicast by leveraging the multicast overlay','Key management'
'group key management is one of the basic building blocks in collaborative and group&#45;oriented applications in mobile adhoc networks &#40;manets&#41;. group key establishment involves creating and distributing a common secret for all group members. however, key management for a large and dynamic group is a difficult problem because of scalability and security. modification of membership requires the group key to be refreshed to ensure backward and forward secrecy. in this paper, we propose a simple and efficient group key &#40;segk&#41; management scheme for manets. group members compute the group key in a distributed manner.an efficient group key management scheme for mobile ad hoc networks','Key management'
'many group key management schemes such as those proposed by wallner et al, wong et al are based on a multilevel, logical hierarchy (or tree) of keyencrypting keys. but lkh is based on the hypothesis that the tree is maintained in a balanced manner. this paper proposes a multi-link tree protocol based on lkh. ltp keeps the structure balanced and has a better performance. we analyze multi-ltp and multi- lkh\'s performance in detail. we prove that 3-degree ltp is the best structure among all the ltp structures, which is also better than lkh.an efficient group key management scheme','Key management'
'key management for large dynamic groups is adifficult problem because of scalability and security.skdc method is very weak in scalability. logical keyhierarchy (lkh) algorithm proposed in 1997 has greatlyimproved scalability for key management in largedynamic groups. in 1998 one-way-function tree (oft)method appeared to obtain more efficiency than lkh. butoft method has security breaches in collusion attacks. inthis paper a new method will be presented, which is moreefficient than oft and resistant to collusion attacks.an efficient key management for large dynamic groups','Key management'
'in this paper we propose a variant of rsa public key scheme, called \"hidden exponent rsa\". based on this new scheme, we devised an efficient key distribution/ management scheme for secure communication among devices in the context of pervasive computing, with emphasis on the simplicity and efficiency of the protocol. we show the new scheme is secure under the strong rsa assumption.an efficient key management scheme for pervasive computing','Key management'
'wireless sensor networks(wsn) are randomly deployed in adversary or dangerous environments so that the security of wsn is extremely important. it is necessary to use encryption system to provide security. w propose an efficient key management scheme for wsn. the main purpose of this scheme is to solve secure communications and key management. the proposed scheme allows to evict the compromised node from the network and employs a multi-tier network architecture in which secure sessions are established only between sensor nodes and gateways. the proposed scheme is based on the theory of combinatorial optimization and provides an approach to maintain security while members have changed in groups.an efficient key management scheme for secure sensor networks','Key management'
'the elliptic curve cryptosystem is considered to be the strongest public-key cryptosystem known today and is preferred over the rsa cryptosystem because the key length for secure rsa has increased over recent years, and this has put a heavier processing load on its applications. an efficient key management and derivation scheme based on the elliptic curve cryptosystem is proposed in this paper to solve the hierarchical access control problem. each class in the hierarchy is allowed to select its own secret key. the problem of efficiently adding or deleting classes can be solved without the necessity of regenerating keys for all the users in the hierarchy, as was the case in previous schemes. the scheme is shown much more efficiently and flexibly than the schemes proposed previously.an efficient key-management scheme for hierarchical access control based on elliptic curve cryptosystem','Key management'
'we investigate the group key management problem for broadcasting applications. previous work showed that, in handling key updates, batch rekeying can be more cost effective than individual rekeying. one model for batch rekeying is to assume that every user has probability p of being replaced by a new user during a batch period with the total number of users unchanged. under this model, it was recently shown that an optimal key tree can be constructed in linear time when p is a constant and in o(n^4) time when p->0. in this paper, we investigate more efficient algorithms for the case p->0, i.e., when membership changes are sparse. we design an o(n) heuristic algorithm for the sparse case and show that it produces a nearly 2-approximation to the optimal key tree. simulation results show that its performance is even better in practice. we also design a refined heuristic algorithm and show that it achieves an approximation ratio of 1+@e for any fixed @e>0 and n, as p->0. finally, we give another approximation algorithm for any p@?(0,0.693) which is shown to be quite good by our simulations.approximately optimal trees for group key management with batch updates','Key management'
'centralized directory services and accounts management project','Key management'
'there are numerous applications that require secure group communication. much recent attention has been focused on secure multicasting over the internet. when such systems are required to manage large groups which undergo frequent fluctuations in group membership, the need for efficient encryption key management becomes critical. this paper presents a combinatorial formulation of the multicast key management problem that is applicable not only to the specific problem of multicast key management, but also to the general problem of managing keys for any type of trusted group communication, regardless of the underlyingtransmission method between group participants. specifically, we describe exclusion basis systems, show exactly when they exist, and demonstrate that such systems represent improvements over the current binary tree-based key management systems and other relatedsystems.combinatorial optimization of multicast key management','Key management'
'crypto key management','Key management'
'this article focuses on cryptographic key management systems (kms) for scada systems environments. it first gives a generic view on the constraints, requirements and desired technical properties in scada contexts. then, the most widespread solutions are presented, before discussing how they meet such conditions. the work done by different initiatives on this issue is also introduced. finally, perspectives and research directions are proposed in consequence. the article aims at presenting open issues on the area, to foster discussion and research, according to the authors&#8217; view.cryptographic key management for scada systems-issues and perspectives','Key management'
'there are two main issues concerning data security on networks; controlling access and the vulnerability of data communication links. a brief introduction to the various techniques which may be applied to these concerns are given in this paper.cryptographic key management','Key management'
'most existing solutions to group security in mobile ad hoc networks (manets) rely on a multicast core based tree (cbt) for key distribution. such solutions, although suitable for systems with low mobility and static characteristics, are unsuitable for dynamic and sparse groups with changing neighborhoods. in this paper, we propose an entirely decentralized key generation mechanism, employing a central trusted entity only during initialization. using our approach, keys can be established between group members with absolutely no prior communication. the solution relies on threshold cryptography and introduces a novel concept of node-group-key (ngk) mapping. we have provided an extensive analytical model for the computations involved and communication costs and have also provided a lie detection mechanism. simulation results show appreciable performance improvement and enhanced robustness.distributed key management for dynamic groups in manets','Key management'
'security of mobile ad hoc networks is built upon a reliable key management system to generate and distribute symmetric encryption/decryption keys for communicating parties. while central servers generate and distribute the keys in traditional wired networks, distributive key management systems are used in mobile ad hoc networks where central approaches will fail in such dynamic, high mobility networks. threshold cryptography1 has been proposed to provide a reliable, distributive key management for networks. in an (n, t+1) threshold system, there are n servers to provide key generation and distribution when needed for the whole network. from these n servers, any x servers (tdistributive key management for mobile ad hoc networks','Key management'
'grid computing is a newly developed technology for complex systems with large-scale resource sharing and multi-institutional collaboration. the prominent feature of grid computing is the collaboration of multiple entities to perform collaborative tasks that rely on two fundamental functions: communication and resource sharing. since the internet is not security-oriented by design, there exist various attacks, in particular malicious internal and external users. securing grid communication and controlling access to shared resources in a fine-tuned manner are important issues for grid services. this paper proposes an elegant dual-level key management (dlkm) mechanism using an innovative concept/construction of access control polynomial (acp) and one-way functions. the first level provides a flexible and secure group communication technology while the second level offers hierarchical access control. complexity analysis and simulation demonstrate the efficiency and effectiveness of the proposed dlkm in both computational grid and data grid. an example is illustrated.dual-level key management for secure grid communication in dynamic and hierarchical groups','Key management'
'energy efficiency is an essential requirement for wireless sensor networks while security must also be ensured for mission-critical applications. in this paper, we present an energy-efficient distributed deterministic key management scheme (eddk) for resource-constrained wireless sensor networks. eddk mainly focuses on the establishment and maintenance of the pairwise keys as well as the local cluster keys and can fix some flaws in some existing key management schemes. not only can the neighbor table constructed during key establishment provide the security for key maintenance and data transfer, but it can also be used to effectively manage the storage and update of the keys. by using the elliptic curve digital signature algorithm in eddk, both new and mobile sensor nodes can join or rejoin a sensor network securely. unlike some centralized and location-based key management schemes, eddk does not depend on such infrastructure as base stations and robots and thus has a high level of flexibility. experiments and analyses show that eddk has a very low overhead in terms of computation, communication, and storage.eddk','Key management'
'effect of key generators on the automatic search for flaws in key management schemes','Key management'
'mobile ad hoc networks involve communications over a shared wireless channel without any centralized infrastructure. consequently, in an optimal solution, management and security services depend exclusively on network members. the main contribution of this paper is an efficient public key management scheme that is suitable for fully self-organized mobile ad hoc networks where all nodes play identical roles. our approach implies that the operations of creating, storing, distributing, and revoking nodes\' public keys are carried out locally by the nodes themselves. the goal of the presented methods is the improvement in the process of building local certificate repositories of nodes. in order to do it, an authentication solution based on the web of trust concept is combined with an element of routing based on the multipoint relay concept introduced in the optimized link state routing protocol. our proposal leads to a good tradeoff among security, overhead, and flexibility. experimental results show a considerable decrease in resource consumption while carrying out the certificate verification process.efficient public key certificate management for mobile ad hoc networks','Key management'
'this paper presents a method to increase the accountability of certificate management by making it intractable for the certification authority (ca) to create contradictory statements about the validity of a certificate. the core of the method is a new primitive, undeniable attester, that allows someone to commit to some set s of bitstrings by publishing a short digest of s and to give attestations for any x that it is or is not a member of s. such an attestation can be verified by obtaining in authenticated way the published digest and applying a verification algorithm to the triple of the bitstring, the attestation and the digest. the most important feature of this primitive is intractability of creating two contradictory proofs for the same candidate element x and digest. we give an efficient construction for undeniable attesters based on authenticated search trees. we show that the construction also applies to sets of more structured elements. we also show that undeniable attesters exist iff collision-resistant hash functions exist.eliminating counterevidence with applications to accountable certificate management','Key management'
'copyright protection is a major issue in online content distribution services and many key-management schemes have been proposed for protecting content. tree-based schemes aim at reducing the load on the server, and do not give consideration to that on clients. this scheme is not fully suitable for devices with low computational capacity. on the other hand, the load on clients is low in a star-based scheme. however, the load on the server becomes large in proportion to the number of clients. this structure is far from scalable. we propose a key-management scheme that is the intermediate scheme of a star-based scheme and a tree-based scheme. then, we evaluate both the load on the server and clients in key-management scheme using the three structures. we find that the load on server and that on clients in our scheme have a relation of trade-off. we can construct optimal key-management structure satisfying system requirements using our scheme. furthermore, the loads on both the server and clients in our scheme are lower than tree-based structure scheme under a certain condition.evaluation of dual-structure key-management scheme suitable for mobile services','Key management'
'experts systems applied to the analysis of key management schemes','Key management'
'this paper proposes the design of multi-user authentication in the multi-application based environment and role-based access control by using pki authentication and x.509 privilege management infrastructure (pmi). a binding model of rbac authorization based on attribute certificate (ac) and public key certificate (pkc) is presented. especially, the way of attribute mapping between pkc, bridge ac, and role ac is illustrated. in addition, the activity-based policy enforcement is introduced to make the system respond to malicious activities more appropriately. at a core, the multi agent system approach is applied to automate the flexible and effective management of user authentication, role delegation as well as system accountability. finally, we reported our ongoing implementation status and demonstrated that our proposed model is a potential solution to support strong authentication and dynamic authorization in the multi-user and multi-application environment.exploiting x.509 certificate and multi-agent system architecture for role-based access control and authentication management','Key management'
'although there is a substantial amount of work on formal requirements for two and three-party key distribution protocols, very little has been done on requirements for group protocols. however, since the latter have security requirements that can differ in important but subtle ways, we believe that a rigorous expression of these requirements can be useful in determining whether a given protocol can satisfy an application\'s needs. in this paper we make a first step in providing a formal understanding of security requirements for group key distribution by using the npatrl language, a temporal requirement specification language for use with the nrl protocol analyzer. we specify the requirements for gdoi, a protocol being proposed as an ietf standard, which we are formally specifying and verifying in cooperation with the msec working group.formalizing gdoi group key management requirements in npatrl','Key management'
'key establishment plays a central role in authentication and encryption in wireless sensor networks, especially when they are mainly deployed in hostile environments. because of the strict constraints in power, processing and storage, designing an efficient key establishment protocol is not a trivial task. compare with traditional public key cryptography, symmetric key cryptographic with key predistribution mechanism is more suitable for large-scale wireless sensor networks. most of previous solutions have some issues on performance and security capabilities. in this paper, we propose a novel key predistribution model using pre-deployment knowledge to take advantage in terms of network connectivity, resilience against node compromised, memory requirement and energy for transmission.hgkm','Key management'
'we present a hierarchical key management scheme using cryptographic hash function and newton\'s polynomial interpolation for users key and system resources management. a similar technique has been proposed in 2002 by shen and chen, but their scheme suffers large computational overhead and security weakness. we show that our scheme is secure and efficient in comparisons to the shen and chen\'s scheme.hierarchical key management scheme using polynomial interpolation','Key management'
'in secure group communications, the time cost associated with key updates in the events of member join and departure is an important aspect of quality of service, especially in large groups with highly dynamic membership. to achieve better time efficiency, we propose a join-exit-tree (jet) key management framework. first, a special key tree topology with join and exit subtrees is introduced to handle key updates for dynamic membership. then, optimization techniques are employed to determine the capacities of join and exit subtrees for achieving the best time efficiency, and algorithms are designed to dynamically update the join and exit trees. we show that, on average, the asymptotic time cost for each member join/departure event is reduced to o(log (log n)) from the previous cost of o(log n), where n is the group size. our experimental results based on simulated user activities as well as the real mbone data demonstrate that the proposed jet scheme can significantly improve the time efficiency, while maintaining low communication and computation cost, of tree-based contributory key management.jet','Key management'
'confidential data stored on mass storage devices is atrisk to be disclosed to persons getting physical or administratoraccess to the device. encrypting the data reducesthis risk, at the cost of more cumbersome administration.in this publication, we examine the problem of encrypteddata storage in a grid computing environment,where storage capacity and data is shared across organizationalboundaries. we propose an architecture thatallows users to store and share encrypted data in this environment.access to decryption keys is granted based onthe grids data access permissions. the system is thereforeusable as an additional security feature togetherwith a classical access control mechanism. data ownerscan choose different tradeoffs of security versusefficiency. storage servers need not to be trusted and commonaccess control models are supported.key management for encrypted data storage in distributed systems','Key management'
'as distributed computing systems grow in size, complexity and variety of applications, the protection of sensitive data against unauthorized disclosure and tampering becomes increasingly important. in this paper, a cryptographic role-based key management (rbem) is developed for the access control in distributed systems. this paper presents features of the rbem that includes simple rules for key generation, key managements for dynamic hierarchy, algorithms for key generation/modification, and procedures of object assignments. the rbem is extended from its prime design for one local domain to the design for multiple local domains. the rbem is decentralized such that each local domain is managed by its local domain security administrator and modifications in any local domain do not affect the keys of roles or objects in other local domains. this paper presents a platform for the comprehensive assessment of the rbem for the role-based access control. compared with typical key-management methods, the rbem updates much less number of keys for the roles and objects when new roles are added to the role hierarchy. this paper presents three typical case studies for illustrating the efficiency of the rbem.key management for role hierarchy in distributed systems','Key management'
'as enterprises increasingly turn to encryption to meet data-security demands, they need open standards for encryption key management. three standards bodies--the ieee, the internet engineering task force (ietf), and the organization for the advancement of structured information standards (oasis)--have recently chartered working groups on key management. a high degree of consensus within and among the standards bodies might lead to the pre-standard release of products that will be viable for the long term by the end of this year.key management standards hit the fast track','Key management'
'key management','Key management'
'the p1619.3 standard seeks to make interoperable key management possible by abstracting the infrastructure of a cryptographic system into three core components.key-management infrastructure for protecting stored data','Key management'
'secure dynamic conferencing (sdc) is a scenario where given a group of participants, any subset of participants can form a privileged subgroup, called a conference, and communicate securely among themselves. the existing sdc schemes belong to two classes: centralized and distributed. the former incurs the single-point of failure, the central point of attack and performance bottleneck. the two existing distributed dynamic conferencing schemes, which are based on public key cryptosystems, are inefficient and imply that anyone, as long as having a pair of public and private keys, can be in the group, thus, lacking the concept of group and the group membership management. in this paper, we first introduce two new concepts based on the well-known key tree scheme: sponsors and co-distributors and then, propose a new distributed dynamic conferencing scheme by designing an efficient algorithm for finding a sponsor or co-distributors. the new scheme enforces group/conference membership management and surpasses all the existing sdc schemes in terms of simplicity,efficiency and scalability.ktdckm-sdc','Key management'
'management of a public key certification infrastructure&#8212;experiences from the deteberkom project bmsec','Key management'
'we study the problem of designing a storage efficient secure multicast key management scheme based on one-way function trees (oft) for a prespecified key update communication overhead. canetti, malkin and nissim presented a hybrid model that divides a group of n members into clusters of m members and assigns each cluster to one leaf node of a key, tree. using the model, we formulate a constrained optimization problem to minimize the center storage in terms of the cluster size m. due to the monotonicity of the center storage with respect to m, we convert the constrained optimization into a fixed point equation and derive the optimal m* explicitly. we show that the asymptotic value of the optimal m*, given as &#181; + a-1/logea loge &#181; with &#181; = o(log n) and a being the degree of a key tree, leads to the mini real storage as o (n/logn), when the update communication constraint is given as o(log n). we present an explicit design algorithm that achieves minimal center storage for a given update communication constraint.minimizing center key storage in hybrid one-way function based group key management with communication constraints','Key management'
'many security protocols were designed to run over inherently insecure channels such as ad hoc radio or the packet-switched internet. however, they can be strengthened with additional transmissions over other lower-capacity channels, commonly found in ubicomp environments, that offer a different mix of security properties. a single protocol might exploit different channels for different messages in its trace. the authors demonstrate this point by presenting and discussing several security protocols for ubicomp pairing and group key agreement. this article is part of a special issue on security and privacy.multichannel security protocols','Key management'
'a mobile ad hoc network (manet) is the cooperative engagement of a collection of wireless mobile nodes without the aid of any established infrastructure or centralized administration. the conventional security solutions to provide key management through accessing trusted authorities or centralized servers are infeasible for this new environment since mobile ad hoc networks are characterized by the absence of any infrastructure, frequent mobility, and wireless links. in this paper, we propose an on-demand, fully localized, and hop-by-hop public key management scheme for manets. it can be performed by generating public&#x002f;private key pairs by nodes themselves, issuing certificates to neighboring nodes, holding these certificates in their certificate repositories, and providing authentication service adaptive quickly to the dynamic topology of the network without relying on any servers. also, our scheme can be performed successfully as long as there is a physical communication line between two nodes, and it is accustomed well to the on-demand routing for manets. copyright &#169; 2006 john wiley & sons, ltd.on-demand public-key management for mobile ad hoc networks','Key management'
'we consider a network where users can issue certificates that identify the public keys of other users in the network. the issued certificates in a network constitute a set of certificate chains between users. a user u can obtain the public key of another user v from a certificate chain from u to v in the network. for the certificate chain from u to v, u is called the source of the chain and v is called the destination of the chain. certificates in each chain are dispersed between the source and destination of the chain such that the following condition holds. if any user u needs to securely send messages to any other user v in the network, then u can use the certificates stored in u and v to obtain the public key of v (then u can use the public key of v to set up a shared key with v to securely send messages to v). the cost of dispersing certificates in a set of chains among the source and destination users in a network is measured by the total number of certificates that need to be stored in all users. a dispersal of a set of certificate chains in a network is optimal if no other dispersal of the same chain set has a strictly lower cost. in this paper, we show that the problem of computing optimal dispersal of a given chain set is np-complete. thus, minimizing the total number of certificates stored in all users is np--complete. we identify three special classes of chain sets that are of practical interests and devise three polynomial-time algorithms that compute optimal dispersals for each class. we also present two polynomial-time extensions of these algorithms for more general classes of chain sets.optimal dispersal of certificate chains','Key management'
'multicast and broadcast are efficient ways to deliver messages to a group of recipients in a network. due to the growing security concerns in various applications, messages are often encrypted with a secret group key. the key tree model which has been widely adopted maintains a set of keys in a tree structure so that in case of group member change, the group key can be updated in a secure and efficient way. in this paper, we focus on the updating cost incurred by member deletions. to implement a sequence of member deletions in any key tree, a certain number of encrypted messages need to be broadcast to accomplish the updates. our goal is to identify the best key tree which can minimize the worst-case deletion cost (i.e., the amortized cost over n member deletions). we prove that there is an optimal tree in which each internal node has at most five children and each internal node with at least one non-leaf child has exactly three children. based on these characterizations, we present a dynamic programming algorithm that computes an optimal key tree in o(n^2) time.optimizing deletion cost for secure multicast key management','Key management'
'ad-hoc sensor networks (asns) are ad-hoc mobile networks that consist of sensor nodes with limited computation and communication capabilities. in order to protect communication security among sensor nodes, a cryptographic method is needed by ad-hoc sensor networks. most ad-hoc sensor networks are deployed in groups to perform tasks. a group-based key management (gbkm) scheme has been proposed to extend random key pre-distribution schemes (rps). in this paper, we compare the performance of the gbkm and rps by further simulations and mathematic analysis [further  this doesn\'t quite fit here. keywords: ad-hoc sensor networks, key management, group-based.performance analysis group-based key management','Key management'
'recently, many group key management approachesbased on the use of logical key trees have been proposed toaddress the issue of scalable group rekeying that is neededto support secure communications for large and dynamicgroups. in this paper, we present two optimizations for logicalkey tree organizations that utilize information aboutthe characteristics of group members to further reduce theoverhead of group rekeying. first, we propose a partitionedkey tree organization that exploits the temporal patterns ofgroup member joins and departures to reduce the overheadof rekeying. using an analytic model, we show that our optimizationcan achieve up to 31.4\% reduction in key serverbandwidth overhead over the unoptimized scheme. second,we propose an approach under which the key tree is organizedbased on the loss probabilities of group members.our analysis shows this optimization can reduce the rekeyingoverhead by up to 12.1\%.performance optimizations for group key management schemes','Key management'
'as evidenced by the proliferation of phishing attacks and keystroke loggers, we know that human beings are not wellequipped to make trust decisions about when to use their passwords or other personal credentials. public key cryptography can reduce this risk of attack, because authentication using pki is designed to not give away sensitive data. however, using private keys on standard platforms exposes the user to \"keyjacking\"; mobile users wishing to use keypairs on an unfamiliar and potentially untrusted workstation face even more obstacles. in this paper we present the design and prototype of porki, a software application for mobile devices that offers an alternative solution to the portable key problem. through the use of temporary keypairs, proxy certificates, and wireless protocols, porki enables a user to employ her pki credentials on any bluetoothenabled workstation, including those not part of her organization\'s network, and even those that might be malicious. moreover, by crafting xacml policy statements that limit the key usage to the workstation\'s trustworthiness level, and inserting these statements into extensions of the proxy certificates, porki provides the user or the relying party with the ability to limit the amount of trust that can be put in the temporary keypair used on that workstation, and thus the scope of a potential compromise.porki','Key management'
'as group-oriented services become the focal point of ad hoc network applications, securing the group communications becomes a default requirement. in this paper, we address the problem of group access in secure multicast communications for wireless ad hoc networks. we argue that energy expenditure is a scarce resource for the energy-limited ad hoc network devices and introduce a cross-layer approach for designing energy-efficient, balanced key distribution trees to perform key management. to conserve energy, we incorporate the network topology (node location), the \"power proximity\" between network nodes and the path loss characteristics of the medium in the key distribution tree design. we develop new algorithms for homogeneous as well as heterogeneous environments and derive their computational complexity. we present simulation studies showing the improvements achieved for three different but common environments of interest, thus illustrating the need for cross-layer design approaches for security in wireless networks.power proximity based key management for secure multicast in ad hoc networks','Key management'
'with the advent of digital technologies and wideninginternet bandwidth in recent years there has been amarked rise in new multimedia services.these servicesinclude teleconferencing, pay-per-view, interactivesimulation, software updates and real-time delivery ofstock market information.security in groupcommunications is an important requirement when thedelivery includes confidential or copyrighted data.proposals for multicast security solutions so far arecomplex and often require trust in intermediatecomponents or are inefficient.in this paper we present aframework using proxy key encryptions for scalablemulticast key management.the solution guaranteesperfect forward and backward secrecy.it has very lowcomplexity and overhead.we propose a simple keymanagement solution and discuss its properties.proxy encryptions for secure multicast key management','Key management'
'mobile location-based services (lbs) have raised privacy concerns amongst mobile phone users who may need to supply their identity and location information to untrustworthy third parties in order to access these applications. widespread acceptance of such services may therefore depend on how privacy sensitive information will be handled in order to restore users\' confidence in what could become the \"killer app\" of 3g networks. in this paper, we present a proxy-based public key infrastructure that allows lbs mobile users to shield their identities using pseudonyms and protect their communications using mediated identity-based encryption. in particular, we propose the first identity-based signature scheme applicable in a mediated environment. furthermore, we introduce a process whereby a mobile user is only required to own one private key in order to protect her communications when accessing lbs under different pseudonyms. the identity-based character of the pki simplifies significantly key management by removing the need for digital certificates. its mediated architecture makes key revocation easier and more efficient as it allows for instant revocation of security capabilities.pseudonym management using mediated identity-based cryptography','Key management'
'recently, many protocols in the internet are proposing the use of public key cryptography in support of integrity and authentication security services. however, each of these protocols lacks a globally available public key distribution and management system. a secure version of the domain name system (dns) is being developed which, conveniently, provides an infrastructure ideally suited for the distribution and management of public keys. we propose how this infrastructure of the secure dns could be exploited by today\'s users of the internet to distribute and manage their personal public keys.public key distribution with secure dns','Key management'
'public key management for x.25 network security','Key management'
'the internet of things (iot) is envisioned as a natural evolution of the internet, promising to enable ubiquitous connections for pervasive objects. the evolutionary merging of heterogeneous wireless networks is inevitable for smooth migration to iot; for example, in a typical application of iot--smart homes--there exist sensor-radio frequency identification (rfid) hybrid networks. the communication security between sensor (or rfid tags) and home control center is critical, whereas an appropriate key management scheme is a prerequisite for communication security. in this paper, we propose a compromise resilient key management scheme including key agreement schemes and key evolution policies to tackle existing remarkable asymmetry with respect to computation resources of hybrid networks. in particular, a forward and backward secure key evolution policy with formal proof is proposed. we also propose a quality of service (qos)-aware enhancement method by measuring several metrics such as data assurance priority, attacking risk, and remaining power percentage. security parameter negotiation and a tuning method are proposed, based on reactive measurement in real time. our proposed scheme is built on abstract cryptographic primitives such as trapdoor permutation, pseudorandom function, pseudorandom number generator, one-way function and hash function so as to retain flexibility for concrete options. security and performance for proposed key agreement schemes and key evolution policies are compared in detail. three qos-aware security strategies are proposed: performance first, security second (pfss); security first, performance second (sfps); and performance security made balanced (psmb). measurement-based negotiation of security parameters is also proposed in terms of algorithm prototype.qos-aware and compromise-resilient key management scheme for heterogeneous wireless internet of things','Key management'
'as typical wireless sensor networks (wsns) have resource limitations, predistribution of secret keys is possibly the most practical approach for secure network communications. in this paper, we propose a key management scheme based on random key predistribution for heterogeneous wireless sensor networks (hsns). as large-scale homogeneous networks suffer from high costs of communication, computation, and storage requirements, the hsns are preferred because they provide better performance and security solutions for scalable applications in dynamic environments. we consider hierarchical hsn consisting of a small number high-end sensors and a large number of low-end sensors. to address storage overhead problem in the constraint sensor nodes, we incorporate a key generation process, where instead of generating a large pool of random keys, a key pool is represented by a small number of generation keys. for a given generation key and a publicly known seed value, a keyed-hash function generates a key chain; these key chains collectively make a key pool. as dynamic network topology is native to wsns, the proposed scheme allows dynamic addition and removal of nodes. this paper also reports the implementation and the performance of the proposed scheme on crossbow\'s micaz motes running tinyos. the results indicate that the proposed scheme can be applied efficiently in resource-constrained sensor networks. we evaluate the computation and storage costs of two keyed-hash algorithms for key chain generation, hmac-sha1 and hmac-md5.scalable and efficient key management for heterogeneous sensor networks','Key management'
'we propose a scalable key management scheme forsensor networks consisting of a large-scale randomdeployment of commodity sensor nodes that areanonymous. the proposed scheme relies on a location-basedvirtual network infrastructure and is built upona combinatorial formulation of the group keymanagement problem. primary features of our schemeinclude autonomously computing group keys, anddynamically computing, using a simple hash function,the mapping of nodes to group keys. the scheme scaleswell in the size of the network and supports dynamicsetup and management of arbitrary structures ofsecure group communications in large-scale wirelesssensor network.scalable cryptographic key management in wireless sensor networks','Key management'
'resource-constraint nature of wireless sensor network (wsn) turns the security issue certainly into a big challenge. an efficient key management scheme is the pre-requisite to ensure security in wsn. in this paper we present two versions of a secured key management protocol (seccosen) adopted on cosen, a chain oriented sensor network, which is used for periodic data collection. seccosen uses partial key pre-distribution and symmetric crypto-graphy techniques. where as one version of seccosen protocol uses shared partial keys in a sensor chain the other version uses private partial keys. both versions of seccosen show high resilience to different security attacks. the protocol outperforms other random key pre-distribution protocols in the sense that it requires lower space, lower communication overhead and offers very high session key candidates. a comparative discussion between the two versions is also provided.seccosen - a key management scheme for securing chain oriented sensor networks','Key management'
'in mobile ad hoc networks, due to unreliable wireless media, host mobility and lack of infrastructure, providing secure communications is a big challenge in this unique network environment. usually cryptography techniques are used for secure communications in wired and wireless networks. the asymmetric cryptography is widely used because of its versatileness (authentication, integrity, and confidentiality) and simplicity for key distribution. however, this approach relies on a centralized framework of public key infrastructure (pki). the symmetric approach has computation efficiency, yet it suffers from potential attacks on key agreement or key distribution. in fact, any cryptographic means is ineffective if the key management is weak. key management is a central aspect for security in mobile ad hoc networks. in mobile ad hoc networks, the computational load and complexity for key management is strongly subject to restriction of the node\'s available resources and the dynamic nature of network topology. in this paper, we propose a secure and efficient key management framework (sekm) for mobile ad hoc networks. sekm builds pki by applying a secret sharing scheme and an underlying multicast server group. in sekm, the server group creates a view of the certification authority (ca) and provides certificate update service for all nodes, including the servers themselves. a ticket scheme is introduced for efficient certificate service. in addition, an efficient server group updating scheme is proposed.secure and efficient key management in mobile ad hoc networks','Key management'
'employing public key-based security architecture is inevitable for the advanced security applications in the mobile networks. however, key storage management problems have arisen, because the public key computation is still the large overhead to usim, and the mobile equipment has potential threats of the key leakage or loss. in order to solve such shortcomings, we improve the key-insulated models and propose \"trust delegation\" model that the overall security computations are operated in me, while the initial private key still remains in the secure storage in usim. our model is resilient against not only key exposure but also key loss. finally, we show that the overall transactions can be reduced to one-third than current 3gpp generic authentication architecture.secure and efficient public key management in next generation mobile networks','Key management'
'in contrast with conventional networks, mobile ad hoc networks usually do not provide online access to trusted authorities or to centralized servers, and they exhibit frequent partitioning due to link and node failures and to node mobility. for these reasons, traditional security solutions that require online trusted authorities or certificate repositories are not well-suited for securing ad hoc networks. in this paper, we propose a fully self-organized public-key management system that allows users to generate their public-private key pairs, to issue certificates, and to perform authentication regardless of the network partitions and without any centralized services. furthermore, our approach does not require any trusted authority, not even in the system initialization phase.self-organized public-key management for mobile ad hoc networks','Key management'
'the &#937; key management service','Key management'
'the efficiency of group key management is a key issue in secure group communications, since the group key must be renewed each time the group membership changes. this paper examines the performance of a periodic rekeying technique for the key management of groups that exhibit bursty behaviour. experimental results demonstrate that this technique costs far fewer key generations than existing approaches, and is therefore more efficient in terms of computation. when the pre-processing of joining members can be performed during the rekeying period, this technique reduces the rekeying time at the end of the rekeying period to the lower bound.the efficiency of periodic rekeying in dynamic group key management','Key management'
'an abstract business process contains a description the protocol that a business process engages in without revealing the internal computation of the process. this description provides the information necessary to compose the process with other web services. bpel supports this by providing distinct dialects for specifying abstract and executable processes. unfortunately, bpel does not prevent complex computations from being included in an abstract process. this complicates the protocol description, unnecessarily reveals implementation details, and makes it difficult to analyze correctness. we propose some restrictions on the data manipulation constructs that can be used in an abstract bpel process. the restrictions permit a full description of a protocol while hiding computation. a restricted abstract process can easily be converted into an abstract bpel process or expanded into an executable bpel process. based on these restrictions we propose a formal model for a business process and use it as the basis of an algorithm for demonstrating the correctness of a protocol described by a restricted abstract process. we then sketch an algorithm for synthesizing a protocol based on a formal specification of its outcome and the tasks available for its construction.a model for abstract process specification, verification and composition','Logic and verification'
'part is a system for pla testing and verification, intended to be properly interfaced with other existing tools to generate a comprehensive design environment. to this purpose, it provides several facilities, among which the capability of generating fault population on the basis of layout information. part aims at producing a very compact test set for all detectable crosspoint defects, using limited amounts of run time and storage. this is achieved by means of an efficient partitioning algorithm together with powerful heuristics. test minimality is ensured by a simple procedure. in the present paper these are discussed, experimental results are given and a comparison with competing strategies is made.a new integrated system for pla testing and verification','Logic and verification'
'a verification technique for gated clock','Logic and verification'
'in software product line (spl) engineering, software products are build in families rather than individually. many critical software are nowadays build as spls and most of them obey hard real-time requirements. formal methods for verifying spls are thus crucial and actively studied. the verification problem for spl is, however, more complicated than for individual systems; the large number of different software products multiplies the complexity of spl model-checking. recently, promising model-checking approaches have been developed specifically for spls. they leverage the commonality between the products to reduce the verification effort. however, none of them considers real time. in this paper, we combine existing spl verification methods with established model-checking procedures for real-time systems. we introduce featured timed automata (fta), a formalism that extends the classical timed automata with constructs for modelling variability. we show that fta model-checking can be achieved through a smart combination of real-time and spl model checking.behavioural modelling and verification of real-time software product lines','Logic and verification'
'post-silicon validation is a necessary step in a design\'s verification process. pre-silicon techniques such as simulation and emulation are limited in scope and volume as compared to what can be achieved on the silicon itself. some parts of the verification, such as full-system functional verification, cannot be practically covered with current pre-silicon technologies. this panel brings together experts from industry, academia, and eda to review the differences and similarities between pre- and post-silicon, discuss how the fundamental aspects of verification are affected by these differences, and explore how the gaps between the two worlds can be bridged.bridging pre-silicon verification and post-silicon validation','Logic and verification'
'once a design is both retimed and sequentially optimized, sequential equivalence verification becomes very hard since retiming breaks the equivalence of the retimed sub-blocks although the design equivalence is preserved. this paper presents a novel compositional algorithm to verify sequential equivalence of large designs that are not only retimed but also optimized sequentially and combinationally. with a new notion of conditional equivalence in the presence of retiming, the proposed compositional algorithm performs hierarchical verification by checking whether each sub-block is conditionally equivalent, then checking whether the conditions are justified on their parent block by temporal equivalence. this is the first compositional algorithm handling both retiming and sequential optimizations hierarchically. the proposed approach is completely automatic and orthogonal to any existing sequential equivalence checker. the experimental results show that the proposed algorithm can handle large industrial designs that cannot be verified by the existing methods on sequential equivalence checking.compositional verification of retiming and sequential optimizations','Logic and verification'
'over the last few years we have been working on bringing simple and pragmatic program specification and verification to programming languages targeting the microsoft .net platform. in this talk i will discuss the motivation and trade-offs influencing our design. the specifications and static verification are targeted at the general developer, not the verification enthusiast. it is thus important to us to use a single form of specifications that meets three simultaneous goals: 1. specifications serve as documentation. they must be as readable as possible. 2. specifications should be executable. this motivates writing specifications for testing and immediate perceived benefit, without consideration of static verification. 3. finally, specifications should be usable in static verification. our specification approach is language-agnostic in that we use idiomatic code written in the developer\'s source language to express pre-conditions and post-conditions. preconditions and postconditions are expressed as calls to the static methods. special dummy methods are used to refer to the method result value as well as referring to the old value of an expression, meaning the value of the expression on method entry.language-agnostic specification and verification','Logic and verification'
'to help a user specify and verify quantified queries --- a class of database queries known to be very challenging for all but the most expert users --- one can question the user on whether certain data objects are answers or non-answers to her intended query. in this paper, we analyze the number of questions needed to learn or verify qhorn queries, a special class of boolean quantified queries whose underlying form is conjunctions of quantified horn expressions. we provide optimal polynomial-question and polynomial-time learning and verification algorithms for two subclasses of the class qhorn with upper constant limits on a query\'s causal density.learning and verifying quantified boolean queries by example','Logic and verification'
'novel verification framework combining structural and obdd methods in a synthesis environment','Logic and verification'
'social commitments have been widely studied to represent business contracts among agents with different competing objectives in communicating multi-agent systems. however, their formal verification is still an open issue. this paper proposes a novel model-checking algorithm to address this problem. we define a new temporal logic, ctlc, which extends ctl with modalities for social commitments and their fulfillment and violation. the verification technique is based on symbolic model checking that uses ordered binary decision diagrams to give a compact representation of the system. we also prove that the problem of model checking ctlc is polynomial-time reducible to the problem of model checking ctlk, the combination of ctl with modalities for knowledge. we finally present the full implementation of the proposed algorithm by extending the mcmas symbolic model checker and report on the experimental results obtained when verifying the netbill protocol.on the verification of social commitments and time','Logic and verification'
'program specification and verification in vdm','Logic and verification'
'program verification','Logic and verification'
'prototype of a verification tool','Logic and verification'
'residue bdd and its application to the verification of arithmetic circuits','Logic and verification'
'concurrent separation logic provides a way of reasoning about the usage of resources in concurrent programs. proofs in the logic all track the transfer of ownership of portions of memory between concurrent processes, mirroring design principles for concurrent systems programs. this allows the safe treatment of \"daring\" concurrent programs, that access shared memory without explicit protection, outside of critical sections; canonical examples of such daring concurrency are resource managers of various kinds. in this talk i will describe the underpinnings of the concurrent separation logic, andallillustrate it with experimental tools -- smallfoot and space invader -- that are being developed to do automatic proofs with the logic.separation logic and concurrent resource management','Logic and verification'
'having a solid verification program that ensures a &#8220;system is built right&#8221; plays a crucial role in both government and commercial space programs, since even the smallest error in a requirement, design-analysis, test, or inspection at every level of a system development could cause a major system malfunction and would be almost impossible to troubleshoot and repair once in space. according to the space-launch vehicles (sv-lvs) failure related data that have been collected by aerospace corporation since the 1960s, almost all of the post-launch failures could have been prevented if their problems had been discovered prior to the shipment of these vehicles to their launch sites. in particular, it was found that the majority of these vehicles could have been successful if deficiencies were corrected during early phase (requirement and design phases) or lower system level of their systems development. in general, any latent problems discovered later on in system development would significantly impact cost and schedule of space programs, even if they were successfully corrected prior to launch. this paper presents a systematic approach to planning and executing verification of space systems based on a distributed-space system verification program. it enforces a standardized set of modular-management processes at every level and phase of space system development activities. this distributed verification program approach tries to enforce a standard set of management processes in order to prevent total system program responsibility (tspr) philosophy, a &#8220;faster, better, cheaper&#8221; approach, from being unintentionally adopted at any level of space system development due to schedule and cost pressures. this space systems verification approach is now explained in an u.s. dod best practice document and also included in the aerospace corporation\'s specification and standard list. it has also been adopted as a compliance document in some major u.s. space programs, such as gps block iii, and national security space programs. &#169; 2007 wiley periodicals, inc. syst eng this article is a u.s. government work and, as such, is in the public domain in the united states of america.space systems verification program and management process','Logic and verification'
'this talk gives an overview of meta-programming, with an emphasis on recent developments in extending existing specification and verification technology to meta-programs.specification and verification of meta-programs','Logic and verification'
'specification and verification','Logic and verification'
'structure sharing for quantified terms: fundamentals','Logic and verification'
'substructural verification and computational feasibility','Logic and verification'
'system verification','Logic and verification'
'predicate abstraction is a powerful technique to reduce the state space of a program to a finite and affordable number of states. it produces a conservative over-approximation where concrete states are grouped together according to a given set of predicates. a precise abstraction contains the minimal set of transitions with regards to the predicates, but as a result is computationally expensive. most model checkers therefore approximate the abstraction to alleviate the computation of the abstract system by trading off precision with cost. however, approximation results in a higher number of refinement iterations, since it can produce more false counterexamples than its precise counterpart. the refinement loop can become prohibitively expensive for large programs. this paper proposes a new abstraction refinement technique that combines slow and precise predicate abstraction techniques with fast and imprecise ones. it allows computing the abstraction quickly, but keeps it precise enough to avoid too many refinement iterations. we implemented the new algorithm in a state-of-the-art software model checker. our tests with various real life benchmarks show that the new approach systematically outperforms both precise and imprecise techniques.the synergy of precise and fast abstractions for program verification','Logic and verification'
'bytecode verification is one of the key security functions of several architectures for mobile and embedded code, including java, java card, and .net. over the past few years, its formal correctness has been studied extensively by academia and industry, using general-purpose theorem provers. the objective of our work is to facilitate such endeavors by providing a dedicated environment for establishing the correctness of bytecode verification within a proof assistant.the environment, called jakarta, exploits a methodology that casts the correctness of bytecode verification relatively to a defensive virtual machine that performs checks at run-time and to an offensive one that does not; it can be summarized as stating that the two machines coincide on programs that pass bytecode verification. such a methodology has been used successfully to prove the correctness of the java card bytecode verifier and may potentially be applied to many similar problems. one definite advantage of the methodology is that it is amenable to automation. indeed, jakarta automates the construction of an offensive virtual machine and a bytecode verifier from a defensive machine, and the proofs of correctness of the bytecode verifier.we illustrate the principles of jakarta on a simple low-level language extended with subroutines and discuss its usefulness to proving the correctness of the java card platform.tool-assisted specification and verification of typed low-level languages','Logic and verification'
'modeling hardware through atomic guard/action transitions with interleaving semantics is popular, owing to the conceptual clarity of modeling and verifying the high level behavior of hardware. in mapping such specifications into hardware, designers often decompose each specification transition into sequences of implementation transitions taking one clock cycle each. some implementation transitions realizing a specification transition overlap. the implementation transitions realizing different specification transitions can also overlap. we present a formal theory of refinement, showing how a collection of such implementation transitions can be shown to realize a specification. we present a modular refinement verification approach by developing abstraction and assume-guarantee principles that allow implementation transitions realizing a single specification transition to be situated in sufficiently general environments. illustrated on a non-trivial vhdl cache coherence engine, our work may allow designers to design high performance controllers without being constrained by fixed automated synthesis scripts, and still conduct modular verification.transaction based modeling and verification of hardware protocols','Logic and verification'
'validation and verification of decision making rules','Logic and verification'
'verification and behavior abstraction towards a tractable verification technique for large distributed systems','Logic and verification'
'verification and normalization of sentences','Logic and verification'
'verification and validation of bayesian knowledge-bases','Logic and verification'
'our ability to verify complex hardware lags far behind our capacity to design and fabricate it. we argue that this gap is partly due to the limitations of rtl models when used for verification. higher level models such as systemc and systemverilog aim to raise the level of abstraction to enhance designer productivity; however, they largely provide for executable but not analyzable descriptions. we propose the use of formally analyzable design models at two distinct levels above rtl: the architecture and the microarchitecture level. at both these levels, we describe concurrent units of data computation termed transactions. the architecture level describes the computation/state updates in the transactions and their interaction through shared data. the microarchitecture level adds to this the resource usage in the transactions as well as their interaction based on shared resources. we then illustrate the applicability of these models in a top-down verification methodology which addresses several concerns of current methodologies.verification driven formal architecture and microarchitecture modeling','Logic and verification'
'we consider verification problems for transition systems enriched with a metric structure. we believe that these metric transition systems are particularly suitable for the analysis of cyber-physical systems in which metrics can be naturally defined on the numerical variables of the embedded software and on the continuous states of the physical environment. we consider verification of bounded and unbounded safety properties, as well as bounded liveness properties. the transition systems we consider are nondeterministic, finitely branching, and with a finite set of initial states. therefore, bounded safety/liveness properties can always be verified by exhaustive exploration of the system trajectories. however, this approach may be intractable in practice, as the number of trajectories usually grows exponentially with respect to the considered bound. furthermore, since the system we consider can have an infinite set of states, exhaustive exploration cannot be used for unbounded safety verification. for bounded safety properties, we propose an algorithm which combines exploration of the system trajectories and state space reduction using merging based on a bisimulation metric. the main novelty compared to an algorithm presented recently by lerda et al. [2008] consists in introducing a tuning parameter that improves the performance drastically. we also establish a procedure that allows us to prove unbounded safety from the result of the bounded safety algorithm via a refinement step. we then adapt the algorithm to handle bounded liveness verification. finally, the effectiveness of the approach is demonstrated by applying it to the analysis of implementations of an embedded control loop.verification of safety and liveness properties of metric transition systems','Logic and verification'
'we describe two checkers for verifying termination and reduction properties about higher-order logic programs. the reduction checker verifies that the result of a program execution is structurally smaller than (or equal to) the inputs to the program. the termination checker guarantees that the inputs of the recursive calls are structurally smaller than the inputs of the original call, taking into account reduction properties. at the heart of both checkers lies an inference system to reason about structural properties, which are described by higher-order subterm relations. this approach provides a logical foundation for proving properties such as termination and reduction and factors the effort required for each one of them. moreover, it allows the study of proof-theoretical properties, soundness, and completeness and different optimizations. the termination and reduction checker are implemented as part of the twelf system and have been used on a wide variety of examples, including proofs about typed assembly language and those in the area of proof-carrying code.verifying termination and reduction properties about higher-order logic programs','Logic and verification'
'adapting to changes in its environment dynamically is a very important aspect of workflow systems. in this paper, we propose a component-based workflow system architecture specifically designed for this purpose. to allow for easy modification of workflow instances, an instance is designed as an object that contains all the necessary data and control information as well as its execution history. this feature facilitates to dynamically modify the process definition on instance basis at run time. the system is designed to consist of functional components like, basic enactment service, history manager, workflow monitoring tool, dynamic modification tool, etc. the clients of the system are coded as network-transportable applets written in java so that the end user can activate workflow system components by connecting to the workflow domain manager over the internet. in this paper we also present a workflow process definition language flowdl, its graphical representation flowgraph and a workflow process modification language flowml and illustrate how the modification process is handled.a component-based workflow system with dynamic modifications','Malicious design modifications'
'for the macrocell design style and for routing problems in which the routing regions are irregular, two-dimensional routers are often necessary. in this paper, a new routing technique that can be applied for general two-layer detailed routing problems, including switchboxes, channels, and partially routed areas, is presented. the routing regions that can be handled are very general: the boundaries can be described by any rectilinear edges, the pins can be on or inside the boundaries of the region, and the obstructions can be of any shape and size. the technique is based on an algorithm that routes the nets in the routing region incrementally and intelligently, and allows modifications and rip-up of nets when an existing shortest path is \"far\" from optimal or when no path exists. the modification steps (also called weak modification) relocate some segments of nets already routed to find a shorter path or to make room for a blocked net. the rip-up and reroute steps (called strong modifiction) remove segments of nets already routed to make room for a blocked connection; these steps are invoked only if weak modification fails. the algorithm has been rigorously proven to complete in finite time and its complexity has been analyzed. the algorithm has been implemented in the \"c\" programming language. many test cases have been run, and on all the examples known in the literature the router has performed as well as or better than existing algorithms. in particular, burstein\'s difficult switchbox example has been routed using one less column than the original data. in addition, the router has routed difficult channels such as deutsch\'s in density and has performed better than or as well as yacr-ii on all the channels available to us.a detailed router based on incremental routing modifications','Malicious design modifications'
'particle swarm optimization (pso) is a population-based optimization technique that can be applied to a wide range of problems. here, we first investigate the behavior of particles in the pso using a monte carlo method. the results reveal the essence of the trajectory of particles during iterations and the reasons why the pso lacks a global search ability in the last stage of iterations. then, we report a novel pso with a moderate-random-search strategy (mrpso), which enhances the ability of particles to explore the solution spaces more effectively and increases their convergence rates. furthermore, a new mutation strategy is used, which makes it easier for particles in hybrid mrpso (hmrpso) to find the global optimum and which also seeks a balance between the exploration of new regions and the exploitation of the already sampled regions in the solution spaces. thirteen benchmark functions are employed to test the performance of the hmrpso. the results show that the new pso algorithm performs much better than other pso algorithms for each multimodal and unimodal function. furthermore, compared with recent evolutionary algorithms, experimental results empirically demonstrate that the proposed framework yields promising search performance.a new particle swarm algorithm and its globally convergent modifications','Malicious design modifications'
'recent investigations show a remarkable convergence among contemporary unification-based formalisms for syntactic description. this convergence is now itself becoming an object of study, and there is an increasing recognition of the need for explicit characterizations of the properties that relate and distinguish similar grammar formalisms. the paper proposes a series of changes in the formalism of generalized phrase structure grammar that throw light on its relation to functional unification grammar.the essential contribution is a generalization of cooccurrence restrictions, which become the principal and unifying device of gpsg. introducing category cooccurrence restrictions (ccrs) for local trees (in analogy to feature cooccurrence restrictions for categories) provides a genuine gain in expressiveness for the formalism. other devices, such as feature instantiation principles and linear precedence statements can be regarded as special cases of ccrs. the proposals lead to a modified notion of unification itself.a proposal for modifications in the formalism of gpsg','Malicious design modifications'
'in this paper, we propose a novel migration method. in this method, the resultant placement retains the structure of the original placement, called model placement, as much as possible. for this purpose, we minimize the sum of the difference in area between the model placement and the relocated one and the total amount of displacement between them. moreover, to achieve a short runtime, we limit the solution space and change the packing origin in the optimization process. we construct the system on sequence-pair. experimental results show that our approach preserves the chip area and the overall circuit structure with 98\% less runtime than that realized by naive simulated annealing.a relocation method for circuit modifications','Malicious design modifications'
'a theory of program modifications','Malicious design modifications'
'wet paper coding is a technique that can be used to embed secrets into some specific pixels, called dry pixels, of a digital image. the fully exploiting modifications method (fem) is a method applied to convey secrets securely from senders to receivers using a stego-image that is slightly modified from the original digital image. based on fem, before embedding, we randomly choose pixels in the image to define as wet or dry and treat every pair of two consecutive pixels as a token. if the token consists of at least one dry pixel, it is judged as embeddable; otherwise, it is unembeddable. experimental results show that our proposed method can achieve very good visual quality and high embedding payload.a wet image data hiding scheme based on coordinate modifications','Malicious design modifications'
'algorithm modifications for parallel operation of a multigrid navier-stokes solver','Malicious design modifications'
'algorithmic modifications to the theory of evidential reasoning','Malicious design modifications'
'as software engineers collaboratively develop software, they need to often analyze past and present program modifications implemented by other developers. while several techniques focus on tool support for investigating past and present software modifications, do these techniques indeed address developers\' awareness interests that are important to them? we conducted an initial focus group study and a web survey to understand in which task contexts and how often particular types of awareness-interests arise. our preliminary study results indicate that developers have daily information needs about code changes that affect or interfere with their code, yet it is extremely challenging for them to identify relevant events out of a large number of change-events.an exploratory study of awareness interests about software modifications','Malicious design modifications'
'image fingerprinting technique plays an important role in digital rights management and related applications, especially in the detection of the illegal use of image works. although existing image fingerprinting methods perform well in many cases, they are not robust enough to sophisticated modifications like image embedding and image combining. this paper proposes an image fingerprinting approach which is robust to such modifications. firstly, we introduce an sift-based algorithm to extract image keypoints as the unique fingerprint. secondly, a technique based on geometry isomorphic relationship is utilized to select valid keypoints in the queried image. finally, by calculating the matched keypoints pairs between the queried image and the pre-registered image, we can make the decision whether they are homologous. experimental results show that the proposed method is robust to image embedding and combining, without a high computational complexity.an image fingerprinting method robust to complicated image modifications','Malicious design modifications'
'an indirect robust continuous-time adaptive controller with minimal modifications','Malicious design modifications'
'an integrated model for activity-dependent synaptic modifications','Malicious design modifications'
'some modifications of the secant method for solving nonlinear equations are revisited and the local order of convergence is found in a direct symbolic computation. to do this, a development of the inverse of the first order divided differences of a function of several variables in two points is presented. a generalisation of the efficiency index used in the scalar case to several variables is also analysed in order to use the most competitive algorithm.analysing the efficiency of some modifications of the secant method','Malicious design modifications'
'architectural modifications for optimizing mappings on morphosys rc-system','Malicious design modifications'
'in this paper we study the question of how an aimed arm movement is modified in response to a sudden change in target location occurring during the reaction or movement time. earlier monkey and human studies demonstrated that aimed arm movements can be elicited in quick succession, without appreciable delays in responding to the target displacement, beyond the normal reaction time. nevertheless, it is not yet clear how this motor task is performed. a first guess is that when a new visual stimulus appears the old plan is aborted and a new one conceived. upon analyzing human arm movements, however, we find that the observations can be well accounted for by a different movement modification scheme. it appears that a new plan is vectorially added to the original plan. among the implications of this result is the possibility of parallel planning of elemental movements and further support for the idea that arm movements are internally represented in terms of hand motion through external space.arm trajectory modifications during reaching towards visual targets','Malicious design modifications'
'in this article, we investigate four variations (d-hsm, d-hsw, d-hse, and d-hsew) of a novel indexing technique called d-hs designed for use in case-based reasoning (cbr) systems. all d-hs modifications are based on a matrix of cases indexed by their discretized attribute values. the main differences between them are in their attribute discretization stratagem and similarity determination metric. d-hsm uses a fixed number of intervals and simple intersection as a similarity metric; d-hsw uses the same discretization approach and a weighted intersection; d-hse uses information gain to define the intervals and simple intersection as similarity metric; d-hsew is a combination of d-hse and d-hsw. benefits of using d-hs include ease of case and similarity knowledge maintenance, simplicity, accuracy, and speed in comparison to conventional approaches widely used in cbr. we present results from the analysis of 20 case bases for classification problems and 15 case bases for regression problems. we demonstrate the improvements in accuracy and&#x002f;or efficiency of each d-hs modification in comparison to traditional k-nn, r-tree, c4,5, and m5 techniques and show it to be a very attractive approach for indexing case bases. we also illuminate potential areas for further improvement of the d-hs approach. &#169; 2007 wiley periodicals, inc. int j int syst 22: 353&#8211;383, 2007.assessment of four modifications of a novel indexing technique for case-based reasoning','Malicious design modifications'
'software maintenance can consume up to 70\% of the effort spent on a software project, with more than half of this devoted to understanding the system. performing a software inspection is expected to contribute to comprehension of the software. the question is: at what cognition levels do novice developers operate during a checklist-based code inspection followed by a code modification? this paper reports on a pilot study of bloom\'s taxonomy levels observed during a checklist-based inspection and while adding new functionality unrelated to the defects detected. bloom\'s taxonomy was used to categorise think-aloud data recorded while performing these activities. results show the checklist-based reading technique facilitates inspectors to function at the highest cognitive level within the taxonomy and indicates that using inspections with novice developers to improve cognition and understanding may assist integrating developers into existing project teams.checklist inspections and modifications','Malicious design modifications'
'given an undirected graph g=(v,e) and a nonnegative integer k, the np-hard cluster editing problem asks whether g can be transformed into a disjoint union of cliques by modifying at most k edges. in this work, we study how \'\'local degree bounds\'\' influence the complexity of cluster editing and of the related cluster deletion problem which allows only edge deletions. we show that even for graphs with constant maximum degree cluster editing and cluster deletion are np-hard and that this implies np-hardness even if every vertex is incident with only a constant number of edge modifications. we further show that under some complexity-theoretic assumptions both cluster editing and cluster deletion cannot be solved within a running time that is subexponential in k, |v|, or |e|. finally, we present a problem kernelization for the combined parameter \'\'number d of clusters and maximum number t of modifications incident with a vertex\'\' thus showing that cluster editing and cluster deletion become easier in case the number of clusters is upper-bounded.cluster editing with locally bounded modifications','Malicious design modifications'
'in model-driven engineering, models are primary artifacts and can evolve heavily during their life cycle. therefore, versioning of models is a key technique which has to be offered by an integrated development environment for model-driven engineering. in contrast to textbased versioning systems we present an approach which takes abstract syntax structures in model states and operational features into account. considering the abstract syntax of models as graphs, we define model revisions as graph modifications which are not necessarily rule-based. building up on the dpo approach to graph transformations, we define two different kinds of conflict detection: (1) the check for operation-based conflicts, and (2) the check for state-based conflicts on results of merged graph modifications.conflict detection for model versioning based on graph modifications','Malicious design modifications'
'monte-carlo tree search algorithms (mcts [4,6]), including upper confidence trees (uct [9]), are known for their impressive ability in high dimensional control problems. whilst the main testbed is the game of go, there are increasingly many applications [13,12,7]; these algorithms are now widely accepted as strong candidates for highdimensional control applications. unfortunately, it is known that for optimal performance on a given problem, mcts requires some tuning; this tuning is often handcrafted or automated, with in some cases a loss of consistency, i.e. a bad behavior asymptotically in the computational power. this highly undesirable property led to a stupid behavior of our main mcts program mogo in a real-world situation described in section this is a big trouble for our several works on automatic parameter tuning [3] and the genetic programming of new features in mogo. we will see in this paper: - a theoretical analysis of mcts consistency; - detailed examples of consistent and inconsistent known algorithms; - how to modify a mcts implementation in order to ensure consistency, independently of themodifications to the \"scoring\"module (the module which is automatically tuned and genetically programmed in mogo); - as a by product of this work, we\'ll see the interesting property that some heavily tuned mcts implementations are better than uct in the sense that they do not visit the complete tree (whereas uct asymptotically does), whilst preserving the consistency at least if \"consistency\" modifications above have been made.consistency modifications for automatically tuned monte-carlo tree search','Malicious design modifications'
'researchers become increasingly interested in developing tools for evaluating the correctness of business process models. we present a methodology for content-based validation of changes to business processes, relying on an automatic extraction of business logic from real-life business process repositories. each process step in a repository is automatically transformed to a descriptor - containing objects, actions, and related qualifiers. from the collection of descriptors we induce taxonomies of action sequence, object lifecycle, and object and action hierarchies that form the logical foundation of the presented validation process. the method utilizes these taxonomies to identify process deficiencies that may occur due to process model modification, and suggests alternatives in order to correct and validate the models.content-based validation of business process modifications','Malicious design modifications'
'in this paper, a predictive norm-optimal iterative learning control algorithm from amann, owens, and rogers (int. j. control 69 (2) (1998) 203-226) is analyzed. the main new result of this is that any of the predictive inputs from the predictive algorithm can be used in the control of the plant. this results in a faster convergence rate than that obtained with the approach proposed by amann, owens, and rogers. furthermore, the nature of the convergence of this new scheme is analysed in detail in terms of the free parameters of the algorithm.convex modifications to an iterative learning control law','Malicious design modifications'
'model checking is a promising technology, which has been applied for verification of many hardware and software systems. in this paper, we introduce the concept of model update towards the development of an automatic system modification tool that extends model checking functions. we define primitive update operations on the models of computation tree logic (ctl) and formalize the principle of minimal change for ctl model update. these primitive update operations, together with the underlying minimal change principle, serve as the foundation for ctl model update. essential semantic and computational characterizations are provided for our ctl model update approach. we then describe a formal algorithm that implements this approach. we also illustrate two case studies of ctl model updates for the well-known microwave oven example and the andrew file system 1, from which we further propose a method to optimize the update results in complex system modifications.ctl model update for system modifications','Malicious design modifications'
'the paper deals with the modification made to the general electrospinning setup. the emphasis is given to characterize the designs based on their applicability. four basic categories are identified, namely, patterned fibers, fiber yarns, multicomponent, and deposition area of the fiber mat obtained. the mathematical modeling to better understand the physics behind the modification made to the general electrospinning setup is presented. emphasis is given to critically analyse these categories on the basis of the applications served by them. each of these categories is found to serve a specific poll of advanced application enabling the researchers to make a calculated choice for the design of the electrospinning setup for particular application.design modifications in electrospinning setup for advanced applications','Malicious design modifications'
'java&#39;s features such as system platform independence, dynamic and network oriented architecture, robustness as well as growing number of common standards make it a language of choice for many projects. however an increasing complexity of created software and requirement for high stability and high quality of applications make it desirable for a developer to inspect, monitor, debug or in any way alter java programs behaviour on-the-fly. the main goal of this paper is to present the design of a system for instrumenting java classes at runtime. this system is to aid developer in modifying program by adding fragments of code at specific locations that implement some new functionality. this allows programmer to enhance classes with logging, monitoring, caching or any other capabilities that are required at run-time.dynamic instrumentation of distributed java applications using bytecode modifications','Malicious design modifications'
'we describe a higher-order interprocedural technique, called dynamic resolution, for the automatic parallelization of procedures that destructively manipulate dynamic dags. dynamic resolution detects shared data and correctly coordinates access to this data at run time. in pointer-unsafe languages such as c, dynamic resolution requires programmer identification of acyclic data structures and use of dynamic resolution\'s macros for pointer manipulations. in pointer-safe languages such as ml, cyclicity can often be inferred by the compiler and parallelization via dynamic resolution can be completely automatic. this paper empirically studies the performance of dynamic resolution. our study reveals that dynamic resolution applied to statically unparallelizable programs can outperform the optimized sequential versions on fast shared-memory multiprocessors. in particular, dynamic-resolution implementations of two problems (dag rewrite and in-place list quicksort) using three processors already outperform their sequential counterparts. for both problems, the absolute performance of dynamic resolution steadily improves as processors are added. we also observe that dynamic resolution can offset its run-time overheads in the presence of some shared structure. dynamic resolution is the first technique that can automatically and effectively parallelize dag rewrite and destructive list quicksort.dynamic parallelization of modifications to directed acyclic graphs','Malicious design modifications'
'dynamic resolution: a runtime technique for the parallelzation of modifications to directed acyclic graphs','Malicious design modifications'
'this aim of this review is to describe the dynamics of learning-induced cellular modifications in the rat piriform (olfactory) cortex after olfactory discrimination learning and to describe their functional significance to long-term memory consolidation. the first change to occur is in the intrinsic properties of the neurons. one day after learning, pyramidal neurons show enhanced neuronal excitability. this enhancement results from reduction in calcium-dependent conductance that mediates the post burst after-hyperpolarization. such enhanced excitability lasts for 3 days and is followed by a series of synaptic modifications. several forms of long-term enhancement in synaptic connections between layer ii pyramidal neurons in the piriform cortex accompany olfactory learning. enhanced synaptic release is indicated by reduced paired-pulse facilitation. post-synaptic enhancement of synaptic transmission is indicated by reduced rise time of post-synaptic potentials and formation of new synaptic connections is indicated by increased spine density along dendrites of these neurons. such modifications last for up to 5 days. thus, olfactory discrimination rule learning is accompanied by a series of cellular modifications which occur and then disappear at different times. these modifications overlap partially, allowing the maintenance of the cortical system in a &#x2018;learning mode&#x2019; in which memories for specific odors can be acquired rapidly and efficiently.dynamics of learning-induced cellular modifications in the cortex','Malicious design modifications'
'efficient testing of software modifications','Malicious design modifications'
'endomorphisms of undirected modifications of directed graphs','Malicious design modifications'
'summary: the determination of annealing temperature is a critical step in pcr design. this parameter is typically derived from the melting temperature of the pcr primers, so for successful pcr work it is important to determine the melting temperature of primer accurately. we introduced several enhancements in the widely used primer design program primer3. the improvements include a formula for calculating melting temperature and a salt correction formula. also, the new version can take into account the effects of divalent cations, which are included in most pcr buffers. another modification enables using lowercase masked template sequences for primer design. availability: features described in this article have been implemented into the development code of primer3 and will be available in future versions (version 1.1 and newer) of primer3. also, a modified version is compiled under the name of mprimer3 which is distributed independently. the web-based version of mprimer3 is available at http://bioinfo.ebc.ee/mprimer3/ and the binary code is freely downloadable from the url http://bioinfo.ebc.ee/download/.  contact: maido.remm@ut.ee enhancements and modifications of primer design program primer3','Malicious design modifications'
'we consider the task of automatically extracting post-translational modification events from biomedical scientific publications. building on the success of event extraction for phosphorylation events in the bionlp\'09 shared task, we extend the event annotation approach to four major new post-transitional modification event types. we present a new targeted corpus of 157 pubmed abstracts annotated for over 1000 proteins and 400 post-translational modification events identifying the modified proteins and sites. experiments with a state-of-the-art event extraction system show that the events can be extracted with 52\% precision and 36\% recall (42\% f-score), suggesting remaining challenges in the extraction of the events. the annotated corpus is freely available in the bionlp\'09 shared task format at the ge-nia project homepage.event extraction for post-translational modifications','Malicious design modifications'
'there are many variants of the original self-organizing neural map algorithm proposed by kohonen. one of the most recent is the evolving tree, a tree-shaped self-organizing network which has many interesting characteristics. this network builds a tree structure splitting the input dataset during learning. this paper presents a speed-up modification of the original training algorithm useful when the evolving tree network is used with complex data as images or video. after a measurement of the effectiveness an application of the modified algorithm in image segmentation is presented.evolving tree algorithm modifications','Malicious design modifications'
'in the course of a search session, searchers often modify their queries several times. in most previous work analyzing search logs, the addition of terms to a query is identified with query specification and the removal of terms with query generalization. by analyzing the result sets that motivated searchers to make modifications, we show that this interpretation is not always correct. in fact, our experiments indicate that in the majority of cases the modifications have the opposite functions. terms are often removed to get rid of irrelevant results matching only part of the query and thus to make the result set more specific. similarly, terms are often added to retrieve more diverse results. we propose an alternative interpretation of term additions and removals and show that it explains the deviant modification behavior that was observed.explaining query modifications','Malicious design modifications'
'when searching the www, users often desire results restricted to a particular document category. ideally, a user would be able to filter results with a text classifier to minimize false positive results; however, current search engines allow only simple query modifications. to automate the process of generating effective query modifications, we introduce a sensitivity analysis-based method for extracting rules from nonlinear support vector machines. the proposed method allows the user to specify a desired precision while attempting to maximize the recall. our method performs several levels of dimensionality reduction and is vastly faster than searching the combination feature space; moreover, it is very effective on real-world data.extracting query modifications from nonlinear svms','Malicious design modifications'
'factor analysis and experiment design in high-performance liquid chromatography. iii.  influence of mobile phase modifications on the selectivity of chalcones on a diol stationary phase','Malicious design modifications'
'first steps to a formal framework for multilevel database modifications','Malicious design modifications'
'flexibility achieved by modifications to gte minicomputer','Malicious design modifications'
'flexible block cipher with provably inequivalent cryptalgorithm modifications','Malicious design modifications'
'augmented reality applications overlap virtual objects over a real scene considering the context. today, more advanced applications also make use of diminished reality, which removes real objects from a scene. this paper describes a novel approach that combines augmented reality and diminished reality techniques to modify real objects in augmented reality applications. the proposed approach removes an object and replaces it with its purposely-modified replica. the solution uses dynamic texture techniques and inpaint to enhance the visual response of the performed modification. the results are promising considering both realism of the modified real object and performance of the application.geometric modifications applied to real elements in augmented reality','Malicious design modifications'
'in this paper, we examine the properties of some typical instance of global image features to provide clues to image retrieval by efficient selection of them as content. it is performed by calculating the feature difference between images including variously transformed images. image data might be caused image variations by typical modification or deterioration originated from image collection process. we intend to classify the transformed images as identical group and investigate the appropriate kind of feature which is robust against the image variation and difference measuring method distinguishable from other group. the result can be applied for image identification against environmental modifications or identical photographic image grouping working with local feature.global feature properties by image modifications and variations','Malicious design modifications'
'the following paper is a description of a boeing modeling technique (bmt) system. the system was derived from gpss ii-b and contains some desirable gpss/360 capabilities in addition to extended capabilities and modifications.gpss extended capabilities and modifications','Malicious design modifications'
'identifying faulty modifications in software maintenance','Malicious design modifications'
'ieee 802.15.4 is a popular choice for mac/phy protocols in low power and low data rate wireless sensor networks. in this paper, we suggest several modifications to beaconless ieee 802.15.4 mac operation and evaluate their impact on the performance via stochastic modeling and simulations. we found that the utility of these modifications is strongly dependent on the traffic load on the network. accordingly, we make recommendations regarding how these modifications should be used in view of the prevalent traffic load on the network.ieee 802.15.4 modifications and their impact','Malicious design modifications'
'improved fast encryption algorithm for multimedia ifea-m is modified to withstand recently discovered vulnerability to differential known plaintext-ciphertext attack. proposed here algorithms pifea-m and i2fea-m have about 10\% and 30\% better performance respectively than that of ifeam.ifea-m modifications resistant to differential attack','Malicious design modifications'
'abstract users looking for documents within specific categories may have a difficult time locating valuable documents using general purpose search engines. we present an automated method for learning query modifications that can dramatically improve precision for locating pages within specified categories using web search engines. we also present a classification procedure that can recognize pages in a specific category with high precision, using textual content, text location, and html structure. evaluation shows that the approach is highly effective for locating personal homepages and calls for papers. these algorithms are used to improve category specific search in the inquirus 2 search engine.improving category specific web search by learning query modifications','Malicious design modifications'
'during the re-engineering of legacy software systems, a good knowledge of the history of past modifications on the system is important to recover the design of the system and transfer its functionalities. in the absence of a reliable revision history, development teams often rely on system experts to identify hidden history and recover software design. in this paper, we propose a new technique to infer the history of repository file modifications of a software system using only past released versions of the system. the proposed technique relies on nearest-neighbor clone detection using the manhattan distance. we performed an empirical evaluation of the technique using tomcat, jhotdraw and adempiere svn information as our oracle of file operations, and obtained an average precision of 97\% and an average recall of 98\%. our evaluation also highlighted the phenomena of implicit moves, which are, moves between a system\'s versions, that are not recorded in the svn repository. in the absence of revision history and software experts, development teams can make use of the proposed technique during the re-engineering of their legacy systems.inferring repository file structure modifications using nearest-neighbor clone detection','Malicious design modifications'
'when a user requests content from a cloud service provider, sometimes the content sent by the provider is modified inflight by third-party entities. to our knowledge, there is no comprehensive study that examines the extent and primary root causes of the content modification problem. we design a lightweight experiment and instrument a vast number of clients in the wild to make two additional dns queries every day. we identify candidate rogue servers and develop a measurement methodology to determine, for each candidate rogue server, whether the server is performing inflight modifications or not. in total, we discover 349 servers as malicious, that is, as modifying content inflight, and more than 1.9\% of all us clients are affected by these malicious servers. we investigate the root causes of the problem. we identify 9 isps, whose clients are predominately affected. we find that the root cause is not sophisticated transparent in-network services, but instead local dns servers in the problematic isps.inflight modifications of content','Malicious design modifications'
'learning classifier systems represent a technique by which various characteristics of a given problem space may be deduced and presented to the user in a readable format. in this paper we present results from the use of xcs on simple tasks with the general multi-variable features typically found in problems addressed by an interactive evolutionary design process. that is, we examine the behaviour of xcs with versions of a well-known single-step task and consider the speed of learning and the ability to respond to changes. we introduce a simple form of supervised learning for xcs with the aim of improving its performance with respect to these two measures. results show that improvements can be made under the new learning scheme and that other aspects of xcs can also play a significant role.initial modifications to xcs for use in interactive evolutionary design','Malicious design modifications'
'laboratory test modifications','Malicious design modifications'
'we start with a large matrix a whose structure is simple, say, with unit entries on the first subdiagonal and superdiagonal. its eigenvalues and eigenvectors are known. we modify a in m widely spaced rows and columns. then the \"new eigenvectors\" are nearly a sum of spikes xj = t|j-r| centered at the positions r of the modified rows. the new eigenvalues are given almost exactly by $\\pm \\sqrt{4+\\mu^2}$, where $\\mu$ is an eigenvalue of the m by m modification.we extend this analysis to a larger class of structured matrices. for a banded toeplitz matrix, our experiments show similar spikes centered around modified rows, and we have a conjecture on the structure of the new eigenvectors. for a single diagonal modification of the adjacency matrix of an infinite two-dimensional grid, we find the new eigenvalue from an elliptic integral (and we don\'t yet recognize the eigenvector).localized eigenvectors from widely spaced matrix modifications','Malicious design modifications'
'we present a new class of exponential integrators for ordinary differential equations: locally exact modifications of known numerical schemes. local exactness means that they preserve the linearization of the original system at every point. in particular, locally exact integrators preserve all fixed points and are a-stable. we apply this approach to popular schemes including euler schemes, the implicit midpoint rule, and the trapezoidal rule. we found locally exact modifications of discrete gradient schemes (for symmetric discrete gradients and coordinate increment discrete gradients) preserving their main geometric property: exact conservation of the energy integral (for arbitrary multidimensional hamiltonian systems in canonical coordinates). numerical experiments for a two-dimensional anharmonic oscillator show that locally exact schemes have very good accuracy in the neighbourhood of stable equilibrium, much higher than suggested by the order of new schemes (locally exact modification sometimes increases the order but in many cases leaves it unchanged).locally exact modifications of numerical schemes','Malicious design modifications'
'it is usually considered that evolutionary algorithms are highly parallel. in fact, the theoretical speed-ups for parallel optimization are far better than empirical results; this suggests that evolutionary algorithms, for large numbers of processors, are not so efficient. in this paper, we show that in many cases automatic parallelization provably provides better results than the standard parallelization consisting of simply increasing the population size &#955;. a corollary of these results is that logarithmic bounds on the speed-up (as a function of the number of computing units) are tight within constant factors. importantly, we propose a simple modification, termed log(&#955;)-correction, which strongly improves several important algorithms when &#955; is large.log(&#955;) modifications for optimal parallelism','Malicious design modifications'
'we consider the problem of minimizing the condition number of a low rank modification of a matrix. analytical results show that the minimum, which is not necessarily unique, can be obtained and expressed by a small number of eigenpairs or singular pairs. the symmetric and the nonsymmetric cases are analyzed, and numerical experiments illustrate the analytical observations.minimizing the condition number for small rank modifications','Malicious design modifications'
'this paper compares four different redundancy methods, which includes parity code, partial duplication and their combinations, with two standard methods (duplex and triple module redundancy). two main attributes are observed: the total size of system including overhead caused by redundancy addition and steady-state availability - dependability parameter defining the readiness for correct service of a system.miscellaneous types of partial duplication modifications for availability improvements','Malicious design modifications'
'insider threat is considered as a serious issue in all organizations. sophisticated insiders can override threat prevention tools and carry on their attacks with new techniques. one such technique which remains to be an advantage for insiders to attack a database is dependency relationship among data items. this paper investigates the ways by which an authorized insider detects dependencies in order to perform malicious write operations. the paper introduces a new term &#39;threshold&#39;, which defines the constraints and limits a write operation could take. having threshold as the key factor, the paper proposes two different attack prevention systems which involve log and dependency graphs that aid in monitoring malicious activities and ultimately secure the data items in a database. our proposed systems continuously monitor all the data items to prevent malicious operations, but the priority is to secure the most sensitive data items first since any damage to them can hinder the functions of critical applications that use the database. by prioritizing the data items, delay of the system is reduced in addition to mitigating insider threats arising from write operations.mitigation of malicious modifications by insiders in databases','Malicious design modifications'
'in vansteenkiste and de schutter propose a modified modal method to simulate diffusion processes with non-homogeneous boundary conditions. moreover, the hybrid implementation of the modal simulation is discussed by the authors from an exclusively theoretical point of view. the modification of the modal simulation consists of a transformation of non-homogeneous boundary conditions into homogeneous conditions. since the modified method considered in presumes the analytic solution of the original partial differential equation possible only under certain restrictions, this paper is concerned with two other proved modal methods for the simulation of non-homogeneous boundary value problems. in addition to this, some critical remarks are made both on the analog/hybrid modal simulation and on the hybrid implementation shown in.modal simulation method modifications and implementations','Malicious design modifications'
'one of the main emerging challenges in legal documentation is to capture the meaning and the semantics of normative content using nlp techniques, and to isolate the relevant part of the linguistic speech. the last five years have seen an explosion in xml schemas and dtds whose focus in modelling legal resources their focus was on structure. now that the basic elements of textual descriptiveness are well formalized, we can use this knowledge to proceed with content. this paper presents a detailed methodology for classifying modificatory provisions in depth and providing all the necessary information for semiautomatically managing the consolidation process. the methodology is based on an empirical legal analysis of about 29,000 italian acts, where we bring out regularities in the language associated with some modifications, and where we define patterns of proprieties for each type of modificatory provision. the list of verbs and the frames inferred through this empirical legal analysis have been used by the nlp group at the university of turin to refine a syntactical nlp parser for isolating and representing the sentences as syntactic trees, and the pattern will be used by the light semantic interpreter module to indentify the parameters of modificatory provisions.model regularity of legal language in active modifications','Malicious design modifications'
'maintenance and evolution of software systems require to modify or exchange system components. in many cases, we would like the new component versions to be backward compatible to the old ones, at least for the use in the given context. whereas on the program level formal techniques to precisely define and verify backward compatibility are under development, the situation on the system level is less mature. a system component c has not only communication interfaces to other system components, but also to human users or the environment of the system. in such scenarios, compatibility checking of different versions of c needs more than program analysis: &#183; the behavior of the users are not part of the program, but needs to be considered for the overall system behavior. if the user interaction in the new version is different from the old one, the notion of compatibility needs clarification. &#183; analyzing the user interface code makes checking technically difficult. &#183; we suggest to use behavioral software models for compatibility checking. in our approach, the underlying system, the old and new component, and the nondeterministic behavior of the environment are modeled with the concurrent object-oriented behavioral modeling language abs. abstracting from implementation details, the checking becomes simpler than on the program level.model-based compatibility checking of system modifications','Malicious design modifications'
'chip-chip is a powerful tool for epigenetic research. however, current statistical methods are developed primarily for detecting transcription factor binding sites, and there is currently no satisfactory method for incorporating covariates such as time, hormone levels, and genotypes. in this study, we develop a varying coefficient model for epigenetic modifications such as histone acetylation and dna methylation. by taking into account the special features of chip-chip data, a plug-in type method is derived for bandwidth selection in the local linear fitting of the varying coefficient model. our results show that analyses using the proposed varying coefficient model can effectively detect diverse characteristics of epigenetic modifications over genomic regions as well as across different treatment conditions.modeling epigenetic modifications under multiple treatment conditions','Malicious design modifications'
'repurposing of multimedia-based learning resources is an important issue in e-learning, as economic success of content production depends on how intensively content is used. repurposing does not only mean reuse &#8221;as is&#8221;, but also comprises modifications of the contents to suit a different learning or teaching context, as well as reuse of fragments of a large learning resource. this paper introduces a method for modeling multimedia content modifications based on an ontology-based content representation. a theoretical background for modeling modifications of multimedia contents independent of the particular format is provided. also a practical implementation is presented and discussed.modeling modifications of multimedia learning resources using ontology-based representations','Malicious design modifications'
'many fault-detection problems fall into the following model:  there is a set of $n$ items, some of which are defective.  the goal is to identify the defective items by using the  minimum number of tests. each test is on a subset of items  and tells whether the subset contains a defective item or  not. let $m_{\\alpha}(d, n) (m_{\\alpha}(d\\,|\\,n))$ denote the  maximum number of tests for an algorithm $\\alpha$ to  identify $d$ defectives from a set of $n$ items provided  that $d$, the number of defective items, is known (unknown)  before the testing. let $m(d, n) = \\min_{\\alpha}  m_{\\alpha}(d, n)$. an algorithm $\\alpha$ is called a {\\it  competitive algorithm\\/} if there exist constants $c$ and  $a$ such that for all $n > d > 0$, $m_{\\alpha}(d\\,|\\,n) \\leq  cm(d, n) + a$. this paper confirms a recent conjecture that  there exists a bisecting algorithm $a$ such that  $m_a(d\\,|\\,n) \\leq 2m(d, n) + 1$. also, an algorithm $b$  such that $m_b(d\\,|\\,n) \\leq 1.65m(d, n) + 10$ is presented. modifications of competitive group testing','Malicious design modifications'
'we describe two variants of ecdsa one of which is secure, in the random oracle model, against existential forgery but suffers from the notion of duplicate signatures. the second variant is also secure against existential forgery but we argue that it is likely to possess only four natural duplicate signatures. our variants of ecdsa are analogous to the variants of dsa as proposed by brickell et al. however, we show that the ecdsa variants have better exact security properties.modifications of ecdsa','Malicious design modifications'
'in this paper modifications of intensifiers have been done for the injection into the receptive field of the fuzzy neural networks. algorithmic developments for these modifiers are carried out for single-input single-output (siso) as well as for multi-input multi-output (mimo) fuzzy neural networks. this work can be beneficial for applications in different fields such as image processing, pattern recognition, control engineering, etc. the effects of the modified intensifiers on the localized fuzzy receptive field strengths as well as the overall performances of the fuzzy neural networks have been studied. simulation results have been presented using complex nonlinear dynamical system (mimo case study) suffering from uncertainties. also, comparative studies with previous works have been given, exhibit improved performances using the proposed technique.modifications of intensifiers and fuzzy neuronal receptive fields','Malicious design modifications'
'the known families of binary sequences having asymptotic merit factor 6.0 are modifications to the families of legendre sequences and jacobi sequences. in this paper, we show that at n = pq, there are many suitable modifications other than the jacobi or modified jacobi sequences. furthermore, we will give three new modifications to the character sequences of length n = pq. based on these new modifications, for any pair of large p and q, we can construct a binary sequence of length 2pq so that such families of sequences have asymptotic merit factor 6.0 without cyclic shifting of the base sequences.modifications of modified jacobi sequences','Malicious design modifications'
'three basic properties of boolean functions to be useful for cryptographic purposes are balancedness, high algebraic degree, and high nonlinearity. in addition, strict avalanche criteria and propagation characteristics are required for design of s-boxes. we introduce methods to modify the patterson-wiedemann (19983, 1990) and bent functions to achieve the above cryptographic properties. in the process, we are able to answer some open questions about boolean functionsmodifications of patterson-wiedemann functions for cryptographic applications','Malicious design modifications'
'we present a survey of the different kinds of pid regulators that have found applications in industrial automation. we present their advantages and disadvantages.modifications of pid regulators','Malicious design modifications'
'hierarchical generalized linear models (hglms) have become popular in data analysis. however, their maximum likelihood (ml) and restricted maximum likelihood (reml) estimators are often difficult to compute, especially when the random effects are correlated; this is because obtaining the likelihood function involves high-dimensional integration. recently, an h-likelihood method that does not involve numerical integration has been proposed. in this study, we show how an h-likelihood method can be implemented by modifying the existing ml and reml procedures. a small simulation study is carried out to investigate the performances of the proposed methods for hglms with correlated random effects.modifications of reml algorithm for hglms','Malicious design modifications'
'one of the most popular hash algorithms is the sha-0, proposed by nist. however, researchers have already found security flaws in sha-0, thereby also posing a threat against other algorithms of the sha family. in this paper we present two simple modifications which can be easily incorporated into the original sha-0 algorithm to make it secure against one of its most basic attack methodologies. we further show that the modified algorithm performs equally well as the original one when compared against standard metrics that are used to evaluate hash functions. we have developed a prototype tool to compare and evaluate the modified and the original sha-0 algorithm.modifications of sha-0 to prevent attacks','Malicious design modifications'
'in 1994 burrows and wheeler [3] described a universal data compression algorithm (bw-algorithm, for short) which achieved compression rates that were close to the best known compression rates. due to it\'s simplicity, the algorithm can be implemented with relatively low complexity. fenwick [5] described ideas to improve the efficiency (i.e. the compression rate) and complexity of the bw-algorithm. he also discusses relationships of the algorithm with other compression methods. schindler [12] proposed a burrows and wheeler transformation (bwt, for short) that is based on a limited ordering. this speeds up the algorithm for compression, but slows it down for decompression and slightly decreases the efficiency. larsson [8] describes relationship of the bwt with suffix trees and with context trees. sadakane [11] suggests a method to compute the bwt faster, and compares it to other methods. recently balkenhol and kurtz [1] gave a thorough analysis of the bwt from an information theoretic point of view. they described implementation techniques for data compression algorithms based on the bwt, and developed a program with a better compression rate.in this paper we improve upon these previous results on the bw-algorithm. based on the context tree model, we consider the speci_c statistical properties of the data at the output of the bwt. we describe six important properties, three of which have not been described elsewhere. these considerations lead to modifications of the coding method, which in turn improve the coding efficiency. we shortly describe how to compute the bwt with low complexity in time and space, using suffix trees in two different representations. finally, we present experimental results about the compression rate and running time of our method, and compare these results to previous achievements. more references on the methods described in this paper can be found in [1, 5].modifications of the burrows and wheeler data compression algorithm','Malicious design modifications'
'modifications of the euclidean algorithm are presented for determining the period from a sparse set of noisy measurements. the elements of the set are the noisy occurrence times of a periodic event with (perhaps very many) missing measurements. this problem arises in radar pulse repetition interval (pri) analysis, in bit synchronization in communications, and in other scenarios. the proposed algorithms are computationally straightforward and converge quickly. a robust version is developed that is stable despite the presence of arbitrary outliers. the euclidean algorithm approach is justified by a theorem that shows that, for a set of randomly chosen positive integers, the probability that they do not all share a common prime factor approaches one quickly as the cardinality of the set increases. in the noise-free case, this implies that the algorithm produces the correct answer with only 10 data samples, independent of the percentage of missing measurements. in the case of noisy data, simulation results show, for example, good estimation of the period from 100 data samples with 50\% of the measurements missing and 25\% of the data samples being arbitrary outliersmodifications of the euclidean algorithm for isolating periodicities from a sparse set of noisy measurements','Malicious design modifications'
'in spite of being a classical method for solving differential equations, the method of variation of parameters continues having a great interest in theoretical and practical applications, as in astrodynamics. in this paper we analyse this method providing some modifications and generalised theoretical results. finally, we present an application to the determination of the ephemeris of an artificial satellite, showing the benefits of the method of variation of parameters for this kind of problems.modifications of the method of variation of parameters','Malicious design modifications'
'modifications of the splitting method for constructing economical difference schemes','Malicious design modifications'
'modifications of the wolfe line search rules to satisfy second-order optimality conditions in unconstrained optimization','Malicious design modifications'
'the inherent uncertainty pervasive over the real world often forces business decisions to be made using uncertain data. the conventional relational model does not have the ability to handle uncertain data. in recent years, several approaches have been proposed in the literature for representing uncertain data by extending the relational model, primarily using probability theory. the aspect of database modification, however, has not been addressed in prior research. it is clear that any modification of existing probabilistic data, based on new information, amounts to the revision of one\'s belief about real-world objects. in this paper, we examine the aspect of belief revision and develop a generalized algorithm that can be used for the modification of existing data in a probabilistic relational database. the belief revision scheme is shown to beclosed,consistent, andcomplete.modifications of uncertain data','Malicious design modifications'
'an algorithm of wavelet domain data quantization aimed at improving compression efficiency is presented. threshold data selection is proposed as more effective uniform quantization modification than zero-zone increasing. to fit adaptively threshold value to local image features, the estimation of significance expectation for each wavelet coefficient was included into thresholding procedure. the remaining data are uniformly quantized without any changes of bin boundaries.as a result, more effective low-cost quantization scheme was constructed. it allows significantly increase image compression efficiency. experimental rate-distortion curve shows the same distortion for decreased bit rates even up to 20\% in comparison to standard uniform quantization. such quantization technique was applied in wavelet coder with optimized schemes of decomposition and zerotree based coding. its compression efficiency is competitive with the most efficient methods across all natural images tested.modifications of uniform quantization applied in wavelet coder','Malicious design modifications'
'in different river catchments in europe, pesticide concentrations in surface waters frequently exceed the standards, possibly resulting in negative impacts on aquatic fauna and flora. pesticides can enter river systems both immediately after application, i.e. as a direct loss, or with some time delay due to runoff or leaching. we define a direct loss as the sum of point losses and drift losses on an application day that will reach the river immediately after or during application. point losses are due to the clean-up of spray equipment, leaking tools, waste water treatment plants etc. different studies demonstrated the importance of direct losses. in small river systems, their contribution accounts for 30 to 90\% of the pesticide load to surface water. as many studies and models only partly take into account these direct losses or even not at all, we attempted to model the dynamic occurrence of pesticides also coming from these sources. for this purpose, some modifications and extensions to the swat (soil and water assessment tool) model were made. special attention was paid to closing mass balances and implementing an estimator for total direct losses, drift and point losses. to verify the modifications we focused on the use of the herbicide atrazine in the nil, a small and hilly river basin in the centre of belgium. the modified swat code resulted in a better correspondence between measured and simulated atrazine concentrations and loads, in particular for direct losses. for the year 1998, the nash-sutcliffe coefficient improved from a value of -2.63 to 0.66. in addition, the modelling results of the test case revealed that the contribution of drift losses to the total pesticide load in the river system is rather small: even without a \'non spray zone\', they account for only 1\% of the total load. point sources, on the other hand, contribute for 22\% up to 70\% of the pesticide load and need to be considered in pesticide pollution management. the resulting model needs further testing for other pesticides and other catchments. in future, the model can be used for comparison of different measures that can be taken to minimise pesticide fluxes towards river systems and in performing realistic risk assessments.modifications to the swat code for modelling direct pesticide losses','Malicious design modifications'
'several important topological concepts such as regularity, local compactness, and local boundedness are investigated in the category of lattice-valued convergence spaces. these definitions coincide with the usual ones in familiar categories.modifications','Malicious design modifications'
'given a sparse symmetric positive definite matrix $\\mathbf{aa}\\tr$ and an associated sparse cholesky factorization $\\mathbf{ldl}\\tr$ or $\\mathbf{ll}\\tr$, we develop sparse techniques for updating the factorization after either adding a collection of columns to a or deleting a collection of columns from a. our techniques are based on an analysis and manipulation of the underlying graph structure, using the framework developed in an earlier paper on rank-1 modifications [t. a. davis and w. w. hager, siam j. matrix anal. appl., 20 (1999), pp. 606--627]. computationally, the multiple-rank update has better memory traffic and executes much faster than an equivalent series of rank-1 updates since the multiple-rank update makes one pass through l computing the new entries, while a series of rank-1 updates requires multiple passes through l.multiple-rank modifications of a sparse cholesky factorization','Malicious design modifications'
'mumps evolving: modifications to the standard','Malicious design modifications'
'the fuzzy c-means (fcm) algorithm and various modifications of it with focus on practical applications in both industry and science are discussed. the general methodology is presented, as well as some well-known and also some less known modifications. it is demonstrated that the simple structure of the fcm algorithm allows for cluster analysis with non-typical and implicitly defined distance measures. examples are residual distance for regression purposes, prediction sorting and penalised clustering criteria. specialised applications of fuzzy clustering to be used for a sequential clustering strategy and for semi-supervised clustering are also discussed.new modifications and applications of fuzzy c-means methodology','Malicious design modifications'
'this paper proposes a framework based on defeasible logic (dl) to reason about normative modifications. we show how to express them in dl and how the logic deals with conflicts between temporalised normative modifications. some comments will be given with regard to the phenomenon of retroactivity.norm modifications in defeasible logic','Malicious design modifications'
'we describe two main classes of one-sided trigonometric and hyperbolic jacobi-type algorithms for computing eigenvalues and eigenvectors of hermitian matrices. these types of algorithms exhibit significant advantages over many other eigenvalue algorithms. if the matrices permit, both types of algorithms compute the eigenvalues and eigenvectors with high relative accuracy. we present novel parallelization techniques for both trigonometric and hyperbolic classes of algorithms, as well as some new ideas on how pivoting in each cycle of the algorithm can improve the speed of the parallel one-sided algorithms. these parallelization approaches are applicable to both distributed-memory and shared-memory machines. the numerical testing performed indicates that the hyperbolic algorithms may be superior to the trigonometric ones, although, in theory, the latter seem more natural.novel modifications of parallel jacobi algorithms','Malicious design modifications'
'dependence analysis on an extended finite state machine (efsm) representation of the requirements of a system under test has been used in requirements-based regression testing for regression test suite (rts) reduction (reducing the size of a given test suite by eliminating redundancies), for rts prioritization (ordering test cases in a given test suite for early fault detection) or for rts selection (selecting a subset of a test suite covering the identified dependencies). these particular uses of dependence analysis are based on definitions of various types of control and data dependencies (between transitions in an efsm) caused by a given set of modifications on the requirements. this abstract considers the definitions of data dependencies, gives examples of incompleteness of existing definitions, and presents insights on completing these definitions.on capturing effects of modifications as data dependencies','Malicious design modifications'
'on reducing transitions through data modifications','Malicious design modifications'
'for the logic design of digital controllers, efficient synthesis algorithms have been published. most of these assume, that a completely new hardware consisting of the fewest number of components has to be built. the implementation of those algorithms form an important part of a cad system for logic design. in order to be accepted in practice, a cad system has to take into account additional requirements. one of these concerns with the minimization of hardware changes caused by subsequent modifications of the control task which often become necessary after the completion of the hardware implementation. this paper describes a method which solves this problem for an universal structure of microprogrammed controllers. this method is implemented as a module of the cad program system loge. loge supports the logic design of hardwired, microprogrammed, and microprocessor based controllers. loge already proved to be a powerful design tool in several industrial applications.optimization of the influence of problem modifications on given microprogrammed controllers','Malicious design modifications'
'we present new pragmatic constructs for easing programming in visual graph rewriting programming languages. the first is a modification to the rewriting process for nodes the host graph, where nodes specified as \'once only\' in the lhs of a rewrite match at most once with a corresponding node in the host graph. this reduces the previously common use of tags to indicate the progress of matching in the graph. the second modification controls the application of lhs graphs, where those specified as \'single match\' are tested against the host graph one time only. this reduces the need for control flags to indicate the progress of execution of transformations.pragmatic graph rewriting modifications','Malicious design modifications'
'propagating modifications to mobile policies','Malicious design modifications'
'in this paper we present a methodology to semi-automatically propagate semantic modifications on local schemas of a federated database to the integrated schema. our approach is based on a declarative schema integration methodology, called sim, which accomplishes schema integration by resolving a set of equivalence correspondences between arbitrarily complex local subschemas. this paper extends this methodology with a more diversified set of semantic correspondences and with an approach to integrate schemas incrementally. on this basis, local schema changes can be modelled as semantic correspondences between subschemas, which extend or alter the correspondences that have led to the existing integrated schema. propagating these changes to the integrated schema then amounts in incrementally revising the affected original schema correspondences and reintegrating the affected portions of the integrated schema.propagation of sematic modifications to an integrated schema','Malicious design modifications'
'propagation of structural modifications to an integrated schema','Malicious design modifications'
'this paper deals with changes in existing it systems resulting from rfid-based process modifications in maritime container logistics. the article demonstrates how a soa enables this it adoption by functioning as a software layer between process and legacy system to enable flexible processes. in addition, the paper shows how we applied the service principles during the design and the implementation phase and perform the implementation by developing a physical miniature prototype to visually show how the modifications can easily be implemented by service-based computing.realizing process modifications in container terminals with soa','Malicious design modifications'
'this work is concerned with the modification of the gray level or color distribution of digital images. a common drawback of classical methods aiming at such modifications is the revealing of artefacts or the attenuation of details and textures. in this work, we propose a generic filtering method enabling, given the original image and the radiometrically corrected one, to suppress artefacts while preserving details. the approach relies on the key observation that artefacts correspond to spatial irregularity of the so-called transportation map, defined as the difference between the original and the corrected image. the proposed method draws on the nonlocal yaroslavsky filter to regularize the transportation map. the efficiency of the method is shown on various radiometric modifications: contrast equalization, midway histogram, color enhancement, and color transfer. a comparison with related approaches is also provided.removing artefacts from color and contrast modifications','Malicious design modifications'
'we consider the following (re)optimization problem: given a minimum-cost hamiltonian cycle of a complete non-negatively real weighted graph g=(v,e,c) obeying the strengthened triangle inequality (i.e., for some strength factor$\\frac{1}{2} \\leq \\beta &lt;1$, we have that &#8704;x,y,z&#8712;v, c(x,y)&#8804;&#946;(c(x,z)&#43;c(y,z))), and given a set of k edge weight modifications producing a new weighted graph still obeying the strengthened triangle inequality, find a minimum-cost hamiltonian cycle of the modified graph. this problem is known to be np-hard already for a single edge weight modification. however, in this case, if both the input and the modified graph obey the strengthened triangle inequality and the respective strength factors are fixed (i.e., independent of |v|), then it has been shown that the problem admits a ptas (which just consists of either returning the old optimal cycle, or instead computing -- for finitely many inputs -- a new optimal solution from scratch, depending on the required accuracy in the approximation). in this paper we first extend the analysis of the ptas to show its applicability for all k=o(1), and then we provide a large set of experiments showing that, in most practical circumstances, altering (uniformly at random) even several edge weights does not affect the goodness of the old optimal solution.reoptimizing the strengthened metric tsp on multiple edge weight modifications','Malicious design modifications'
'algorithms are developed for computing the coefficients in the three-term recurrence relation of repeatedly modified orthogonal polynomials, the modifications involving division of the orthogonality measure by a linear function with real or complex coefficient. the respective gaussian quadrature rules can be used to account for simple or multiple poles that may be present in the integrand. several examples are given to illustrate this.repeated modifications of orthogonal polynomials by linear divisors','Malicious design modifications'
'the work studies the possibilities of metallic materials hardening, especially carbon steel and low alloyed steel, presenting the novelty elements related to the technological procedure, experimental installation, and study of quality, study of thermal processing, with a view to obtaining and determining the characteristics and structure of the superficial layers, the influence of the main metallurgical and technological factors. the originality of the suggested method of superficial hardening of plastic deformation tools by phase zone modification consists in the followings: the plastic deformation tools manufactured of low alloyed steels (c&lt;0, 3\%) are packed in boxes with hardening mixtures (50\% mixtures of metallic powders of ni, cr, v, mo + 50\% carburising mixtures); hardening is achieved by induction, being made concomitantly with the enrichment with carbon of the superficial layers and a micro-alloying of the layers with hard elements, achieving complex carbides.research regarding the superficial hardening by phase zone modifications','Malicious design modifications'
'schema modifications in the lispo2 persistent object-oriented language','Malicious design modifications'
'the paper presents modifications of the hill cipher generating dynamic encryption key matrix. the presented modifications provide better security and encryption quality than known ones. it also presents a protocol for secure key exchange similar to diffie-hellman\'s one, but using commutative matrices and matrix multiplication operations instead of time-consuming exponentiation.secure hill cipher modifications and key exchange protocol','Malicious design modifications'
'the performance of content-based image retrieval systems (cbirs) is typically evaluated via benchmarking their capacity to match images despite various generic distortions such as crops, rescalings or picture in picture (pip) attacks, which are the most challenging. distortions are made in a very generic manner, by applying a set of transformations that are completely independent from the systems later performing recognition tasks. recently, studies have shown that exploiting the finest details of the various techniques used in a cbirs offers the opportunity to create distortions that dramatically reduce the recognition performance. such a security perspective is taken in this paper. instead of creating generic pip distortions, it proposes a creation scheme able to delude the recognition capabilities of a cbirs that is representative of state of the art techniques as it relies on sift, high-dimensional k-nearest neighbors searches and geometrical robustification steps. experiments using 100,000 real-world images confirm the effectiveness of these security-oriented pip visual modifications.security-oriented picture-in-picture visual modifications','Malicious design modifications'
'this paper considers a number of modifications that can be applied to the congestion control algorithm of a tcp sender without requiring the co-operation either of the network or of the receiver, analyzing their impact on the performance of the protocol. we use a theoretical approach based on the use of queueing networks for the description of the protocol dynamics and a fixed point approximation to derive the working point of the ip network. our results show that in presence of short lived connections the impact of the transient behavior of tcp on the network performance is dominant, and major performance improvements can be obtained only if the transient behavior is improved.sender-side tcp modifications','Malicious design modifications'
'most current algorithms for blind steganalysis of images are based on a two-stages approach: first, features are extracted in order to reduce dimensionality and to highlight potential manipulations; second, a classifier trained on pairs of clean and stego images finds a decision rule for these features to detect stego images. thereby, vector components might vary significantly in their values, hence normalization of the feature vectors is crucial. furthermore, most classifiers contain free parameters, and an automatic model selection step has to be carried out for adapting these parameters. however, the commonly used cross-validation destroys some information needed by the classifier because of the arbitrary splitting of image pairs (stego and clean version) in the training set. in this paper, we propose simple modifications of normalization and for standard cross-validation. in our experiments, we show that these methods lead to a significant improvement of the standard blind steganalyzer of lyu and farid.simple algorithmic modifications for improving blind steganalysis performance','Malicious design modifications'
'we study the problem of modifying a given elimination ordering through local reorderings. we present new theoretical results on equivalent orderings, including a new characterization of such orderings. based on these results, we define the notion of k-optimality for an elimination ordering, and we describe how to use this in a practical context to modify a given elimination ordering to obtain less fill. we experiment with different values of k, and report on percentage of fill that is actually reduced from an already good initial ordering, like minimum degree.simple and efficient modifications of elimination orderings','Malicious design modifications'
'simple modifications of monotonicity-preserving limiters','Malicious design modifications'
'in this paper, carnap\'s modal logic c is reconstructed. it is shown that the carnapian approach enables us to create some epistemic logics in a relatively straight-forward way. these epistemic modifications of c are axiomatized and one of them is compared with intuitionistic logic. at the end of the paper, some connections between this epistemic logic and medvedev\'s logic of finite problems and inquisitive semantics are shortly discussed.some modifications of carnap\'s modal logic','Malicious design modifications'
'low-dimensional simplex evolution ldse is a real-coded evolutionary algorithm for global optimization. in this paper, we introduce three techniques to improve its performance: low-dimensional reproduction ldr, normal struggle ns and variable dimension vd. ldr tries to preserve the elite by keeping some of its randomly chosen components. ldr can also help the offspring individuals to escape from the hyperplane determined by their parents. ns tries to enhance its local search capability by allowing unlucky individual search around the best vertex of &lt;italic&gt;m&lt;/italic&gt;-simplex. vd tries to draw lessons from recent failure by making further exploitation on its most promising sub-facet. numerical results show that these techniques can improve the efficiency and reliability of ldse considerably. the convergence properties are then analysed by finite markov chains. it shows that the original ldse might fail to converge, but modified ldse with the above three techniques will converge for any initial population. to evaluate the convergence speed of modified ldse, an estimation of its first passage time of reaching the global minimum is provided.some modifications of low-dimensional simplex evolution and their convergence','Malicious design modifications'
'we present a graph drawing algorithm that was developed as an extension of the hierarchical layout method of sugiyama [4]. the main features of the algorithm are that the edges are orthogonal and that each edge has at most two bends.some modifications of sugiyama approach','Malicious design modifications'
'some modifications of the algorithm for constructing classification trees that are helpful in the processing of noise spectra in the technical diagnostics of the nuclear power plant are presented.some modifications of the algorithm for construction of classification trees','Malicious design modifications'
'structural modifications in net theory','Malicious design modifications'
'it is well known that limit periodic continued fractions can be accelerated by modifications using converging factors. in this paper the repeated use of modifications is studied in the case of a constant converging factor and in the case of aitken\'s @d^2 process. based on these modifications, a method for controlling the error is proposed.successive modifications of limit periodic continued fractions','Malicious design modifications'
'robustness and preservation of stability and synchronization in the presence of structural changes is an important issue in the study of chaotic dynamical systems. in this work we present a methodology to establish conditions for preservation of stability in dynamical system in terms of linear matrix polynomial evaluation. the idea is to construct a group of dynamical transformations under which stability is retained along the stable, unstable and synchronization manifolds using simultaneous schur decompositions.synchronization preservation under linear polynomial modifications','Malicious design modifications'
'based on an existing modeller that can generate realistic and controllable whole-body models, we introduce our modifier synthesizer for obtaining higher level of manipulations of body models by using parameters such as fat percentage and hip-to-waist ratio. users are assisted in automatically modifying an existing model by controlling the parameters provided. on any synthesized model, the underlying bone and skin structure is properly adjusted, so that the model remains completely animatable using the underlying skeleton. based on statistical analysis of data models, we demonstrate the use of body attributes as parameters in controlling the shape modification of the body models while maintaining the distinctiveness of the individual as much as possible.synthesizing animatable body models with parameterized shape modifications','Malicious design modifications'
'a way out of the retransmission quagmire.tcp thin-stream modifications','Malicious design modifications'
'recently there has been a renewed interest in performing topological modifications on hexahedral meshes to enable clean up, mesh improvement, generation of more complex hexahedral topologies, and a deepened understanding of methods for producing hexahedral topologies in increasingly complex geometries. additionally, generation of all-hexahedral topologies in arbitrary models remains an open-problem in the computational geometry community. in this paper we provide surveys of important research efforts in local hexahedral topology modification, and provide some formalization of many of these methods. we also provide some additional proofs giving credance to the community held notion that these topology modifications are feasible despite the historic difficulty in developing the methodologies for performing the modifications. additionally, some formalization of modification operations will be provided for hexahedral sheet-based methods and a demonstration of how these operations are related to the atomic operations proposed by tautges et&#x00a0;al.topological modifications of hexahedral meshes via sheet operations: a theoretical study','Malicious design modifications'
' we are concerned with the automatic semantic interpretation of legal modificatory provisions. we propose a novel approach which pairs deep syntactic parsing and a fine-grained taxonomy of legal modifications. although still in a developmental stage, the implemented system can be used to annotate with meta-information modificatory provisions of normainrete documents. towards semantic interpretation of legal modifications through deep syntactic analysis','Malicious design modifications'
'genes acquire many changes and modifications during evolution producing many orthologous and paralogous. one of the main challenges for biologist is tracing back the origin of modified genes in the genomes of various organisms. tracing genes modifications and developments from the ancestors to offsprings is however a massive task for biologists. thus, computer programs can help biologists tracking down those changes through sequence alignment. in this paper, we present an algorithm, called genetracer and based on sequence alignment, that, given two ancestor sequences and their offspring one, tracks down genes modification in the ancestor sequences and finds related parts of each ancestor in the offspring one.tracking genes modifications in the pedigree through genetracer algorithm','Malicious design modifications'
'system modeling is a widely used technique to modelstate-based systems. system models are frequently largeand complex and are hard to understand. in addition,they are frequently modified because of specificationchanges. understanding the effect of these changes onthe model and the system may be very difficult for largemodels. in this paper, we present an approach that maysupport understanding the effect of model modifications.the goal is identify these parts of the model that mayexhibit different behavior because of the modification. inthis approach, the difference between the original modeland the modified model is identified and then affectedparts of the model are computed based on modeldependence analysis. our initial experience shows thatthe approach may be helpful in understanding the effectof modifications on the system.understanding modifications in state-based models','Malicious design modifications'
'this paper proposes some variants of temporal defeasible logic (tdl) to reason about normative modifications. these variants make it possible to differentiate cases in which, for example, modifications at some time change legal rules but their conclusions persist afterwards from cases where also their conclusions are blocked.variants of temporal defeasible logics for modelling norm modifications','Malicious design modifications'
'we propose a method of speech watermarking based on modifications to line spectral frequencies (lsfs) of original speech. lsfs were derived from each frame with linear prediction (lp) analysis and watermarks were embedded into them by using the quantization index modulation (qim) of different quantization steps. we took into consideration inaudibility and robustness that were influenced by minor modifications to lsfs. the proposed approach was evaluated with two kinds of experiments with respect to inaudibility and robustness against different speech codecs and general processing. the results from the evaluations revealed that the proposed approach not only had high rate of bit detection while keeping the original sound quality undistorted but also good robustness against general speech processing.watermarking method for speech signals based on modifications to lsfs','Malicious design modifications'
'this paper discusses techniques for embedding data into three-dimensional (3-d) polygonal models of geometry. given objects consisting of points, lines, (connected) polygons, or curved surfaces, the algorithms described in produce polygonal models with data embedded into either their vertex coordinates, their vertex topology (connectivity), or both. such data embedding can be used, for example, for copyright notification, copyright protection, theft deterrence, and inventory of 3-d polygonal models. a description of the background and requirements is followed by a discussion of where, and by what fundamental methods, data can be embedded into 3-d polygonal models. the paper then presents several data embedding algorithms, with examples, based on these fundamental methods. by means of these algorithms and examples, we show that the embedding of data into 3-d polygonal models is a practicable techniquewatermarking three-dimensional polygonal models through geometric and topological modifications','Malicious design modifications'
'this paper deals with the problem of decision making in the context of forward collision mitigation system design. the authors present a multilevel collision mitigation (cm) approach that allows a flexible tradeoff between potential benefit and the risk associated with driver acceptability and product liability. due to its practical relevance, algorithms that allow for an efficient incorporation of both sensor and prediction uncertainties are further outlined. the performance tradeoffs that come along with different parameterizations are investigated by means of stochastic simulations on three dangerous traffic situations, namely 1) rear-end collisions due to an unexpected braking, 2) cutting-in vehicles, and 3) crossing traffic at intersections. the results show that an overly conservative cm system sacrifices much of its potential benefit. however, it is pointed out that the vision of accident-free driving can be achieved only through cooperative driving strategiesa multilevel collision mitigation approach&#8212;its situation assessment, decision making, and performance tradeoffs','Malware and its mitigation'
'as a wealth of data services is becoming available on the web, building and querying web applications that effectively integrate their content is increasingly important. however, schema integration and ontology matching with the aim of registering data services often requires a knowledge-intensive, tedious, and error-prone manual process. we tackle this issue by presenting a bottom-up, semi-automatic service registration process that refers to an external knowledge base and uses simple text processing techniques in order to minimize and possibly avoid the contribution of domain experts in the annotation of data services. the first by-product of this process is a representation of the domain of data services as an entity-relationship diagram, whose entities are named after concepts of the external knowledge base matching service terminology rather than being manually created to accommodate an application-specific ontology. second, a three-layer annotation of service semantics (service interfaces, access patterns, service marts) describing how services &#8220;play&#8221; with such domain elements is also automatically constructed at registration time. when evaluated against heterogeneous existing data services and with a synthetic service dataset constructed using google fusion tables, the approach yields good results in terms of data representation accuracy. we subsequently demonstrate that natural language processing methods can be used to decompose and match simple queries to the data services represented in three layers according to the preceding methodology with satisfactory results. we show how semantic annotations are used at query time to convert the user\'s request into an executable logical query. globally, our findings show that the proposed registration method is effective in creating a uniform semantic representation of data services, suitable for building web applications and answering search queries.a bottom-up, knowledge-aware approach to integrating and querying web data services','Management and querying of encrypted data'
'in this paper we address the important issue of establishing a formal background for the management of semistructured data. we define a data model and propose an algebra for xml. the algebra, clearly inspired by relational algebra, is quite intuitive; nevertheless it is able to represent most of xquery expressions.a data model and an algebra for querying xml documents','Management and querying of encrypted data'
'indexing video data is essential for providing content based access. in this paper, we consider how database technology can offer an integrated framework for modeling and querying video data. as many concerns in video (e.g., modeling and querying) are also found in databases, databases provide an interesting angle to attack many of the problems. from a video applications perspective, database systems provide a nice basis for future video systems. more generally, database research will provide solutions to many video issues even if these are partial or fragmented. from a database perspective, video applications provide beautiful challenges. next generation database systems will need to provide support for multimedia data (e.g., image, video, audio). these data types require new techniques for their management (i.e., storing, modeling, querying, etc.). hence new solutions are significant.this paper develops a data model and a rule-based query language for video content based indexing and retrieval. the data model is designed around the object and constraint paradigms. a video sequence is split into a set of fragments. each fragment can be analyzed to extract the information (i.e., symbolic descriptions) of interest that can be put into a database. this database can then be searched to find information of interest. two types of information are considered: (1) the entities (i.e., objects) of interest in the domain of a video sequence, (2) video frames which contain these entities. to represent these information, our data model allows facts as well as objects and constraints. we present a declarative, rule-based, constraint query language that can be used to infer relationships about information represented in the model. the language has a clear declarative and operational semantics.a database approach for modeling and querying video data','Management and querying of encrypted data'
'a crucial prerequisite for the deployment and success of peer-to-peer data management applications is the availability of metadata in a way that makes it easy to access and combine data from different sources and domains. in this paper, we argue for a unified and distributed infrastructure providing a repository for semantic data by offering location transparency and advanced query services. after discussing the challenges of such an approach, we present our solution which applies extended sparql-like query features for dealing with large and possibly heterogeneous data sets. we focus on the integration into efficient distributed query processing and evaluate our approach in a series of experiments.a dht-based infrastructure for ad-hoc integration and querying of semantic data','Management and querying of encrypted data'
'the richness of semi-structured data allows data of varied and inconsistent structures to be stored in a single database.such data can be represented as a graph, and queries can beconstructed using path expressions, which describe traversals through the graph.instead of providing optimal performance for a limitedrange of path expressions, we propose a mechanism whichis shown to have consistent and high performance for pathexpressions of any complexity, including those with descendant operators (path wildcards). we further detail mechanisms which employ our index to perform more complexprocessing, such as evaluating both path expressions containing links and entire (sub) queries containing path basedpredicates. performance is shown to be independent of thenumber of terms in the path expression, even where thesecontain wildcards. experiments show that our index is fasterthan conventional methods by up to two orders of magnitudefor certain query types, is small, and scales well.a fast and versatile ath index for querying semi-structured data','Management and querying of encrypted data'
'a foundation for capturing and querying complex multidimensional data','Management and querying of encrypted data'
'genealogy information is becoming increasingly abundant in light of modern genetics and the study of diseases and risk factors. as the volume of this structured pedigree data expands, there is a pressing need for better ways to manage, store, and efficiently query this data. building on recent advances in semistructured data management and proven relational database technology, we propose a general-purpose pedigree query language (pql) and evaluation framework for elegantly expressing and efficiently evaluating queries on this data. in this paper, we describe how the problem of modeling and querying pedigree data differs from xml, present an overview of pql, and present efficient evaluation for key parts of the language. experimental results using real data show significant (>850\%) performance improvement for complex queries over na&#239;ve evaluation.a framework for querying pedigree data','Management and querying of encrypted data'
'any attribute in rough relational database (rrdb) can be multi-valued, and has an indiscernibility relation in its domain. currently, the research on rough data querying mainly discussed some simple select-querying. that is, selecting the tuples whose attribute\'s value is equal to a constant from a single table. the main idea of its implementation is to expand the original search conditions according to the indiscernibility relation in attribute\'s domain. because the search conditions after being expanded need more calculation, the querying becomes very slow. in this paper, we present a solution called rough data querying based on encoding. firstly encode the data of multi-valued attribute into the single-valued data according to the indiscernibility relation in attribute\'s domain, and then execute the querying on the single-valued data despite the indiscernibility relation to make the implementation of rough data querying much simpler and more efficient.a high efficiency approach to querying rough data','Management and querying of encrypted data'
'in this paper, we present the design of system of databases (sydb). we also give the design and implementation of a java api for global querying and updates on the sydb. the databases may be heterogeneous. the api allows for queries and updates that have global references to schema elements of multiple databases to be executed in a seamless manner. the api can be used to develop collaborative applications that need access to several independent databases on the network. one such collaborative application, called the calendar application, is illustrated in the paper. in this application each individual keeps their schedule information in their personal database. the users can schedule meetings with others, view others schedules, cancel meetings, etc. we implement the api using direct jdbc connections to databases.a java api for global querying and updates for a system of databases','Management and querying of encrypted data'
'this paper demonstrates the generation of a linear-time query-answering algorithm based on the constructive proof of higman\'s lemma by murthy and russell [proceedings of the 5th ieee symposium on logic in computer science, 1990, p. 257-267]. the target problem is linear-time evaluation of a fixed disjunctive monadic query on an indefinite database over linearly ordered domains, first posed by van der meyden [proceedings of the 11th acm sigact-sigmod-sigart symposium on principles of database systems, 1992, p. 331-345]. van der meyden showed the existence of a linear-time algorithm, but an explicit construction has, until now, not been reported.a linear time algorithm for monadic querying of indefinite data over linearly ordered domains','Management and querying of encrypted data'
'we describe the development of the cerealab ontology, an ontology of molecular and phenotypic cereals data, that allows identifying the correlation between the phenotype of a plant with its molecular data. it is realised by integrating public web databases with the database developed by the research group of the cerealab laboratory. integration is obtained semi&#45;automatically by using the mediator environment for multiple information sources &#40;momis&#41; system, a data integration system developed by the database group of the university of modena and reggio emilia, and allows querying the integrated data sources regardless of the specific languages of the source databases.a mediator&#45;based approach to ontology generation and querying of molecular and phenotypic cereals data','Management and querying of encrypted data'
'the emerging trend of outsourcing database to third parties motivates the research of protecting sensitive data from database service providers. the bucket-based method is one of the feasible methods to store those data encrypted. since a bucket may contain many different attribute values, this method also introduces some false query results when performing queries over encrypted databases. strategies are needed to balance the data secrecy and query efficiency. in this paper, we use the expected number of false hits in query results to measure the query efficiency and formulate the tradeoff between security and efficiency as an optimization problem. constrained by a given range of the bucket set entropies, a local search based method is applied to regulate the elements between neighboring bucket pairs and reconstruct a new set of buckets with fewer false hits. we validate this method with some experiments on range queries. experiment results show that it can decrease the number of false hits in two different range query behaviors.a method for reducing false hits in querying encrypted databases','Management and querying of encrypted data'
'the encryption mechanism is the common method to protect the sensitive data in database from various attacks. but once the data in databases is encrypted, the efficiency of dbms will fall. to enhance the security of the sensitive data and to improve the efficiency of the encrypted database, one effective method is to build index on the encrypted field, and firstly executes the query on the index to get fewer records to decrypt in this paper. this paper proposes an index mechanism of bucket index on the character data, which has close relationships with all of its characters. the index tries to translate the character string into a numeric data, on which the primary query will be processed to filtrate the records roughly. only the rest records need to be decrypted, and it will save lots of time. so the cipher index can improve the query efficiency greatly.a method of bucket index over encrypted character data in database','Management and querying of encrypted data'
'the encryption mechanism is an effective way to protect the sensitive data in database from various attacks. when the data is encrypted, the query performance degrades greatly. how to query encrypted data efficiently becomes a challenge. in this paper, a scheme to support query over encrypted data is proposed. firstly, we extend two-phase framework to complete query, constructing different indexes for different data types. a new method to construct bucket index for numeric data is proposed and bloom filter compression algorithm is used for the character string, which will be saved in database as a numeric data. we analyzed the false positive probability of bloom filter to get optimal parameters. experiment results show the performance of our scheme has improved.a method of query over encrypted data in database','Management and querying of encrypted data'
'data warehpuses store large volumes of data according to a multidimensional model dimensions representing different axesof analysis. olap systems (online analytical processing) provide the ability to interactively explore the data warehouse. rising volumes and complexity of data favor the use of morepowerful distributed computing architectures. computing grids in particular are built for decentralized management of heterogeneousdistributed resources. their lack of centralized control however conflicts with classic centralized data warehouse models. to take advantage of a computing grid infrastracture to operate a data warehouse several problems need to be solved. first the data warehouse must be uniquely indentified and judiciously partitioned to allow effecient distribution querying and exchange among the nodes of the grid. we propose a data model based on \"chunks\" as atomic entities of warehouse data that can be uniquely identified. we then build contiguous blocks of these chunks to obtain suitable fragments of the data warehouse. the fragments stored on each grid node must be indexed in a uniform way to effectively interact with existing gridservices. our indexing structure consists of a lattice structure mapping queries to warehouse fragments and specialized spatial index structure formed by x-trees providing the information neccessary for optimized query evaluation plans.a model for distributing and querying a data warehouse on a computing grid','Management and querying of encrypted data'
'the key problem in the reverse analysis of encrypted programmable logic devices (plds) using logic analysis techniques is how to acquire effective and self-contained data set, especially for sequential plds. an efficient data acquisition algorithm is presented which suits for large scale and multi-state synchronous plds. the algorithm builds non-complete state graph and finds the shortest path for state migration from initial state to each active state. the algorithm has ideal time and space cost and has been used in our pld reverse analysis system.a novel data acquisition algorithm for the reverse analysis of encrypted synchronous plds','Management and querying of encrypted data'
'with the rapid growth of data, it is desirable to outsource data on remote storage server. the emergency of cloud computing makes the dream true and more and more sensitive data are being centralized into cloud for sharing. since the public cloud server cannot be fully trusted in protecting them, encryption is a promising way to keep confidentiality but leads to high communication and computation overhead for some useful data operations. searchable encryption initiated by song et al. provides an efficient solution to support for keyword-based search directly on encrypted data. nevertheless, existing work depends on key sharing among authorized users, which inevitably causes the risks of key exposure and abuse. in this paper, the keyword search over encrypted data with differential privileges is addressed. we provide a novel framework for secure outsourcing and sharing of encrypted data on hybrid cloud. the framework is full-featured: i) it enables authorized users to perform keyword-based search directly on encrypted data without sharing the same private key, ii) it provides two-layered access control to achieve fine-grained sharing of encrypted data. the security analysis shows that the proposed generic construction satisfies the requirements of message privacy and keyword privacy.a novel framework for outsourcing and sharing searchable encrypted data on hybrid cloud','Management and querying of encrypted data'
'cloud computing technologies become more and more popular every year, as many organizations tend to outsource their data utilizing robust and fast services of clouds while lowering the cost of hardware ownership. although its benefits are welcomed, privacy is still a remaining concern that needs to be addressed. we propose an efficient privacy-preserving search method over encrypted cloud data that utilizes minhash functions. most of the work in literature can only support a single feature search in queries which reduces the effectiveness. one of the main advantages of our proposed method is the capability of multi-keyword search in a single query. the proposed method is proved to satisfy adaptive semantic security definition. we also combine an effective ranking capability that is based on term frequency-inverse document frequency (tf-idf) values of keyword document pairs. our analysis demonstrates that the proposed scheme is proved to be privacy-preserving, efficient and effective.a practical and secure multi-keyword search method over encrypted cloud data','Management and querying of encrypted data'
'xquery is a powerful and convenient language that is designed for querying the data in xml documents. in this paper, we address how to optimally query encrypted xml documents using xquery, with the key point being how to eliminate redundant decryption so as to accelerate the querying. we propose a processing model that can automatically and appropriately translate the xquery statements for encrypted xml documents. furthermore, we show that xml schema is significantly associated with queries over xml documents. the implementation and experimental results demonstrate the practicality of the proposed model.a processing model for the optimal querying of encrypted xml documents in xquery','Management and querying of encrypted data'
'in this paper we introduce and formally define query by browsing (qbb), a scalable, relationally complete visual query language based on the desktop user interface paradigm and tuple relational calculus that allows the formulation of complex queries over relational, entity-relationship, object-oriented and xml data sources on a variety of handheld and desktop platforms. it is to our knowledge the first visual query language to combine the important characteristics of usability, scalability, expressive power and flexibility. we support these claims by demonstrating the similarity of the qbb paradigm to the popular desktop user interface paradigm, by relating it to relational calculus and relational algebra and by describing chiromancer ii, a web-based implementation of the qbb paradigm for handheld devices. we also discuss ways in which non-relational sources can be represented and queried and compare qbb to related work in the area of visual query languages for a variety of data models. we finally offer conclusions and thoughts for future work.a relationally complete visual query language for heterogeneous data sources and pervasive querying','Management and querying of encrypted data'
'a schema-based approach to modeling and querying www data','Management and querying of encrypted data'
'using database encryption to protect data in some situations where access control is not solely enough is inevitable. database encryption provides an additional layer of protection to conventional access control techniques. it prevents unauthorized users, including intruders breaking into a network, from viewing the sensitive data. as a result data keeps protected even in the incident that database is successfully attacked or stolen. however, data encryption and decryption process result in database performance degradation. in the situation where all the information is stored in encrypted form, one cannot make the selection on the database content any more. data should be decrypted first, so an unwilling tradeoff between the security and the performance is normally forced. the appropriate approaches to increase the performance are methods to deal directly with the encrypted data without firstly decrypting them. this paper while introduce some methods for searching on encrypted data, provides a comparison study between current methods in terms of performance and security.a survey on querying encrypted data for database as a service','Management and querying of encrypted data'
'we present a tool called gis for indexing and querying a large database of labeled, undirected graphs. such graphs can model chemical compounds, represent contact maps constructed from 3d structure of proteins, and so forth. gis supports exact subgraph matching and approximate graph matching queries. it adopts a suite of new techniques and algorithms for (a) fast construction of disk-based indexes with small index sizes, and (b) efficient query processing with high precision of matching. during the demo, the user can index real graph datasets using a recommendation facility in gis, pose exact subgraph matching and approximate graph matching queries, and view matching graphs using the jmol browser.a tool for fast indexing and querying of graphs','Management and querying of encrypted data'
'a toolkit to facilitate the querying and integration of tabular data from semistructured documents','Management and querying of encrypted data'
'a visual approach to multimedia querying and presentation','Management and querying of encrypted data'
'we present a visual tool based in xquery for querying and exploring xml documents. the tool is based in a simple but effective visual metaphor and a results visualization technique based in fisheye trees. the application fulfills the expectations of facilitating the manipulation of xml documents for inexperienced users, offering original interaction features, improving upon similar work. as xml is one of the main metalanguages of the web, this tool may have a broad impact.a visual tool for querying and exploring xml data','Management and querying of encrypted data'
'open architectures meet the demands of increasingly complex and diverse networks by adding programmability to some or all components of the network infrastructure. the effectiveness of this programmability, however, is severely reduced in the domain of encrypted data streams. because encrypted streams are opaque, existing solutions to adapting encrypted data are limited to either performing link-level encryption across each node, or restricting adaptations to those that do not require knowledge of the content. these restrictions either reduce the functionality of the system or compromise the security of the data stream. this research examines an alternative method of adapting encrypted streams through intelligent tagging and layering of the data content. sending separately encrypted layers of the data allows for adaptation of the data stream without actually decrypting the data along any point of the connection.adapting encrypted data streams in open architectures','Management and querying of encrypted data'
'information hiding enables the seller to hide additional bits into multimedia content in an imperceptible manner. in most information hiding schemes for copyright protection, the seller always knows the embedded information uniquely identifying the buyer. however, it causes both framing issue and repudiation issue. to solve these problems, a variety of watermarking protocols have been proposed based on the encrypted information hiding scheme. in this paper, a new encrypted information hiding scheme with adaptive capacity is proposed, and it can improve the resistance against jpeg compression after decryption and detect image tampering at the information extractor. the blind information extracting employs the same threshold criterion and parameters as information embedding. the proposed encrypted information hiding scheme is a new solution to implementing the existing watermarking protocols.adaptive encrypted information hiding scheme for images','Management and querying of encrypted data'
'algebraic operations on encrypted relational databases','Management and querying of encrypted data'
'at acisp\'07, bringer et al. introduced a new protocol for achieving biometric authentication with a private information retrieval (pir) scheme. their proposal is made to enforce the privacy of biometric data of users. we improve their work in several aspects. firstly, we show how to replace the basic pir scheme they used with lipmaa\'s which has ones of the best known communication complexity. secondly, we combine it with secure sketches to enable a strict separation between on one hand biometric data which remain the same all along a lifetime and stay encrypted during the protocol execution, and on the other hand temporary data generated for the need of the authentication to a service provider. our proposition exploits homomorphic properties of goldwasser-micali and paillier cryptosystems.an authentication protocol with encrypted biometric data','Management and querying of encrypted data'
'wireless sensor networks have a wide spectrum of civil and military application that call for security. in-network data aggregation has been shown to improve scalability, prolong sensor network lifetimes and diminish computational demands. however, aggregator in the network that play the role of the data aggregation will consume much more energy than common nodes and will quit the mission in advance due to energy exhausting because it must decrypt the data first and re-encrypt them after aggregating the data, moreover, it will bring complex key management to ensure the security of corresponding keys. our scheme was designed specifically to address above problems, cluster head can aggregate the encrypted data without decrypting first and re-encrypting. our scheme can achieve the security equal to one-time pad; moreover, combined with the mechanism of dynamic cluster head recycling, this protocol can balance energy consumption of the whole network and prolong its life. furthermore, this scheme can resist the attack of node capture, improves the survivability of network. through simulation, we show that our scheme can save about 3-5 times energy consumption than basic scheme according to different density of network and average hops from node to base station.an efficient and secure aggregation of encrypted data for wireless sensor network based on dynamic cluster','Management and querying of encrypted data'
'modern exploit payloads in commercial and open-source penetration testing frameworks have grown much more advanced than the traditional shellcode they replaced. these payloads permit interactive access without launching a shell, network proxying, and many other rich features. available payload frameworks have several limitations, however. they make little use of encryption to secure delivery and communications, especially in earlier stage payloads. in addition, their richer features require a constant network connection to the penetration tester, making them unsuitable against mobile clients, such as laptops, pdas, and smart phones. this work introduces a first-stage exploit payload that is able to securely establish an encrypted channel using elgamal key agreement and the rc4 stream cipher. the key agreement implementation requires only modular exponentiation and rc4 also lends itself to an implementation requiring a very small amount of executable code. this secure channel is used to send further executable code and deliver a fully-featured interpreter to execute mission logic written in the high-level lua scripting language. this scripting environment permits secure code delivery as well as disconnected operation and execution of penetration testing mission logic.an encrypted payload protocol and target-side scripting engine','Management and querying of encrypted data'
'scalable distributed data structures (sdds) store data in a file of key-based records distributed over many storage sites. the number of storage sites utilized grows and shrinks with the storage needs of applications, but transparently to them. an application can search records by key or by content in parallel at all storage sites. the need for privacy of the data at the storage sites might require the encryption of the records. however, the scheme needs to preserve the capability to search in parallel. we propose a scheme that achieves this goal. we create a collection of additional sdds indices. we encrypt these so that we can still perform string searches performed in parallel at the storage sites. we present the scheme and evaluate its strength as well as storage and access performance.an encrypted, content searchable scalable distributed data structure','Management and querying of encrypted data'
'an extension of sql for querying graph relations','Management and querying of encrypted data'
'in this paper, we propose the simultaneous use of cryptography and transformation functions in biometric-based identification systems aiming to increase the security level of biometric data as well as the performance of these systems. additionally, we aim to keep a reasonable efficiency level of these data through the use of more elaborated classification structures, such as ensemble systems. with this proposal, we intend to have a robust and secure identification system using signature data.an investigation of ensemble systems applied to encrypted and cancellable biometric data','Management and querying of encrypted data'
'the miel++ system integrates data expressed in two different formalisms: a relational database and an xml database. the xml database is filled with data semi-automatically retrieved from the web, which have been semantically enriched according to the ontology used in the relational database. these data may be imprecise and represented as possibility distributions. the miel++ querying system scans the two databases simultaneously in a transparent way for the end-user. to scan the xml database, the miel query is translated into an xml tree query. in this paper, we propose to introduce flexibility into the query processing of the xml database, in order to take into account the imperfections due to the semantic enrichment of its data. this flexibility relies on fuzzy queries and query rewriting which consists in generating a set of approximate queries from an original query using three transformation techniques: deletion, renaming and insertion of query nodes.approximate querying of xml fuzzy data','Management and querying of encrypted data'
'obtaining fast and good quality approximations to data distributions is a problem of central interest to database management. a variety of popular database applications including, approximate querying, similarity searching and data mining in most application domains, rely on such good quality approximations. histogram based approximation is a very popular method in database theory and practice to succinctly represent a data distribution in a space efficient manner.in this paper, we place the problem of histogram construction into perspective and we generalize it by raising the requirement of a finite data set and/or known data set size. we consider the case of an infinite data set on which data arrive continuously forming an infinite data stream. in this context, we present the first single pass algorithms capable of constructing histograms of provable good quality. we present algorithms for the fixed window variant of the basic histogram construction problem, supporting incremental maintenance of the histograms. the proposed algorithms trade accuracy for speed and allow for a graceful tradeoff between the two, based on application requirements.in the case of approximate queries on infinite data streams, we present a detailed experimental evaluation comparing our algorithms with other applicable techniques using real data sets, demonstrating the superiority of our proposal.approximating a data stream for querying and estimation','Management and querying of encrypted data'
'the semantic web provides the foundation for semantic architecture to support the transparent exchange of information and knowledge among collaborating e-business organizations. recent advances in semantic web based technologies offer means for organizations to exchange knowledge in a meaningful way [1]. in spite of these developments, major challenges remain for developers of semantic web applications, such as the availability of semantic web content [2], and ontology based information retrieval [3]. in this paper, an architecture aimed at addressing these issues is presented. an easy to use annotation tool is deployed, providing a convenient mechanism for web site owners to mark up their web pages with rdf metadata. search and coordination activities are carried out by a system of multiagents designed for such environments. the architecture is demonstrated in the accommodation services domain of the australian tourism industry.architecture for automated annotation and ontology based querying of semantic web resources','Management and querying of encrypted data'
'comparing and querying the protein-protein interaction (ppi) networks of different organisms is important to infer knowledge about conservation across species. known methods that perform these tasks operate symmetrically, i.e., they do not assign a distinct role to the input ppi networks. however, in most cases, the input networks are indeed distinguishable on the basis of how the corresponding organism is biologically well characterized. in this paper a new idea is developed, that is, to exploit differences in the characterization of organisms at hand in order to devise methods for comparing their ppi networks. we use the ppi network (called master) of the best characterized organism as a fingerprint to guide the alignment process to the second input network (called slave), so that generated results preferably retain the structural characteristics of the master network. technically, this is obtained by generating from the master a finite automaton, called alignment model, which is then fed with (a linearization of) the slave for the purpose of extracting, via the viterbi algorithm, matching subgraphs. we propose an approach able to perform global alignment and network querying, and we apply it on ppi networks. we tested our method showing that the results it returns are biologically relevant.asymmetric comparison and querying of biological networks','Management and querying of encrypted data'
'the encrypted key exchange (eke) protocol is augmented so that hosts do not store cleartext passwords. consequently, adversaries who obtain the one-way encrypted password file may (i) successfully mimic (spoof) the host to the user, and (ii) mount dictionary attacks against the encrypted passwords, but cannot mimic the user to the host. moreover, the important security properties of eke are preserved&#8212;an active network attacker obtains insufficient information to mount dictionary attacks. two ways to accomplish this are shown, one using digital signatures and one that relies on a family of commutative one-way functions.augmented encrypted key exchange','Management and querying of encrypted data'
'a public-key cryptosystem is usually used for key management, in particular to session key management. the paper presents a method for handling a batch of concurrent keys with homomorphic public-key cryptosystems such as rsa, paillier and elgamal. theorematically, regardless shacham and boneh proved that it is impossible to provide batch rsa encryption of messages with a single certificate, the present result is positive when the messages are small. practically, the present method is compliant to the de facto standard ssl/tls handshake and increases the ssl system performance.batch decryption of encrypted short messages and its application on concurrent ssl handshakes','Management and querying of encrypted data'
'realising a biometric identification scheme with the constraint of storing only encrypted data is an exciting challenge. whereas a recent cryptographic primitive described by bringer &lt;em&gt;et al.&lt;/em&gt; and named error-tolerant searchable encryption achieves such a goal, the associated construction is not scalable to large databases. this paper shows how to move away from the model of bringer &lt;em&gt;et al.&lt;/em&gt; , and proposes to use symmetric searchable encryption (sse) as the baseline for biometric identification. the use of symmetric cryptography enables to achieve reasonable computational costs for each identification request. this paper also provides a realistic security model for this problem, which is stronger than the one for sse. in particular, the construction for biometric identification is resilient to statistical attacks, an aspect yet to be considered in the previous constructions of sse. as a practical example, parameters for the realisation of our scheme are provided in the case of iris recognition. biometric identification over encrypted data made feasible','Management and querying of encrypted data'
'searchable encryption schemes provide an important mechanism to cryptographically protect data while keeping it available to be searched and accessed. in a common approach for their construction, the encrypting entity chooses one or several keywords that describe the content of each encrypted record of data. to perform a search, a user obtains a trapdoor for a keyword of her interest and uses this trapdoor to find all the data described by this keyword. we present a searchable encryption scheme that allows users to privately search by keywords on encrypted data in a public key setting and decrypt the search results. to this end, we define and implement two primitives: public key encryption with &lt;em&gt;oblivious&lt;/em&gt; keyword search (peoks) and &lt;em&gt;committed blind anonymous&lt;/em&gt; identity-based encryption (ibe). peoks is an extension of public key encryption with keyword search (peks) in which users can obtain trapdoors from the secret key holder without revealing the keywords. furthermore, we define committed blind trapdoor extraction, which facilitates the definition of authorisation policies to describe which trapdoor a particular user can request. we construct a peoks scheme by using our other primitive, which we believe to be the first blind and anonymous ibe scheme. we apply our peoks scheme to build a public key encrypted database that permits authorised private searches, i.e., neither the keywords nor the search results are revealed. blind and anonymous identity-based encryption and authorised private searches on public key encrypted data','Management and querying of encrypted data'
'bioinformatics needs high-throughput computing and huge data storage to understand datasets such as ones produced by complete genome projects. but these data are linked to patients, and used in scientific or industrial processes such as drug design and gene function identification. these use cases need to have a certain level of confidentiality and integrity to preserve the patient privacy or the patent secret. obviously important in a local computing context such as supercomputer or cluster, these requirements are exarcebated in the context of a grid such egee, where the computing and storage resources are distributed across a worldwide platform. we have developed the encfile encrypted files management system, deployed on the production platform of the egee project, and associated to encrypted representative biological resources. thus we provided grid users with a user-friendly component that doesn\'t require any user privileges. moreover, our encfile system can be also deployed on other distributed systems as it is not linked to the egee grid components.building an encrypted file system on the egee grid','Management and querying of encrypted data'
'no abstract available.call for papers for special issue on online analysis and querying of continuous data streams','Management and querying of encrypted data'
'we present the genetegra system, an ontology-based information integration environment. we show its ability to query multiple data sources, and we evaluate the relative performance of different data repositories. genetegra uses semantic web standards to resolve the semantic and syntactic diversity of the large and increasingly complex body of publicly available data. genetegra contains mechanisms to create ontology models of data sources using the owl 2 web ontology language, and to define, plan, and execute queries against these models using the sparql query language. data source formats supported include relational databases and xml and rdf data sources. experimental results have been obtained to show that genetegra obtains equivalent results from different data repositories containing the same data, illustrating the ability of the methods proposed in querying heterogeneous sources using the same modeling paradigm.cancer data integration and querying with genetegra','Management and querying of encrypted data'
'capturing and querying multiple aspects of semistructured data','Management and querying of encrypted data'
'scientific workflows are commonly used to model and execute large-scale scientific experiments. they represent key resources for scientists and are enacted and managed by scientific workflow management systems (swfms). each swfms has its particular approach to execute workflows and to capture and manage their provenance data. due to the large scale of experiments, it may be unviable to analyze provenance data only after the end of the execution. a single experiment may demand weeks to run, even in high performance computing environments. thus scientists need to monitor the experiment during its execution, and this can be done through provenance data. runtime provenance analysis allows for scientists to monitor workflow execution and to take actions before the end of it (i.e. workflow steering). this provenance data can also be used to fine-tune the parallel execution of the workflow dynamically. we use the prov data model as a basic framework for modeling and providing runtime provenance as a database that can be queried even during the execution. this database is agnostic of swfms and workflow engine. we show the benefits of representing and sharing runtime provenance data for improving the experiment management as well as the analysis of the scientific data.capturing and querying workflow runtime provenance with prov','Management and querying of encrypted data'
'combining the power of searching and querying','Management and querying of encrypted data'
'signal processing tools working directly on encrypted data could provide an efficient solution to application scenarios where sensitive signals must be protected from an untrusted processing device. in this paper, we consider the data expansion required to pass from the plaintext to the encrypted representation of signals, due to the use of cryptosystems operating on very large algebraic structures. a general composite signal representation allowing us to pack together a number of signal samples and process them as a unique sample is proposed. the proposed representation permits us to speed up linear operations on encrypted signals via parallel processing and to reduce the size of the encrypted signal. a case study-1-d linear filtering-shows the merits of the proposed representation and provides some insights regarding the signal processing algorithms more suited to work on the composite representation.composite signal representation for fast and storage-efficient processing of encrypted signals','Management and querying of encrypted data'
'chaotic mixing based encryption schemes for visual data are shown to be robust to lossy compression as long as the security requirements are not too high. this property facilitates the application of these ciphers in scenarios where lossy compression is applied to encrypted material &#8211; which is impossible in case traditional ciphers should be employed. if high security is required chaotic mixing loses its robustness to compression, still the lower computational demand may be an argument in favor of chaotic mixing as compared to traditional ciphers when visual data is to be encrypted.compression of encrypted visual data','Management and querying of encrypted data'
'what if you want to query a search engine, but don\'t want to tell the search engine what you are looking for? is there a way that you can encrypt your query, such that the search engine can process your query without your decryption key, and send back an (encrypted) response that is well-formed and concise (up to some upper bound on length that you specify)? the answer is yes, if you use a \"fully homomorphic\" encryption scheme. as another application, you can store your encrypted data in the \"cloud\", and later ask the server to retrieve only those files that contain a particular (boolean) combination of keywords, without the server being able to \"see\" either these keywords or your files. we will present a recent fully homomorphic encryption scheme. in particular, we will highlight the main ideas of the construction, discuss issues concerning the scheme\'s performance, and mention other applications. computing on encrypted data','Management and querying of encrypted data'
'concept based querying of semistructured data','Management and querying of encrypted data'
'concepts for modeling and querying list-structured data','Management and querying of encrypted data'
'we consider the problem of comparing two encrypted numbers and its extension &#8211; transferring one of the two secrets, depending on the result of comparison. we show how to efficiently apply our solutions to practical settings, such as auctions with the semi-honest auctioneer, proxy selling, etc. we propose a new primitive, conditional encrypted mapping, which captures common security properties of one round protocols in a variety of settings, which may be of independent interest.conditional encrypted mapping and comparing encrypted numbers','Management and querying of encrypted data'
'now, security and authenticity of data is a big challenge. to solve this problem, we propose an innovative method to authenticate the digital documents. in this paper, we propose a new method, where the marks obtained by a candidate will also be encoded in qr code tm in encrypted form, so that if an intruder tries to change the marks in the mark sheet then he can not do that in the qr code tm, because the encryption key is unknown to him. in this method, we encrypt the mark sheet data using the ttjsa encryption algorithm. the encrypted marks are entered inside qr code and that qr code is also printed with the original data of the mark sheet. the marks can then be retrieved from the qr code and can be decrypted using ttjsa decryption algorithm and then it can be verified with marks already there in the mark sheet.confidential encrypted data hiding and retrieval using qr authentication system','Management and querying of encrypted data'
'searchable encryption allows a party to search over encrypted data without decrypting it. prior schemes in the symmetric setting deal only with exact or similar keyword matches. we describe a scheme for the problem of wildcard searches over encrypted data to make search queries more flexible, provide a security proof for our scheme and compare the computational, communication and space complexity with existing schemes. we develop an efficient scheme, using pseudorandom functions and bloom filters, that supports wildcard searches over encrypted data. the scheme also supports conjunctive wildcard searches, efficient and secure updates and is more efficient than previous solutions. besides, our construction is independent of the encryption method of the remote data and is practical to use in real world applications.conjunctive wildcard search over encrypted data','Management and querying of encrypted data'
'we construct public-key systems that support comparison queries (x &#8805; a) on encrypted data as well as more general queries such as subset queries (x&#8712; s). furthermore, these systems support arbitrary conjunctive queries (p1 &#8743; ... &#8743; pl) without leaking information on individual conjuncts. we present a general framework for constructing and analyzing public-key systems supporting queries on encrypted data.conjunctive, subset, and range queries on encrypted data','Management and querying of encrypted data'
'we present kadop, a distributed infrastructure for warehousing xml resources in a peer-to-peer framework. kadop allows users to build a shared, distributed repository of resources such as xml documents, semantic information about such documents, web services, and collections of such items. kadop leverages several existing technologies and models: it uses distributed hash tables as a peer communication layer, and activexml as a model for constructing and querying the resources in the peer network.constructing and querying peer-to-peer warehouses of xml resources','Management and querying of encrypted data'
'in this paper we present a method of storing semantic rdf instances data in relational databases. this method is based on the \"table-per-class\" approach, and present an algorithm of automatic relational schema building for abox data. we also developed query processor that rewrites sparql queries to sql queries and has some inferencing capabilities. tests on our prototype system demonstrate that the rewritten queries can be answered by rdbms in an efficient and scalable way.construction and querying of relational schema for ontology instances data','Management and querying of encrypted data'
'users of computerized dictionaries require powerful and flexible tools for analyzing and manipulating the information in them. this paper discusses a system for grammatically describing and parsing entries from machine-readable dictionary tapes and a lexical data base representation for storing the dictionary information. it also describes a language for querying, formatting, and maintaining dictionaries and other lexical data stored with that representation.creating and querying lexical data bases','Management and querying of encrypted data'
'data compression using encrypted text','Management and querying of encrypted data'
'we propose the data node encrypted file system (dnefs), which uses on-the-fly encryption and decryption of file system data nodes to efficiently and securely delete data on flash memory systems. dnefs is a generic modification of existing flash file systems or controllers that enables secure data deletion while preserving the underlying systems\' desirable properties: application-independence, fine-grained data access, wear-levelling, and efficiency. we describe dnefs both abstractly and in the context of the flash file system ubifs. we propose ubifsec, which integrates dnefs into ubifs. we implement ubifsec by extending ubifs\'s linux implementation and we integrate ubifsec in the android operating system running on a google nexus one smartphone. we show that it is efficient and usable; android os and applications (including video and audio playback) run normally on top of ubifsec. to the best of our knowledge, this work presents the first comprehensive and fully-implemented secure deletion solution that works within the specification of flash memory.data node encrypted file system','Management and querying of encrypted data'
'the declarative programming paradigms used in constraint languages can lead to powerful extensions of relational database query languages. this is an overview of recent progress on such constraint query languages, with a list of related open theoretical computer science questions.database querying and constraint programming','Management and querying of encrypted data'
'the increasing size of distributed data sources often leads to large amounts of answers in response to user queries (information overload), and to long response times. moreover, querying systems do not sufficiently take into account and make use of the user-related information (preferences,context,etc.) that constitute their profile. in this, they are far apart from personalized systems that adapt their behavior or service to the user. although user-adaptive systems are mostly found on the world wide web, the personalization concept is more general. this paper discusses the possibility and implications of combining the use of summaries with a personalized service, in the querying of a database. the personalization aspect consists in allowing the users to choose their own querying vocabulary, by means of linguistic variables, while the use of summaries increases the efficiency of the querying. several ways of doing so are identified and discussed. their advantages and drawbacks are exposed. in some cases, querying algorithms have already been proposed. in other cases, the missing querying algorithms are given.database querying with personalized vocabulary using data summaries','Management and querying of encrypted data'
'we examine the security requirements for creating a deniable file system (dfs), and the efficacy with which the truecrypt disk-encryption software meets those requirements. we find that the windows vista operating system itself, microsoft word, and google desktop all compromise the deniability of a truecrypt dfs. while staged in the context of truecrypt, our research highlights several fundamental challenges to the creation and use of any dfs: even when the file system may be deniable in the pure, mathematical sense, we find that the environment surrounding that file system can undermine its deniability, as well as its contents. we hypothesize some extensions of our discoveries to regular (non-deniable) encrypted file systems. finally, we suggest approaches for overcoming these challenges on modern operating systems like windows. we analyzed truecrypt version 5.1a (latest available version during the writing of the paper); truecrypt v6 introduces new features, including the ability to create deniable operating systems, which we have not studied.defeating encrypted and deniable file systems','Management and querying of encrypted data'
'design and analysis of data structures for querying image databases','Management and querying of encrypted data'
'security and privacy concerns as well as legal considerations force many companies to encrypt the sensitive data in databases. however, storing the data in an encrypted format entails non-negligible performance penalties while processing queries. in this paper, we address several design issues related to querying encrypted data in relational databases. based on our experiments, we propose new and efficient techniques to reduce the cost of cryptographic operations while processing different types of queries. our techniques enable us not only to overlap the cryptographic operations with the io latencies but also to reduce the number of block cipher operations with the help of selective decryption capabilities.design and analysis of querying encrypted data in relational databases','Management and querying of encrypted data'
'the memory cards are widely used in several applications. this paper is intended to design and construct of the encrypted card reader. the basic building block in this design is the card reader, amplifier, multiplexer, and pc parallel port. this card can be used in license, passport, credit card, id card, bank card, health care, travel and entertainment cards and etc. in this paper, data on a card can be protected against unauthorized viewing.design and implementation of encrypted card reader','Management and querying of encrypted data'
'we show how to specify and use the metadata for a virtual and relational data integration system under the local-as-view (lav) approach. we use xml and ruleml for representing metadata, like the global and local schemas, the mappings between the former and the latter, and global integrity constraints. xquery is used to retrieve relevant information for query planning. the system uses an extended inverse rules algorithm for computing certain answers that is provably correct for monotone relational global queries. for query answering, evaluation engines for answer set programs on relational databases are used. the programs declaratively specify the legal instances of the integration system. designing, specifying and querying metadata for virtual data integration systems','Management and querying of encrypted data'
'the problem of the inability of machines to interpret and process information published on web pages caused the development of a web of data, next to the web of documents. the idea is known as the semantic web, where links between information are established in a way that machines can understand and interpret. with its development, new applications were introduced to query and process this linked data. additionally the open data initiative was launched with the goal to publish governmental, scientific, and cultural data freely accessible on the web. often, this open data is offered in a semi-structured form, like csv files, but can also be transformed into linked data format. with this linked open data, programs can be created that efficiently process queries and find information. this work is supposed to integrate the support for discovery queries into an existing lod cache engine. the goal is to develop a new approach that processes sparql queries and augments the result with discovered information from different (online) sources. thus, the approach can help users to explore new information and knowledge more easily. users should not worry about what particular data is stored locally and which identifiers are used. to do so, we plan to extend the rewriting process during logical optimization of sparql queries.discovery querying in linked open data','Management and querying of encrypted data'
'password-based key-server protocols are susceptible to password chaining attacks, in which an enemy uses knowledge of a user\'s current password to learn all future passwords. as a result, the exposure of a single password effectively compromises all future communications by that user. the same protocols also tend to be vulnerable to dictionary attacks against user passwords. bellovin and merrit[1] presented a hybrid of symmetric- and public-key cryptography called encrypted key exchange (eke) that cleanly solves the dictionary attack problem. this paper presents an extension of their ideas called dual-workfactor encrypted key exchange that preserves eke\'s strength against dictionary attacks but also efficiently prevents passive password-chaining attacks.dual-workfactor encrypted key exchange','Management and querying of encrypted data'
'the erosion of trust put in traditional database servers and in database service providers and the growing interest for different forms of selective data dissemination are different factors that lead to move the access-control from servers to clients. different data encryption and key dissemination schemes have been proposed to serve this purpose. by compiling the access-control rules into the encryption process, all these methods suffer from a static way of sharing data. with the emergence of hardware security elements on client devices, more dynamic client-based access-control schemes can be devised. this paper proposes a tamper-resistant client-based xml access-right controller supporting flexible and dynamic access-control policies. the access-control engine is embedded in a hardware-secure device and, therefore, must cope with specific hardware resources. this engine benefits from a dedicated index to quickly converge toward the authorized parts of a potentially streaming xml document. pending situations (i.e., where data delivery is conditioned by predicates, which apply to values encountered afterward in the document stream) are handled gracefully, skipping, whenever possible the pending elements and reassembling relevant parts when the pending situation is solved. additional security mechanisms guarantee that (1) the input document is protected from any form of tampering and (2) no forbidden information can be gained by replay attacks on different versions of the xml document and of the access-control rules. performance measurements on synthetic and real datasets demonstrate the effectiveness of the approach. finally, the paper reports on two experiments conducted with a prototype running on a secured hardware platform.dynamic access-control policies on xml encrypted data','Management and querying of encrypted data'
'data sets involving linear ordered sequences are a recurring theme in bioinformatics. dynamic query tools that support exploration of these data sets can be useful for identifying patterns of interest. this paper describes the use of one such tool - timesearcher - to interactively explore linear sequence data sets taken from two bioinformatics problems. microarray time course data sets involve expression levels for large numbers of genes over multiple time points. timesearcher can be used to interactively search these data sets for genes with expression profiles of interest. the occurrence frequencies of short sequences of dna in aligned exons can be used to identify sequences that play a role in the pre-mrna splicing. timesearcher can be used to search these data sets for candidate splicing signals.dynamic querying for pattern identification in microarray and genomic data','Management and querying of encrypted data'
'data streaming has established itself as a viable communication abstraction in data-intensive parallel and distributed computations, occurring in applications such as scientific visualization, performance monitoring, and large-scale data transfer. a known problem in large-scale event communication is tailoring the data received at the consumer. it is the general problem of extracting data of interest from a data source, a problem that the database community has successfully addressed with sql queries, a time tested, user-friendly way for noncomputer scientists to access data. by leveraging the efficiency of query processing provided by relational queries, the dquob system provides a conceptual relational data model and sql query access over streaming data. queries can be used to extract data, combine streams, and create new streams. the language augments queries with an action to enable more complex data transformations such as fourier transforms. the dquob system has been applied to two large-scale distributed applications: a safety critical autonomous robotics simulation and scientific software visualization for global atmospheric transport modeling. in this paper, we present the dquob system and the results of performance evaluation undertaken to assess its applicability in data-intensive wide-area computations, where the benefit of portable data transformation must be evaluated against the cost of continuous query evaluation.dynamic querying of streaming data with the dquob system','Management and querying of encrypted data'
'traditional databases mainly focus on the processing of deterministic data. however, information is often uncertain in practical applications. this paper aims to provide a basic framework for managing possibilistic xml (extensible markup language) data queries in a dynamic environment. existing efforts are mainly made on querying xml data towards the representation of crisp concepts based on the static labeling schemes. once an updating operation is involved, these static labeling scheme approaches often need to search the whole original xml document again to relabel all the labels of the nodes. this re-labeling obviously sacrifices the processing performance. different from the prior work, we adopt a novel dynamic encoding scheme which is tailored for both static and dynamic possibilistic xml documents to effectively avoid re-labeling after updates. on this basis, we propose an efficient algorithm to handle the problem of dynamic twig queries in possibilistic xml documents. finally, we report our experimental results to show that our algorithm is superior to previous approaches.dynamically querying possibilistic xml data','Management and querying of encrypted data'
'wireless sensor networks (wsns) are ad-hoc networks composed of tiny devices with limited computation and energy capacities. for such devices, data transmission is a very energy-consuming operation. it thus becomes essential to the lifetime of a wsn to minimize the number of bits sent by each device. one wellknown approach is to aggregate sensor data (e.g., by adding) along the path from sensors to the sink. aggregation becomes especially challenging if end-to-end privacy between sensors and the sink is required. in this paper, we propose a simple and provably secure additively homomorphic stream cipher that allows ef- ficient aggregation of encrypted data. the new cipher only uses modular additions (with very small moduli) and is therefore very well suited for cpu-constrained devices. we show that aggregation based on this cipher can be used to eficiently compute statistical values such as mean, variance and standard deviation of sensed data, while achieving signi?cant bandwidth gain.e.cient aggregation of encrypted data in wireless sensor networks','Management and querying of encrypted data'
'most tools to recognize the application associated with network connections use well-known signatures as basis for their classification. this approach is very effective in enterprise and campus networks to pinpoint forbidden applications (peer to peer, for instance) or security threats. however, it is easy to use encryption to evade these mechanisms. in particular, secure sockets layer (ssl) libraries such as openssl are widely available and can easily be used to encrypt any type of traffic. in this paper, we propose a method to detect applications in ssl encrypted connections. our method uses only the size of the first few packets of an ssl connection to recognize the application, which enables an early classification. we test our method on packet traces collected on two campus networks and on manually-encrypted traces. our results show that we are able to recognize the application in an ssl connection with more than 85\% accuracy.early recognition of encrypted applications','Management and querying of encrypted data'
'efficient and effective querying by image content','Management and querying of encrypted data'
'wireless sensor networks (wsns) are composed of tiny devices with limited computation and battery capacities. for such resource-constrained devices, data transmission is a very energy-consuming operation. to maximize wsn lifetime, it is essential to minimize the number of bits sent and received by each device. one natural approach is to aggregate sensor data along the path from sensors to the sink. aggregation is especially challenging if end-to-end privacy between sensors and the sink (or aggregate integrity) is required. in this article, we propose a simple and provably secure encryption scheme that allows efficient additive aggregation of encrypted data. only one modular addition is necessary for ciphertext aggregation. the security of the scheme is based on the indistinguishability property of a pseudorandom function (prf), a standard cryptographic primitive. we show that aggregation based on this scheme can be used to efficiently compute statistical values, such as mean, variance, and standard deviation of sensed data, while achieving significant bandwidth savings. to protect the integrity of the aggregated data, we construct an end-to-end aggregate authentication scheme that is secure against outsider-only attacks, also based on the indistinguishability property of prfs.efficient and provably secure aggregation of encrypted data in wireless sensor networks','Management and querying of encrypted data'
'wireless sensor networks are now in widespread use to monitor regions, detect events and acquire information. to reduce the amount of sending data, an aggregation approach can be applied along the path from sensors to the sink. however, usually the carried information contains confidential data. therefore, an end-to-end secure aggregation approach is required to ensure a healthy data reception. end-to-end encryption schemes that support operations over cypher-text have been proved important for private party sensor network implementations. unfortunately, nowadays these methods are very complex and not suitable for sensor nodes having limited resources. in this paper, we propose a secure end-to-end encrypted-data aggregation scheme. it is based on elliptic curve cryptography that exploits a smaller key size. additionally, it allows the use of higher number of operations on cypher-texts and prevents the distinction between two identical texts from their cryptograms. these properties permit to our approach to achieve higher security levels than existing cryptosystems in sensor networks. our experiments show that our proposed secure aggregation method significantly reduces computation and communication overhead and can be practically implemented in on-the-shelf sensor platforms. by using homomorphic encryption on elliptic curves, we thus have realized an efficient and secure data aggregation in sensor networks.efficient and robust secure aggregation of encrypted data in sensor networks','Management and querying of encrypted data'
'information search and document retrieval from a remote database (e.g. cloud server) requires submitting the search terms to the database holder. however, the search terms may contain sensitive information that must be kept secret from the database holder. moreover, the privacy concerns apply to the relevant documents retrieved by the user in the later stage since they may also contain sensitive data and reveal information about sensitive search terms. a related protocol, private information retrieval (pir), provides useful cryptographic tools to hide the queried search terms and the data retrieved from the database while returning most relevant documents to the user. in this paper, we propose a practical privacy-preserving ranked keyword search scheme based on pir that allows multi-keyword queries with ranking capability. the proposed scheme increases the security of the keyword search scheme while still satisfying efficient computation and communication requirements. to the best of our knowledge the majority of previous works are not efficient for assumed scenario where documents are large files. our scheme outperforms the most efficient proposals in literature in terms of time complexity by several orders of magnitude.efficient and secure ranked multi-keyword search on encrypted cloud data','Management and querying of encrypted data'
'with the rapidly increasing popularity of xml as a data format, there is a large demand for efficient techniques in storing and querying xml documents. however xml is by nature verbose, due to repeatedly used tags that describe data. for this reason the storage requirements of xml can be excessive and lead to increased costs for data manipulation. therefore, it seems natural to use compression techniques to increase the efficiency of storing and querying xml data. in this paper, we propose a new approach called scqx for storing, compressing and querying xml documents. this approach compresses the structure of an xml document based on exploiting repetitive consecutive tags in the structure, and then scqx stores the compressed xml structure and the data separately in a robust storage structure that includes a set of access support structures to guarantee fast query performance. moreover, scqx supports querying of the compressed xml structure directly and efficiently without requiring decompression. an experimental evaluation on sets of xml data shows the effectiveness of our approach.efficient compression and querying of xml repositories','Management and querying of encrypted data'
'multi-dimensional spatial data are obtained when a number of data acquisition devices are deployed at different locations to measure a certain set of attributes of the study subject. how to manipulate these spatial data remains a challenge to the database community, especially when the spatial locations are represented in 3d. in this work, we establish a data model to handle multi-dimensional spatial data with three spatial dimensions. in particular, firstly, a clustering algorithm is applied to group the data set into \"point clouds\". secondly, each cloud is considered as a 3d spatial convex object and triangulated into a set of tetrahedrons. thirdly, all tetrahedron sets are stored in the database and spatial analysis is performed. in this paper, we focus on defining 3d spatial operations and relationships for 3d spatial elements (points, segments, triangles and tetrahedrons), and further applying these operations on 3d spatial objects, where each object is composed of a set of tetrahedrons.efficient data modeling and querying system for multi-dimensional spatial data','Management and querying of encrypted data'
'we propose a representation of spatio-temporal objects with continuous and cyclic or acyclic periodic movements. we also describe an extended relational algebra query language for databases with such objects. we show that the new spatio-temporal databases are closed under the extended relational algebra queries, and each fixed relational algebra query can be evaluated in ptime in the size of the input database.efficient querying and animation of periodic spatio-temporal databases','Management and querying of encrypted data'
'network accountability, forensic analysis, and failure diagnosis are becoming increasingly important for network management and security. such capabilities often utilize network provenance - the ability to issue queries over network meta-data. for example, network provenance may be used to trace the path a message traverses on the network as well as to determine how message data were derived and which parties were involved in its derivation. this paper presents the design and implementation of exspan, a generic and extensible framework that achieves efficient network provenance in a distributed environment. we utilize the database notion of data provenance to \"explain\" the existence of any network state, providing a versatile mechanism for network provenance. to achieve such flexibility at internet-scale, exspan uses declarative networking in which network protocols can be modeled as continuous queries over distributed streams and specified concisely in a declarative query language. we extend existing data models for provenance developed in database literature to enable distribution at internet-scale, and investigate numerous optimization techniques to maintain and query distributed network provenance efficiently. the exspan prototype is developed using rapidnet, a declarative networking platform based on the emerging ns-3 toolkit. experiments over a simulated network and an actual deployment in a testbed environment demonstrate that our system supports a wide range of distributed provenance computations efficiently, resulting in significant reductions in bandwidth costs compared to traditional approaches.efficient querying and maintenance of network provenance at internet-scale','Management and querying of encrypted data'
'the contemporary web is formed mainly by documents, from which it is rather complicated to automatically retrieve hidden structured and interlinked information. the idea of linked data based primarily on rdf data triples seems to successfully follow this drawback. in recent years, a significant effort was made not only in a theoretical research, but also in the amount of linked data globally available. since rdf triples are modelled as graph data, we cannot directly adopt the existing solutions from relational databases or xml technologies. thus, several research questions remain opened. the purpose of our ongoing research effort is to propose an efficient framework for querying linked data. this requires finding the compromise between storing data in local storages and accessing them directly on-demand in distributed data sources. next, we need to choose a suitable querying language, propose auxiliary indexing structures and, finally, to define an ordering model for query results. the theoretical research will be supplemented by a prototype implementation and experiments over real-world data.efficient querying of distributed linked data','Management and querying of encrypted data'
'this paper describes the design of a censorship-resistant distributed file sharing protocol which has been implemented on top of gnunet, an anonymous, reputation-based network. we focus on the encoding layer of the gnunet file-sharing protocol which supports efficient dissemination of encrypted data as well as queries over encrypted data. the main idea advocated in this paper is that simple cryptographic techniques are sufficient to engineer an efficient data encoding that can make it significantly harder to selectively censor information. our encoding allows users to share files encrypted under descriptive keys which are the basis for querying the network for content. a key property of our encoding is that intermediaries can filter invalid encrypted replies without being able to decrypt the query or the reply. files are stored in small chunks which are distributed and replicated automatically by the gnunet infrastructure. additionally, data files may be stored in plaintext or encrypted form or as a combination of both and encrypted on demand.efficient sharing of encrypted data','Management and querying of encrypted data'
'in recent years, due to the appealing features of cloud computing, large amount of data have been stored in the cloud. although cloud based services offer many advantages, privacy and security of the sensitive data is a big concern. to mitigate the concerns, it is desirable to outsource sensitive data in encrypted form. encrypted storage protects the data against illegal access, but it complicates some basic, yet important functionality such as the search on the data. to achieve search over encrypted data without compromising the privacy, considerable amount of searchable encryption schemes have been proposed in the literature. however, almost all of them handle exact query matching but not similarity matching, a crucial requirement for real world applications. although some sophisticated secure multi-party computation based cryptographic techniques are available for similarity tests, they are computationally intensive and do not scale for large data sources. in this paper, we propose an efficient scheme for similarity search over encrypted data. to do so, we utilize a state-of-the-art algorithm for fast near neighbor search in high dimensional spaces called locality sensitive hashing. to ensure the confidentiality of the sensitive data, we provide a rigorous security definition and prove the security of the proposed scheme under the provided definition. in addition, we provide a real world application of the proposed scheme and verify the theoretical results with empirical observations on a real dataset.efficient similarity search over encrypted data','Management and querying of encrypted data'
'the need to analyze structured data for various business intelligence applications such as customer churn analysis, social network analysis, telecom network monitoring etc., is well known. however, the potential size to which such data will scale in future will make solutions that revolve around data warehouses hard to scale. as data sizes grow the movement of data from the warehouse to archives becomes more frequent. current file based archive models make the archived data unusable for any type of insight extraction. in this paper, we present an active archival solution for data warehouses that makes use of hadoop distributed file system (hdfs) to store the data in an always available and cost-effective manner. we investigate various structured data storage schemes within hdfs and empirical evaluations show that a combination of universal scheme model and column store is best suited for the active archival solution.efficiently querying archived data using hadoop','Management and querying of encrypted data'
'extensible markup language (xml) is emerging as a de facto standard for information exchange among various applications on the world-wide web. there has been a growing need for developing high-performance techniques to query large xml data repositories efficiently. one important problem in xml query processing is twig pattern matching , that is, finding in an xml data tree d all matches that satisfy a specified twig (or path) query pattern q. in this survey we review, classify, and compare major techniques for twig pattern matching.specifically, we consider two classes of major xml queryprocessing techniques: the relational approach and the native approach. the relational approach directly utilizes existing relational database systems to store and query xml data, which enables the use of all important techniques that have been developed for relational databases, while in the native approach, specialized storage and query-processing systems tailored for xml data are developed from scratch to further improve xml query performance. as implied by existing work, xml data querying and management are developing in the direction of integrating the relational approach with the native approach, which could result in higher query-processing performance and also significantly reduce system-reengineering costs.efficiently querying large xml data repositories','Management and querying of encrypted data'
'applications that query data streams in order to identify trends, patterns, or anomalies can often benefit from comparing the live stream data with archived historical stream data. however, searching this historical data in real time has been considered so far to be prohibitively expensive. one of the main bottlenecks is the update costs of the indices over the archived data. in this paper, we address this problem by using our highly-efficient bitmap indexing technology (called fastbit) and demonstrate that the index update operations are sufficiently efficient for this bottleneck to be removed. we describe our prototype system based on the telegraphcq streaming query processor and the fastbit bitmap index. we present a detailed performance evaluation of our system using a complex query workload for analyzing real network traffic data. the combined system uses telegraphcq to analyze streams of traffic information and fastbit to correlate current behaviors with historical trends. we demonstrate that our system can simultaneously analyze (1) live streams with high data rates and (2) a large repository of historical stream data.enabling real-time querying of live and historical stream data','Management and querying of encrypted data'
'companies can optimize their supply chain if they exchange item-level data, i.e. item-specific data gathered with the help of radio frequency identification or 2d bar codes. data can either be distributed over the repositories of each company or stored in a central repository. the distributed approach requires \"discovering\" the repositories which contain data about the queried item. thus, data access is slow. the central approach does not require discovery, but the data owner has to relinquish access control to the repository provider. both approaches are not satisfactory. in this paper we present an encryption scheme for exchanging item-level data by storing it in a central repository. it allows the data owner to enforce access control on an item-level by managing the corresponding keys. furthermore, data remains confidential even against the repository provider. thus we eliminate the main problem of the central approach. we provide formal proofs that the proposed encryption scheme is secure. then, we evaluate the encryption scheme with databases containing up to 50 million tuples. results show that the encryption scheme is fast, scalable and that it can be parallelized very efficiently. our encryption scheme thereby reconciles the conflict between security and performance in item-level data repositories.encrypted searchable storage of rfid tracking data','Management and querying of encrypted data'
'the deployment of share data spaces in open, possibly hostile, environments arises the need of protecting the confidentiality of the data space content. existing approaches focus on access control mechanisms that protect the data space from untrusted agents. the basic assumption is that the hosts (and their administrators) where the data space is deployed have to be trusted. encryption schemes can be used to protect the data space content from malicious hosts. however, these schemes do not allow searching on encrypted data. in this paper we present a novel encryption scheme that allows tuple matching on completely encrypted tuples. since the data space does not need to decrypt tuples to perform the search, tuple confidentiality can be guaranteed even when the data space is deployed on malicious hosts (or an adversary gains access to the host). our scheme does not require authorised agents to share keys for inserting and retrieving tuples. each authorised agent can encrypt, decrypt, and search encrypted tuples without having to know other agents\' keys. this is beneficial inasmuch as it simplifies the task of key management. an implementation of an encrypted data space based on this scheme is described and some preliminary performance results are given.encrypted shared data spaces','Management and querying of encrypted data'
'we target at one newly introduced security concern which is not fully addressed when moving (encrypted) data to the cloud, namely, the security of the search results from the cloud. the cloud storage provider (csp) might be compromised or simply act maliciously for their own good, which yields incorrect search results. in this paper, we exploit hidden vector encryption to tackle this important security problem. our construction enables csps to provide a proof of the search results to be verified later by the cloud storage users. in particular, this proposed scheme handles equality and range searches on encrypted data. users can verified the correctness of the search results without decrypting for the corresponding file contents. any tampering with the search results by csps will be caught, while any incorrect charge against csps for tampering search results can be rectified. finally, we present extensive security and performance analysis to show the security and practicality of our scheme.ensuring correctness of range searches on encrypted cloud data','Management and querying of encrypted data'
'a feature that has become desirable for low-power mobile devices with limited computing and energy resources is the ability to select a security configuration in order to create a trade-off between security and other important parameters such as performance and energy consumption. selective encryption can be used to create this trade-off by only encrypting chosen units of the information. in this paper, we continue the investigation of the confidentiality implications of selective encryption by applying entropy on a generic selective encryption scheme. by using the concept of run-length vector from run-length encoding theory, an expression is derived for entropy of selectively encrypted strings when the number of encrypted substrings, containing one symbol, and the order of the language change.entropy of selectively encrypted strings','Management and querying of encrypted data'
'in their daily routine, enterprise decision makers use to analyze huge amounts of information in order to sustain their decisions and, consequently, ensuring success of enterprises business activities. through time, on-line analytical processing systems have contributed decisively to the decision making process improvement, not only by granting extremely flexible data manipulation mechanisms, but also allowing the materialization of the analysis indexes required. however, that analytical \"power\" uses to exhaust the computational resources, especially disk space and processing time, especially materializing specialized views. besides, as time goes by, multidimensional databases become very large, being its management very difficult. aiming to optimize maintenance and operationality of such databases, we design a system that is able to restructure them in useful time and reduce multidimensional query processing time, according to the exploitation trends of knowledge workers. in this paper, we present the system\'s structure, its correspondent cost model, query and maintenance algorithms, restructuring strategies, and, finally, its distribution through several processing olap nodes.estimating querying and maintenance costs for restructuring data cubes','Management and querying of encrypted data'
'we present a public-key encryption scheme with the following properties. given a branching program p and an encryption c&#8242; of an input x, it is possible to efficiently compute a succinct ciphertext c&#8242; from which p(x) can be efficiently decoded using the secret key. the size of c&#8242; depends polynomially on the size of x and the length of p, but does not further depend on the size of p. as interesting special cases, one can efficiently evaluate finite automata, decision trees, and obdds on encrypted data, where the size of the resulting ciphertext c&#8242; does not depend on the size of the object being evaluated. these are the first general representation models for which such a feasibility result is shown. our main construction generalizes the approach of kushilevitz and ostrovsky (focs 1997) for constructing single-server private information retrieval protocols. we also show how to strengthen the above so that c&#8242; does not contain additional information about p (other than p(x) for some x) even if the public key and the ciphertext c are maliciously formed. this yields a two-message secure protocol for evaluating a length-bounded branching program p held by a server on an input x held by a client. a distinctive feature of this protocol is that it hides the size of the server\'s input p from the client. in particular, the client\'s work is independent of the size of p.evaluating branching programs on encrypted data','Management and querying of encrypted data'
'randomness tests have been employed in encrypted data identification since encrypted data have high randomness. however, the evaluation of various randomness tests is absent. the nist statistical test suite is performed on encrypted and compressed data, which are derived from seven different types of files. and an index, differentiability, is introduced to indicate the ability of a test to identify the encrypted data. results show the effect of differentiability, and suggest that cumulative sums test have the best differentiability. furthermore, some compressed data are similar to encrypted data and the identification of them is a major point of future work.evaluation of encrypted data identification methods based on randomness test','Management and querying of encrypted data'
'while telemedicine technology greatly increases the quality of medical care with less health care costs, it also raises critical challenges for data security and privacy in the course of data entry, data review, patient education, video conference, and electronic monitoring. to meet these various needs, a number of different of security strategies can be utilized. in this study, we aim to secure user-related telemedicine data by storing and distributing them in an encrypted form, so that on the one hand the requirement on secure medical data management is satisfied, while on the other hand efficient processing, particularly the efficient querying of encrypted medical data can still be guaranteed. given the increasing use of extensible markup language (xml) in describing and interchanging data between systems and users, querying over encrypted xml telemedicine data becomes more and more important. in [3], we develop an effective hash-based approach to enable efficient querying over encrypted xml data. the basic idea is to augment encrypted xml data with encodings which characterize the topology and content of tree-structured xml data, and then filter out candidate xml data for decryption and query execution by examining xpath queries against these encodings. a generic framework is provided which consists of three phases, namely, \"query preparation\", \"query pre-processing\" and \"query execution\", for the proposed search strategy. this paper presents a comprehensive experimental evaluation of querying over encrypted xml-formated telemedicine data.experimental evaluation of query processing on encrypted telemedicine data','Management and querying of encrypted data'
'different from the traditional public key encryption, searchable public key encryption allows a data owner to encrypt his data under a user\'s public key in such a way that the user can generate search token keys using her secret key and then query an encryption storage server. on receiving such a search token key, the server filters all or related stored encryptions and returns matched ones as response. searchable pubic key encryption has many promising applications. unfortunately, existing schemes either only support simple query predicates, such as equality queries and conjunctive queries, or have a superpolynomial blowup in ciphertext size and search token key size. in this paper, based on the key-policy attribute-based encryption scheme proposed by lewko et al. recently, we present a new construction of searchable public key encryption. compared to previous works in this field, our construction is much more expressive and efficient and is proven secure in the standard model.expressive search on encrypted data','Management and querying of encrypted data'
'a dimension in a data warehouse (dw) is an abstract concept that groups data that share a common semantic meaning. the dimensions are modeled using a hierarchical schema of categories. a dimension is called strict if every element of each category has exactly one ancestor in each parent category, and covering if each element of a category has an ancestor in each parent category. if a dimension is strict and covering we can use pre-computed results at lower levels to answer queries at higher levels. this capability of computing summaries is vital for efficiency purposes. nevertheless, when dimensions are not strict/covering it is important to know their strictness and covering constraints to keep the capability of obtaining correct summarizations. real world dimensions might fail to satisfy these constraints, and, in these cases, it is important to find ways to fix the dimensions (correct them) or find ways to get correct answers to queries posed on inconsistent dimensions. a minimal repair is a new dimension that satisfies the strictness and covering constraints, and that is obtained from the original dimension through a minimum number of changes. the set of minimal repairs can be used as a tool to compute answers to aggregate queries in the presence of inconsistencies. however, computing all of them is np-hard. in this paper, instead of trying to find all possible minimal repairs, we define a single compatible repair that is consistent with respect to both strictness and covering constraints, is close to the inconsistent dimension, can be computed efficiently and can be used to compute approximate answers to aggregate queries. in order to define the compatible repair we defined the notion of extended dimension that supports sets of elements in categories.extended dimensions for cleaning and querying inconsistent data warehouses','Management and querying of encrypted data'
'extending olap querying to external object databases','Management and querying of encrypted data'
'many applications work with graph-structured data. as graphs grow in size, indexing becomes essential to ensure sufficient query performance. we present the gripp index structure (graph indexing based on pre- and postorder numbering) for answering reachability queries in graphs. gripp requires only linear time and space. using gripp, we can answer reachability queries on graphs with 5 million nodes on average in less than 5 milliseconds, which is unrivaled by previous methods. we evaluate the performance and scalability of our approach on real and synthetic random and scale-free graphs and compare our approach to existing indexing schemes. gripp is implemented as stored procedure inside a relational database management system and can therefore very easily be integrated into existing graph-oriented applications.fast and practical indexing and querying of very large graphs','Management and querying of encrypted data'
'we propose techniques based on bitmap-based indices to efficiently store heterogeneous (i.e., time-varying schema) streaming data, supporting fast archiving and query throughput while being space-efficient. we also present the architecture and performance evaluation of a system implementation.fast archiving and querying of heterogeneous sensor data streams','Management and querying of encrypted data'
'in distribution of data encrypted to multiple users, there is a problem that the system will become low efficient if a single key center has to generate keys for a large number of users. besides, it is a risk that some users could deliberately disclose their keys for some benefits. we in this paper give a key leakage discovering scheme where users are partitioned into groups and groups are hierarchically organized. in our scheme, users in upper-level groups can delegate keys for users in lower-level groups, which alleviates the key generation burden of the trusted third party. as an interesting feature, our scheme provide a key leakage discovering measure that if some users in groups leaked their decryption keys then at least one of them can be found out. this enables the data owners to accuse the illegal users when they infringed the copyright. at last, we analyze the performance of our system.finding key leakage in hierarchical distribution of encrypted data','Management and querying of encrypted data'
'there has been an explosion of hyperlinked data in many domains, e.g., the biological web. expressive query languages and effective ranking techniques are required to convert this data into browsable knowledge. we propose the graph information discovery (gid) framework to support sophisticated user queries on a rich web of annotated and hyperlinked data entries, where query answers need to be ranked in terms of some customized ranking criteria, e.g., pagerank or objectrank. gid has a data model that includes a schema graph and a data graph, and an intuitive query interface. the gid framework allows users to easily formulate queries consisting of sequences of hard filters (selection predicates) and soft filters (ranking criteria); it can also be combined with other specialized graph query languages to enhance their ranking capabilities. gid queries have a well-defined semantics and are implemented by a set of physical operators, each of which produces a ranked result graph. we discuss rewriting opportunities to provide an efficient evaluation of gid queries. soft filters are a key feature of gid and they are implemented using authority flow ranking techniques; these are query dependent rankings and are expensive to compute at runtime. we present approximate optimization techniques for gid soft filter queries based on the properties of random walks, and using novel path-length-bound and graph-sampling approximation techniques. we experimentally validate our optimization techniques on large biological and bibliographic datasets. our techniques can produce high quality (top k) answers with a savings of up to an order of magnitude, in comparison to the evaluation time for the exact solution.flexible and efficient querying and ranking on hyperlinked data sources','Management and querying of encrypted data'
'this paper deals with efficient fuzzy query processing in a distributed database context. the idea suggested is to forward the query only to the data sources that are likely to provide \"good answers\" to the query, which entails to define a way to assess each source by means of a \"compact\" representation of its content. a solution based on fuzzy summaries is outlined, and different approaches to fuzzy summaries are presented and compared. the query/summary matching aspect is tackled for each of these approaches and a general processing strategy is described.flexible querying of multiple data sources through fuzzy summaries','Management and querying of encrypted data'
'the problem of dealing with semi-structured information is an increasingly studied topic by both communities of researchers in database management and in information retrieval.dbms and irss are the two main types of systems aimed at representing, storing and retrieving information.when semi-structured information is considered, the main problems addressed in the design of these systems are to represent and to inquiry semi-structured data and structured documents respectively.flexible querying of semi-structured information','Management and querying of encrypted data'
'this article provides a general discussion about how flexible querying can be applied to semistructured data (ssd). we adapt flexible querying ideas, already used for classically structured databases, to xquery-like querying of ssd for managing users\' priority and preferences, but also for tackling with the variability of ssd underlying structures. indeed flexible querying seems to be still more useful for ssd than for classical databases, because of the potential structural heterogeneity of the former. fuzzy sets are useful for expressing flexible requirements on attribute values and for estimating the degree of similarity of tags, or attribute labels, with elements present in the request. priorities are introduced in the request for specifying the relative importance of elementary requirements in terms of their semantic contents, but also preferences about the location of information in the structure. the evaluation of the queries uses a qualitative scale with a finite number of levels, and retrieved pieces of ssd are rank-ordered using a lexicographic vector procedure. illustrative examples are provided. &#169; 2007 wiley periodicals, inc. int j int syst 22: 723&#8211;737, 2007.flexible querying of semistructured data: a fuzzy-set-based approach','Management and querying of encrypted data'
'encrypted files captured by acquiring a bit-by-bit image in the process of conventional forensic investigation are practically impossible to decrypt without knowing the key and the method of encryption. the windows operating system provides the option to encrypt files using an encryption driver bundled with the new technology file system (ntfs) file system, the so-called encrypting file system (efs). efs files can be manipulated transparently by the owner and the system administrator as long as they reside in an ntfs file system. in this article we demonstrate the methodology of extracting efs-decrypted files from a live system. the method of extraction is built around a software utility, robocopy, which does not modify any metadata of the file system during extraction. the hash value for the encrypted data calculated before and after the extraction is identical, so this approach can be considered to be forensically sound. we present a scenario that shows that live system investigation is indispensable in obtaining complete information about the system being examined. this information would be lost if conventional methods were applied, even when supplemented by the capture and analysis of physical memory.forensic extraction of efs-encrypted files in live system investigation','Management and querying of encrypted data'
'reviews a new definition of fuzzy functional dependency based on conditional probability and its application to approximate data reduction related to the operation of projection in classical relational databases in order to construct fuzzy integrity constraints. we introduce the concept of partial fuzzy functional dependency, which expresses the fact that a given attribute x does not determine y completely, but in the partial area of x it might determine y. finally, we discuss another application of fuzzy functional dependency in constructing fuzzy query relations for data querying and approximate joins of two or more fuzzy query relations in the framework of an extended query system.fuzzy functional dependency and its application to approximate data querying','Management and querying of encrypted data'
'in this paper, we present a new method, called multiview fuzzy querying, which permits to query incomplete, imprecise and heterogeneously structured data stored in a relational database. this method has been implemented in the miel software. miel is used to query the sym\'previus database which gathers information about the behavior of pathogenic germs in food products. in this database, data are incomplete because information about all possible food products and all possible germs is not available; data are heterogeneous because they come from various sources (scientific bibliography, industrial data, etc); data may be imprecise because of the complexity of the underlying biological processes that are involved. to deal with heterogeneity, miel queries the database through several views simultaneously. to query incomplete data, miel proposes to use a fuzzy set, expressing the query preferences of the user. fuzzy sets may be defined on a hierarchized domain of values, called an ontology (values of the domain are connected using the a kind of semantic link). miel also proposes two optional functionalities to help the user query the database: i) miel can use the ontology to enlarge the querying in order to retrieve the nearest data corresponding to the selection criteria; and ii) miel proposes fuzzy completion rules to help the user formulate his/her query. to query imprecise data stored in the database with fuzzy selection criteria, miel uses fuzzy pattern matching.fuzzy querying of incomplete, imprecise, and heterogeneously structured data in the relational model using ontologies and rules','Management and querying of encrypted data'
'this paper describes a geometric query model for dynamic exploring multidimensional data. an application of the model for solving the problem of meterials selection in product design is discussed. data from database are interpreted geometrically as multidimensional points. a query window is a query solid of any shape specified by its location. the queries are formulated with geometric objects and operations over them. the geometric objects and operations are described with implicit functions. the process of query specification is visualized. the user poses the queries through graphics interface accessing dynamically multidimensional points, geometric primitives and applying geometric operations over them.geometric querying for dynamic exploration of multidimensional data','Management and querying of encrypted data'
'in this paper, we have proposed a cryptosystem using state of the art technology. we used the minimum possible compressed speech file for encryption. additionally we applied data compression to speech files and added further compression. then we applied data encryption techniques to minimize security threats. we applied the symmetric and asymmetric encryption algorithms to the compressed speech files. this was followed by hiding these files into cover images using steganography. this camouflages the secret data and reduces chances of eavesdropping. by using compression, the capacity has been increased. however, after applying encryption, the size of the encrypted data becomes much greater than the input file size, therefore hiding it in a cover demands more capacity. we have analyzed the size of the cipher texts of the compressed speech data for further enhancement of the capacity.hiding encrypted speech using steganography','Management and querying of encrypted data'
'ieee p1619 will enable interoperable encryption of data at rest on storage devices.ieee standard for encrypted storage','Management and querying of encrypted data'
'signal processing in the encrypted domain is a new technology for protecting valuable signals from insecure signal processing. although there has been some research in the area, this field of research is still in its infancy. in this paper, we propose a scheme for implementing discrete wavelet transform (dwt) and multi-resolution analysis (mra) in a homomorphic encrypted domain. we first suggest a framework for performing dwt and inverse discrete wavelet transform (idwt) in the encrypted domain and then conduct an analysis of data expansion and quantization errors on the framework. in order to solve the problem of data expansion, which is very important in practical applications, we then present a novel method for reducing data expansion in the case that both dwt and idwt are performed. with the proposed method, multi-level dwt/idwt can be performed with less data expansion in a homomorphic encrypted domain. we address the case of a two-dimensional haar wavelet transform and give some experiments to demonstrate the advantages of our improved method. to the best of our knowledge, this is the first paper on the implementation of dwt and mra in the encrypted domain.implementation of the discrete wavelet transform and multiresolution analysis in the encrypted domain','Management and querying of encrypted data'
'a dynamic accumulator is an algorithm, which merges a large set of elements into a constant-size value such that for an element accumulated, there is a witness confirming that the element was included into the value, with a property that accumulated elements can be dynamically added and deleted into/from the original set. recently wang et al. presented a dynamic accumulator for batch updates at icics 2007. however, their construction suffers from two serious problems. we analyze them and propose a way to repair their scheme. we use the accumulator to construct a new scheme for common secure indices with conjunctive keyword-based retrieval.improvement of a dynamic accumulator at icics 07 and its application in multi-user keyword-based retrieval on encrypted data','Management and querying of encrypted data'
'linked data principles allow for processing sparql queries on-the-fly by dereferencing uris. link-traversal query approaches for linked data have the benefit of up-to-date results and decentralised execution, but operate only on explicit data from dereferenced documents, affecting recall. in this paper, we show how inferable knowledge--specifically that found through owl:sameas and rdfs reasoning--can improve recall in this setting. we first analyse a corpus featuring 7 million linked data sources and 2.1 billion quadruples: we (1) measure expected recall by only considering dereferenceable information, (2) measure the improvement in recall given by considering rdfs:seealso links as previous proposals did. we further propose and measure the impact of additionally considering (3) owl:sameas links, and (4) applying lightweight rdfs reasoning for finding more results, relying on static schema information. we evaluate different configurations for live queries covering different shapes and domains, generated from random walks over our corpus.improving the recall of live linked data querying through reasoning','Management and querying of encrypted data'
'this paper introduces a new approach for efficiently indexing and querying constantly evolving data. traditional data index structures suffer from frequent updating cost and result in unsatisfactory performance when data changes constantly. existing approaches try to reduce index updating cost by using a simple linear or recursive function to define the data evolution, however, in many applications, the data evolution is far too complex to be accurately described by a simple function. we propose to take each constantly evolving data as a time series and use the arima (autoregressive integrated moving average) methodology to analyze and model it. the model enables making effective forecasts for the data. the index is developed based on the forecasting intervals. as long as the data changes within its corresponding forecasting interval, only its current value in the leaf node needs to be updated and no further update needs to be done to the index structure. the model parameters and the index structure can be dynamically adjusted. experiments show that the forecasting interval index (fi-index) significantly outperforms traditional indexes in a high updating environment.indexing and querying constantly evolving data using time series analysis','Management and querying of encrypted data'
'indexing and querying xml data for regular path expressions','Management and querying of encrypted data'
'beyond mere automation of tasks, a major potential of human resource information systems &#40;hris&#41; is to informate human resource management &#40;hrm&#41;. within current hris, the informate function is realised based on a data querying approach. given a major innovation in data analysis subsumed under the concept of &#39;data mining&#39;, possibly valuable potentials to informate hrm are lost while overlooking the data mining approach. therefore our paper aims at a conceptual evaluation of both approaches. we therefore discuss and evaluate data mining as a novel approach compared to data querying as the conventional approach to informating hrm. based on a robust framework of informational contributions, our analysis reveals interesting potentials of data mining to generate explicative and prognostic information. thus data mining enriches and complements the conventional querying approach. furthermore, recommendations for future research are derived in order to deepen the knowledge on the contributions of data mining to informate hrm.informating hrm&#58; a comparison of data querying and data mining','Management and querying of encrypted data'
'in this paper we present a very practical ciphertext-only cryptanalysis of gsm (global system for mobile communications) encrypted communication, and various active attacks on the gsm protocols. these attacks can even break into gsm networks that use &#x201c;unbreakable&#x201d; ciphers. we first describe a ciphertext-only attack on a5/2 that requires a few dozen milliseconds of encrypted off-the-air cellular conversation and finds the correct key in less than a second on a personal computer. we extend this attack to a (more complex) ciphertext-only attack on a5/1. we then describe new (active) attacks on the protocols of networks that use a5/1, a5/3, or even gprs (general packet radio service). these attacks exploit flaws in the gsm protocols, and they work whenever the mobile phone supports a weak cipher such as a5/2. we emphasize that these attacks are on the protocols, and are thus applicable whenever the cellular phone supports a weak cipher, for example, they are also applicable for attacking a5/3 networks using the cryptanalysis of a5/1. unlike previous attacks on gsm that require unrealistic information, like long known-plaintext periods, our attacks are very practical and do not require any knowledge of the content of the conversation. furthermore, we describe how to fortify the attacks to withstand reception errors. as a result, our attacks allow attackers to tap conversations and decrypt them either in real-time, or at any later time. we present several attack scenarios such as call hijacking, altering of data messages and call theft.instant ciphertext-only cryptanalysis of gsm encrypted communication','Management and querying of encrypted data'
'this paper proposes a way to integrate cleanly relational databases and xml documents. the main idea is to draw a clear line of demarcation between the two concepts by modelling xml documents as a new atomic sql type. the standardised xml tools like xpath, xquery, xslt are then user-defined functions that operate on this type. well-defined interoperability is guaranteed by, on the one hand, defining a standard way to markup sql relations as xml documents and, thus, to make them accessible to the xml tools; on the other hand, xpath and xquery queries run against the xml portion of the database can use the same predefined schema to make their results accessible to the sql language for further processing. additionally, a method for set-oriented evaluation of regular path expressions is presented that integrates into our implementation framework.integrated querying of xml data in rdbmss','Management and querying of encrypted data'
'xml instances are not necessarily self-contained but may have connections to remote xml data residing on other servers. in this paper, we show that-in spite of its minor support and use in the xml world-the xlink language provides a powerful mechanism for expressing such links both from the modeling point of view and for actually querying interlinked xml data: in our dbxlink approach, the links are not seen as explicit links (where the users must be aware of the links and traverse them explicitly in their queries), but define views that combine into a logical, transparent xml model which serves as an external schema and can be queried by xpath/xquery. we motivate the underlying modeling and give a concise and declarative specification as an xml-to-xml mapping. we also describe the implementation of the model as an extension of the exist [exist: an open source native xml database, http://exist-db.org/] xml database system. the approach can be applied both for distribution of data and for integration of data from autonomous sources.integrating and querying distributed xml data via xlink','Management and querying of encrypted data'
'programs and a database\'s schema contain complex data and control dependencies that make modifying the schema along with multiple portions of the source code difficult to change. in this paper, we address the problem of exploring and analyzing those dependencies that exist between a program and a database\'s schema using keyword search techniques inside a database management system (dbms). as a result, we present qdpc, a novel system that allows the integration and flexible querying within a dbms of source code and a database\'s schema. the integration focuses on obtaining the approximate matches that exist between source files (classes, function and variable names) and the database\'s schema (table names and column names), and then storing them in summarization tables inside a dbms. these summarization tables are then analyzed with sql queries to find matches that are related to a set of keywords provided by the user. it is possible to perform additional analysis of the discovered matches by computing aggregations over the obtained matches, and to perform sophisticated analysis by computing olap cubes. in our experiments, we show that we obtain an efficient integration and allow complex analysis of the dependencies inside the dbms. furthermore, we show that searching for data dependencies and building olap cubes can be obtained in an efficient manner. our system opens up the possibility of using the keyword search for software engineering applications.integrating and querying source code of programs working on a database','Management and querying of encrypted data'
'we present the query-driven exploration of semistructured dataand meta-data with conflicts and partial knowledge (quest) system for supporting the integration of scientific data and taxonomies in the presence of misalignments and conflicts. quest relies on a novel constraint-based data model that captures both value and structural conflicts and enables researchers to observe and resolve such misalignments in the integrated data by considering the context provided by the data requirements of given research questions.integrating and querying taxonomies with quest in the presence of conflicts','Management and querying of encrypted data'
'identification of patterns in time series data sets is a task that arises in a wide variety of application domains [4]. this paper presents a user interface for the timebox query model of rectangular regions that specify constraints over time series data sets. a prototype application based on timeboxes is presented. collaborations with potential users will guide the design of enhanced functionality. usability tests and controlled experiments will be conducted to evaluate the timebox query model.interactive querying of time series data','Management and querying of encrypted data'
'current approaches to cell motion analysis rely on cell tracking. in certain cases, the trajectories of each cell is not as informative as a representation of the overall motion in the scene. in this paper, we extend a cell motion descriptor and provide methods for the intuitive visualization and querying of cell motion. our approach allows for searches of scale- and rotation-invariant motion signatures, and we develop a desktop application that researchers can use to query biomedical video quickly and efficiently. we demonstrate this application on synthetic video sets and &lt;em&gt;in vivo&lt;/em&gt; microscopy video of cells in a mouse liver. intuitive visualization and querying of cell motion','Management and querying of encrypted data'
'the scalability of a wireless sensor network has been of interest and importance. we use a constrained optimization framework to derive fundamental scaling laws for both unstructured sensor networks (which use blind sequential search for querying) and structured sensor networks (which use efficient hash-based querying). we find that the scalability of a sensor network\'s performance depends upon whether or not the increase in energy and storage resources with more nodes is outweighed by the concomitant application-specific increase in event and query loads. we have figured out the theoretical scaling laws for the networks of 2 dimensional deployment in our previous work [2]. we report on our work-in-progress aimed at extending the scaling laws to networks of various dimensional deployment. as a recent achievement, we find that m&#8901;q1/2 must be o(n d?1/2d) for unstructured networks, and m&#8901;q d/d+1 must be o(n d?1/d) for structured networks, to ensure scalable network performance, where m is the number of events sensed by a network over a finite period of deployment, q is the number of queries for each event, d is the dimension of deployment, and n is the size of the network. these conditions determine (i) whether or not the energy requirement per node grows without bound with the network size for a fixed-duration deployment, (ii) whether or not there exists a maximum network size that can be operated for a specified duration on a fixed energy budget, and (iii) whether the network lifetime increases or decreases with the size of the network for a fixed energy budget. an interesting finding of this extension is that 3d uniform deployments are inherently more scalable than 2d uniform deployments, which in turn are more scalable than 1d uniform deployments.is data-centric storage and querying scalable?','Management and querying of encrypted data'
'this paper considers the problem of an attacker disrupting an encrypted victim wireless ad hoc network through jamming. jamming is broken down into layers and this paper focuses on jamming at the transport/network layer. jamming at this layer exploits aodv and tcp protocols and is shown to be very effective in simulated and real networks when it can sense victim packet types, but the encryption is assumed to mask the entire header and contents of the packet so that only packet size, timing, and sequence is available to the attacker for sensing. a sensor is developed and tested on live data. the classification is found to be highly reliable for many packet types. the relative roles of size, timing, and sequence are discussed along with the implications for making networks more secure.jamming and sensing of encrypted wireless ad hoc networks','Management and querying of encrypted data'
'confidential data stored on mass storage devices is atrisk to be disclosed to persons getting physical or administratoraccess to the device. encrypting the data reducesthis risk, at the cost of more cumbersome administration.in this publication, we examine the problem of encrypteddata storage in a grid computing environment,where storage capacity and data is shared across organizationalboundaries. we propose an architecture thatallows users to store and share encrypted data in this environment.access to decryption keys is granted based onthe grids data access permissions. the system is thereforeusable as an additional security feature togetherwith a classical access control mechanism. data ownerscan choose different tradeoffs of security versusefficiency. storage servers need not to be trusted and commonaccess control models are supported.key management for encrypted data storage in distributed systems','Management and querying of encrypted data'
'cryptography is now popularized and is widely used anywhere for many aims such as data confidentiality and integrity. the key of cryptography has a lifetime, thus a key update issue is well-known to be one of hard problems in practice. according to the national institute of standards and technology sp 800-57, the lifetime of the cryptographic key and the lifetime of encrypted data are strictly limited. in other words, the encrypted data is required to be periodically re-encrypted. in this paper, we point out that this key update issue is now crucial in network storage and propose a key update mechanism efficiently reducing the communication and computation cost of re-encryption.key update mechanism for network storage of encrypted data','Management and querying of encrypted data'
'we consider the following problem: a user stores encrypted documents on an untrusted server, and wishes to retrieve all documents containing some keywords without any loss of data confidentiality. conjunctive keyword searches on encrypted data have been studied by numerous researchers over the past few years, and all existing schemes use keyword fields as compulsory information. this however is impractical for many applications. in this paper, we propose a scheme of keyword field-free conjunctive keyword searches on encrypted data, which affirmatively answers an open problem asked by golle &lt;em&gt;et al.&lt;/em&gt; at &lt;em&gt;acns&lt;/em&gt; 2004. furthermore, the proposed scheme is extended to the dynamic group setting. security analysis of our constructions is given in the paper. keyword field-free conjunctive keyword searches on encrypted data and extension for dynamic groups','Management and querying of encrypted data'
'voice over ip (voip) has become a popular protocol for making phone calls over the internet. due to the potential transit of sensitive conversations over untrusted network infrastructure, it is well understood that the contents of a voip session should be encrypted. however, we demonstrate that current cryptographic techniques do not provide adequate protection when the underlying audio is encoded using bandwidth-saving variable bit rate (vbr) coders. explicitly, we use the length of encrypted voip packets to tackle the challenging task of identifying the language of the conversation. our empirical analysis of 2,066 native speakers of 21 different languages shows that a substantial amount of information can be discerned from encrypted voip traffic. for instance, our 21-way classifier achieves 66\% accuracy, almost a 14-fold improvement over random guessing. for 14 of the 21 languages, the accuracy is greater than 90\%. we achieve an overall binary classification (e.g., \"is this a spanish or english conversation?\") rate of 86.6\%. our analysis highlights what we believe to be interesting new privacy issues in voip.language identification of encrypted voip traffic','Management and querying of encrypted data'
'developers need to access persistent xml data programmatically. object-oriented access is often the preferred method. translating xml data into objects or vice-versa is a hard problem due to the data model mismatch and the difficulty of query translation. our prototype addresses this problem by transforming object-based queries and updates into queries and updates on xml using declarative mappings between classes and xml schema types. our prototype extends the ado.net entity framework and leverages its object-relational mapping capabilities. we demonstrate how a developer can interact with stored relational and xml data using the language integrated query (linq) feature of .net. we show how linq queries are translated into a combination of sql and xquery. finally, we illustrate how explicit mappings facilitate data independence upon database refactoring.language-integrated querying of xml data in sql server','Management and querying of encrypted data'
'this paper emphasizes the development and application of technologies to effective guarantying the confidentiality and integrity of patient data in gridenabled biomedical applications. by strongly focusing on the interaction between security technologies and the human environment, this paper relates the experience of developing a model for the storage and management of encrypted medical data in the grid. the ideas and concepts behind the proposed solution are briefly explained, as well as the components implementing the model in an ogsa compliant architecture. trencadis, a project for managing dicom structured reporting objects for use in a valencian cyberinfrastructure for medical imaging in oncology (cvimo), is currently adopting the architecture as a core component.long-term storage and management of encrypted biomedical data in real scenarios','Management and querying of encrypted data'
'a data warehouse (dw) is a database that integrates external data sources (edss) for the purpose of advanced data analysis. the methods of designing a dw usually assume that a dw has a static schema and structures of dimensions. in practice, schema and dimensions&#8217; structures often change as the result of the evolution of edss, changes of the real world represented in a dw, new user requirements, new versions of software being installed, and system tuning activities. examples of various change scenarios can be found in [1,8].managing and querying versions of multiversion data warehouse','Management and querying of encrypted data'
'we study a joint scheduling and coding problem for collecting multi-snapshots spatial data in a resource constrained sensor network. motivated by a distributed coding scheme for single snapshot data collection [7], we generalize the scenario to include multi-snapshots and general coding schemes. associating a utility function with the recovered data, we aim to maximize the expected utility gain through joint coding and scheduling. we first assume non-mixed coding where coding is only allowed for data of the same snapshot. we study the problem of how to schedule (or prioritize) the codewords from multiple snapshots under an archiving model where data from all snapshots are of interests with additive utilities. we formalize the scheduling problem into a multi-armed bandit (mab) problem. we derive the optimal solution using gittins indices, and identify conditions under which a greedy algorithm is optimal. we then consider random mixed coding where data from different snapshots are randomly coded together. we generalize the growth codes in [7] to arbitrary linear-codes-based random mixed coding and show that there exists an optimal degree of coding. various practical issues and the buffer size impact on performance are then discussed.maximizing the data utility of a data archiving & querying system through joint coding and scheduling','Management and querying of encrypted data'
'we have designed a das (data as a service) model of e-commerce by using the semi-structured xml data file system instead of traditional database--the general b / s model, which mainly includes the clients&#8217; data encryption, key management on the server and xml encrypted data querying processing on the service provider. the searching method that we connected the xml index&#8216;s skill and deweyencoding [1,2] based on the level to achieve users&#8217; order query processing of xml encrypted data. this project has greatly reduced the system&#8217;s maintenance costs and improved the system&#8217;s capability and operating efficiency. at the same time it can help the enterprise to promote their benefits under the premise of guaranteeing the system&#8217;s security and stability.model design on das and research of xml encrypted data querying','Management and querying of encrypted data'
'modeling and querying content description and quality adaptation capabilities of audio-visual data','Management and querying of encrypted data'
'although pictorial renditions of statistical data are ubiquitous, few techniques and standards exist to exchange, search and query these graphical representations. we present several improvements to human-graph interaction including (i) a new approach to manage statistical graph knowledge by semantic annotation of graphs that bridges the gap between web 2.0 social tagging and formal, logic-based approaches, (ii) knowledge management and discovery across a non-trivial graph knowledge base and (iii) sophisticated question answering that requires background knowledge.modeling and querying graphical representations of statistical data','Management and querying of encrypted data'
'large organizations have a multitude of data sources across the enterprise and want to obtain business value from all of them. while the majority of these data sources may be consolidated in an enterprise data warehouse, many business units have their own data marts where analysis is carried out against data stored in multidimensional data structures. it is often critical to pose queries which span both these sources. this is a challenge since these sources have differing models and query languages (sql vs mdx). the siebel analytics server enables this requirement to be fulfilled. in this paper, we describe how the multidimensional metadata is modeled relationally within siebel analytics, efficient sql to mdx translation algorithms and the conversion protocols required to convert a multidimensional result into a relational rowset.modeling and querying multidimensional data sources in siebel analytics','Management and querying of encrypted data'
'resource description framework (rdf) and its extension rdf schema (rdfs) are data models to represent information on the web. they use rdf triples to make statements. because of lack of knowledge, some triples are known to be true with a certain degree of belief. existing approaches either assign each triple a probability and assume that triples are statistically independent of each other, or only model statistical relationships over possible objects of a triple. in this paper, we introduce probabilistic rdfs (prdfs) to model statistical relationships among correlated triples by specifying the joint probability distributions over them. syntax and semantics of prdfs are given. since there may exist some truth value assignments for triples that violate the rdfs semantics, an algorithm to check the consistency is provided. finally, we show how to find answers to queries in sparql. the probabilities of the answers are approximated using a monte-carlo algorithm.modeling and querying probabilistic rdfs data sets with correlated triples','Management and querying of encrypted data'
'modeling and querying probabilistic xml data','Management and querying of encrypted data'
'modeling and querying semi-structured data','Management and querying of encrypted data'
'modeling and querying software repositories is an integral part of reverse engineering legacy software systems. current data models and query languages for software repositories lack generality, extensibility, and sufficient formalism. in this paper, we propose an algebraic framework that provides both an extensible data model capable of modeling software information at multiple levels of abstraction, as well as an algebraic applicative query language that can be used effectively to express queries on the data model. we demonstrate the power of the algebraic framework by expressing a wide variety of queries drawn from reverse engineering literature. the algebraic approach combines issues of formal elegance, expressive power, and query processing and optimization within a single framework.modeling and querying software repositories','Management and querying of encrypted data'
'for accessing and processing the information provided on the web, there is a need for extarction, restructuring, and integration of semistructured data from autonomous, heterogeneous sources. in this paper, we regard the web and its contents as a unit, represented in an object-oriented data model: the web structure (inter-document level), given by its hyperlinks, the parse-trees of web pages (intra-document level), and their contents. the model is complemented by a rule-based object-oriented language which is extended by web access capabilities and allows for and navigation in the unified model. we show the practicability of our approach by using the florid system.modeling and querying structure and contents of the web','Management and querying of encrypted data'
'modeling and querying video data','Management and querying of encrypted data'
'the authors discuss data models and query languages for xml data. they propose a data model, called xdt, which takes care of the structural features of xml data. xdt represents xml data by means of labeled oriented graphs. with respect to other previously proposed models, xdt handles links definable in both xml and xll. queries are made through an sql-like language which uses weighted path queries, i.e. path queries based on weighted regular languages and gives, as a result, a set of pairs (node/weight) where node is an \'element\' of an xml document and weight gives information about the relevance of the node.modeling and querying xml-data','Management and querying of encrypted data'
'we propose a new functional framework for modeling, querying and reasoning about olap databases. the framework represents data (data cubes and dimensional hierarchies) and querying constructs as first-order and second-order functional symbols respectively. a polymorphic attribute-based type system is used to annotate the functional symbols with proper type information. furthermore, semantic knowledge about the functional symbols, such as the properties of dimensional hierarchical structures and algebraic identities among query constructs, can be specified by equations which permits equational reasoning on equivalence of olap queries and generalized summarizability of aggregate views.modeling, querying and reasoning about olap databases','Management and querying of encrypted data'
'a number of proposals for integrating geographical (geographical information systems-gis) and multidimensional (data warehouse-dw and online analytical processing-olap) processing are found in the database literature. however, most of the current approaches do not take into account the use of a gdw (geographical data warehouse) metamodel or query language to make available the simultaneous specification of multidimensional and spatial operators. to address this, this paper discusses the uml class diagram of a gdw metamodel and proposes its formal specifications. we then present a formal metamodel for a geographical data cube and propose the geographical multidimensional query language (geomdql) as well. geomdql is based on well-known standards such as the multidimensional expressions (mdx) language and ogc simple features specification for sql and has been specifically defined for spatial olap environments based on a gdw. we also present the geomdql syntax and a discussion regarding the taxonomy of geomdql query types. additionally, aspects related to the geomdql architecture implementation are described, along with a case study involving the brazilian public healthcare system in order to illustrate the proposed query language.modelling and querying geographical data warehouses','Management and querying of encrypted data'
'modelling and querying video data','Management and querying of encrypted data'
'multi-application smart cards and encrypted data processing','Management and querying of encrypted data'
'we design an encryption scheme called multi-dimensional range query over encrypted data (mrqed), to address the privacy concerns related to the sharing of network audit logs and various other applications. our scheme allows a network gateway to encrypt summaries of network flows before submitting them to an untrusted repository. when network intrusions are suspected, an authority can release a key to an auditor, allowing the auditor to decrypt flows whose attributes (e.g., source and destination addresses, port numbers, etc.) fall within specific ranges. however, the privacy of all irrelevant flows are still preserved. we formally define the security for mrqed and prove the security of our construction under the decision bilinear diffie-hellman and decision linear assumptions in certain bilinear groups. we study the practical performance of our construction in the context of network audit logs. apart from network audit logs, our scheme also has interesting applications for financial audit logs, medical privacy, untrusted remote storage, etc. in particular, we show that mrqed implies a solution to its dual problem, which enables investors to trade stocks through a broker in a privacypreserving manner.multi-dimensional range query over encrypted data','Management and querying of encrypted data'
'skype is a popular peer-to-peer (p2p) voice over ip (voip) application evolving quickly since its launch in 2003. however, the ability to traverse network address transla- tion (nat) and bypass firewalls, as well as the induced bandwidth burden due to the super node (sn) mechanism, make skype considerably a threat to enterprise networks se- curity and availability. because skype uses both encryp- tion and overlays, detection and blocking of skype is non- trivial. motivated by the work of biondi and desclaux [3], we adopt the view of skype as a backdoor and we take a forensic approach to analyze it. we share our experience in this paper. with the forensic evidence, we identify a transport layer communication framework for skype. we further formulate a set of socket-based detection and con- trol policies for skype traffics. our detection method is a hybrid between payload and non-payload inspections, with improved accuracy and version sustainability over the tra- ditional payload-only approaches. our solution is practi- cable both inside and outside the nat firewalls. this break- through makes the detection, blocking, and prioritization of skype traffics possible in both the enterprise internal net- works and the internet services providers carrier networks. keywords: enterprise network security, network foren- sics, traffic analysis, skype, blocking, traffic prioritiza- tion, nat traversal, reverse engineeringnetwork forensic on encrypted peer-to-peer voip traffics and the detection, blocking, and prioritization of skype traffics','Management and querying of encrypted data'
'xml documents changing will lead to global index information updating in existing literatures for encrypted xml data querying schemes. there should have a scheme considering updating efficiency of index information for encrypted xml data. the paper presents the number list-based interval labeling scheme (nlbils) for encrypted xml data. the basic idea is that if there have not enough space available for nodes inserting, the labeling process assigns a number for the sub-tree to be inserted, and then start with a new labeling process for each node in the sub-tree. the labeling result of each node will be consisting of a number list with its parent\'s nodes label. the proposed scheme makes index information maintenance more efficient, and it is easy to update xml data with decreasing the number of affected nodes to the lowest. the evaluation results show that the proposed scheme supports index information maintenance in an efficient way. the proposed scheme can be used to build structural index information for encrypted xml data query processing.number list-based interval labeling scheme for structural index building of encrypted xml data','Management and querying of encrypted data'
'when it is desired to transmit redundant data over an insecure and bandwidth-constrained channel, it is customary to first compress the data and then encrypt it. in this paper, we investigate the novelty of reversing the order of these steps, i.e., first encrypting and then compressing, without compromising either the compression efficiency or the information-theoretic security. although counter-intuitive, we show surprisingly that, through the use of coding with side information principles, this reversal of order is indeed possible in some settings of interest without loss of either optimal coding efficiency or perfect secrecy. we show that in certain scenarios our scheme requires no more randomness in the encryption key than the conventional system where compression precedes encryption. in addition to proving the theoretical feasibility of this reversal of operations, we also describe a system which implements compression of encrypted data.on compressing encrypted data','Management and querying of encrypted data'
'this paper investigates compression of encrypted data. it has been previously shown that data encrypted with vernam\'s scheme, also known as the one-time pad, can be compressed without knowledge of the secret key, therefore this result can be applied to stream ciphers used in practice. however, it was not known how to compress data encrypted with non-stream ciphers. in this paper, we address the problem of compressing data encrypted with block ciphers, such as the advanced encryption standard (aes) used in conjunction with one of the commonly employed chaining modes. we show that such data can be feasibly compressed without knowledge of the key. we present performance results for practical code constructions used to compress binary sources.on compression of data encrypted with block ciphers','Management and querying of encrypted data'
'we consider video sequences that have been encrypted uncompressed. since encryption masks the source, traditional data compression algorithms are rendered ineffective. however, it has been shown that through the use of distributed source-coding techniques, the compression of encrypted data is in fact possible. this means that it is possible to reduce data size without requiring that the data be compressed prior to encryption. indeed, under some reasonable conditions, neither security nor compression efficiency need be sacrificed when compression is performed on the encrypted data (johnson et al., 2004). in this paper we develop an algorithm for the practical lossless compression of encrypted gray scale video. our method is based on considering the temporal correlations in video. this move to temporal dependence builds on our previous work on memoryless sources, and one- and two-dimensional markov sources. for comparison, a motion-compensated lossless video encoder can compress each unencrypted frame of the standard \"foreman&#148; test video sequence by about 57\%. our algorithm can compress compress the same frames, after encryption, by about 33\%.on compression of encrypted video','Management and querying of encrypted data'
'in this paper we address the problem of consistency for cryptographic file systems. a cryptographic file system protects the users&#39; data from the file server, which is possibly untrusted and might exhibit byzantine behavior, by encrypting the data before sending it to the server. the consistency of the encrypted file objects that implement a cryptographic file system relies on the consistency of the two components used to implement them: the file storage protocol and the key distribution protocol. we first define two generic classes of consistency conditions that extend and generalize existing consistency conditions. we then formally define consistency for encrypted file objects in a generic way: for any consistency conditions for the key and file objects belonging to one of the two classes of consistency conditions considered, we define a corresponding consistency condition for encrypted file objects. we finally provide, in our main result, necessary and sufficient conditions for the consistency of the key distribution and file storage protocols under which the encrypted storage is consistent. our framework allows the composition of existing key distribution and file storage protocols to build consistent encrypted file objects and simplifies complex proofs for showing the consistency of encrypted storage.on consistency of encrypted files','Management and querying of encrypted data'
'a data warehouse (dw) is fed with data that come from external data sources that are production systems. external data sources, which are usually autonomous, often change not only their content but also their structure. the evolution of external data sources has to be reflected in a dw, that uses the sources. traditional dw systems offer a limited support for handling dynamics in their structure and content. a promising approach to handling changes in dw structure and content is based on a multiversion data warehouse. in such a dw, each dw version describes a schema and data at certain period of time or a given business scenario, created for simulation purposes. in order to appropriately analyze multiversion data, an extension to a traditional sql language is required. in this paper we propose an approach to querying a multiversion dw. to this end, we extended a sql language and built a multiversion query language interface with functionality that allows: (1) expressing queries that address several dw versions and (2) presenting their results annotated with metadata information.on querying versions of multiversion data warehouse','Management and querying of encrypted data'
'the ability to perform effective xml data retrieval in the absence of schema knowledge has recently received considerable attention. the majority of relevant proposals employs heuristics that identify groups of meaningfully related nodes using information extracted from the input data. these heuristics are employed to effectively prune the search space of all possible node combinations and their popularity is evident by the large number of such heuristics and the systems that use them. however, a comprehensive study detailing the relative merits of these heuristics has not been performed thus far. one of the challenges in performing this study is the fact that these techniques have been proposed within different and not directly comparable contexts. in this paper, we attempt to fill this gap. in particular, we first abstract the common selection problem that is tackled by the relatedness heuristics and show how each heuristic addresses this problem. we then identify data categories where the assumptions made by each heuristic are valid and draw insights on their possible effectiveness. our findings can help systems implementors understand the strengths and weaknesses of each heuristic and provide simple guidelines for the applicability of each one.on the effectiveness of flexible querying heuristics for xml data','Management and querying of encrypted data'
'we are interested in providing databases with fuzzy data representation and flexible querying capabilities. this work concerns with a performance analysis based on statistical techniques to evaluate methods for fuzzy queries over fuzzy data. we introduce the application of derivation principle for this kind of queries and show its practical benefit using a prototype that we have built on top of oracle\'s dbms.on the performance of fuzzy data querying','Management and querying of encrypted data'
'xslt is the prime example of an xml query language based on tree-walking. indeed, stripped down, xslt is just a tree-walking tree-transducer equipped with registers and look-ahead. motivated by this connection, we want to pinpoint the computational power of devices based on tree-walking. we show that in the absence of unique identifiers even very powerful extensions of the tree-walking paradigm are not relationally complete. that is, these extensions do not capture all of first-order logic. in contrast, when unique identifiers are available, we show that various restrictions allow to capture logspace, ptime, pspace, and exptime. these complexity classes are defined w.r.t. a turing machine model working directly on (attributed) trees. when no attributes are present, relational storage does not add power; whether look-ahead adds power is related to the open question whether tree-walking captures the regular tree languages.on the power of walking for querying tree-structured data','Management and querying of encrypted data'
'optimized querying of integrated data over the web','Management and querying of encrypted data'
'the setting in which alice wishes to disclose log entries to bob, only of which contain a keyword specified by him was studied. suppose that alice also wishes to keep other entries secret to bob, even after the disclosure, then how could alice convince bob that the undisclosed entries surely do not contain the keyword? such can be ensured with a variation of searchable encryption scheme; with alice\'s help, bob can search through the encrypted log and specify the entries which contain the keyword. it is a common step to extend a simple search to a search with boolean queries, such as and and or. however, it is not simple to disclose entries which contain more than a single keyword, without leaking information on the undisclosed entries. in this paper, a scheme to disclose entries which match a boolean query is proposed. by using a bloom filter as an encoded index, the scheme reduces the frequencies of comparison and the size of matching data, at the sacrifice of faultlessness.partial disclosure of searchable encrypted data with support for boolean queries','Management and querying of encrypted data'
'when multiple users access a network storage system for cloud computing, security becomes a key factor in the service, as well as performance and reliability.the \"encrypt-on-disk\'\' scheme effectively protects transmitted and stored data in network storage. however, this scheme has the problem of revocation for shared files. active revocation is safe but has denial periods to allow immediate reencryption, while lazy revocation has no denial period but is unsafe during the delay.we propose intelligent storage nodes capable of handling active revocation in storage without the denial period by adopting a primary--backup configuration.this approach provides a good combination of security and availability by replication.however, the reencryption process negatively affects the update performance. delaying the reencryption process and disk write on the backup node improves performance with no ill effect on security and a small decrease of mttdl for the simple primary-backup configuration.we evaluate the performance of the proposed approaches by experiments, and the reliability by estimation.performance and reliability of a revocation method utilizing encrypted backup data','Management and querying of encrypted data'
'it is desirable to store data on data storage servers such as mail servers and file servers in encrypted form to reduce security and privacy risks. however, this usually implies that one has to sacrifice functionality for security. for example, if a client wishes to retrieve only documents containing certain words, it was not previously known how to let the data storage server perform the search and answer the query without loss of data confidentiality.in this paper, we describe our cryptographic schemes for the problem of searching on encrypted data and provide proofs of security for the resulting crypto systems. our techniques have a number of crucial advantages. they are provably secure: they provide provable secrecy for encryption, in the sense that the untrusted server cannot learn anything about the plain text when only given the ciphertext; they provide query isolation for searches, meaning that the untrusted server cannot learn anything more about the plaintext than the search result; they provide controlled searching, so that the untrusted server cannot search for an arbitrary word without the user\'s authorization; they also support hidden queries, so that the user may ask the untrusted server to search for a secret word without revealing the word to the server. the algorithms we present are simple, fast (for a document of length $n$, the encryption and search algorithms only need o(n) stream cipher and block cipher operations), and introduce almost no space and communication overhead, and hence are practical to use today.practical techniques for searches on encrypted data','Management and querying of encrypted data'
'data confidentiality is a major concern in database systems. encryption is a useful tool for protecting the confidentiality of sensitive data. however, when data is encrypted, performing queries becomes more challenging. in this paper, we study efficient and provably secure methods for queries on encrypted data stored in an outsourced database that may be susceptible to compromise. specifically, we show that, in our system, even if an intruder breaks into the database and observes some interactions between the database and its users, he only learns very little about the data stored in the database and the queries performed on the data. our work consists of several components. first, we consider databases in which each attribute has a finite domain and give a basic solution for certain kinds of queries on such databases. then, we present two enhanced solutions, one with a stronger security guarantee and the other with accelerated queries. in addition to providing proofs of our security guarantees, we provide empirical performance evaluations. our experiments demonstrate that our solutions are fast on large-sized real data.privacy-preserving queries on encrypted data','Management and querying of encrypted data'
'probabilistic data modeling and querying for location-based data warehouses','Management and querying of encrypted data'
'monomi is a system for securely executing analytical workloads over sensitive data on an untrusted database server. monomi works by encrypting the entire database and running queries over the encrypted data. monomi introduces split client/server query execution, which can execute arbitrarily complex queries over encrypted data, as well as several techniques that improve performance for such workloads, including per-row precomputation, space-efficient encryption, grouped homomorphic addition, and pre-filtering. since these optimizations are good for some queries but not others, monomi introduces a designer for choosing an efficient physical design at the server for a given workload, and a planner to choose an efficient execution plan for a given query at runtime. a prototype of monomi running on top of postgres can execute most of the queries from the tpc-h benchmark with a median overhead of only 1.24&#215; (ranging from 1.03&#215;to 2.33&#215;) compared to an un-encrypted postgres database where a compromised server would reveal all data.processing analytical queries over encrypted data','Management and querying of encrypted data'
'we give a generic methodology to unlinkably anonymize cryptographic schemes in bilinear groups using the boneh-goh-nissim cryptosystem and &lt;emphasis type=\"smallcaps\"&gt;nizk&lt;/emphasis&gt; proofs in the line of groth, ostrovsky and sahai. we illustrate our techniques by presenting the first instantiation of anonymous proxy signatures (in the standard model), a recent primitive unifying the functionalities and strong security notions of group and proxy signatures. to construct our scheme, we introduce various efficient &lt;emphasis type=\"smallcaps\"&gt;nizk&lt;/emphasis&gt; and witness-indistinguishable proofs. proofs on encrypted values in bilinear groups and an application to anonymity of signatures','Management and querying of encrypted data'
'the processing and encryption of multimedia content are generally considered sequential and independent operations. in certain multimedia content processing scenarios, it is, however, desirable to carry out processing directly on encrypted signals. the field of secure signal processing poses significant challenges for both signal processing and cryptography research; only few ready-togo fully integrated solutions are available. this study first concisely summarizes cryptographic primitives used in existing solutions to processing of encrypted signals, and discusses implications of the security requirements on these solutions. the study then continues to describe two domains in which secure signal processing has been taken up as a challenge, namely, analysis and retrieval of multimedia content, as well as multimedia content protection. in each domain, state-of-the-art algorithms are described. finally, the study discusses the challenges and open issues in the field of secure signal processing.protection and retrieval of encrypted multimedia content','Management and querying of encrypted data'
'new means of communication are constantly emerg- ing, some of which may constitute resource mis- use of an organisation\'s network system. identify- ing the protocols used is straight-forward when in- specting network logs, but we focus on the problem of identifying the underlying protocol present in an unknown tcp connection. actions are difficult to detect if the underlying protocol is encrypted and tunneled through a proxy server or ssh. we use a graph-comparison approach to build profiles of sev- eral protocols, and attempt to classify an unknown, encrypted protocol against these profiles using only the visible behaviour of the protocol being tunneled-- the size, timing and direction of packets.protocol identification of encrypted network traffic','Management and querying of encrypted data'
'querying aggregate data','Management and querying of encrypted data'
'a lightweight method for querying and browsing multiple relational and xml data sources is presented. the approach is based on a simple abstraction of relational and xml data models. a query language for the abstraction to which xpath and sql map to naturally is introduced. a unique feature of the query language is that it unifies structured queries and keyword-based searches.querying and browsing xml and relational data sources','Management and querying of encrypted data'
'the management of uncertainty in large databases has recently attracted tremendous research interest. data uncertainty is inherent in many emerging and important applications, including locationbased services, wireless sensor networks, biometric and biological databases, and data stream applications. in these systems, it is important to manage data uncertainty carefully, in order to make correct decisions and provide high-quality services to users. to enable the development of these applications, uncertain database systems have been proposed. they consider data uncertainty as a \"first-class citizen\", and use generic data models to capture uncertainty, as well as provide query operators that return answers with statistical confidences. we summarize our work on uncertain databases in recent years. we explain how data uncertainty can be modeled, and present a classification of probabilistic queries (e.g., range query and nearest-neighbor query). we further study how probabilistic queries can be efficiently evaluated and indexed. we also highlight the issue of removing uncertainty under a stringent cleaning budget, with an attempt of generating high-quality probabilistic answers.querying and cleaning uncertain data','Management and querying of encrypted data'
'although data stored in xml is of increasing importance, most existing data repositories are still managed by relational database systems. in light of this, recent xml database research has focused on extending relational database systems to handle xml data efficiently. while there are many issues in processing xml data efficiently, containment queries are the queries that often appear and need to be optimized. recently, structural joins have been proposed to process containment queries efficiently. to date, structural join algorithms are mostly based on stacks and/or external b-tree indices. most of these prototypes have been implemented on object databases. this paper proposes an efficient structural join algorithm that can be implemented on top of existing relational databases. experiments show that our method performs far more superior than previous work in both queries and updates.querying and maintaining ordered xml data using relational databases','Management and querying of encrypted data'
'querying and mining data streams','Management and querying of encrypted data'
'the last decade has witnessed a tremendous growths of interests in applications that deal with querying and mining of time series data. numerous representation methods for dimensionality reduction and similarity measures geared towards time series have been introduced. each individual work introducing a particular method has made specific claims and, aside from the occasional theoretical justifications, provided quantitative experimental observations. however, for the most part, the comparative aspects of these experiments were too narrowly focused on demonstrating the benefits of the proposed methods over some of the previously introduced ones. in order to provide a comprehensive validation, we conducted an extensive set of time series experiments re-implementing 8 different representation methods and 9 similarity measures and their variants, and testing their effectiveness on 38 time series data sets from a wide variety of application domains. in this paper, we give an overview of these different techniques and present our comparative experimental findings regarding their effectiveness. our experiments have provided both a unified validation of some of the existing achievements, and in some cases, suggested that certain claims in the literature may be unduly optimistic.querying and mining of time series data','Management and querying of encrypted data'
'xml represents both content and structure of documents. taking advantage of the document structure promises to greatly improve the retrieval precision. in this article, we present a retrieval technique that adopts the similarity measure of the vector space model, incorporates the document structure, and supports structured queries. our query model is based on tree matching as a simple and elegant means to formulate queries without knowing the exact structure of the data. using this query model we propose a logical document concept by deciding on the document boundaries at query time. we combine structured queries and term-based ranking by extending the term concept to structural terms that include substructures of queries and documents. the notions of term frequency and inverse document frequency are adapted to logical documents and structural terms. we introduce an efficient technique to calculate all necessary term frequencies and inverse document frequencies at query time. by adjusting parameters of the retrieval process we are able to model two contrary approaches: the classical vector space model, and the original tree matching approach.querying and ranking xml documents','Management and querying of encrypted data'
'the problem of extracting consistent information from relational databases violating integrity constraints on numerical data is addressed. in particular, aggregate constraints defined as linear inequalities on aggregate-sum queries on input data are considered. the notion of repair as consistent set of updates at attribute-value level is exploited, and the characterization of several data-complexity issues related to repairing data and computing consistent query answers is provided. moreover, a method for computing &#8220;reasonable&#8221; repairs of inconsistent numerical databases is provided, for a restricted but expressive class of aggregate constraints. several experiments are presented which assess the effectiveness of the proposed approach in real-life application scenarios.querying and repairing inconsistent numerical databases','Management and querying of encrypted data'
'the problem of repairing xml data which are inconsistent and incomplete with respect to a set of integrity constraints and a dtd is addressed. the existence of repairs (i.e. minimal sets of update operations making data consistent) is investigated and shown to be undecidable in the general case. this pro-blem is shown to be still undecidable when data are interpreted as &#8220;incomplete&#8221; (so that they could be repaired by performing insert operations only). however, it becomes decidable when particular classes of constraints are considered. the existence of repairs is proved to be decidable and, in particular, $\\mathcal{np}$-complete, if inconsistent data are interpreted as &#8220;dirty&#8221; data (so that repairs are data-cleaning operations consisting in only deletions). the existence of general repairs (containing both insert and delete operations) for special classes of integrity constraints (functional dependencies) is also investigated. finally, for all the cases where the existence of a repair is decidable, the complexity of providing consistent answers to a query (issued on inconsistent data) is characterized.querying and repairing inconsistent xml data','Management and querying of encrypted data'
'in both industry and the research community it is now common to represent workflow schemas and enactments using xml. as a matter of fact, more and more enterprise application integration platforms (e.g., excelon, bea, iplanet, etc.) are using xml to represent workflows within or across enterprise boundaries. in this paper we explore the ability of modern xml query languages (specifically, the w3c xml algebra underlying the forthcoming xquery) to query and manipulate workflow schemas and enactments represented as xml data. the paper focuses on a simple, yet expressive, model called workflow query model (wqm) offering four primary constructs: sequence, choice, parallel, and loop. then three classes of queries are considered against wqm workflows: simple (e.g., to check the status of enactments), traversal (e.g., to check the relationship between tasks, or check the expected running time of a schema), and schema construction (e.g., to create new schemas from a library of workflow components). this querying functionality is quite useful for specifying, enacting and supervising e-services in various e-commerce application contexts and it can be easily specified using the w3c xml query algebra.querying and splicing of xml workflows','Management and querying of encrypted data'
'querying and visualization of geometric data','Management and querying of encrypted data'
'the term \"big data\" became a buzzword and is widely used in both research and industrial worlds. typically the concept of big data assumes a variety of different sources of information and velocity of complex analytical processing, rather than just a huge and growing volume of data. all variety, velocity, and volume create new research challenges, as nearly all techniques and tools commonly used in data processing have to be re-considered. variety and uncertainty of big data require a mixture of exact and similarity search and grouping of complex objects based on different attributes. high-level declarative query languages are important in this context due to expressiveness and potential for optimization. in this talk we are mostly interested in an algebraic layer for complex query processing which resides between user interface (most likely, graphical) and execution engine in layered system architecture. we analyze the applicability of existing models and query languages. we describe a systematic approach to similarity handling of complex objects, simultaneous application of different similarity measures and querying paradigms, complex searching and querying, combined semi-structured and unstructured search. we introduce the adaptive abstract operations based on the concept of fuzzy set, which are needed to support uniform handling of different kinds of similarity processing. to ensure an efficient implementation, approximate algorithms with controlled quality are required to enable quality versus performance trade-off for timeliness of similarity processing. uniform and adaptive operations enable high-level declarative definition of complex queries and provide options for optimization.querying big data','Management and querying of encrypted data'
'big data poses new challenges to query answering, from computational complexity theory to query evaluation techniques. several questions arise. what query classes can be considered tractable in the context of big data? how can we make query answering feasible on big data? what should we do about the quality of the data, the other side of big data? this paper aims to provide an overview of recent advances in tackling these questions, using social network analysis as an example.querying big social data','Management and querying of encrypted data'
'the large size of most data warehouses (typically hundreds of gigabytes to terabytes) results in non-trivial storage costs and makes compression techniques attractive. for the most part, page-level compression (as opposed to attribute or record level schemes) has been shown to achieve the greatest reductions in storage size for databases. a key issue with such schemes is how to quickly access the data to answer queries, since individual tuple boundaries are lost. in this paper we introduce an approach that aims to maintain the benefits of page-level compression (i.e., large reductions in storage size), while at the same time improving query performance through an efficient signature file indexing scheme. the approach uses an attribute-level signature generation method that exploits the value distribution of each attribute in a data warehouse. we provide an extensive theoretical analysis of this approach in which we compare our approach with a recently proposed indexing technique, encoded bitmapped indexing, along a number of important metrics including query processing, insertion, and storage costs. results show that our approach is preferred in many situations that are likely to occur in practice. we have also implemented a prototype system which validates our analytical findings.querying compressed data in data warehouses','Management and querying of encrypted data'
'the management of legal domains is gaining great importance in the context of data management. in fact, the geographical distribution of data as implied -- for example -- by cloud-based services requires that the legal restrictions and obligations are to be taken into account whenever data circulates across different legal domains. in this paper, we start to investigate an approach for coping with the complex issues that arise when dealing with data spanning different legal domains. our approach consists of a conceptual model that takes into account the notion of legal domain (to be paired with the corresponding data) and a reference architecture for implementing our approach in an actual relational dbms.querying data across different legal domains','Management and querying of encrypted data'
'many advanced data management operations (e.g., incremental maintenance, trust assessment, debugging schema mappings, keyword search over databases, or query answering in probabilistic databases), involve computations that look at how a tuple was produced, e.g., to determine its score or existence. this requires answers to queries such as, \"is this data derivable from trusted tuples?\"; \"what tuples are derived from this relation?\"; or \"what score should this answer receive, given initial scores of the base tuples?\". such questions can be answered by consulting the provenance of query results. in recent years there has been significant progress on formal models for provenance. however, the issues of provenance storage, maintenance, and querying have not yet been addressed in an application-independent way. in this paper, we adopt the most general formalism for tuple-based provenance, semiring provenance. we develop a query language for provenance, which can express all of the aforementioned types of queries, as well as many more; we propose storage, processing and indexing schemes for data provenance in support of these queries; and we experimentally validate the feasibility of provenance querying and the benefits of our indexing techniques across a variety of application classes and queries.querying data provenance','Management and querying of encrypted data'
'we study the problem of querying data sources that accept only a limited set of queries, such as sources accessible by web services which can implement very large (potentially infinite) families of queries. we revisit a classical setting in which the application queries are conjunctive queries and the source accepts families of conjunctive queries specified as the expansions of a (potentially recursive) datalog program. we say that query q is expressible by the program p if it is equivalent to some expansion of p. q is supported by p if it has an equivalent rewriting using some finite set of p\'s expansions. we present the first study of expressibility and support for sources that satisfy integrity constraints, which is generally the case in practice.querying data sources that export infinite sets of views','Management and querying of encrypted data'
'we study the problem of querying data sources that accept only a limited set of queries, such as sources accessible by web services which can implement very large (potentially infinite) families of queries. we revisit a classical setting in which the application queries are conjunctive queries and the source accepts families of conjunctive queries specified as the expansions of a (potentially recursive) datalog program with parameters. we say that query q is expressible by the program $\\ensuremath{\\mathcal{p}}$ if it is equivalent to some expansion of $\\ensuremath{\\mathcal{p}}$. q is supported by $\\ensuremath{\\mathcal{p}}$ if it has an equivalent rewriting using some finite set of $\\ensuremath{\\mathcal{p}}$ &#x2019;s expansions. we present the first study of expressibility and support for sources that satisfy integrity constraints, which is generally the case in practice.querying data sources that export infinite sets of&#x00a0;views','Management and querying of encrypted data'
'data sources on the web are often accessible through web interfaces that present them as relational tables, but require certain attributes to be mandatorily selected, e.g., via a web form. in a scenario where we integrate a set of such sources, and we pose queries over them, the values needed to access a source may have to be retrieved from other sources that are possibly not even mentioned in the query: answering queries at best can then be done only with a potentially recursive query plan that gets all obtainable answers to the query. since data sources are typically distributed over a network, a major cost indicator for the execution of a query plan is the number of accesses to remote sources. in this paper we present an optimization technique for conjunctive queries that produces a query plan that: (1) minimizes the number of accesses according to a strong notion of minimality; (2) excludes all sources that are not relevant for the query. we introduce toorjah, a prototype system that answers queries posed on sources with limitations by means of optimized query plans. toorjah adopts a strategy that is aimed to retrieve answers as early as possible during query processing, and to present them to the user as they are computed. we provide experimental evidence of the effectiveness of our optimization, by showing the reduction of the number of accesses in a large number of cases.querying data under access limitations','Management and querying of encrypted data'
'the role of database knowledge is usually limited to the evaluation of data queries. in this paper we argue that when this knowledge is of substantial volume and complexity, there is genuine need to query this repository of information. moreover, since users of the database may not be able to distinguish between information that is data and information that is knowledge, access to knowledge and data should be provided with a single, coherent instrument. we provide an informal review of various kinds of knowledge queries, with possible syntax and semantics. we then formalize a framework of knowledge-rich databases, and a simple query language consisting of a pair of retrieve and describe statements. the retrieve statement is for querying the data (it corresponds to the basic retrieval statement of various knowledge-rich database systems). the describe statement is for querying the knowledge. essentially, it inquires about the meaning of a concept under specified circumstances. we provide algorithms for evaluating sound and finite knowledge answers to describe queries, and we demonstrate them with examples.querying database knowledge','Management and querying of encrypted data'
'data security is a serious concern when we migrate data to a cloud dbms. database encryption, where sensitive columns are encrypted before they are stored in the cloud, has been proposed as a mechanism to address such data security concerns. the intuitive expectation is that an adversary cannot &#8220;learn&#8221; anything about the encrypted columns, since she does not have access to the encryption key. however, query processing becomes a challenge since it needs to &#8220;look inside&#8221; the data. this tutorial explores the space of designs studied in prior work on processing queries over encrypted data. we cover approaches based on both classic client-server and involving the use of a trusted hardware module where data can be securely decrypted. we discuss the privacy challenges that arise in both approaches and how they may be addressed. briefly, supporting the full complexity of a modern dbms including complex queries, transactions and stored procedures leads to significant challenges that we survey.querying encrypted data','Management and querying of encrypted data'
'this paper proposes techniques to query encrypted xml documents. such a problem predominantly occurs in \"database as a service&#148; (das) architectures, where a client may outsource data to a service provider that provides data management services. security is of paramount concern, as the service provider itself may be untrusted. encryption offers a natural solution to preserve the confidentiality of the client\'s data. the challenge now is to execute queries over the encrypted data, without decrypting them at the server side. in this paper we develop: 1) primitives using which a client can specify the sensitive parts of the xml documents; 2) mechanisms to map the xml documents to encrypted representations that hides sensitive portions of the documents; and 3) techniques to run spj (selection-projection-join) queries over encrypted xml documents. a strategy, where indices/ancillary information is maintained along with the encrypted xml documents is exploited, which helps in pruning the search space during query processing.querying encrypted xml documents','Management and querying of encrypted data'
'graph databases have gained renewed interest in the last years, due to its applications in areas such as the semantic web and social networks analysis. we study the problem of querying graph databases, and, in particular, the expressiveness and complexity of evaluation for several general-purpose query languages, such as the regular path queries and its extensions with conjunctions and inverses. we distinguish between two semantics for these languages. the first one, based on simple paths, easily leads to intractability, while the second one, based on arbitrary paths, allows tractable evaluation for an expressive family of languages. we also study two recent extensions of these languages that have been motivated by modern applications of graph databases. the first one allows to treat paths as first-class citizens, while the second one permits to express queries that combine the topology of the graph with its underlying data.querying graph databases','Management and querying of encrypted data'
'graphs have great expressive power to describe the complex relationships among data objects, and there are large graph datasets available such as web data, semi-structured data and xml data. in this paper, we describe our work on querying graph-structured data, including graph labeling methods, reachability joins, and graph pattern matching. we show that we can base on the graph labeling of complex xml and semi-structured data to process path queries and we devise join primitives for matching graph patterns. novel aspects about using such join primitives for graph pattern matching are addressed.querying graph-structured data','Management and querying of encrypted data'
'in this paper, we propose a new tunable index scheme, called iminmax($\\theta$), that maps points in high-dimensional spaces to single-dimensional values determined by their maximum or minimum values among all dimensions. by varying the tuning &#8220;knob&#8221;, $\\theta$, we can obtain different families of iminmax structures that are optimized for different distributions of data sets. the transformed data can then be indexed using existing single-dimensional indexing structures such as the b+-trees. queries in the high-dimensional space have to be transformed into queries in the single-dimensional space and evaluated there. we present efficient algorithms for evaluating window queries as range queries on the single-dimensional space. we conducted an extensive performance study to evaluate the effectiveness of the proposed schemes. our results show that iminmax($\\theta$) outperforms existing techniques, including the pyramid scheme and va-file, by a wide margin. we then describe how iminmax could be used in approximate k-nearest neighbor (knn) search, and we present a comparative study against the recently proposed idistance, a specialized knn indexing method.querying high-dimensional data in single-dimensional space','Management and querying of encrypted data'
'sensors are used to monitor some physical phenomena such as contamination, climate, building, and so on. the sensors collect and communicate their readings to the sensor databases for making decisions and answering various user queries. due to continuous changes and possible errors in these values, the data values recorded in sensor databases may differ from the actual status. queries using these values can yield incorrect and misleading answers. in order to manage the imprecision between the actual sensor value and the database value, the framework representing imprecision of sensor data has been proposed, in which each data value is represented as an interval. in this paper, we examine the situation when answer imprecision can be represented qualitatively and quantitatively. in particular, we propose two new kinds of imprecise queries called top-k imprecise query and imprecise threshold query. also, we investigate techniques for evaluating the qualitative and quantitative queries.querying imprecise data in sensor databases','Management and querying of encrypted data'
'language integrated query (linq) is part of the upcoming version 3.5 of the .net framework. as a combination of apis and enhancements to the .net programming languages, linq provides a uniform approach to querying of data across any data source. linq pulls the querying experience into the programming language space, providing full static typing and tool support. linq is built to be pluggable, allowing data source providers to insert their own query engines. using the new c# 3.0 this demonstration peels apart the layers of linq to show how a smooth user experience on the surface emerges from new language features, naming conventions and metaprogramming facilities.querying in c#','Management and querying of encrypted data'
'with the proliferation of geographic data and resources over the internet, there is an increasing demand for integration services that allow a transparent access to massive repositories of heterogeneous spatial data. recent initiatives such as google earth are likely to encourage other companies or state agencies to publish their (satellite) data over the internet. to fulfill this demand, we need at minimum an efficient geographic integration system. the goal of this demonstration is to show some new and enhanced features of the virgis geographic mediation system.querying mediated geographic data sources','Management and querying of encrypted data'
'representing descriptions of movements in databases and querying them is a basic capability required in mobile data management. in this demonstration, we show for the first time a prototype implementing a data model and query language for moving objects (trajectories) completely integrated into a dbms environment, including query optimization and user interface issues such as animation.querying moving objects in secondo','Management and querying of encrypted data'
'to support the retrieval and fusion of multimedia information from multiple sources and databases, a spatial/temporal query language called ql is proposed. ql is based upon the operator sequence and in practice expressible in sql-like syntax. ql allows a user to specify powerful spatial/temporal queries for both multimedia data sources and multimedia databases, eliminating the need to write different queries for each. a ql query can be processed in the most effective manner by first selecting the suitable transformations of multimedia data to derive the multimedia static schema, and then processing the query with respect to this multimedia static schema.querying multimedia data sources and databases','Management and querying of encrypted data'
'rule mining is an important data mining task that has been applied to numerous real-world applications. often a rule mining system generates a large number of rules and only a small subset of them is really useful in applications. although there exist some systems allowing the user to query the discovered rules, they are less suitable for complex ad hoc querying of multiple data mining rulebases to retrieve interesting rules. in this paper, we propose a new powerful rule query language rule-ql for querying multiple rulebases that is modeled after sql and has rigorous theoretical foundations of a rule-based calculus. in particular, we first propose a rule-based calculus rc based on the first-order logic, and then present the language rule-ql that is at least as expressive as the safe fragment of rc. we also propose a number of efficient query evaluation techniques for rule-ql and test them experimentally on some representative queries to demonstrate the feasibility of rule-ql.querying multiple sets of discovered rules','Management and querying of encrypted data'
'managing and querying information with varying temporal granularities is an important problem in databases. although there is a substantial body of work on temporal granularities for the relational data model, a comprehensive framework is lacking for the object-oriented paradigm. to the best of our knowledge, a formal treatment of temporal queries with multiple granularities has not been considered in the literature. in this paper, we make a step in this direction. we formally introduce the syntax and semantics of expressions involving data with multiple granularities, comparison between data with different granularities, and conversion of data from one granularity to another. we believe that this is an important step towards the development of an object-oriented query language that supports multiple granularities.querying multiple temporal granularity data','Management and querying of encrypted data'
'heirarchically structured directories have recently proliferated with the growth of the internet, and are being used to store not only address books and contact information for people, but also personal profiles, network resource information, and network and service policies. these systems provide a means for managing scale and heterogeneity, while allowing for conceptual unity and autonomy across multiple directory servers in the network, in a way for superior to what conventional relational or object-oriented databases offer. yet, in deployed systems today, much of the data is modeled in an ad hoc manner, and many of the more sophisticated &#8220;queries&#8221; involve navigational access.in this paper, we develop the core of a formal data model for network directories, and propose a sequence of efficiently computable query languages with increasing expressive power. the directory data model can naturally represent rich forms of heterogeneity exhibited in the real world. answers to queries expressible in our query languages can exhibit the same kinds of heterogeneity. we present external memory algorithms for the evaluation of queries posed in our directory query languages, and prove the efficiency of each algorithm in terms of its i/o complexity. our data model and query languages share the flexibility and utility of the recent proposals for semi-structured data models, while at the same time effectively addressing the specific needs of network directory applications, which we demonstrate by means of a representative real-life example.querying network directories','Management and querying of encrypted data'
'now-relative temporal data play an important role in most temporal applications, and their management has been proved to impact in a crucial way the efficiency of temporal databases. though several temporal relational approaches have been developed to deal with now-relative data, none of them has provided a whole temporal algebra to query them. in this paper we overcome such a limitation, by proposing a general algebra which is parametrically adapted to cope with the relational approaches to now-relative data in the literature, i.e., min, max, null and point approaches. besides being general enough to provide a query language for several approaches in the literature, our algebra has been designed in such a way to satisfy several theoretical and practical desiderata: closure with respect to representation languages, correctness with respect to the \"consensus\" bcdm semantics, reducibility to the standard non-temporal algebra (which involves interoperability with non-temporal relational databases), implementability and efficiency. indeed, the experimental evaluation we have drawn on our implementation has shown that only a slight overhead is added by our treatment of now-relative data (with respect to an approach in which such data are not present).querying now-relative data','Management and querying of encrypted data'
'querying of a higher order','Management and querying of encrypted data'
'the availability of executable specification languages allows testing to be carried out soon after or concurrently with the requirements specification phase. in addition, it becomes possible to use these languages for rapid prototyping, making it possible to gather information on properties of the specified target system including its behavior in response to external events. the inspection of software behavior is viewed as the querying of executable specifications. a language rsq is defined for the purpose of constructing queries against executable specifications expressed in rsf, a language for the description of systems with time constraints. a query is able to single out a subclass of possible behaviors based on properties supplied by the query. the integration of rsq with rsf enhances the analytical abilities of the software designer and developer.querying of executable software specifications','Management and querying of encrypted data'
'when gathering data from multiple data sources, users need uniform, transparent access to data. also, when extracting data from several independent, often only partially sound and complete data sources, it is useful to present users with meta-information about the confidence in the answer to a query, based on the number and quality of the sources that participated in constructing the answer. we consider the problem of querying collections of sources with incomplete and partially sound data. we provide a method for checking the consistency of a source collection, we give a tableaux-based characterization for the set of possible worlds consistent with a given source collection and we propose a probabilistic semantics for query answers.querying partially sound and complete data sources','Management and querying of encrypted data'
'in the context of large-scale distributed component-based systems, this article motivates the need for and defines a general open query service. this query service allows for retrieving and selecting components from both repositories and deployed running systems. the problem of retrieving components from repositories has already been tackled by many research works whose purpose is to construct systems by assembling reusable components, typically in the context of cots components. however, the proposed query services allows for an uniform efficient associative access to both repositories and running systems, while beeing based on distributed database techniques, which is the main contribution in this paper.querying reflexive component-based architectures','Management and querying of encrypted data'
'querying semi-structured data','Management and querying of encrypted data'
'in this paper we propose the gem language (gel), a sql-like query language, which is able to extract information from semistructured temporal databases represented according to the graphical semistructured temporal (gem) data model.querying semistructured temporal data','Management and querying of encrypted data'
'the need to perform complex analysis and decision making tasks has motivated growing interest in geographic information systems (gis) as a means to compare different scenarios and simulate the evolution of phenomena. however, data and function complexity may critically affect human interaction and system performances during planning and prevention activities. this is especially true when the scenarios of interest involve space or time in imprecise contexts. in this paper we propose a visual language which drives users to perform queries involving discrete objects by considering their temporal component. moreover, in order to allow queries closer to the user mental model we add a specific hint for relaxing constraints and allowing fuzzy conditions. the visual language will be tested on two specific contexts concerning with the fire risk and the air pollution.querying spatial and temporal data by condition tree','Management and querying of encrypted data'
'querying streaming geospatial image data','Management and querying of encrypted data'
'mashql, a novel query formulation language for querying and mashing up structured data on the web, doesn\'t require users to know the queried data\'s structure or the data itself to adhere to a schema. in this article, the authors address the fact that being a language, not merely an interface (and simultaneously schema-free), mashql faces a particular set of challenges. in particular, the authors propose and evaluate a novel technique for optimizing queries over large data sets to allow instant user interaction.querying the data web','Management and querying of encrypted data'
'the increasing amount of interlinked rdf data has finally made available the necessary building blocks for the web of data. this in turns makes it possible (and interesting) to query such a collection of graphs as an open and decentralized knowledge base. however, despite the fact that there are already implementations of query answering algorithms for the web of data, there is no formal characterization of what a satisfactory answer is expected to be. in this paper, we propose a preliminary model for such an open collection of graphs which goes beyond the standard single-graph rdf semantics, describes three different ways in which a query can be answered, and characterizes them semantically in terms of three incremental restrictions on the relation between the domain of interpretation of each single component graph.querying the web of data','Management and querying of encrypted data'
'with the fast development in mobile and communication technology, the need for moving object databases increases. moving object databases constitute a major ingredient in many applications that are location sensitive. traffic control, fleet management, m-commerce, and e-911 are among those emerging location based services (lbss). along with the vast increase of location information, and the need to efficiently manage and mine those information, spatio-temporal data warehouses arise into the picture requiring new algorithms, new measures, and efficient querying techniques to efficiently utilize the historical information obtained from moving objects. in this paper we present a spatio-temporal data warehouse (stdw) that enables querying location information. experimental results on our query performance are presented.querying trajectory data warehouses','Management and querying of encrypted data'
'data uncertainty arises in many situations. a common approach to query processing uncertain data is to sample many \"possible worlds\" from the uncertain data and to run queries against the possible worlds. however, sampling is not a trivial task, as a randomly sampled possible world may not satisfy known constraints imposed on the data. in this paper, we focus on an important category of constraints, the aggregate constraints. an aggregate constraint is placed on a set of records instead of on a single record, and a real-life system usually has a large number of aggregate constraints. it is a challenging task to find qualified possible worlds in this scenario, since tuple by tuple sampling is extremely inefficient because it rarely leads to a qualified possible world. in this paper, we introduce two approaches for querying uncertain data with aggregate constraints: constraint aware sampling and mcmc sampling. our experiments show that the new approaches lead to high quality query results with reasonable cost.querying uncertain data with aggregate constraints','Management and querying of encrypted data'
'the problem of modeling and managing uncertain data has received a great deal of interest, due to its manifold applications in spatial, temporal, multimedia and sensor databases. there exists a wide range of work covering spatial uncertainty in the static (snapshot) case, where only one point of time is considered. in contrast, the problem of modeling and querying uncertain spatio-temporal data has only been treated as a simple extension of the spatial case, disregarding time dependencies between consecutive timestamps. in this work, we present a framework for efficiently modeling and querying uncertain spatio-temporal data. the key idea of our approach is to model possible object trajectories by stochastic processes. this approach has three major advantages over previous work. first it allows answering queries in accordance with the possible worlds model. second, dependencies between object locations at consecutive points in time are taken into account. and third it is possible to reduce all queries on this model to simple matrix multiplications. based on these concepts we propose efficient solutions for different probabilistic spatio-temporal queries. in an experimental evaluation we show that our approaches are several order of magnitudes faster than state-of-the-art competitors.querying uncertain spatio-temporal data','Management and querying of encrypted data'
'querying virtual videos using path and temporal expressions','Management and querying of encrypted data'
'an object-oriented database methodology for querying web sources of data, structured in xml, is presented in this paper. a querying system is developed based on this methodology using objectstore database system. xml data are converted into objects that can be stored as part of the objectstore database system. several querying interfaces for querying, searching and browsing the database are developed. the methodology presented in this paper is easily adaptable to any source of web data that is represented in xml.querying web data','Management and querying of encrypted data'
'several commercial applications, such as online comparison shopping and process automation, require integrating information that is scattered across multiple websites or xml documents. much research has been devoted to this problem, resulting in several research prototypes and commercial implementations. such systems rely on wrappers that provide relational or other structured interfaces to websites. traditionally, wrappers have been constructed by hand on a per-website basis, constraining the scalability of the system.we introduce a website structure inference mechanism called compact skeletons that is a step in the direction of automated wrapper generation. compact skeletons provide a transformation from websites or other hierarchical data, such as xml documents, to relational tables. we study several classes of compact skeletons and provide polynomial-time algorithms and heuristics for automated construction of compact skeletons from websites. experimental results show that our heuristics work well in practice. we also argue that compact skeletons are a natural extension of commercially deployed techniques for wrapper construction.querying websites using compact skeletons','Management and querying of encrypted data'
'we study the problem of querying xml data sources that accept only a limited set of queries, such as sources accessible by web services which can implement very large (potentially infinite) families of xpath queries. to compactly specify such families of queries we adopt the query set specifications, a formalism close to context-free grammars. we say that query q is expressible by the specification p if it is equivalent to some expansion of p. q is supported by p if it has an equivalent rewriting using some finite set of p\'s expansions. we study the complexity of expressibility and support and identify large classes of xpath queries for which there are efficient (ptime) algorithms. our study considers both the case in which the xml nodes in the results of the queries lose their original identity and the one in which the source exposes persistent node ids.querying xml data sources that export very large sets of views','Management and querying of encrypted data'
'sparql is today the standard access language for semantic web data. in the recent years xml databases have also acquired industrial importance due to the widespread applicability of xml in the web. in this paper we present a framework that bridges the heterogeneity gap and creates an interoperable environment where sparql queries are used to access xml databases. our approach assumes that fairly generic mappings between ontology constructs and xml schema constructs have been automatically derived or manually specified. the mappings are used to automatically translate sparql queries to semantically equivalent xquery queries which are used to access the xml databases. we present the algorithms and the implementation of sparql2xquery framework, which is used for answering sparql queries over xml databases. querying xml data with sparql','Management and querying of encrypted data'
'a limitation of xquery is that a programmer has to be familiar with the shape of the data to query it effectively. and if that shape changes, or if the shape is other than what the programmer expects, the query may fail. one way to avoid this limitation is to transform the data into a desired shape. a data transformation is a rearrangement of data into a new shape. in this paper, we present the semantics and implementation of xmorph 2.0, a shape-polymorphic data transformation language for xml. an xmorph program can act as a query guard. the guard both transforms data to the shape needed by the query and determines whether and how the transformation potentially loses information, a transformation that loses information may lead to a query yielding an inaccurate result. this paper describes how to use xmorph as a query guard, gives a formal semantics for shape-to-shape transformations, documents how xmorph determines how a transformation potentially loses information, and describes the xmorph implementation.querying xml data','Management and querying of encrypted data'
'querying xml views of relational data','Management and querying of encrypted data'
'in their recent paper, \"encrypted key exchange: password-based protocols secure against dictionary attacks,\" bellovin and merritt propose a novel and elegant method for safeguarding weak passwords. this paper discusses a possible weakness in the proposed protocol, develops some enhancements and simplifications, and provides a security analysis of the resultant minimal eke protocol. in addition, the basic 2-party eke model is extended to the 3-party setting; this yields a protocol with some interesting properties. most importantly, this paper illustrates, once again, the subtlety associated with designing password-based protocols.refinement and extension of encrypted key exchange','Management and querying of encrypted data'
'we propose and evaluate different methods to signal position and size of encrypted rois (regions of interest) in jpeg images. after discussing various design choices regarding the encoding of roi coordinates with a minimal amount of bits, we discuss both, existing and newly proposed approaches to signal the encoded coordinates inside jpeg images. by evaluating the different signalling methods on various data sets, we show that several of our proposed encoding methods outperform jbig in this special use case. furthermore, we show that one of our proposed signalling methods allows length-preserving lossless signalling, i.e., storing roi coordinates in a format-compliant way inside the jpeg images without quality loss or change of file size.region of interest signalling for encrypted jpeg images','Management and querying of encrypted data'
'motivated by both established and new applications, we study navigational query languages for graphs (binary relations). the simplest language has only the two operators union and composition, together with the identity relation. we make more powerful languages by adding any of the following operators: intersection; set difference; projection; coprojection; converse; transitive closure; and the diversity relation. all these operators map binary relations to binary relations. we compare the expressive power of all resulting languages. we do this not only for general path queries (queries where the result may be any binary relation) but also for boolean or yes/no queries (expressed by the nonemptiness of an expression). for both cases, we present the complete hasse diagram of relative expressiveness. in particular, the hasse diagram for boolean queries contains nontrivial separations and a few surprising collapses.relative expressive power of navigational querying on graphs','Management and querying of encrypted data'
'we propose using a simple linear constraint relational representation for spatial data derived using 2d shape function interpolations. we show that many queries that could not be done in traditional gis systems can be efficiently expressed and evaluated using linear constraint database systems.representation and querying of interpolation data in constraint databases','Management and querying of encrypted data'
'representing and querying changes in semistructured data','Management and querying of encrypted data'
'modern information systems often store data that has been transformed and integrated from a variety of sources. this integration may obscure the original source semantics of data items. for many tasks, it is important to be able to determine not only where data items originated, but also why they appear in the integration as they do and through what transformation they were derived. this problem is known as data provenance. in this work, we consider data provenance at the schema and mapping level. in particular, we consider how to answer questions such as \"what schema elements in the source(s) contributed to this value\", or \"through what transformations or mappings was this value derived?\" towards this end, we elevate schemas and mappings to first-class citizens that are stored in a repository and are associated with the actual data values. an extended query language, called mxql, is also developed that allows meta-data to be queried as regular data and we describe its implementation. scenario.representing and querying data transformations','Management and querying of encrypted data'
'multidimensional semistructured data (mssd) are semistructured data that present different facets under different contexts. context represents alternative worlds, and is expressed by assigning values to a set of user-defined variables called dimensions. the notion of context has been incorporated in the object exchange model (oem), and the extended model is called multidimensional oem (moem), a graph model for mssd. in this paper, we explain in detail how moem can represent the history of an oem database. we discuss how moem properties are applied in the case of representing oem histories, and show that temporal oem snapshots can be obtained from moem. we present a system that implements the proposed ideas, and we use an example scenario to demonstrate how an underlying moem database accommodates changes in an oem database. furthermore, we show that moem is capable to model changes occurring not only in oem databases, but in multidimensional oem databases as well. the use of multidimensional query language (mql), a query language for mssd, is proposed for querying the history of oem databases and moem databases.representing and querying histories of semistructured databases using multidimensional oem','Management and querying of encrypted data'
'we study the representation and querying of xml with incomplete information. we consider a simple model for xml data and their dtds, a very simple query language, and a representation system for incomplete information in the spirit of the representations systems developed by imielinski and lipski for relational databases. in the scenario we consider, the incomplete information about an xml document is continuously enriched by successive queries to the document. we show that our representation system can represent partial information about the source document acquired by successive queries, and that it can be used to intelligently answer new queries. we also consider the impact on complexity of enriching our representation system or query language with additional features. the results suggest that our approach achieves a practically appealing balance between expressiveness and tractability. the research presented here was motivated by the xyleme project at inria, whose objective it to develop a data warehouse for web xml documents.representing and querying xml with incomplete information','Management and querying of encrypted data'
'data security has become an increasinglyimportant factor in routine work with thedevelopment of applications, and users trend toobtain high security with maximum comfort. in thisarticle a cryptographic file system callednas_cfs used for nas is designed. nas_cfs hassome characteristics as below: adding encryptionfunction to file-system layer allows users to encryptdata transparently, obtaining high security by keymanagement based on session id and user id,attachment and timeouts mechanisms, andnas_cfs is an in-kernel file system of highperformance using stackable mechanism.research and implement of an encrypted file system used to nas','Management and querying of encrypted data'
'the data storaged in multi-purpose asynchronous wireless sensor network should be encrypted to protect against physical attacks. meanwhile,in multi-purpose wireless sensor network,an access control mechanism is needed to differentiate the encrypted data which belong to different applications. howeever, as the author knows, by far, no secure scheme solving this problem has been proposed. in this paper, we introduce an attribute-based encryption scheme to address the encrypted data storage and access control issues, through experiment and analysis we prove that the abe scheme could be applied to multi-purpose asynchronous wsn.research on the encrypted data access control of multi-purpose asynchronous wsn','Management and querying of encrypted data'
'a novel reversible data hiding technique in encrypted images is presented in this paper. instead of embedding data in encrypted images directly, some pixels are estimated before encryption so that additional data can be embedded in the estimating errors. a benchmark encryption algorithm (e.g. aes) is applied to the rest pixels of the image and a special encryption scheme is designed to encrypt the estimating errors. without the encryption key, one cannot get access to the original image. however, provided with the data hiding key only, he can embed in or extract from the encrypted image additional data without knowledge about the original image. moreover, the data extraction and image recovery are free of errors for all images. experiments demonstrate the feasibility and efficiency of the proposed method, especially in aspect of embedding rate versus peak signal-to-noise ratio (psnr).reversibility improved data hiding in encrypted images','Management and querying of encrypted data'
'we use a constrained optimization framework to derive scaling laws for data-centric storage and querying in wireless sensor networks. we consider both unstructured sensor networks, which use blind sequential search for querying, and structured sensor networks, which use efficient hash-based querying. we find that the scalability of a sensor network\'s performance depends upon whether the increase in energy and storage resources with more nodes is outweighed by the concomitant application-specific increase in event and query loads. we derive conditions that determine: 1) whether the energy requirement per node grows without bound with the network size for a fixed-duration deployment, 2) whether there exists a maximum network size that can be operated for a specified duration on a fixed energy budget, and 3) whether the network lifetime increases or decreases with the size of the network for a fixed energy budget. an interesting finding of this work is that three-dimensional (3d) uniform deployments are inherently more scalable than two-dimensional (2d) uniform deployments, which in turn are more scalable than one-dimensional (1d) uniform deployments.scaling laws for data-centric storage and querying in wireless sensor networks','Management and querying of encrypted data'
'schema-based authoring and querying of large hypertexts','Management and querying of encrypted data'
'we need better ways to query large linked data collections such as dbpedia. using the sparql query language requires not only mastering its syntax but also understanding the rdf data model, large ontology vocabularies and uris for denoting entities. natural language interface systems address the problem, but are still subjects of research. we describe a compromise in which non-experts specify a graphical query \"skeleton\" and annotate it with freely chosen words, phrases and entity names. the combination reduces ambiguity and allows the generation of an interpretation that can be translated into sparql. key research contributions are the robust methods that combine statistical association and semantic similarity to map user terms to the most appropriate classes and properties in the underlying ontology.schema-free structured querying of dbpedia data','Management and querying of encrypted data'
'a hidden vector encryption scheme (hve) is a derivation of identity-based encryption, where the public key is actually a vector over a certain alphabet. the decryption key is also derived from such a vector, but this one is also allowed to have \"*\" (or wildcard) entries. decryption is possible as long as these tuples agree on every position except where a \"*\" occurs. these schemes are useful for a variety of applications: they can be used as a building block to construct attribute-based encryption schemes and sophisticated predicate encryption schemes (for e.g. range or subset queries). another interesting application - and our main motivation - is to create searchable encryption schemes that support queries for keywords containing wildcards. here we construct a new hve scheme, based on bilinear groups of prime order, which supports vectors over any alphabet. the resulting ciphertext length is equally shorter than existing schemes, depending on a trade-off. the length of the decryption key and the computational complexity of decryption are both constant, unlike existing schemes where these are both dependent on the amount of non-wildcard symbols associated to the decryption key. our construction hides both the plaintext and public key used for encryption. we prove security in a selective model, under the decision linear assumption.searching keywords with wildcards on encrypted data','Management and querying of encrypted data'
'security is still a major inhibitor of cloud computing. when companies are testing cloud applications, e.g. for storage or databases, they use generated data for fear of data loss. modern encrypted databases where the cryptographic key remains at the client provide a solution to this problem. recent results in cryptography, such order-preserving encryption, and database systems enable the practical use of these systems. we report on our pre-development efforts of implementing such an encrypted database in an in-memory, column store database. we highlight some unsolved research challenges: such as access control, infrequent queries and security vs. performance query optimization. challenges to key management in multi-user environments remain largely unsolved. we give an overview of the architecture and performance benchmarks on our prototype which are very encouraging for practical adoption.searching over encrypted data in cloud systems','Management and querying of encrypted data'
'cloud computing security and reliability are important challenges in the research agenda. for some applications managing sensitive data, cloud security solutions and data-privacy management are the main concerns for organizations that are considering a move to the cloud. the advantages of cloud computing include reduced costs, easy maintenance and re-provisioning of resources, thereby also possibly increasing profits. but the adoption of cloud computing solutions applies only if different security concerns are ensured. this article presents a solution for data storage and data management in internet storage clouds, preserving privacy conditions under the control of cloud users. the proposed solution supports operations over stored encrypted data, including reading, writing and searching based on relevance ranking and multiple keywords. the approach is based on a middleware architecture supported by homomorphic encryption techniques combined with dynamic indexing mechanisms. the solution preserves data-privacy without need to either decipher data during operations in the cloud or transfer the data during searches. the article further describes an implementation prototype of the solution and its evaluation. the evaluation shows that the solution is viable, offers security and privacy control for the user and does not aggravate conditions of data-access latency and availability.searching private data in a cloud encrypted domain','Management and querying of encrypted data'
'the cloud storage based information retrieval service is a promising technology that will form a vigorous market in the near future. although there have been numerous studies proposed about secure data retrieval over encrypted data in cloud services, most of them focus on providing the strict security for the data stored in a third party domain. however, those approaches require stupendous costs centralized on the cloud service provider, which could be a principal impediment to achieve efficient data retrieval in cloud storage. in this paper, we propose an efficient data retrieval scheme using attribute-based encryption. the proposed scheme is best suited for cloud storage systems with massive amount of data. it provides rich expressiveness as regards access control and fast searches with simple comparisons of searching entities. the proposed scheme also guarantees data security and user privacy during the data retrieval process.secure and efficient data retrieval over encrypted data using attribute-based encryption in cloud storage','Management and querying of encrypted data'
'end-to-end encryption schemes that support operations over ciphertext are of utmost importance for commercial private party wireless sensor network implementations to become meaningful and profitable. for wireless sensor networks, we demonstrated in our previous work that privacy homomorphisms, when used for this purpose, offer two striking advantages apart from end-to-end concealment of data and ability to operate on ciphertexts: flexibility by keyless aggregation and conservation and balancing of aggregator backbone energy. we offered proof of concept by applying a certain privacy homomorphism for sensor network applications that rely on the addition operation. but a large class of aggregator functions like median computation or finding maximum/minimum rely exclusively on comparison operations. unfortunately, as shown by rivest, et. al., any privacy homomorphism is insecure even against ciphertext only attacks, if they support comparison operations. in this paper we show that a particular order preserving encryption scheme achieves the above mentioned energy benefits and flexibility when used to support comparison operations over encrypted texts for wireless sensor networks, while also managing to hide the plaintext distribution and being secure against ciphertext only attacks. the scheme is shown to have reasonable memory and computation overhead when applied for wireless sensor networks.secure comparison of encrypted data in wireless sensor networks','Management and querying of encrypted data'
'in an end-to-end encryption model for a wireless sensor network (wsn), the network control center preloads encryption and decryption keys to the sensor nodes and the subscribers respectively, such that a subscriber can use a mobile device in the deployment field to decrypt the sensed data encrypted by the more resource-constrained sensor nodes. this paper proposes sms-sed, a provably secure yet practically efficient key assignment system featuring a discrete time-based access control, to better support a business model where the sensors deployer rents the wsn to customers who desires a higher flexibility beyond subscribing to strictly consecutive periods. in sms-sed, a node or a mobile device stores a secret key of size independent of the total number of sensor nodes and time periods. we evaluated the feasibility of deploying 2000 nodes for 4096 time periods at 1024-bit of security as a case study, studied the trade off of increasing the storage requirement of a node to significantly reduce its computation time, and provided formal security argument in the random oracle model.secure mobile subscription of sensor-encrypted data','Management and querying of encrypted data'
'since obfuscation was brought into the field of cryptography, it has become one of the most difficult and hottest problems. because a general secure obfuscating method, if exists, will lead to the solution of many open problems in cryptography. however, after bark et al.\'s negative impossibility result for general obfuscation became well-known, only a few positive results was brought out. in eurocrypt 2010, hada proposed a secure obfuscator of encrypted signatures (es), which signs a message under alice\'s secret signing key and then encrypts the signature using bob\'s public encryption key. this result is the only few secure obfuscation of complicated cryptographic primitives. in this paper, we consider the obfuscation of encrypted verifiable encrypted signatures (eves). there is a trusted third party (ttp) in our protocol, and eves first generates a verifiable encrypted signature (ves) under alice\'s secret signing key and the ttp\'s public encryption key and then the ves is encrypted using bob\'s public encryption key. we give out the detailed eves protocol and securely obfuscate it. we prove the security requirement of virtual black box property under standard assumptions and the secure obfuscation result will have many practical applications as we issue.secure obfuscation of encrypted verifiable encrypted signatures','Management and querying of encrypted data'
'advances in cloud computing and internet technologies have pushed more and more data owners to outsource their data to remote cloud servers to enjoy with huge data management services in an efficient cost. however, despite its technical advances, cloud computing introduces many new security challenges that need to be addressed well. this is because, data owners, under such new setting, loss the control over their sensitive data. to keep the confidentiality of their sensitive data, data owners usually outsource the encrypted format of their data to the untrusted cloud servers. several approaches have been provided to enable searching the encrypted data. however, the majority of these approaches are limited to handle either a single keyword search or a boolean search but not a multikeyword ranked search, a more efficient model to retrieve the top documents corresponding to the provided keywords. in this paper, we propose a secure multi-keyword ranked search scheme over the encrypted cloud data. such scheme allows an authorized user to retrieve the most relevant documents in a descending order, while preserving the privacy of his search request and the contents of documents he retrieved. to do so, data owner builds his searchable index, and associates with each term document with a relevance score, which facilitates document ranking. the proposed scheme uses two distinct cloud servers, one for storing the secure index, while the other is used to store the encrypted document collection. such new setting prevents leaking the search result, i.e. the document identifiers, to the adversary cloud servers. we have conducted several empirical analyses on a real dataset to demonstrate the performance of our proposed scheme.secure rank-ordered search of multi-keyword trapdoor over encrypted cloud data','Management and querying of encrypted data'
'there has been considerable interest in querying encrypted data, allowing a &#8220;secure database server&#8221; model where the server does not know data values. this paper shows how results from cryptography prove the impossibility of developing a server that meets cryptographic-style definitions of security and is still efficient enough to be practical. the weaker definitions of security supported by previous secure database server proposals have the potential to reveal significant information. we propose a definition of a secure database server that provides probabilistic security guarantees, and sketch how a practical system meeting the definition could be built and proven secure. the primary goal of this paper is to provide a vision of how research in this area should proceed: efficient encrypted database and query processing with provable security properties.security issues in querying encrypted data','Management and querying of encrypted data'
'in a verifiably encrypted signature scheme, signers encrypt their signature under the public key of a trusted third party and prove that they did so correctly. the security properties, due to boneh et al. (eurocrypt 2003), are unforgeability and opacity. this paper proposes two novel fundamental requirements for verifiably encrypted signatures, called &lt;em&gt;extractability&lt;/em&gt; and &lt;em&gt;abuse-freeness&lt;/em&gt; , and analyzes its effects on the established security model. extractability ensures that the trusted third party is always able to extract a valid signature from a valid verifiably encrypted signature and abuse-freeness guarantees that a malicious signer, who cooperates with the trusted party, is not able to forge a verifiably encrypted signature. we further show that both properties are not covered by the model of boneh et al. the second main contribution of this paper is a verifiably encrypted signature scheme, provably secure without random oracles, that is more efficient and greatly improves the public key size of the only other construction in the standard model by lu et al. (eurocrypt 2006). moreover, we present strengthened definitions for unforgeability and opacity in the spirit of strong unforgeability of digital signature schemes. security of verifiably encrypted signatures and a construction without random oracles','Management and querying of encrypted data'
'a large part of the web, actually holding a significant portion of the useful information throughout the web, consists of views on hidden databases, provided by numerous heterogeneous interfaces that are partly human-oriented via web forms (\"deep web\"), and partly based on web services (only machine accessible). in this paper we present an approach for annotating these sources in a way that makes them citizens of the semantic web. we illustrate how queries can be stated in terms of the ontology, and how the annotations are used to selected and access appropriate sources and to answer the queries. semantic annotations and querying of web data sources','Management and querying of encrypted data'
'information integration in the world wide web has evolved to a new framework where the information is represented and manipulated using a wide range of modeling languages. current approaches to data integration use wrappers to convert the different modeling languages into a common data model. in this work we use a nested hypergraph based data model (called hdm) as a common data model for integrating different structured or semi-structured data. we present a hypergraph query language (hql) that allows the integration of the wrapped data sources through the creation of views for mediators, and the querying of the wrapped data sources and the mediator views by the end users. we also show that hql queries (views) can be constructed from other views and/or source schemas using a set of primitive transformations. our integration architecture is flexible and allows some (or all) of the views in a mediator to be materialized.semantic integration and querying of heterogeneous data sources using a hypergraph data model','Management and querying of encrypted data'
'a computational grid infrastructure for biomedical research, called cagrid, is under development by the national cancer institute (nci) as part of the cancer biomedical informatics grid (cabig) initiative. in this paper we present a model that enables users to query an integrated view of cabig data services at a conceptual semantic level. the model is based on semcdi, a formulation to generate an ontology view of cabig semantics and pose queries against this view using the sparql query language complemented with horn rules. we present here a mechanism to process these queries algebraically using our semqa query algebra extension for sparql, in order to create sub-expressions for each data service. we then show how resulting graphs from these sub-expressions are then merged using horn rules.semantic representation and querying of cabig data services','Management and querying of encrypted data'
'current security mechanisms pose a risk for organisations that outsource their data management to untrusted servers. encrypting and decrypting sensitive data at the client side is the normal approach in this situation but has high communication and computation overheads if only a subset of the data is required, for example, selecting records in a database table based on a keyword search. new cryptographic schemes have been proposed that support encrypted queries over encrypted data but all depend on a single set of secret keys, which implies single user access or sharing keys among multiple users, with key revocation requiring costly data re-encryption. in this paper, we propose an encryption scheme where each authorised user in the system has his own keys to encrypt and decrypt data. the scheme supports keyword search which enables the server to return only the encrypted data that satisfies an encrypted query without decrypting it. we provide two constructions of the scheme giving formal proofs of their security. we also report on the results of a prototype implementation.shared and searchable encrypted data for untrusted servers','Management and querying of encrypted data'
'outsourcing database to database service providers demonstrates an emerging computation paradigm in many organizations. to meet the requirement of protecting sensitive data from outside providers, a bucket-based storage method is proposed to store the data encrypted. however, this storage method does not address the access control from the inside users, and an inside user may have enough rights to access the query results. in this paper, we propose a simple method to protect sensitive data both from insiders and outsiders on the bucket-based method. the session keys, used to encrypt sensitive data, are stored encrypted with corresponding tuples on remote databases. by constructing the user abilities to decrypt these encrypted keys on hierarchies, only the users in higher hierarchies are allowed to decrypt the encrypted session keys issued in some lower hierarchies. this method combines key distribution with access control and can be applied in the small enterprises where the users are structured in hierarchies.sharing session keys in encrypted databases','Management and querying of encrypted data'
'while traditional data-management systems focus on evaluating single, ad-hoc queries over static data sets in a centralized setting, several emerging applications require (possibly, continuous) answers to queries on dynamic data that is widely distributed and constantly updated. furthermore, such query answers often need to discount data that is \"stale\", and operate solely on a sliding window of recent data arrivals (e.g., data updates occurring over the last 24 hours). such distributed data streaming applications mandate novel algorithmic solutions that are both time- and space-efficient (to manage high-speed data streams), and also communication-efficient (to deal with physical data distribution). in this paper, we consider the problem of complex query answering over distributed, high-dimensional data streams in the sliding-window model. we introduce a novel sketching technique (termed ecm-sketch) that allows effective summarization of streaming data over both time-based and count-based sliding windows with probabilistic accuracy guarantees. our sketch structure enables point as well as inner-product queries, and can be employed to address a broad range of problems, such as maintaining frequency statistics, finding heavy hitters, and computing quantiles in the sliding-window model. focusing on distributed environments, we demonstrate how ecm-sketches of individual, local streams can be composed to generate a (low-error) ecm-sketch summary of the order-preserving aggregation of all streams; furthermore, we show how ecm-sketches can be exploited for continuous monitoring of sliding-window queries over distributed streams. our extensive experimental study with two real-life data sets validates our theoretical claims and verifies the effectiveness of our techniques. to the best of our knowledge, ours is the first work to address efficient, guaranteed-error complex query answering over distributed data streams in the sliding-window model.sketch-based querying of distributed sliding-window data streams','Management and querying of encrypted data'
'we provide a framework for querying of databases. we model a query as a collection of required conditions and an imperative for combining an object\'s satisfaction to the individual conditions to get its overall satisfaction. we investigate tools that can enrich this process by enabling the inclusion ofmore human focused considerations.we discuss how to include flexible conditions with the use of fuzzy sets. we describe some more sophisticated techniques for aggregating the satisfactions of the individual conditions based on the inclusion of importances and the use of the owa operator. we introduce a new method for aggregating the individual satisfactions that can model a lexicographic relation between the individual requirements. in addition to considering more human focused aspects of the query we look at databases in which the information in the database can have some probabilistic or possibilistic uncertainty.soft querying of standard and uncertain databases','Management and querying of encrypted data'
'there is some private and sensitive data in database, which need to be protected from attacking. in order to reinforce the security of data, an effective mechanism, cryptographic support has been widely used. however, we must make a tradeoff between the performance and the security because encryption and decryption greatly degrade the query performance. to solve such a problem, a novel approach is proposed in this paper that can quickly execute sql query on the encrypted data. for character data, it not only encrypts them, but also turns the character data into characteristic values via a characteristic function and stores them as additional fields. for numerical data, it not only encrypts them, but also creates its b+ tree index before the encryption in order to keep the ordering of each record in the index. furthermore, we give the algorithms of querying the encrypted data based on the storage models. results of sets of experiments validate the functionality and usability of our approach.storage and query over encrypted character and numerical data in database','Management and querying of encrypted data'
'storage and querying of e-commerce data','Management and querying of encrypted data'
'storage and querying of high dimensional sparsely populated data creates new challenge to conventional horizontal model. it requires supporting large number of columns and frequently altering of database schema. the sparsity of data degrades performance in both time and space. a 3-ary vertical representation [5] can be used. but the cardinality of the vertical table grows exponentially when the density of the non-null values increases. it is also difficult to support multiple data types usinga single vertical table. in this paper, we have presented a compressed 1-ary vertical representation where schema evolution is easy and size grows linearly with non-null density. queries can be processed on compressed form of data without decompression. decompression is done only when the result is necessary. we have considered three alternative representations: 3-ary uncompressed vertical, 1-ary compressed bit-array and 1-ary compressed offset. experimental results show the superiority of 1-ary offset representation in both space and time.storage and querying of high dimensional sparsely populated data in compressed representation','Management and querying of encrypted data'
'information imprecision and uncertainty exist in many real-world applications and for this reason fuzzy data management has been extensively investigated in various database management systems. currently, introducing native support for xml data in relational database management systems (rdbms) has attracted considerable interest with a view to leveraging the powerful and reliable data management services provided by rdbms. although there is a rich literature on xml-to-relational storage, none of the existing solutions satisfactorily addresses the problem of storing fuzzy xml data in rdbms. in this paper, we study the methodology of storing and querying fuzzy xml data in relational databases. in particular, we present an edge-based approach to shred fuzzy xml data into relational data. the unique feature of our approach is that no schema information is required for our data storage. on this basis, we present a generic approach to translate path expression queries into sql for processing xml queries.storing and querying fuzzy xml data in relational databases','Management and querying of encrypted data'
'as the popularity of extensible markup language (xml) continues to increase at an astonishing pace, data management systems for storing and querying large repositories of xml data are urgently needed. in this paper, we investigate an object-relational dbms (ordbms) for storing and querying xml data. we present an algorithm, called xorator, for mapping xml documents to tables in an ordbms. an important part of this mapping is assigning a fragment of an xml document to a new xml data type. we demonstrate that using the xo-rator algorithm, an ordbms is usually more efficient than a relational dbms (rdbms). based on an actual implementation in db2 v.7.2, we compare the performance of the xorator algorithm with a well-known algorithm for mapping xml data to an rdbms. our experiments show that the xorator algorithm requires less storage space, has much faster loading times, and in most cases can evaluate queries faster. the primary reason for this performance improvement is that the xorator algorithm results in a database that is smaller in size, and queries that usually have fewer number of joins.storing and querying xml data in object-relational dbmss','Management and querying of encrypted data'
'xml database systems emerge as a result of the acceptance of the xml data model. recent works have followed the promising approach of building xml database management systems on underlying rdbms&#x2019;s. achieving query processing performance reduces to two questions: (i) how should the xml data be decomposed into data that are stored in the rdbms? (ii) how should the xml query be translated into an efficient plan that sends one or more sql queries to the underlying rdbms and combines the data into the xml result? we provide a formal framework for xml schema-driven decompositions, which encompasses the decompositions proposed in prior work and extends them with decompositions that employ denormalized tables and binary-coded xml fragments. we provide corresponding query processing algorithms that translate the xml query conditions into conditions on the relational tables and assemble the decomposed data into the xml query result. our key performance focus is the response time for delivering the first results of a query. the most effective of the described decompositions have been implemented in xcachedb, an xml dbms built on top of a commercial rdbms, which serves as our experimental basis. we present experiments and analysis that point to a class of decompositions, called inlined decompositions, that improve query performance for full results and first results, without significant increase in the size of the database.storing and querying xml data using denormalized relational databases','Management and querying of encrypted data'
'structuring and querying personalized audio using ontologies','Management and querying of encrypted data'
'over recent years, peer to peer (p2p) traffic over the internet has increased dramatically. p2p file sharing applications, such as napster, gnutella, kazaa, bittorrent, skype and pplive, have experienced tremendous success among end users. recent statistics suggests that p2p traffic accounts towards 70\% of internet traffic. p2p traffic becomes a major concern for internet service provider (isp). some isps have implemented shaping or blocking techniques in order to stem the flow of p2p traffic over their networks, which not only affect end users, but also the companies who develop p2p applications. although p2p protocols can be used for transferring illegal content they have great potential for legitimate users, especially those in the business world. in this paper, we examine both encrypted and non-encrypted network traffic from bittorrent, one of the most popular p2p protocols. first hardware and software requirements for this study are described. secondly a torrent\'s life cycle and data capture from the internet are explained. then encrypted and non-encrypted traffic are compared and analysed. our investigations show that p2p is almost impossible to hide as not every packet is encrypted in the encrypted stream.study of encrypted and non-encrypted bittorrent traffic','Management and querying of encrypted data'
'transparent encryption has two main requirements, i.e. security and perceived quality. the perceptual quality aspect has never been thoroughly investigated. in this work, three variants to transparently encrypt jpeg2000 images are compared from a perceptual quality viewpoint. the assessment is based on subjective and objective quality assessment of the transparently encrypted images and if the requirements with respect to desired functionalities can be met by the respective techniques. in particular, we focus on the question if it is possible to predict the subjective quality of the encrypted (and attacked) images as given by the mean opinion score (mos) with state-of-the-art objective quality metrics. additionally, we answer the question which objective quality measure is suited best to determine an image quality for which a certain subjective quality is required.subjective and objective quality assessment of transparently encrypted jpeg2000 images','Management and querying of encrypted data'
'much has been written about the necessity of processing data in the encrypted form. however, no satisfactory method of processing encrypted data has been published to date. ahitub et al. [2] have analyzed the possibilities of using some special algorithms to add encrypted data. rivest et al. [10] have suggested the use of an algorithm based on homomorphic functions for processing encrypted data. the main limitation of this algorithm is that such functions can be broken by solving a set of linear equations, as noted by [2]. the public-key crytosystem described in [11] can be used to multiply encrypted data but cannot be used to add encrypted data and is therefore not appropriate for some practical applications such as bank transactions. abadi, feigenbaum and kilian [1] presented some general theorems concerning the problem of computing with encrypted data and formulated a framework to prove precise statements about what an encrypted instance hides and reveals; they also described encryption schemes for some well-known functions.superimposing encrypted data','Management and querying of encrypted data'
'test of time award talk: executing sql over encrypted data in the database-service-provider model','Management and querying of encrypted data'
'inspired by the cap theorem, we identify three desirable properties when querying the web of data: alignment (results up-to-date with sources), coverage (results covering available remote sources), and efficiency (bounded resources). in this short paper, we show that no system querying the web can meet all three ace properties, but instead must make practical trade-offs that we outline.the ace theorem for querying the web of data','Management and querying of encrypted data'
'we review a recently introduced computation model for streaming and external memory data. an important feature of this model is that it distinguishes between sequentially reading (streaming) data from external memory (through main memory) and randomly accessing external memory data at specific memory locations; it is well-known that the latter is much more expensive in practice. we explain how a number of lower bound results are obtained in this model and how they can be applied for proving lower bounds for xml query processing.the complexity of querying external memory and streaming data','Management and querying of encrypted data'
'the complexity of querying indefinite data about linearly ordered domains','Management and querying of encrypted data'
'according to the special requirements of the watermarking embedding of the medical volume data, an encrypted watermarking algorithm based on arnold scrambling and 3d-dct for medical volume data is presented in this paper. to strengthen the protection of the watermarking information, the original watermarking is preprocessed using the arnold scrambling. the part of sign sequence from 3d-dct coefficients is used as the feature vector, which is utilized to enhance the robustness against conventional attacks and geometrical attacks in different levels. firstly it is described that how to encrypt the watermarking. then we obtain the feature vector of the medical volume data, and embed and extract the watermarking. the result of our experiment shows that the encrypted watermarking algorithm has strong robustness and security.the encrypted watermarking algorithm for medical volume data based on arnold scrambling and 3d-dct','Management and querying of encrypted data'
'querying object evolution in temporal databases is interesting, but neither sql-like algebraic languages nor general purpose languages take evolution into account. this paper presents a language, called the event matching language, that is specifically designed for querying object evolution. the language is a pattern matching language based on the concept of cursor borrowed from snobol4.the event matching language for querying temporal data','Management and querying of encrypted data'
'using the terminology usual in databases, it is possible to view xml as a language for data modeling. to retrieve xml data from xml databases, several query languages have been proposed. the common feature of such languages is the use of regular path expressions. they enable the user to navigate through arbitrary long paths in xml data. if we considered a path content as a vector of path elements, we would be able to model xml paths as points within a multidimensional vector space. this paper introduces a geometric framework for indexing and querying xml data conceived in this way. in consequence, we can use certain data structures for indexing multidimensional points (objects). we use the ub-tree for indexing the vector spaces and the m-tree for indexing the metric spaces. the data structures for indexing the vector spaces lead rather to exact matching queries while the structures for indexing the metric spaces allow us to provide the similarity queries.the geometric framework for exact and similarity querying xml data','Management and querying of encrypted data'
'motivation: the development of glycomics technologies in recent years has produced a sufficient amount of data to begin analyzing the glycan structures present in various organisms and tissues. in particular, glycan profiling using mass spectrometry (ms) and tandem ms has generated a large amount of data that are waiting to be analyzed. the consortium for functional glycomics (cfg) has provided a web resource for obtaining such glycan profiling data easily. although an interactive spectrum viewer is provided on the website as a java applet, it is not necessarily easy to search for particular glycans or to find commonalities between different tissues in a single organism, for example. therefore, to allow users to better take advantage of the valuable glycome data that can be obtained from mass spectra and other leading technologies, we have developed a tool called glycome atlas which is pre-loaded with the data from the cfg and is also able to visualize local glycan profiling data for human and mouse. results: we have developed a tool to allow users to visualize and perform queries of glycome data. this tool, called glycomeatlas, is pre-loaded with glycome data as provided by the cfg. moreover, users can load their own local glycome data into this tool to visualize and perform queries on their own data. availability: this tool is available at the following url: http://www.rings.t.soka.ac.jp/glycomeatlas/gui.html.  contact: kkiyoko@soka.ac.jp the glycomeatlas tool for visualizing and querying glycome data','Management and querying of encrypted data'
'encryption is an effective way to enhance the security of the database. the key management is the important factor to ensure the security of the encrypted database. but the current scheme of the key management has some security problem and is easily attacked. face to this problem, the new scheme about key management base on xml is proposed in this paper. a xml file that has the tree structure is used to mange the working key that is protected by the primary key. it can not only manage the working key efficiently, but also enhance the security of the encrypted database.the key management of the encrypted database based on xml','Management and querying of encrypted data'
'we present a framework for compressing encrypted media, such as images and videos. encryption masks the source, rendering traditional compression algorithms ineffective. by conceiving of the problem as one of distributed source coding, it has been shown in prior work that encrypted data are as compressible as unencrypted data. however, there are two major challenges to realize these theoretical results. the first is the development of models that capture the underlying statistical structure and are compatible with our framework. the second is that since the source is masked by encryption, the compressor does not know what rate to target. we tackle these issues in this paper. we first develop statistical models for images before extending it to videos, where our techniques really gain traction. as an illustration, we compare our results to a state-of-the-art motion-compensated lossless video encoder that requires unencrypted video input. the latter compresses each unencrypted frame of the ldquoforemanrdquo test sequence by 59\% on average. in comparison, our proof-of-concept implementation, working on encrypted data, compresses the same sequence by 33\%. next, we develop and present an adaptive protocol for universal compression and show that it converges to the entropy rate. finally, we demonstrate a complete implementation for encrypted video.toward compression of encrypted images and video sequences','Management and querying of encrypted data'
'towards a unified querying system of both structured and semi-structured imprecise data using fuzzy view','Management and querying of encrypted data'
'keywords queries on various data models have been studied in the past. regardless of the nature of target data, a keyword query poses a common problem: the difficulty in determining a suitable retrieval unit. in this paper, we discuss this problem in a new scenario that assumes several different types of data being stored and queried in a unified framework. a hypothetical general query model that borrows the ideas of our previous model designed for xml and linear video data is presented. we then state the requirements and challenges to realize such a model.towards an integrated framework for querying collection of heterogeneous data','Management and querying of encrypted data'
'encryption schemes that support computation on encrypted data are useful in constructing efficient and intuitively simple cryptographic protocols. however, the approach was previously limited to stand-alone and/or honest-but-curious security. in this work, we apply recent results on \"non-malleable homomorphic encryption\" to construct new protocols with universally composable security against active corruption, for certain interesting tasks. also, we use our techniques to develop non-malleable homomorphic encryption that can handle homomorphic operations involving more than one ciphertext. towards robust computation on encrypted data','Management and querying of encrypted data'
'cloud computing enables an economic paradigm of data service outsourcing, where individuals and enterprise customers can avoid committing large capital outlays in the purchase and management of both software and hardware and the operational overhead therein. despite the tremendous benefits, outsourcing data management to the commercial public cloud is also depriving customers\' direct control over the systems that manage their data, raising security and privacy as the primary obstacles to the adoption of cloud. although data encryption helps protecting data confidentiality, it also obsoletes the traditional data utilization service based on plain text keyword search. thus, enabling an encrypted cloud data search service with privacy-assurance is of paramount importance. considering the large number of data users and huge amount of outsourced data files in cloud, this problem is particularly challenging as it is extremely difficult to meet also the practical requirements of performance, system usability, and high-level user searching experiences. this paper investigates these challenges and defines the problem of fuzzy keyword search over encrypted cloud data, which should be explored for effective data utilization in cloud computing. fuzzy keyword search aims at accommodating various typos and representation inconsistencies in different user searching input for acceptable system usability and overall user searching experience, while protecting keyword privacy. in order to further enrich the spectrum of secure cloud data utilization services, we also study how the notion of fuzzy search naturally supports similarity search, a fundamental and powerful tool that is widely used in information retrieval. we describe the challenges that are not yet met by existing searchable encryption techniques and discuss the research directions and possible technical approaches for these new search functionalities to become a reality. the investigation of the proposed research can become the key for cloud service providers to securely and effectively deliver value from the cloud infrastructure to their enterprise and individual customers, and thus significantly encourage the adoption of cloud computing in a large scale.towards secure and effective utilization over encrypted cloud data','Management and querying of encrypted data'
'efficient power management is vital for increasing the life of sensor nodes in wireless sensor networks (wsn). thus, techniques such as data aggregation have been widely used in wsn to preserve nodes\' energy. despite its appealing and powerful features, data aggregation processes require a high level of security. ensuring accuracy with aggregating data can become an issue even due to single bit errors. hence, many encryption techniques have been proposed to secure the data aggregation process. however, once a wireless sensor is physically compromised, the keys stored on the sensor are exposed, thus rendering the cryptography useless. moreover, most cryptographic approaches restrict the level of collaboration with other security measures -- especially trust management, where behavior observation is vital. in this paper, we propose a trust management approach on encrypted packets to deal with the faulty nodes in sensor networks. we designed a collaborative framework which incorporates the order-preserved encryption scheme [2] and sigmoid trust management approaches. we then discuss how these two can coexist and comprehensively evaluate an integrated opes-trust management scheme in the sensing environment using both the tossim simulator and an actual mote implementation. the results have shown that trust management and opes cryptography does not consume more energy than radio transmission, yet provides a fairly accurate result.trust management of encrypted data aggregation in a sensor network environment','Management and querying of encrypted data'
'we consider a new model for online secure computation on encrypted inputs in the presence of malicious adversaries. the inputs are independent of the circuit computed in the sense that they can be contributed by separate third parties. the model attempts to emulate as closely as possible the model of \"computing with encrypted data\" that was put forth in 1978 by rivest, adleman and dertouzos which involved a single online message. in our model, two parties publish their public keys in an offline stage, after which any party (i.e., any of the two and any third party) can publish encryption of their local inputs. then in an on-line stage, given any common input circuit c and its set of inputs from among the published encryptions, the first party sends a single message to the second party, who completes the computation.two-party computing with encrypted data','Management and querying of encrypted data'
'in this work, a universal reversible data embedding method applicable to any encrypted domain (urdeed) is proposed. urdeed operates completely in the encrypted domain and requires no feature of the signal prior to the encryption process. in particular, urdeed exploits the coding redundancy of the encrypted signal by partitioning it into segments referred to as imaginary codewords (ic\'s). then, ic\'s are entropy encoded by using golomb-rice codewords (grc\'s). finally, each grc is modified to accommodate two bits from the augmented payload. urdeed is designed to preserve the same file-size as that of the original input (encrypted) signal by embedding the quotient part of the grc\'s as side information. moreover, urdeed is consistently reversible and universally applicable to any digital signal encrypted by any encryption method. experimental results show that urdeed achieves an average embedding capacity of ~0.169 bit per every bit of the encrypted (host) signal.universal data embedding in encrypted domain','Management and querying of encrypted data'
'the technologies of mobile communications pervade our society and wireless networks sense the movement of people, generating large volumes of mobility data, such as mobile phone call records and global positioning system (gps) tracks. in this work, we illustrate the striking analytical power of massive collections of trajectory data in unveiling the complexity of human mobility. we present the results of a large-scale experiment, based on the detailed trajectories of tens of thousands private cars with on-board gps receivers, tracked during weeks of ordinary mobile activity. we illustrate the knowledge discovery process that, based on these data, addresses some fundamental questions of mobility analysts: what are the frequent patterns of people\'s travels? how big attractors and extraordinary events influence mobility? how to predict areas of dense traffic in the near future? how to characterize traffic jams and congestions? we also describe m-atlas, the querying and mining language and system that makes this analytical process possible, providing the mechanisms to master the complexity of transforming raw gps tracks into mobility knowledge. m-atlas is centered onto the concept of a trajectory, and the mobility knowledge discovery process can be specified by m-atlas queries that realize data transformations, data-driven estimation of the parameters of the mining methods, the quality assessment of the obtained results, the quantitative and visual exploration of the discovered behavioral patterns and models, the composition of mined patterns, models and data with further analyses and mining, and the incremental mining strategies to address scalability.unveiling the complexity of human mobility by querying and mining massive trajectory data','Management and querying of encrypted data'
'in this paper, we consider databases representing information about moving objects(e.g., vehicles), particularly their location.we address the problems of updating and querying suchdatabases. specifically, the update problem is to determine whenthe location of a moving object in the database (namely itsdatabase location) should be updated. we answer this question by proposing an information cost model that captures uncertainty, deviation, and communication.then we analyze dead-reckoning policies, namely policies that update the database location whenever the distancebetween the actual location and the database location exceeds agiven threshold, x.dead-reckoning is the prevalent approach in military applications,and our cost model enables us to determine the threshold x.we propose several dead-reckoning policies and we compare theirperformance by simulation.then we consider the problem of processing range queriesin the database. an example of a range query is &#8216;retrieve the objects thatare currently inside a given polygon p&#8242;. we propose a probabilisticapproach to solve the problem. namely, the dbms will answersuch a query with a set of objects, each of which is associatedwith a probability that the object is inside p.updating and querying databases that track mobile units','Management and querying of encrypted data'
'we propose an end-user oriented approach to querying repositories of data and provenance in e-science environments. the approach is based on ontology models describing multiple domains  in silico experiments, provenance, data, and applications. those ontologies, integrated in a unified model and containing mappings to underlying data models, allow to query repositories of data and provenance in a unified way, or even combine provenance and data aspects in one query. we demonstrate query translation tools (quatro), built on top of the ontology models, which allow to construct complex queries over both data and provenance repositories, expressed in the terms of the domain familiar to end users. we present, in the context of the virolab virtual laboratory for infectious diseases, examples of construction of complex queries, combining provenance and data model aspects, which can be of practical value to scientists or medical users. keywords: ontology models, end-user oriented database querying, provenance, semantic griduser-oriented querying over repositories of data and provenance','Management and querying of encrypted data'
'in statically analyzing large sample collections, packed and encrypted malware pose a significant challenge to automating the identification of malware attributes and functionality. entropy analysis examines the statistical variation in malware executables, enabling analysts to quickly and efficiently identify packed and encrypted samples.using entropy analysis to find encrypted and packed malware','Management and querying of encrypted data'
'when employing cryptographic tunnels such as the ones provided by secure shell (ssh) to protect their privacy on the internet, users expect two forms of protection. first, they aim at preserving the privacy of their data. second, they expect that their behavior, e.g., the type of applications they use, also remains private. in this paper we report on two statistical traffic analysis techniques that can be used to break the second type of protection when applied to ssh tunnels, at least under some restricting hypothesis. experimental results show how current implementations of ssh can be susceptible to this type of analysis, and illustrate the effectiveness of our two classifiers both in terms of their capabilities in analyzing encrypted traffic and in terms of their relative computational complexity.using gmm and svm-based techniques for the classification of ssh-encrypted traffic','Management and querying of encrypted data'
'this paper proposes an approach for representing and querying semistructured web data, which is based on nested tables allowing internal nested structural variations. our motivation is to reduce the complexity found in typical query languages for semistructured data and to provide users with an alternative for quickly querying data obtained from multiple-record web pages. we show the feasibility of our proposal by developing a prototype for a graphical query interface called qsbye (querying semistructured data by example), which implements a set of qbe-like operations that extends typical nested-relational-algebra operations to handle semistructured data.using nested tables for representing and querying semistructured web data','Management and querying of encrypted data'
'many systems and strategies have been proposed for processing non-terminating data streams. each approach has advantages and disadvantages, including the kinds of queries that can be executed. we present a framework for characterizing the kinds of queries that can be executed over streams based on a notion of compact sets from topology. we first apply our framework to queries over punctuated data streams. previous work on punctuations focused primarily on the behavior of individual query operators. we use our framework to determine if an entire query can benefit from punctuations available from stream sources. we then consider other common strategies proposed in the literature for executing queries over streams, and we discuss how our framework can characterize the kinds of queries each strategy can answer.using punctuation schemes to characterize strategies for querying over data streams','Management and querying of encrypted data'
'vertical partitioning is a well-known technique for optimizing query performance in relational databases. an extreme form of this technique, which we call vectorization, is to store each column separately. we use a generalization of vectorization as the basis for a native xml store. the idea is to decompose an xml document into a set of vectors that contain the data values and a compressed skeleton that describes the structure. in order to query this representation and produce results in the same vectorized format, we consider a practical fragment of xquery and introduce the notion of query graphs and a novel graph reduction algorithm that allows us to leverage relational optimization techniques as well as to reduce the unnecessary loading of data vectors and decompression of skeletons. a preliminary experimental study based on some scientific and synthetic xml data repositories in the order of gigabytes supports the claim that these techniques are scalable and have the potential to provide performance comparable with established relational database technology.vectorizing and querying large xml repositories','Management and querying of encrypted data'
'visualization techniques increase the user involvement in the interactive process of data mining and querying of spatio-temporal data. this paper describes a novel geometric approach to clustering and querying of spatio-temporal data. we propose the uniform geometric model based on function representation of solids to cluster and query time-dependent data. clustering and querying are integrated with visualization techniques in one gui. first, visual clustering with blobby model allows the user to see the result of clustering on the screen for different time points and/or time intervals and set the appropriate parameters interactively. after that, the user gets the data of clusters for the chosen time frames. then, the user can visually query the cluster/clusters he/she is interested in with geometric primitive solids which currently are cubes, spheres/ellipsoids, cylinders, etc. geometric operations of union, intersection and/or subtraction can be performed over the geometric primitive solids to get the final query shape. the user visually clusters spatio-temporal data and queries the clusters with geometric shapes through graphics interface accessing dynamically 3d projections of multidimensional points from database, warehouses or files.with the uniform geometric model of the clustering and querying of spatio-temporal data, 3d visualization tools can be naturally incorporated in one system to allow the user to visualize and query clusters changing over time.visual interactive clustering and querying of spatio-temporal data','Management and querying of encrypted data'
'visual querying and explanation of recommendations from collaborative filtering systems','Management and querying of encrypted data'
'software engineering problems often involve large sets of objects and complex relationships among them. this report proposes that graphical visualization techniques can help engineers understand and solve a class of these problems. to illustrate this, two problems are analyzed and recast using the graphical language graphlog. the first problem is that of simplifying dependencies among components of a system, which translates into removing cycles from a graph. the second problem is that of designing an efficient code overlay structure, which is facilitated in several ways through graphical techniques.visualizing and querying software structures','Management and querying of encrypted data'
'visualizing queries and querying visualizations','Management and querying of encrypted data'
'in this paper, we address the problem of trajectory data streams warehousing and querying, that revealed really challenging as we deal with data (trajectories) for which the order of elements is relevant. we propose an end to end framework in order to make the querying step quite effective. we performed several tests on real world datasets that confirmed the efficiency and effectiveness of the proposed techniques.warehousing and querying trajectory data streams with error estimation','Management and querying of encrypted data'
'recently, halevi et al. (ccs \'11) proposed a cryptographic primitive called proofs of ownership (pow) to enhance security of client-side deduplication in cloud storage. in a proof of ownership scheme, any owner of the same file f can prove to the cloud storage that he/she owns file f in a robust and efficient way, in the bounded leakage setting where a certain amount of efficiently-extractable information about file f is leaked. following this work, we propose a secure client-side deduplication scheme, with the following advantages: our scheme protects data confidentiality (and some partial information) against both outside adversaries and honest-but-curious cloud storage server, while halevi et al. trusts cloud storage server in data confidentiality; our scheme is proved secure w.r.t. any distribution with sufficient min-entropy, while halevi et al. (the last and the most practical construction) is particular to a specific type of distribution (a generalization of \"block-fixing\" distribution) of input files. the cost of our improvements is that we adopt a weaker leakage setting: we allow a bounded amount one-time leakage of a target file before our scheme starts to execute, while halevi et al. allows a bounded amount multi-time leakage of the target file before and after their scheme starts to execute. to the best of our knowledge, previous works on client-side deduplication prior halevi et al. do not consider any leakage setting.weak leakage-resilient client-side deduplication of encrypted data in cloud storage','Management and querying of encrypted data'
'we report on work in progress on the development of sweetinfo (semantic web-enabled exploration of temporal information), a tool for querying and visualizing time-oriented clinical data. sweetinfo is based on an open-source web-based infrastructure that allows clinical investigators to import data and to perform operations on their temporal dimensions. the architecture combines semantic web standards, such as owl and swrl, with advanced web development software, such as the google web toolkit. user interaction with sweetinfo creates owl-based specifications of (1) data operations, such as filtering, grouping, and visualization, and (2) data pipelines for data analyses. both of these can be shared with and adapted by other users via the web. our system meets the functional and nonfunctional specifications derived from the use cases. we will demo how sweetinfoprovides non-technical users the ability to interactively define data pipelines for such complex temporal analyses.web-based querying and temporal visualization of longitudinal clinical data','Management and querying of encrypted data'
'there have been several proposals for transforming and querying xml structures, but very few of them offer a data logic approach. this article describes data structures and some primitives as a framework to efficiently manipulate xml well-formed documents. the implementation of the querying system, based on the proposed framework, is performed in the prolog programming language. results indicate that this approach allows the solution of both deductive and recursive queries from xml documents.xml querying using data logic structures and primitives','Management and querying of encrypted data'
'crowd sourcing is a collaboration model enabled by people-centric web technologies to solve individual, organizational, and societal problems using a dynamically formed crowd of people who respond to an open call for participation. we report on a literature survey of crowd sourcing research, focusing on top journals and conferences in the information systems (is) field. to our knowledge, ours is the first effort of this type in the is discipline. contributions include providing a synopsis of crowd sourcing research to date, a common definition for crowd sourcing, and a conceptual model for guiding future studies of crowd sourcing. we show how existing is literature applies to the elements of that conceptual model: problem, people (problem owner, individual, and crowd), governance, process, technology, and outcome. we close with suggestions for future research.conceptual foundations of crowdsourcing','Mathematical foundations of cryptography'
'the theses of existonness, compoundness, and polyadness are proved. the consistency of these theses with the reversibility principle is founded. existential foundations of the composition paradigm are constructed.existential foundations of the composition paradigm','Mathematical foundations of cryptography'
'extensions and foundations of object-oriented programming','Mathematical foundations of cryptography'
'4thought, a prototype design tool, is based on the notion that design artifacts are complex, formal, mathematical objects that require complementary textual and graphical views to be adequately comprehended. this paper describes the combined use of entity- relationship modelling and graphlog to bridge the textual and graphical views. these techniques are illustrated by an example that is formally specified in z notation.foundations of 4thought','Mathematical foundations of cryptography'
'foundations of aggregation constraints','Mathematical foundations of cryptography'
'ants are generally believed to follow an intensive work routine. numerous tales and fables refer to ants as conscientious workers. nevertheless, biologists have discovered that ants also rest for extended periods of time. this does not only hold for individual ants. interestingly, ant colonies exhibit synchronized activity phases that result from self-organization. in this work, self-synchronization in ant colonies is taken as the inspiring source for a new mechanism of self-synchronized duty-cycling in mobile sensor networks. hereby, we assume that sensor nodes are equipped with energy harvesting capabilities such as, for example, solar cells. we show that the proposed self-synchronization mechanism can be made adaptive depending on variable energy resources. the main objective of this paper is to study and explore the swarm intelligence foundations of self-synchronized duty-cycling. with this purpose in mind, physical constraints such as packet collisions and packet loss are generally not considered.foundations of antcycle','Mathematical foundations of cryptography'
'attack trees have found their way to practice because they have proved to be an intuitive aid in threat analysis. despite, or perhaps thanks to, their apparent simplicity, they have not yet been provided with an unambiguous semantics. we argue that such a formal interpretation is indispensable to precisely understand how attack trees can be manipulated during construction and analysis. we provide a denotational semantics, based on a mapping to attack suites, which abstracts from the internal structure of an attack tree, we study transformations between attack trees, and we study the attribution and projection of an attack tree.foundations of attack trees','Mathematical foundations of cryptography'
'computer tools for cognitively challenging activities are considered useful, to a great extent, because of the support that they provide for human thinking and problem solving. to analyze, specify, and design cognitive support, a suitable analytic framework is required. theories of \"distributed cognition\" have been offered as potentially suitable frameworks, but they have generally failed to plainly articulate comprehensive theories of cognitive support. this paper seeks to clarify the intellectual foundations for studying and designing cognitive support, and aims to put them in a form suitable for design. a framework called rods is described as a type of minimal, lightweight intellectual toolkit. its main aim is to allow analysts to think in high-level cognition-support terms rather than be overwhelmed by task- and technology-specific implementation details. framing usefulness in terms of cognitive support makes it possible to define abstract patterns of what makes tools \"good\". implications are drawn for how the framework may be used for the design of tools in cognitively challenging work domains.foundations of cognitive support','Mathematical foundations of cryptography'
'foundations of computer communications','Mathematical foundations of cryptography'
'revolutionary developments which took place in the 1980\'s have transformed cryptography from a semi-scientific discipline to a respectable field in theoretical computer science. in particular, concepts such as computational indistinguishability, pseudorandomness and zero-knowledge interactive proofs were introduced and classical notions as secure encryption and unforgeable signatures were placed on sound grounds. the resulting field of cryptography, reviewed in this survey, is strongly linked to complexity theory (in contrast to \"classical\" cryptography which is strongly related to information theory.foundations of cryptography','Mathematical foundations of cryptography'
'foundations of denotational semantics','Mathematical foundations of cryptography'
'this chapter accompanies the foundational lecture ondescription logics (dls) at the 7th reasoning web summer school in galway, ireland, 2011. it introduces basic notions and facts about this family of logics which has significantly gained in importance over the recent years as these logics constitute the formal basis for today\'s most expressive ontology languages, the owl (web ontology language) family. we start out from some general remarks and examples demonstrating the modeling capabilities of description logics as well as their relation to first-order predicate logic. then we begin our formal treatment by introducing the syntax of dl knowledge bases which comes in three parts: rbox, tbox and abox. thereafter, we provide the corresponding standard model-theoretic semantics and give a glimpse of the alternative way of defining the semantics via an embedding into first-order logic with equality. we continue with an overview of the naming conventions for dls before we delve into considerations about different notions of semantic alikeness (concept and knowledge base equivalence as well as emulation). these are crucial for investigating the expressivity of dls and performing normalization. we move on by reviewing knowledge representation capabilities brought about by different dl features and their combinations as well as some model-theoretic properties associated thereto. subsequently, we consider typical reasoning tasks occurring in the context of dl knowledge bases. we show how some of these tasks can be reduced to each other, and have a look at different algorithmic approaches to realize automated reasoning in dls. finally, we establish connections between dls and owl. we show how dl knowledge bases can be expressed in owl and, conversely, how owl modeling features can be translated into dls. in our considerations, we focus on the description logic sroiq which underlies the most recent and most expressive yet decidable version of owl called owl 2 dl. we concentrate on the logical aspects and omit data types as well as extralogical features from our treatise. examples and exercises are provided throughout the chapter.foundations of description logics','Mathematical foundations of cryptography'
'directed mutation abandons the so-called random mutation hypothesis postulating mutations to occur at random, regardless of fitness consequences to the resulting offspring. by introducing skewness into the mutation operators, bigger portions of offspring can be created in the area of higher fitness with respect to the elder and thus promising directions of the evolution path can be favored. the aim of this work is to present the foundations of directed mutation as well as different operators in one single place. their characteristics will be presented and their advantages and disadvantages are discussed. furthermore, an application scenario will be presented that shows how evolutionary algorithm and directed mutation can be applied in engineering design. in addition, some experimental results solving a real world optimization task in this scenario are provided. finally some first, preliminary results of a multivariate skew distribution as mutation operator in a covariance matrix adaptation algorithm will be presented.foundations of directed mutation','Mathematical foundations of cryptography'
'some months ago, i asked for comments on the relationship between standards and documentation. having received none, i offer some preliminary views of my own.foundations of documentation','Mathematical foundations of cryptography'
'foundations of equational deduction','Mathematical foundations of cryptography'
'exact rounding of numbers and functions is a fundamental computational problem. this paper introduces the mathematical and computational foundations for exact rounding. we show that all the elementary functions in iso standard (iso/iec 10967) for language independent arithmetic can be exactly rounded, in any format, and to any precision. moreover, a priori complexity bounds can be given for these rounding problems. our conclusions are derived from results in transcendental number theory. foundations of exact rounding','Mathematical foundations of cryptography'
'foundations of fuzzy sets','Mathematical foundations of cryptography'
'garbled circuits, a classical idea rooted in the work of yao, have long been understood as a cryptographic technique, not a cryptographic goal. here we cull out a primitive corresponding to this technique. we call it a garbling scheme. we provide a provable-security treatment for garbling schemes, endowing them with a versatile syntax and multiple security definitions. the most basic of these, privacy, suffices for two-party secure function evaluation (sfe) and private function evaluation (pfe). starting from a prf, we provide an efficient garbling scheme achieving privacy and we analyze its concrete security. we next consider obliviousness and authenticity, properties needed for private and verifiable outsourcing of computation. we extend our scheme to achieve these ends. we provide highly efficient blockcipher-based instantiations of both schemes. our treatment of garbling schemes presages more efficient garbling, more rigorous analyses, and more modularly designed higher-level protocols.foundations of garbled circuits','Mathematical foundations of cryptography'
'recently, a first step toward establishing foundations for group signatures was taken [5], with a treatment of the case where the group is static. however the bulk of existing practical schemes and applications are for dynamic groups, and these involve important new elements and security issues. this paper treats this case, providing foundations for dynamic group signatures, in the form of a model, strong formal definitions of security, and a construction proven secure under general assumptions. we believe this is an important and useful step because it helps bridge the gap between [5] and the previous practical work, and delivers a basis on which existing practical schemes may in future be evaluated or proven secure.foundations of group signatures','Mathematical foundations of cryptography'
'many people believe that emotions and subjective feelings are one and the same and that a goal of human-centered computing is emotion recognition. the first belief is outdated; the second mistaken. for human-centered computing to succeed, a different way of thinking is needed. emotions are species-typical patterns that evolved because of their value in addressing fundamental life tasks. emotions consist of multiple components, of which subjective feelings may be one. they are not directly observable, but inferred from expressive behavior, self-report, physiological indicators, and context. i focus on expressive facial behavior because of its coherence with other indicators and research. among the topics included are measurement, timing, individual differences, dyadic interaction, and inference. i propose that design and implementation of perceptual user interfaces may be better informed by considering the complexity of emotion, its various indicators, measurement, individual differences, dyadic interaction, and problems of inference.foundations of human computing','Mathematical foundations of cryptography'
'first page of the articlefoundations of information theory','Mathematical foundations of cryptography'
'a novel form of labelled transition system is proposed, where the labels are the arrows of a category, and adjacent labels in computations axe required to be composable. such transition systems provide the foundations for modular sos descriptions of programming languages. three fundamental ways of transforming label categories, analogous to monad transformers, are provided, and it is shown that their applications preserve computations in modular sos. the approach is illustrated with fragments taken from a modular sos for ml concurrency primitives.foundations of modular sos','Mathematical foundations of cryptography'
'this paper introduces an approach to the foundations of information science considered in the context of near sets. perceptual information systems (or, more concisely, perceptual systems) provide stepping stones leading to nearness relations, near sets and a framework for classifying perceptual objects. this work has been motivated by an interest in finding a solution to the problem of how one goes about discovering affinities between perceptual granules such as images. near set theory provides a formal basis for observation, comparison and classification of perceptual granules. this is made clear in this article by considering various nearness relations that define coverings of sets of perceptual objects that are near each other. in the near set approach, every perceptual granule is a set of objects that have their origin in the physical world. objects that have, in some degree, affinities are considered perceptually near each other, i.e., objects with similar descriptions. this article includes a comparison of near sets with other approaches to approximate knowledge representation and a sample application in image analysis. the main contribution of this article is the introduction of a formal foundation for near sets and a demonstration that the family of near sets is a gratzer slash lattice.foundations of near sets','Mathematical foundations of cryptography'
'motivated by the recent emergence of ultra-high speed (e.g., 100gb/s) optical networks, some researchers have recently started the study of optical encryption techniques that offer secrecy of information transfer over such networks. despite significant attention to this problem, we are not aware of attempts made to define a formal cryptographic model for the study of such schemes. in this paper we fill this gap by proposing a formal model for the investigation of symmetric encryption with shannon secrecy over a specific type of optical networks frequently studied in the related literature, where the security is imparted through all-optical signal processing. this model is then used to formally prove the shannon secrecy of a construction obtained by carefully combining and improving ideas appeared in the optical network literature. in our construction the key stream rate is the same as the data stream rate, which is optimal for this level of security. foundations of optical encryption','Mathematical foundations of cryptography'
'prior research on organizational trust has not rigorously examined the context specificity of trust nor distinguished between the potentially varying dimensions along which different stakeholders base their trust. as a result, dominant conceptualizations of organizational trust are overly generalized. building on existing research on organizational trust and stakeholder theory, we introduce a more nuanced perspective on the nature of organizational trust. we develop a framework that distinguishes between organizational stakeholders along two dimensions: depth of the relationship (deep or shallow) and locus (internal or external). the framework identifies which of six dimensions of trustworthiness (benevolence, integrity, managerial competence, technical competence, transparency, and identification) will be relevant to which stakeholder type. we test the predictions of our framework using original survey data from 1,298 respondents across four stakeholder groups from four different organizations. the results reveal that the relevant dimensions of trustworthiness vary systematically across different stakeholder types and provide strong support for the validity of the depth and locus dimensions.foundations of organizational trust','Mathematical foundations of cryptography'
'an extended first-order predicate sequent calculus plk with two kinds of negation is introduced as a basis of a new resolution calculus prc (paraconsistent resolution calculus) for handling the property of paraconsistency. herbrand theorem, completeness theorem (with respect to a classical-like semantics) and cut-elimination theorem are proved for plk. the correspondence between plk and prc is shown by using a faithful embedding of plk into the sequent calculus lk for classical logic.foundations of paraconsistent resolution','Mathematical foundations of cryptography'
'we formalize a mathematical approach to probabilistic logic for zero-order logic and derive new inequalities that are necessary and sufficient for consistent probability assignments to propositions. we prove that a complete theory of probabilistic logic requires the a priori assignment of 2k-1 probabilities for a system with k basic propositions. we also show that a proposal due to cheeseman, namely, to regard measures of confidence in knowledge systems as expectations that are conditioned on unknown distributions, does not work in general. we demonstrate this by showing that several certainty measures proposed for expert systems are not consistent with the derived inequalities for probabilistic logics.foundations of probabilistic logic','Mathematical foundations of cryptography'
'foundations of quantum programming','Mathematical foundations of cryptography'
'foundations of rdf databases','Mathematical foundations of cryptography'
'amongst the algorithms for biclustering using some rough sets based steps none of them uses the formal concept of rough bicluster with its lower and upper approximation. in this short article the new foundations of rough biclustering are described. the new relation &#946; generates &#946;&#8722;description classes that build the rough bicluster defined with its lower and upper approximation.foundations of rough biclustering','Mathematical foundations of cryptography'
'testing is potentially the best grounded part of software engineering, since it deals with the well defined situation of a fixed program and a test (a finite collection of input values). however, the fundamental theory of program testing is in disarray. part of the reason is a confusion of the goals of testing---what makes a test (or testing method) \"good.\" i argue that testing\'s primary goal should be to measure the dependability of tested software. in support of this goal, a plausible theory of dependability is needed to suggest and prove results about what test methods should be used, and under what circumstances. although the outlines of dependability theory are not yet clear, it is possible to identify some of the fundamental questions and problems that must be attacked, and to suggest promising approaches and research methods. perhaps the hardest step in this research is admitting that we do not already have the answers.foundations of software testing','Mathematical foundations of cryptography'
'team computing concerns the coordination of teams comprised of software components, devices, and humans operating in ubiquitous computing environments. teams have no central control: elements play their roles autonomously, and overall behavior is emergent. the proposed model promotes resilience to incompleteness and inconsistency in team designs, as well as to run-time faults. team computing is geared towards enabling end users to design and deploy their own teams in domains such as home surveillance, assisted living, and energy management. this paper presents the foundations of team computing, including its operational semantics and a framework to automate the verification of desired behaviors. the paper also discusses the implementation of a software infrastructure for the automatic deployment of systems based on team designs.foundations of team computing','Mathematical foundations of cryptography'
'in this paper, we lay the foundations for a contextualisation of trust, the role it plays, and its different layers within the context of a novel paradigm: social cloud computing. in a social cloud, trust plays a vital role as a collaboration enabler. however, trust is not trivial to define, observe, represent and analyse as precursors to understand exactly what role it plays in the enablement of collaboration. we do this through the definition of structure of a social cloud as a sequence of social and cognitive processes. we then survey research from the domains of computer science, economics and sociology that consider trust in online communities and exchange scenarios to illustrate the complexity of modelling trust in our scenario. finally, we define trust within the context of a social cloud and identify the core components of trust to facilitate its understanding.foundations of trust','Mathematical foundations of cryptography'
'a timed extension of &#960;-calculus with a transaction construct &#8211; the calculus web&#960; &#8211; is studied. the underlying model of web&#960; relies on networks of processes; time proceeds asynchronously at the network level, while it is constrained by the local urgency at the process level. namely process reductions cannot be delayed to favour idle steps. the extensional model &#8211; the timed bisimilarity &#8211; copes with time and asynchrony in a different way with respect to previous proposals. in particular, the discriminating power of timed bisimilarity is weaker when local urgency is dropped. a labelled characterization of timed bisimilarity is also discussed.foundations of web transactions','Mathematical foundations of cryptography'
'this paper provides a logical framework for negotiation between agents that are assumed to be rational, cooperative and truthful. we present a characterisation of the permissible outcomes of a process of negotiation in terms of a set of rationality postulates, as well as a method for constructing exactly the rational outcomes. the framework is extended by describing two modes of negotiation from which an outcome can be reached. in the concessionary mode, agents are required to weaken their demands in order to accommodate the demands of others. in the adaptationist mode, agents are required to adapt to the demands of others in some appropriate fashion. both concession and adaptation are characterised in terms of rationality postulates. we also provide methods for constructing exactly the rational concessions, as well as the rational adaptations. the central result of the paper is the observation that the outcomes obtained from the concessionary and adaptationist modes both correspond to the rational outcomes. we conclude by pointing out the links between negotiation and agm belief change, and providing a glimpse of how this may be used to define a notion of preference-based negotiation.logical foundations of negotiation','Mathematical foundations of cryptography'
'on the foundations of modern cryptography','Mathematical foundations of cryptography'
'overview of foundations','Mathematical foundations of cryptography'
'review of foundations of logic and mathematics applications to computer science and cryptography by yves nievergelt','Mathematical foundations of cryptography'
'review of introduction to cryptography with mathematical foundations and computer implementation by alexander stanoyevitch','Mathematical foundations of cryptography'
'semantic foundations of programming','Mathematical foundations of cryptography'
'some remarks on the foundations of numerical analysis','Mathematical foundations of cryptography'
'in this paper we examine the mechanism of delp (defeasible logic programming). we first study the definition of the defeating relation in a formal setting that allows us to uncover some hidden assumptions, and suggest an alternative definition. then we introduce a game-theoretic characterization of the system. we obtain a new set of truth values arising from games in which arguments for and against a given literal are played out. we study how additional constraints define protocols of admissible attacks. the delp protocol ensures the finiteness of the games, and therefore the existence of winning strategies for the corresponding games. the defeating relation among arguments determines the strategies that will win and consequently the truth values of queries. we find that the delp protocol also excludes the warranting of a literal and its negation.the foundations of delp','Mathematical foundations of cryptography'
'ultimate cryptography','Mathematical foundations of cryptography'
'univalent foundations of mathematics','Mathematical foundations of cryptography'
'security in wireless networks is a notorious problem, suffering from the following dilemma: on the one hand, wireless medium access puts the attacker into a much better position, on the other hand wireless devices most often have resource deficiencies (processing, memory, energy) which make conventional attack countermeasures based on cryptographic solutions impractical. though, there have been many approaches towards lightweight security for wireless networks, this dilemma always persists, at least for the low-cost sector, say, for example, sensor networks or rfids. so far the overwhelming majority of wireless security approaches followed a conventional security paradigm which abstracts the physical communication as a logical channel. we depart from this paradigm, and try to leverage from the physical characteristics of wireless communications as much as possible, thus bringing us again in equality of arms with the attacker. this we coined the security by wireless principle. in the talk, several incarnations of the security by wireless principle are presented. these are taken from wlan as well as wireless sensor network scenarios and show for different security goals that security by wireless designs can lead to interesting security solutions.security by wireless','Mobile and wireless security'
'we propose three theories, which can be regarded as attempts to characterize and establish guaranteed properties of wireless networks: (i) how and to what extent can we deliver packets with hard delay bounds? (ii) how and to what extent can we synchronize clocks in wireless networks? (iii) can we develop provably secure protocols for the entire life-cycle of wireless networks that also optimize a utility measure while operating in a hostile environment? for the first problem, consider an access point serving several clients over unreliable wireless links. suppose packets arrive for/from the clients, with each packet having a hard deadline, after which it is dropped. we characterize precisely the mix of delivery ratios, channel unreliabilities and hard deadline that the access point can guarantee, under some models. for the second problem, consider a wireless network where clocks at the nodes are linear, though with different rates (skews) and offsets. nodes can exchange packets with their neighbors, with direction dependent delays. we characterize precisely to what extent clocks can and cannot be synchronized and delays determined. under a random model the end-to-end error can be kept bounded irrespective of network size. concerning the third problem, traditionally, wireless protocols have been developed to provide performance. as attacks are identified, the protocols are fortified against the identified vulnerabilities. however, holistic guarantees are not provided against other attacks. we seek to reverse this paradigm. we propose a provable approach that guarantees the protocol suite is secure when the nodes are subject to certain assumptions. the protocols take a set of good nodes mingled with unknown malicious nodes from primordial birth to an operating network, while attaining min-max of a utility function. the maximization is over protocols announced and followed by the good nodes, and the minimization is over all behaviors of the malicious nodes. further, the malicious nodes are reduced to either cooperating or jamming. [joint work with vivek borkar, nikolaos freris, scott graham, i-hong hou, yih-chun hu, jonathan ponniah and roberto solis].three theories for delays, clocks and security in wireless networks','Mobile and wireless security'
'network security problems emerge in an endless stream and cause the inestimable damage. to solve network security problems efficiently, it is not enough to make good protection at nodes or protect networks from outside attacks. many network security problems should be solved efficiently in collaborative approaches which can integrate various resources over internet to defense network security. in this paper, we have designed and implemented a collaborative network security platform based on p2p system. the nodes participated in the p2p system can publish their designed defensible services against network security problems. based on the published services, collaborative network applications can be developed easily to solve the network security problems on demand. an experiment against tcp syn flooding attack is demonstrated by the designed defensible services including packets sniffing, forwarding, filtering, and logging services, which can trace the attack origins and filter malicious traffic efficiently.a collaborative network security platform in p2p networks','Mobile platform security'
'effectively responding to a security incident that takes place inside a city, whether that is a routine road accident or a terrorist attack, is a complex and challenging task. security forces and administrative bodies have to cooperate in order to share information, allocate resources and create the optimal response. in this paper we present a distributed simulation framework for studying security operations which take place inside an urban environment. more specifically, we model a cyber-physical system that aims at engaging malicious entities present inside a city. our model includes the physical urban area, the participating actors and the information exchange among the participants. this design allows the evaluation of the effect of different communication patterns on the outcome of an operation. finally, our platform allows us to investigate realistic scenarios with multiple participating actors and also gives us the opportunity to evaluate how different actor roles and responsibilities affect the overall operation outcome.a distributed simulation platform for urban security','Mobile platform security'
'a generic security platform for workstations','Mobile platform security'
'a java-based distributed platform for multilateral security','Mobile platform security'
'we develop a loss model for multi-hop wireless networks based on ieee 802.11 mac. given a multi-hop network topology, connection demands and routes, we model the working of 802.11 mac in dcf mode to find good approximations to average mac layer losses, service times and carried load. the model is defined as an implicit function amongst the variables in the model and solved using a fixed point approach. further, using automatic differentiation (ad) on the implicit function, we perform sensitivity analysis and use it in an optimization framework. as an illustration of how this model can help in design and optimization of wireless networks, we optimize the network throughput by appropriate load splitting along multiple paths. we validate our models using network simulations.a model based platform for design and optimization of multi-hop 802.11 wireless networks','Mobile platform security'
'this paper presents the design, implementation, and evaluation of the rfid guardian, the first-ever unified platform for rfid security and privacy administration. the rfid guardian resembles an \"rfid firewall,\" that monitors and controls access to rfid tags by combining a standard-issue rfid reader with unique rfid tag emulation capabilities. our system provides a platform for both automated and coordinated usage of rfid security mechanisms, offering fine-grained control over rfid-based auditing, key management, access control, and authentication capabilities. we have prototyped the rfid guardian using off-the-shelf components, and our experience has shown that active mobile devices are a valuable tool for managing the security of rfid tags in a variety of applications, including protecting low-cost tags that are unable to regulate their own usage. more philosophically, rfid technology vividly illustrates the difficulties of security administration in a world of increasingly pervasive, decentralized, low-cost, and low-power computing. our paper thus also offers a glimpse of what system administration may be like in the future, when laymen face the responsibility to manage systems of tiny computers that they are barely aware of.a platform for rfid security and privacy administration','Mobile platform security'
'saas platform model as a new innovative software application model in today\'s e-commerce for smes occupied a dominant position. saas platform model revolutionized the delivery of application software and operating mode, reducing the costs of small and medium enterprises to buy, build and maintenance, but there are a series of information security risks, such as security, reliability, and so on. most previous studies tended to explore the application of the saas model, while information security issues of the sme saas platform model are relatively unconcerned. in this study, the authors analyze the sme needs for saas platform model, point out the information security risks of sme platform model and propose specific preventive measures for information security risks.a research for sme saas platform model of information security','Mobile platform security'
'to meet the requirements in developing distributed simulation technology, a grid-based distributed simulation platform (gdsp), which was based on latest grid technology and high level architecture (hla), was proposed. gdsp is the base infrastructure of service-oriented simulation support environment. and it can run simulations on wide area network efficiently, realize share and reuse of simulation resources, and improve load balancing capability of the system. gdsp introduces an information assurance challenge and details of a simulation must be guarded from unauthorized access. however, hla doesn&#8217;t provide any security mechanism. so a security policy and a corresponding security architecture for gdsp were developed and discussed in detail. the security architecture uses proxy and multiple certifications mechanisms to provide an authentication and access control infrastructure that allows simulations based on gdsp to operate securely in heterogeneous environments. at last, the performance impact of the security architecture on gdsp was tested and the result showed the impact was little.a security architecture for grid-based distributed simulation platform','Mobile platform security'
'an authentication system based on trusted platform is presented in distributed network, network nodes are distributed in each domain, the domain authentication server based-tpm granted a certificate to each trusted member of domain and authorize trust token to access network source in different domain, in this paper, many security technologies are applied, member with trust token can span domain boundaries, transmit security message and access resource, authentication can prevent message from tampering and counterfeiting based on dolve-yao attack, and shorten the response time of authentication.a security authentication system based on trusted platform','Mobile platform security'
'the paper introduces a data security assurance mechanism, which is already applied in a security data exchange platform (sdep). then, the paper describes the security design of sdep in details. by using the security solution, information sender and receiver can build a security communication channel, and transfer lots of private files.a security design of data exchange platform','Mobile platform security'
'this paper describes the security framework that is to be developed for the generic grid platform created for the project gredia. this platform is composed of several components that need to be secured. the platform uses the ogsa standards, so that the security framework will follow gsi, the portion of globus that implements security. thus, we will show the security features that gsi already provides and we will outline which others need to be created or enhanced.a security framework for a workflow-based grid development platform','Mobile platform security'
'remote platform integrity attestation is a method by which a client attests its hardware and software configuration to a remote server. the goal of remote integrity platform attestation is to enable a remote challenger to determine the level of trust in the integrity of another attestator\'s platform configuration. however, the existing integrity reporting protocol is vulnerable to man-in-the-middle(mitm) attacks. in this paper, we describe this kind of attacks against protocols for remote platform integrity attestation, and propose a security-enhanced remote platform integrity attestation scheme for preventing man-in-the-middle attacks.a security-enhanced remote platform integrity attestation scheme','Mobile platform security'
'mobile network operators aims are offering new services to be competitive in the telecommunications market. however, the migration cost towards the third generation (3g) network is so high. this paper presents a proposal to offer a next generation services platform based on session initiation protocol (sip). this solution is easy to implement and its cost is relatively low. our solution takes into account the ip multimedia subssytem (ims) service architecture specification proposed by the third generation partnership project (3gpp) providing ubiquitous characteristics for mobile users. in this way, our proposal becomes as a tool to allow an easy transition from 2.5 to 3rd generation networks.a sip based next generation services platform','Mobile platform security'
'intel has recently produced several new capabilities to enhance security on the platform that have been released or will be released in the near future. in this presentation i will give a review of these capabilities and discuss their benefit to the security of the platform.a vision for platform security','Mobile platform security'
'this paper describes a novel android-based opportunistic platform for mobile computing applications. it has the aim to incentive the growth of practical experiences that should give an answer to the following question: can opportunistic networks actually compete with cellular networks to support urban-wide mobile computing applications?an opportunistic platform for android-based mobile devices','Mobile platform security'
'this paper discusses the use of cyber security exercises and competitions to produce data valuable for security research. cyber security exercises and competitions are primarily arranged to train participants and/or to offer competence contests for those with a profound interest in security. this paper discusses how exercises and competitions can be used as a basis for experimentation in the security field. the conjecture is that (1) they make it possible to control a number of variables of relevance to security and (2) the results can be used to study several topics in the security field in a meaningful way. among other things, they can be used to validate security metrics and to assess the impact of different protective measures on the security of a system.cyber security exercises and competitions as a platform for cyber security experiments','Mobile platform security'
'design of a security platform for corba based application','Mobile platform security'
'wireless sensor networks are rapidly emerging as an important area in ubiquitous computing industry. they are very tiny devices with limited energy, memory, transmission range, and computation power. so, wireless sensor networks are vulnerable to security attack such as sinkhole attack, wormhole attack, and simple power exhaustion attack.however, wireless sensor networks must be secured from obstructing the delivery of correct sensor data and from forging sensor data. the security and privacy problem related to wireless sensor network&#8217;s application has become a serious issue in the real service environment.in this paper, we propose our management platform and security framework for wireless sensor networks. the proposed framework has advantages as regards secure association and intrusion detection. furthermore, the proposed mechanism can be applied to ubiquitous application such as u-city, u-healthcare, u-defense as a secure wireless sensor network management platform.design of network management platform and security framework for wsn','Mobile platform security'
'in recent years, the home automation has seen a rapid introduction of network enabled digital technology. home automation offers new opportunities to increase the connectivity of devices within the home for the purpose of home safety. home automation has been a trend linking to a wide range of home appliances through the local area network(lan) and also saves energy, improving the quality of life. in this paper, we use zigbee based home automation system and wi-fi network are inregrated through a common arm board and personal computer.development and validation on integrated dynamic security monitoring platform','Mobile platform security'
'binary attestation in trusted computing platforms provide the ability to reason about the state of a system using hash measurements. property based attestation, an extension of binary attestation enables more meaningful attestation by abstracting low level binary values to high level security properties or functions of systems. in this paper, we propose tesm: a trust enhanced secure model for trusted computing platforms. we argue that given the nature of both binary and property based attestation mechanisms, an attestation requester cannot be absolutely certain if an attesting platform will behave as it is expected to behave. tesm uses a hybrid trust model based on subjective logic to combine \'hard\' trust from measurements and properties and \'soft\' trust from past experiences and recommendations to reduce such uncertainties. we believe that such a model will enable better reasoning about the trustworthiness of attesting platforms and thereby facilitate better security decision making.dynamic trust enhanced security model for trusted platform based services','Mobile platform security'
'agent mobility requires additional security standards. while the theoretical aspects of mobile agent security have been widely studied, there are few studies about the security standards of current agent platforms. in this paper, test cases are proposed to assess agent platform security. these tests focus on malicious agents trying to attack other agents or the agency. currently, they have been carried out for two agent platforms: jade and semoa. these tests show which of the known theoretical security problems are relevant in practice. furthermore, they reveal how these problems were addressed by the respective platform and what security flaws are present.evaluating mobile agent platform security','Mobile platform security'
'evros is an always-available, secure service-delivery platformfor enterprise remote access and device management. evros satisfiesthe broad necessities of the key players in today\'sremote-access/device-management space: the information technology(it) administrator, who needs an affordable means to minimizethreat reaction latencies and continuously monitor the health ofoff-site laptops; the roaming employee, who needs a streamlinednetwork access experience with conflict-free automatic backups andsoftware updates; and the broadband wireless access serviceprovider, who needs compelling applications to expand thesubscription base and increase network utilization during off-peakhours. the platform is built on three components, the evros card,the evros gateway, and the evros management server. the evros cardis a laptop personal computer (pc) card with an embedded wirelesswide area network (wwan) modem, processor, battery, and nonvolatilememory. it independently establishes the internet protocol security(ipsec) tunnel that secures the remote access connection, and itswwan interface makes the laptop reachable by the it organizationvirtually anytime and anywhere. the evros gateway is an enhancedsecure access server that deploys within the network perimeter. ithosts the enterprise end points of the ipsec tunnels and exploitsthe extended reachability of the laptop to improve theeffectiveness of all remote management functions. the evrosmanagement server is a software suite for the management of allevros components. &#169; 2007 alcatel-lucent.evros: a service-delivery platform for extending security coverage and it reach','Mobile platform security'
'sora is a fully programmable, high performance software radio platform based on commodity general-purpose pc. in this demonstration, we illustrate the main features of the sora platform that provide researchers flexible and powerful means to conduct wireless experiments at different levels with various goals. specifically, the demonstrator will show four useful applications for wireless research that are built based on the sora platform: 1) a capture tool that allows one to take a snapshot on a wireless channel; 2) a signal generation tool that allows one to transmit arbitrary baseband wave-form over the air, from a monophonic tone to a complex modulated frame; 3) an on-line real-time receiving application that uses the sora user-mode extension; and 4) a fully featured software radio wifi driver (softwifi) that can seamlessly inter-operate with commercial wifi cards.experimenting software radio with the sora platform','Mobile platform security'
'we describe the key ideas behind our implementation of distributed beamforming on a gnu-radio based software-defined radio platform. distributed beamforming is a cooperative transmission scheme whereby a number of nodes in a wireless network organize themselves into a virtual antenna array and focus their transmission in the direction of the intended receiver, potentially achieving orders of magnitude improvements in energy efficiency. this technique has been extensively studied over the past decade and its practical feasibility has been demonstrated in multiple experimental prototypes. our contributions in the work reported in this paper are three-fold: (a) the first ever all-wireless implementation of distributed beamforming without any secondary wired channels for clock distribution or channel feedback, (b) a novel digital baseband approach to synchronization of high frequency rf signals that requires no hardware modifications, and (c) an implementation of distributed beamforming on a standard, open platform that allows easy reuse and extension. we describe the design of our system in detail, present some initial results and discuss future directions for this work.fully wireless implementation of distributed beamforming on a software-defined radio platform','Mobile platform security'
'in this talk, eric blossom, founder and maintainer of the gnu radio project, will present an overview of the current capabilities and applications of gnu radio. in addition, he will discuss future plans relevant to networking as well as the port of gnu radio to the cell broadband engine. 30 to 200 gflops makes new things possible!gnu radio as an experimental platform','Mobile platform security'
'secure multicast applications of multimedia contents, such as internet tv, pay per view, satellite tv, etc., need to maintain a high number of keys. in these applications, a user contracts a group of channels or even specific content (films, sports, etc.) which do not have to coincide with the services contracted by other users, so different keys are needed to encrypt the contents. these keys must be recalculated, encrypted and redistributed when a user joins or unjoins a specific group in order to prevent users who do not belong to a group from being able to access the contents. original algorithms generate only one group key for all users, so this key must be recalculated and resent when a user joins or unjoins in the user group. this is an important problem, because a group key could be changed even when one content is performing. this paper presents a high performance implementation of one of the most employed algorithms of group key maintenance, the lkh algorithm, using reconfigurable hardware and a very high and realistic number of users (8,388,609). the performance obtained by this study improves a lot other results found in the literature in terms of both performance and number of users.hardware security platform for multicast communications','Mobile platform security'
'as security is becoming a significant obstacle to wider deployment of wireless lans, ieee 802.1x and eapow has been put forward to enhance authentication in wlan. at present, some existing client solution such as xsupplicant or wire1x has only applies to one platform, linux or windows, and is ill-considered of eapow. this paper discusses an integrated multi-platform eapow-based wlan aaa solution -- imewas, its architecture and module design and implementation. the integrated flexible and secure solution has been proved feasible.imewas--a integrated multi-platform eapow-based wlan aaa solution','Mobile platform security'
'first page of the articleinside java 2 platform security','Mobile platform security'
'the purpose of this demo is to introduce a new low-power wireless device for wireless sensor networks (wsn) operating in the ism 2.4ghz band: the magonode. thanks to a highly efficient rf front-end, that extends the radio range and increases link reliability, the magonode features out-standing rf performance, still containing energy consumption. the platform has been designed at the department of computer, control, and management engineering antonio ruberti of the university of rome la sapienza in collaboration with the spin-off wsense [1].introducing the magonode platform','Mobile platform security'
'is cross-platform security possible?','Mobile platform security'
'there are some security threats when mobile agent codes are loaded or running on destination host platforms. those threats are detecting, draining or altering the agent&#39;s intention. in order to protect the code from these attacks, we discuss some measures to assure the original code security with agent code obfuscation, encryption, self-defining classloaders and rebuilding the jvm system classloader in this paper.mobile code security on destination platform','Mobile platform security'
'network security involves the security of data transmission, the confidentiality of communications, the legitimacy of key management; it is a comprehensive application of information security technology. shopping on the internet in the business community, online transactions in the financial community have become the future trend of development, and information security plays a very important role. the use of network information security is closely related to the field of e-commerce applications. information security issue is resolved the better, the application of e-commerce becomes more widely. an urgent need to request a secure communications system ensures that the online exchange of information security with the current state of development of the issue a preliminary study. ict in the process of resolving the security issue, the use of visual c ++ language design security platform, applies the information security technology to the theoretical knowledge in practice.network security platform design based on www information system','Mobile platform security'
'the java card&#8482; 2.1.1 runtime environment (jcre) specification [1] describes a secure virtual machine environment for smart cards that facilitates the post-issuance loading and installation of applets, via an optional \"installer\". the open platform (op) card specification [2] provides a robust specification for that installer. it identifies the on-card security features necessary to safeguard the various actors that are involved in a smart card system, including card issuers, application providers as well as cardholders. such is the nature of information security these days it is necessary to demonstrate the trustworthiness of the op approach. the common criteria (iso 15408:1999) [3] presents an obvious course of action. a \"protection profile\", termed op3 [4] has therefore been produced in order to ensure the benefit of common criteria evaluation of the op installer, and by virtue of specifying the security requirements of the underlying operating system and integrated circuitry, of java card&#8482; and the chip-card platform itself. evaluation will demonstrate that the op security requirements are correctly implemented and cannot be bypassed, deactivated, corrupted or otherwise circumvented - at least to a given level of confidence (an eal in common criteria terms). this is an amazingly useful first step. however, there are important off-card assets that the smart card does not protect. common criteria evaluation does nothing to mitigate the risks to those assets. a common criteria evaluation will make assumptions about the environment of the target of evaluation. evaluation does nothing to validate those assumptions. the assumptions usually concern the compromise of security data held off-card. it therefore makes little sense to rely just on the cc evaluation of just the smart card in order to establish and maintain the security of the overall system. other steps are necessary. the paper describes what is being done to progress the common criteria evaluation of op and what else is necessary to ensure confidence in the security of the overall system. researches indicate that common criteria evaluation at a modest level of evaluation (e.g. eal 4) together with an \"information security management system\" (isms), as specified in bs 7799:1999 part 2 [5] -particularly to address the off-card security issues-reduces the need for smart card evaluation at higher eals.open platform security','Mobile platform security'
'a platform independent security architecture for use in multi-processor system-on-chip integrated circuits, primarily oriented for mobile phones and handheld devices, is presented. the suggested architecture provides an enhanced security protection scheme for use in smartphones, pda\'s, as well as other similar systems. sensitive data storage facilities, cryptographic engines, and physical protection mechanisms such as debug port access control are presented and described in detail. the proposed secure architecture has been incorporated as a part of an application processor which in turn is embedded in a number of smartphone and handheld products. system operation is discussed and hardware realization and experimental results of the proposed architecture are described.platform independent overall security architecture in multi-processor system-on-chip integrated circuits for use in mobile phones and handheld devices','Mobile platform security'
'we propose a set of quantitative metrics to empirically evaluate security quality levels on an open (software) engineering service bus (engsb) platform.quantitative software security measurement in an engineering service bus platform','Mobile platform security'
'as a kind of new technology, cloud computing has become a study highlight in both industrial field and academic field. as the users have to save their data in the cloud to use cloud computing, the security of cloud could affect users\' trust in cloud computing service directly. the thesis discusses the architecture and tier of cloud computing, analyzes the basic system structure of cloud computing platform, designs a security framework of cloud computing platform, describes the general process of such framework, and discusses the principal functions of the key modules.research on cloud computing security platform','Mobile platform security'
'mobile network operators are adapting femtocells in order to simplify their network architecture for increased coverage, performance, and greater revenue opportunities. while emerging as a new low-cost technology which assures best connectivity, it has also introduced a range of new potential security risks for the mobile network operators. in this paper, we analyze these security issues and demonstrate the weaknesses of femtocell security. we demonstrate several security flaws that allowing attackers to gain root access and to install malicious applications on the femtocell. furthermore, we experimentally evaluate and show a wide range of possible threats to femtocell; including compromise of femtocell credentials; physical, configuration, and protocol attacks; user data and identity privacy attacks. the vulnerabilities we found suggest that commercial-available femtocells fail to fulfill 3gpp security requirements and could expose operator network elements to the attacker. our findings and successful attacks exhibit the need for further research to bridge the gap between theoretical and practical security of femtocell devices.security analysis of a femtocell device','Mobile platform security'
'security in wireless networks is a notorious problem, suffering from the following dilemma: on the one hand, wireless medium access puts the attacker into a much better position, on the other hand wireless devices most often have resource deficiencies (processing, memory, energy) which make conventional attack countermeasures based on cryptographic solutions impractical. though, there have been many approaches towards lightweight security for wireless networks, this dilemma always persists, at least for the low-cost sector, say, for example, sensor networks or rfids. so far the overwhelming majority of wireless security approaches followed a conventional security paradigm which abstracts the physical communication as a logical channel. we depart from this paradigm, and try to leverage from the physical characteristics of wireless communications as much as possible, thus bringing us again in equality of arms with the attacker. this we coined the security by wireless principle. in the talk, several incarnations of the security by wireless principle are presented. these are taken from wlan as well as wireless sensor network scenarios and show for different security goals that security by wireless designs can lead to interesting security solutions.security by wireless','Mobile platform security'
'synchronous e-training is emerging as an alternative for developing human resources training plans in large organizations. real-time communications are used to emulate face-to-face interaction that occurs in on-campus learning environments. however, the security concerns that a synchronous e-training platform must face may compromise the integrity, availability and confidentiality of corporate information, which may lead to serious economic and legal consequences. the disclosure of corporate information or the unauthorized participation in e-training activities must be prevented. in this paper, the security issues in synchronous e-training are identified, and the threats to a real e-training platform are analyzed. the platform is organized into four virtual networks with different security requirements and vulnerabilities. the platform assumes that multicast communications are available in the underlying corporate network. the threats affecting each element of the platform and their impact on e-training activities are discussed. finally, a security scheme is proposed fixing the aforementioned vulnerabilities. digital certificates and encryption algorithms solve most of the vulnerabilities, but other techniques such as access control lists and user skills on security basics are essential. most of the proposed scheme is applicable to other real-time communication systems, since the e-training platform is built using standard technologies commonly used in voice over ip systems.security issues in a synchronous e-training platform','Mobile platform security'
'security watch','Mobile platform security'
'over the last few years, the success of gps-enabled pdas has finally instigated a breakthrough of mobile devices. many people now already have a device that can connect to the internet and run untrusted code, typically a cell-phone or pda. having such a large interconnected and powerful computing base presents some new security issues. in order to counter new threats, the traditional security architectures need to be overhauled to support a new and more flexible way of securely executing mobile code. this article describes the concept of security-by-contract (sxc) and its implementation on the .net platform. this new model allows users to guarantee that an untrusted application remains within the boundaries of acceptable behavior, as defined by the user herself. a number of different techniques will be presented that can be employed to enforce this behavior. in order to support the sxc paradigm, some new steps can be introduced in the application development process. in addition to building an application, developers can create an application contract and securely bind this contract to the application. the application deployment process supports legacy applications developed without such contracts, but it can support more advanced enforcement technologies for those applications that are sxc-aware.security-by-contract on the .net platform','Mobile platform security'
'this paper describes a small-scale software radio platform that has been developed and implemented to investigate the new mobile network concept of network-assisted device-to-device (na-d2d) communications. the implementation includes mechanisms for mode selection, direct-link quality estimation, and resource allocation. we show that significant benefits were reaped from this real-world implementation despite the relatively simplistic scenario emulated by our indoor laboratory setup. apart from the obvious advantages of testing the concept over a real wireless medium with its associated physical characteristics, other unforeseen factors were also uncovered. asymmetries in radio channels, variations in supposedly identical node hardware, and timing issues, were just some of the experiences encountered and fed back into the research design process which resulted in significant refinements and improvements made to the mechanisms and algorithms of the na-d2d communications concept studied.software radio platform for network-assisted device-to-device (na-d2d) concepts','Mobile platform security'
'home security systems have existed for many years. there are security devices such as digital cameras or sensors deployed in the home or office. we present an osgi based security architecture for a home security system. this system consists of service location protocol (slp), session initiation protocol (sip) and open service gateway initiative (osgi) to achieve automated services, device discovery and management. we also implement security programs in the osgi platform for detecting home environment intrusion. if intrusion occurs, the security programs will acquire the intruder automatically. the camera system will capture the intrusion and inform the user through remote control. we also describe our prototype implementation.supporting home security system in osgi platform','Mobile platform security'
'scalable trusted computing seeks to apply and extend the fundamental technologies of trusted computing to large-scale systems. to provide the functionality demanded by users, bootstrapping a trusted platform is but the first of many steps in a complex, evolving mesh of components. the bigger picture involves building up many additional layers to allow computing and communication across large-scale systems, while delivering a system retaining some hint of the original trust goal. not to be lost in the shuffle is the most important element: the system\'s human users. unlike 40 years ago, they cannot all be assumed to be computer experts, under the employ of government agencies which provide rigorous and regular training, always on tightly controlled hardware and software platforms. it seems obvious that the design of scalable trusted computing systems necessarily must involve, as an immutable design constraint, realistic expectations of the actions and capabilities of normal human users. experience shows otherwise. the security community does not have a strong track record of learning from user studies, nor of acknowledging that it is generally impossible to predict the actions of ordinary users other than by observing (e.g., through user experience studies) the actions such users actually take in the precise target conditions. we assert that because the design of scalable trusted computing systems spans the full spectrum from hardware to software to human users, experts in all these areas are essential to the end-goal of scalable trusted computing.system security, platform security and usability','Mobile platform security'
'the security of mobile operating systems becomes more and more important with the increasing number and increasing use of mobile devices. with the advances in operating systems security new concepts are introduced for increasing the security of current mobile operating systems. this paper introduces a testing system for the platform security architecture (psa) of the mobile operating system symbian os. the overall design of our testing system strives for avoiding redundancy in the testcase specification as a central goal. we present design and implementation of selected testing areas and some issues that we found with the psa implementation on current mobile devices.testing the symbian os platform security architecture','Mobile platform security'
'the rapid development of mobile platform leads to growing demand for network communications, which suffer from increasingly severe security threats as well. considering the urgent security demand and the limitation of battery capacity, the power consumption of the encryption algorithms plays a pivotal role for mobile device. aiming at the above problem, this paper proposes a performance evaluation system on android platform, which can evaluate the state parameters of device battery. our work provides elaborate analysis and exploration of 10 security algorithms\' energy consumption through extensive simulation results. with the popularity of mobile network and the rapid increase of related applications, it is significant to reduce the power consumption of security encryption algorithms on mobile devices in both academic field and industry.the evaluation of security algorithms on mobile platform','Mobile platform security'
'the network can be a powerful platform at the core of an advanced mobility security architecture. there are several unique benefits of using the network to provide security. virtually all traffic - good and bad - traverses the network. also, the network sees traffic from many places and can correlate data to find problems. the network has unlimited battery and processing power, is independent of end devices, and cannot easily be circumvented (as can software on devices). at&#38;t is investing significant research resources in order to realize the vision of the network as a mobility security platform. project marconi is instrumenting the mobility network to be able to detect and act upon malicious traffic. project saturn / smart mobile computing will provide a more secure environment for mobile devices that today can bypass the protection of a security perimeter. and, a new host-assisted, network-based architecture will enable fine grained detection, mitigation, and recovery on mobile devices. current research challenges include determining the theoretical subset of attacks that can be detected in the network, and defining algorithms to do this at an extreme scale in near real time.the network as a mobility security platform','Mobile platform security'
'this paper presents a pilot study on the development of a reconfigurable computing platform for use in prototyping software defined radio (sdr) applications and building technical knowledge in this specialist area. sdr is becoming an increasingly popular approach for building experimental radio and radar systems, giving researchers significant flexibility in choosing bandwidths, modulation and other operational parameters traditionally fixed by front-end hardware. the sdr approach involves constructing and testing radio applications on reusable platforms, thereby reducing costs and time spent changing physical layer hardware. this paper discusses use of the reconfigurable hardware interface for compting and radio (rhino) platform as a hardware platform for novice engineers to develop sdr skills and to prototype radio systems.the rhino platform','Mobile platform security'
'we present some issues relevant to the design of a secure platform for distributed mobile computing, that goes beyond existing ad-hoc approaches to software mobility. this platform aims to support wide-area computing applications such as active network infrastructures or network supervision tools. our contribution is two-fold: the first part of the paper is a survey of the security features of a few languages and virtual machines as regards authentication, access control, and communications security. we then discuss a possible architecture for a secure virtual machine for distributed mobile computing based on interesting features found in the implementations studied.towards a secure platform for distributed mobile object computing','Mobile platform security'
'the purpose of this paper is to present a modified scheduling algorithm, called traffic-oriented scheduling algorithm for aerial platform based routing and mac (aprmac) protocol for wireless sensor networks (wsns) to maximise throughput. this algorithm keeps the characteristics of aprmac, which is energy efficient, contention free and inherent routes establishment. the advantages of aprmac and the novel scheduling algorithm are analysed. a simulation is carried out for both crossing chain and random topologies to validate the scheme and for comparison of performance with the original delay-oriented aprmac and a well-known schedule-based mac protocol, trama. the results show that aprmac offers higher energy efficiency against trama. the results also show that the traffic-oriented scheduling algorithm can provide better throughput performance then the original delay-oriented one.traffic-oriented scheduling algorithm for aerial platform based routing and mac for wsns','Mobile platform security'
'we present usc sdr, a wireless platform designed for easy-to-program, high data rate, real time wireless experimentation. the design of our platform aims at removing most of the bottlenecks encountered in the design of current software radio architectures, e.g. the requirement to program new schemes in an fpga, and the difficulty to run real-time experiments for a long time. the architecture combines generic pci fpga development boards with radio front-ends built as self-sufficient daughterboards. the daughterboards are connected to the fpgas, which in turn are plugged into the pcie slots of a general-purpose server. interestingly, the connection of the daughterboards to the fpga cards is implemented through a standard fmc (fpga mezzanine card) interface, such that the same rf front-end can be reused with future fpga generations. in this way, usc sdr is not limited to a specific fpga choice and does not require a complete re-design in order to accommodate for future more powerful fpgas. the hardware is supported by a real-time software architecture where signal processing tasks, phy and mac layer algorithms can be programmed as user-level applications. as an example, we will showcase a massive mimo testbed built using a single server with multiple pcie slots.usc sdr, an easy-to-program, high data rate, real time software radio platform','Mobile platform security'
'heterogeneity is a given these days in almost any it system installation. how can developers ensure that required security properties can be correctly implemented and deployed across all the platforms on which an application such as a web service must run?why cross-platform security','Mobile platform security'
'wireless security requires slightly different thinking from wired security because it gives potential attackers easy transport medium access. this access significantly increases the threat that any security architecture must address. wireless networking broadcast nature makes traditional link-layer attacks readily available to anyone. wireless network security based on the ieee 802.11 standard has received a lot of negative attention, since it is coupled with several design errors and security problems. ieee 802.11 uses spread-spectrum signaling technology, which the military depends on for secure communications. newer architectures are becoming available to dramatically increase the security of 802.11-based networks.wireless security is different','Mobile platform security'
'a look back at wireless technology through the years.wlan security','Mobile platform security'
'in this paper, we propose a novel single sign-on (sso) approach based on multi-agent system (mas) and public key infrastructure (pki) authentication scheme. this allows the model to benefit from key advantages of the two schemes, i.e. the capability of the multi-agent technique and the strength of pki. in addition we also deal with the issue of agent service disruption and recovery as well as real-time client privilege management. we apply mas concept to facilitate multi-application authentication and authorization process for multiple concurrent users. depending on the type, an agent serves such various functions as client certificate validation, authorization check, access granting, administration, application delegation scheduling. pki is employed to create trust among agents. finally, we proved our idea with real implementation and testing.a robust single sign-on model based on multi-agent system and pki','Multi-factor authentication'
'the feature of the peer-to-peer system such as user anonymity, open nature makes that some peers are not responsible for their distributing inauthentic information. an effective solution is that set up the trust model in the p2p system. this paper presents a novel global trust model based on confirmation theory. the reputation of peers is calculated by c-f model according the bartering history, and then the peer decides to download the sharing file from which the peer has good reputation. analyses and simulations show that this trust model can discard the malicious peer from p2p system effectively and improve the rate of successful barter greatly.a trust model of p2p system based on confirmation theory','Multi-factor authentication'
'this paper presents an aspect-oriented approach to access control in mobile agent systems, where a multipoint security check mechanism visualizes the services of a host as individual components and access of each will require authentication. since the security check crosscuts various functional components of mobile agents and their platforms, it is naturally handled in the aspect-oriented paradigm. this approach is suitable not only for developing mobile agent systems from scratch, but also for enhancing legacy mobile agent systems. we demonstrate our approach through a case study.an aspect-oriented approach to mobile agent access control','Multi-factor authentication'
'in stream authentication protocols used for large-scale data dissemination in autonomuous systems, authentication is based on the timing of the publication of keys, and depends on trust of the receiver in the sender and belief on whether an intruder can have prior knowledge of a key before it is published by a protocol. many existing logics and approaches have successfully been applied to specify other types of authentication protocols, but most of them are not appropriate for analysing stream authentication protocols. we therefore consider a fibred modal logic that combines a belief logic with a linear-time temporal logic which can be used to analyse time-varying aspects of certain problems. with this logical system one is able to build theories of trust for analysing stream authentication protocols, which can deal with not only agent beliefs but also the timing properties of an autonomous agent-based system.analysing stream authentication protocols in autonomous agent-based systems','Multi-factor authentication'
'the contribution of this paper is a mechanism which links authentication to audit using weak identities and takes identity out of the trust management envelope. although our protocol supports weaker versions of anonymity it is still useful even if anonymity is not required, due to the ability to reduce trust assumptions which it provides. we illustrate the protocol with an example of authorization in a role based access mechanism.anonymous authentication','Multi-factor authentication'
'approximate message authentication code (amac) is a recently introduced cryptographic primitive with several applications in the areas of cryptography and coding theory. briefly speaking, amacs represent a way to provide data authentication that is tolerant to acceptable modifications of the original message. although constructs had been proposed for this primitive, no security analysis or even modeling had been done. in this paper we propose a rigorous model for the design and security analysis of amacs. we then present two amac constructions with desirable efficiency and security properties. amac is a useful primitive with several applications of different nature. a major one, that we study in this paper, is that of entity authentication via biometric techniques or passwords over noisy channels. we present a formal model for the design and analysis of biometric entity authentication schemes and show simple and natural constructions of such schemes starting from any amac.approximate message authentication and biometric entity authentication','Multi-factor authentication'
'alphanumeric authentication, by means of a secret, is not only a powerful mechanism, in theory, but prevails over all its competitors in practice. however, it is clearly inadequate in a world where increasing numbers of systems and services require people to authenticate in a shared space, while being actively observed. this new reality places pressure on a password mechanism never intended for use in such a context. asterisks may obfuscate alphanumeric characters on entry but popular systems, e.g. apple iphone and nintendo wii, regularly require users to use an on-screen keyboard for character input. this may not be a real concern within the context of secluded space but inadvertly reveals a secret within shared space. such a secret has an economic cost in terms of replacement, recall and revenue, all of which affect the financial return of the offending systems and services. in this paper, we present and evaluate a graphical authentication mechanism, tetrad, which appears to have the potential to address these specific concerns.armchair authentication','Multi-factor authentication'
'the pr&#234;t &#224; voter end-to-end verifiable voting system makes use of receipts, retained by voters, to provide individual verifiability that their vote has been recorded as cast. the paper discusses issues around the production and acceptance of receipts, and presents an alternative approach to individual verifiability based on authentication codes. these codes are constructed, in the encrypted domain, by the peered web bulletin board when the vote is cast, and provide the voter with an assurance that their vote has been properly received. the approach is designed to work in a uniform way with ranked elections and single preference elections.authentication codes','Multi-factor authentication'
'my talk is on authentication components. what i mean by an authentication component is basically a reusable building block. i&#8217;m talking about building blocks in a strictly engineering sense (there is very little novel cryptograph use involved), building blocks that are useful to system designers when they&#8217;re designing a system and need a protocol for doing something, and they don&#8217;t want to reinvent all the cryptographic stuff themselves. usually they are not experts in that either, so it&#8217;s a good thing that they don&#8217;t always reinvent things from scratch.authentication components','Multi-factor authentication'
'authentication confidences','Multi-factor authentication'
'the purpose of this paper is to illustrate the use of one-way functions and public-key algorithms in some authentication procedures. special emphasis is given to the problem of mutual authentication of a card and a point-of-sale terminal in an off-line situation.authentication procedures','Multi-factor authentication'
'authentication and secrecy have been widely investigated in security protocols. they are closely related to each other and variants of definitions have been proposed, which focus on the concepts of corresponding assertion and key distribution. this paper proposes an &lt;em&gt;on-the-fly model checking&lt;/em&gt;method based on the pushdown system to verify the authentication of recursive protocols with an unbounded number of principals. by experiments of the maude implementation, we find the recursive authentication protocol, which was verified in the sense of (weak) key distribution, has a flaw in the sense of correspondence assertion.authentication revisited','Multi-factor authentication'
'suppose a principal in a cryptographic protocol creates and transmits a message containing a new value v, which it later receives back in cryptographically altered form. it can conclude that some principal possessing the relevant key has transformed the message containing v. in some circumstances, this must be a regular participant of the protocol, not the penetrator. an inference of this kind is an authentication test. we introduce two main kinds of authentication test. an outgoing test is one in which the new value v is transmitted in encrypted form, and only a regular participant can extract it from that form. an incoming test is one in which v is received back in encrypted form, and only a regular participant can put it in that form. we combine these two tests with a supplementary idea, the unsolicited test, and a related method for checking that certain values remain secret. together, they determine what authentication properties are achieved by a wide range of cryptographic protocols. in this paper, we introduce authentication tests and illustrate their power, giving new and straightforward proofs of security goals for several protocols. we also illustrate how to use the authentication tests as a heuristic for finding attacks against incorrect protocols. finally, we suggest a protocol design process. we express these ideas in the strand space formalism [21], and prove them correct elsewhere [8].authentication tests','Multi-factor authentication'
'graphical passwords are an alternative to alphanumeric passwords in which users click on images to authenticate themselves rather than type alphanumeric strings. we have developed one such system, called passpoints, and evaluated it with human users. the results of the evaluation were promising with respect to rmemorability of the graphical password. in this study we expand our human factors testing by studying two issues: the effect of tolerance, or margin of error, in clicking on the password points and the effect of the image used in the password system. in our tolerance study, results show that accurate memory for the password is strongly reduced when using a small tolerance (10 x 10 pixels) around the user\'s password points. this may occur because users fail to encode the password points in memory in the precise manner that is necessary to remember the password over a lapse of time. in our image study we compared user performance on four everyday images. the results indicate that there were few significant differences in performance of the images. this preliminary result suggests that many images may support memorability in graphical password systems.authentication using graphical passwords','Multi-factor authentication'
authentication,'Multi-factor authentication'
'to increase the security of handheld devices, we propose awareless authentication. since insensible input prevents the leakage of the key information, it can provide more secure authentication scheme. experiments that used a pressure sensor show that users can input a preset rhythm by insensible finger motion, and the boundary between insensible and sensible is extended by adding vibration while input.awareless authentication','Multi-factor authentication'
'as mobile devices pervade physical space, the familiar authentication patterns are becoming insufficient: besides entity authentication, many applications require, e.g., location authentication. many interesting protocols have been proposed and implemented to provide such strengthened forms of authentication, but there are very few proofs that such protocols satisfy the required security properties. in some cases, the proofs can be provided in the symbolic model. more often, various physical factors invalidate the perfect cryptography assumption, and the symbolic model does not apply. in such cases, the protocol cannot be secure in an absolute logical sense, but only with a high probability. but while probabilistic reasoning is thus necessary, the analysis in the full computational model may not be warranted, since the protocol security does not depend on any computational assumptions, or on attacker\'s computational power, but only on some guessing chances. we refine the dolev-yao algebraic method for protocol analysis by a probabilistic model of guessing, needed to analyze protocols that mix weak cryptography with physical properties of nonstandard communication channels. applying this model, we provide a precise security proof for a proximity authentication protocol, due to hancke and kuhn, that uses probabilistic reasoning to achieve its goals.bayesian authentication','Multi-factor authentication'
'authentication plays a vital role in various applications. even though a number of effective methods and techniques are available, new ideas and techniques are necessary to overcome some of the limitations of these systems. at the present time, biometrics is increasingly gaining popularity in security related applications. biometrics measure individual&#8217;s unique physical or behavioral characteristic to recognize or authenticate their identity and using this technology is far better than any other technology. the word biometrics is very often used as a synonym for the perfect security as proper design and implementation of the biometric system can indeed increase the overall security. there are many biometric technologies to suit different types of applications. this paper discusses the advantages and applications of various biometric technologies.biometric authentication','Multi-factor authentication'
'concerns on widespread use of biometric authentication systems are primarily centered around template security, revocability, and privacy. the use of cryptographic primitives to bolster the authentication process can alleviate some of these concerns as shown by biometric cryptosystems. in this paper, we propose a provably secure and blind biometric authentication protocol, which addresses the concerns of user\'s privacy, template protection, and trust issues. the protocol is blind in the sense that it reveals only the identity, and no additional information about the user or the biometric to the authenticating server or vice-versa. as the protocol is based on asymmetric encryption of the biometric data, it captures the advantages of biometric authentication as well as the security of public key cryptography. the authentication protocol can run over public networks and provide nonrepudiable identity verification. the encryption also provides template protection, the ability to revoke enrolled templates, and alleviates the concerns on privacy in widespread use of biometrics. the proposed approach makes no restrictive assumptions on the biometric data and is hence applicable to multiple biometrics. such a protocol has significant advantages over existing biometric cryptosystems, which use a biometric to secure a secret key, which in turn is used for authentication. we analyze the security of the protocol under various attack scenarios. experimental results on four biometric datasets (face, iris, hand geometry, and fingerprint) show that carrying out the authentication in the encrypted domain does not affect the accuracy, while the encryption key acts as an additional layer of security.blind authentication','Multi-factor authentication'
'this paper addresses a problem that has arisen in building distributed systems in which incomplete tmst exists and program composition is necessary. the problem is to permit authentication for both access control and accounting when cascading invocations. the problem can be identified as one of providing cascaded authentication. we have developed a mechanism we call passports that are passed along with each stage of the cascade and digitally signed at each transition. the information thus signed is that which is critical to the authentication. the contributions of the work are both in recognizing the problem and in devising a solution that is efficient enough to be usable, although there will be some cost associated with such a mechanism.cascaded authentication','Multi-factor authentication'
'network behaviour analysis techniques are designed to detect intrusions and other undesirable behaviour in computer networks by analysing the traffic statistics. we present an efficient framework for integration of anomaly detection algorithms working on the identical input data. this framework is based on high&#45;speed network traffic acquisition subsystem and on trust modelling, a well&#45;established set of techniques from the multi&#45;agent system field. trust&#45;based integration of algorithms results in classification with lower error rate, especially in terms of false positives. the presented system is suitable for both online and offline processing, and introduces a relatively low computational overhead compared to deployment of isolated anomaly detection algorithms.collaborative approach to network behaviour analysis based on hardware&#45;accelerated flowmon probes','Multi-factor authentication'
commentary,'Multi-factor authentication'
'reputation mechanisms provide a promising alternative to the traditional security methods for preventing malicious behavior in online transactions. however, obtaining correct reputation information is not trivial. in the absence of objective authorities (or trusted third parties) which can oversee every transaction, mechanism designers have to ensure that it is rational for the participating parties to report the truth. in this paper we describe a complete reputation mechanism for the online hotel booking industry that is efficient (i.e. the equilibrium behavior is cooperative) and incentive compatible. our mechanism discovers the true outcome of an interaction by analyzing the two reports coming from the agents involved in the interaction. based on side payments, such a mechanism makes it profitable for long-run agents to commit to always report the truth.confess \" an incentive compatible reputation mechanism for the online hotel booking industry','Multi-factor authentication'
'confidentially using authentication','Multi-factor authentication'
'digital image manipulation software is readily accessible from the internet today, thus, digital data is very easy to be tampered without having any clue. under this circumstance, integrity verification has become an important issue in the digital world. the aim of this paper is to present an in-depth analysis on the methods of detecting image tampering. we introduce the notion of image content authentication and the features required to design an effective authentication scheme. we review some algorithms and frequently used security mechanisms. we also analyze and discuss the performance trade-offs and related security issues among existing technologies. compared with those techniques and protocols based on digital signatures or watermarking, majority of the proposed image hash methods emphasize more on content authentication rather than strict integrity.content-based image authentication','Multi-factor authentication'
'interactions between resources as well as services are one of the fundamental characteristics in the distributed multi-application environments. in such environments, attribute-based access control (abac) mechanisms are gaining in popularity while the role-based access control (rbac) mechanism is widely accepted as a general mechanism for authorization management. this paper proposes a new access control model, crbac, which aims to combine the advantages of rbac and abac, and integrates all kinds of constraints into the rbac model. unlike other work in this area, which only incorporates one or a few particular attribute constraints into rbac, this paper analyses and abstracts the generic properties of the attribute constraints imposed on authorization systems. based on these analyses and generalization, two constraints templates are presented, called authorization mapping constraint template and behaviour constraint template. the former template is able to automate the user-role and role-permission mapping, while the latter is used to restrict the behaviours of the authorization entities. the attribute constraints are classified into these two templates. moreover, the state mechanism is introduced to build up the constraints among the statuses of the entities, and reflect the outcomes of the authorization control as well. based on the presented templates and the state mechanism, the execution model is developed. a use case is proposed to show the authorization process of our proposed model. the extensive analyses are conducted to show its multi-grained constraints by comparing with other models.crbac','Multi-factor authentication'
'this paper proposes the design of multi-user authentication in the multi-application based environment and role-based access control by using pki authentication and x.509 privilege management infrastructure (pmi). a binding model of rbac authorization based on attribute certificate (ac) and public key certificate (pkc) is presented. especially, the way of attribute mapping between pkc, bridge ac, and role ac is illustrated. in addition, the activity-based policy enforcement is introduced to make the system respond to malicious activities more appropriately. at a core, the multi agent system approach is applied to automate the flexible and effective management of user authentication, role delegation as well as system accountability. finally, we reported our ongoing implementation status and demonstrated that our proposed model is a potential solution to support strong authentication and dynamic authorization in the multi-user and multi-application environment.exploiting x.509 certificate and multi-agent system architecture for role-based access control and authentication management','Multi-factor authentication'
'a new type of authentication, call group authentication, which authenticates all users belonging to the same group is proposed in this paper. the group authentication is specially designed for group-oriented applications. the group authentication is no longer a one-to-one type of authentication as most conventional user authentication schemes which have one prover and one verifier; but, it is a many-to-many type of authentication which has multiple provers and multiple verifiers. we propose a basic $(t)$-secure $(m)$-user $(n)$-group authentication scheme (($(t, m, n)$) gas), where $(t)$ is the threshold of the proposed scheme, $(m)$ is the number of users participated in the group authentication, and $(n)$ is the number of members of the group, which is based on shamir\'s ($(t, n)$) secret sharing (ss) scheme. the basic scheme can only work properly in synchronous communications. we also propose asynchronous ($(t, m, n)$) gass, one is a gas with one-time authentication and the other is a gas with multiple authentications. the ($(t, m, n)$) gas is very efficient since it is sufficient to authenticate all users at once if all users are group members; however, if there are nonmembers, it can be used as a preprocess before applying conventional user authentication to identify nonmembers.group authentication','Multi-factor authentication'
'information authentication','Multi-factor authentication'
'in this article, we discuss how to shape a mas infrastructure to support an agent-oriented, role-based access control model (rbac-mas). first, we introduce the rbac model, and show how it can be extended to capture the essential features of agent systems. then, we extrapolate the core requirements of an infrastructure for rbac-mas, and depict a possible approach based on accs (agent coordination contexts). the conceptual framework for an rbac-mas infrastructure exploiting accs is subsequently formalized through a process-algebraic description of the main infrastructure entities: this is meant to serve as a formal specification of both the infrastructure and the language for expressing roles, operations, and policies in rbac-mas.infrastructure for rbac-mas','Multi-factor authentication'
'heuristic computing has consolidated into two streams of research (personification software and smart products) [1]. cognitive science is one of these fields and is attracting research effort based on multi-agent system (mas). this research requires the formation of a voluntarily trust relationship in order for collaboration to occur, otherwise the imposed goal(s) may be aborted or fail completely [2,3]. an agent transportation layer adaption system (atlas) communications framework has been constructed to pass messages between separate agent systems. discussion about confined frameworks have recently been extended to enable individual students associated with our knowledge-based and intellingent information and engineering systems (kes) centre to fast track the development of their research concepts. a plug \'n\' play concept based on a multi-agent blackboard architecture forms the basis of this research. this paper highlights the core architecture, we believe is required for multi-agent system (mas) developers to achieve such flexibility. agent teams can provide the ability to adapt and dynamically organize. the model described, concentrates on the blackboard design constructs to represents all functional blocks required to automate the processes required to complete any decomposed goals. discussion in this paper is limited to the formative work within the foundation layers of that framework.interoperability within multi-agent system','Multi-factor authentication'
'passwords are ubiquitous, and users and service providers alike rely on them for their security. however, good passwords may sometimes be hard to remember. for years, security practitioners have battled with the dilemma of how to authenticate people who have forgotten their passwords. existing approaches suffer from high false positive and false negative rates, where the former is often due to low entropy or public availability of information, whereas the latter often is due to unclear or changing answers, or ambiguous or fault prone entry of the same. good security questions should be based on long-lived personal preferences and knowledge, and avoid publicly available information. we show that many of the questions used by online matchmaking services are suitable as security questions. we first describe a new user interface approach suitable to such security questions that is offering a reduced risks of incorrect entry. we then detail the findings of experiments aimed at quantifying the security of our proposed method.love and authentication','Multi-factor authentication'
'first page of the articlemessage authentication','Multi-factor authentication'
'to develop theories to specify and reason about various aspects of multi-agent systems, many researchers have proposed the use of modal logics such as belief logics, logics of knowledge, and logics of norms. as multi-agent systems operate in dynamic environments, there is also a need to model the evolution of multi-agent systems through time. in order to introduce a temporal dimension to a belief logic, we combine it with a linear-time temporal logic using a powerful technique called fibring for combining logics. we describe a labelled modal tableaux system for the resulting fibred belief logic (fl) which can be used to automatically verify correctness of inter-agent stream authentication protocols. with the resulting fibred belief logic and its associated modal tableaux, one is able to build theories of trust for the description of, and reasoning about, multi-agent systems operating in dynamic environments.modal tableaux for verifying stream authentication protocols','Multi-factor authentication'
'online applications have become ubiquitous and userid/password access control remains firmly in the forefront. this presentation will take a high-level look at the authentication practices used by some of the online applications encountered by typical users to illustrate the good, the bad and the ugly among current practices. the technical controls used to control access will be identified and discussed. tradeoff issues that govern which controls are used in various circumstances will also be explored.online authentication','Multi-factor authentication'
'parasitic authentication','Multi-factor authentication'
'the people who invented computer passwords obviously never met charlie fox. during my high school years, i worked saturdays and summers at hampton general store, in the small new england village of hampton, connecticut. charlie fox was the postman. for many years, every day at 3:30 pm charlie would come into the store, having finished his route. every day he took down the phone book, looked up his home telephone number, and called his wife to ask for her grocery order. he was a pleasant and perfectly normal fellow, but one who just couldn\'t remember a phone number, even his own.password-based authentication','Multi-factor authentication'
'frequently, communication between two principals reveals their identities and presence to third parties. these privacy breaches can occur even if security protocols are in use; indeed, they may even be caused by security protocols. however, with some care, security protocols can provide authentication for principals that wish to communicate while protecting them from monitoring by third parties. this paper discusses the problem of private authentication and presents two protocols for private authentication of mobile principals. in particular, our protocols allow two mobile principals to communicate when they meet at a location if they wish to do so, without the danger of tracking by third parties. the protocols do not make the (dubious) assumption that the principals share a long-term secret or that they get help from an infrastructure of ubiquitous on-line authorities.private authentication','Multi-factor authentication'
'mobile users are often faced with a trade-off between security and convenience. either users do not use any security lock and risk compromising their data, or they use security locks but then have to inconveniently authenticate every time they use the device. rather than exploring a new authentication scheme, we address the problem of deciding when to surface authentication and for which applications. we believe reducing the number of times a user is requested to authenticate lowers the barrier of entry for users who currently do not use any security. progressive authentication, the approach we propose, combines multiple signals (biometric, continuity, possession) to determine a level of confidence in a user\'s authenticity. based on this confidence level and the degree of protection the user has configured for his applications, the system determines whether access to them requires authentication. we built a prototype running on modern phones to demonstrate progressive authentication and used it in a lab study with nine users. compared to the state-of-theart, the system is able to reduce the number of required authentications by 42\% and still provide acceptable security guarantees, thus representing an attractive solution for users who do not use any security mechanism on their devices.progressive authentication','Multi-factor authentication'
'a new approach to efficient authentication of quotations from trusted sources but embedded in messages created by unknown authors is proposed. two different techniques for efficient generation of authenticable quotations using cascaded hashing are proposed for conventional quotations, and an efficient two-dimensional signature generation technique is proposed for two-dimensional quotations from spreadsheets. a technique for associating the signature with the quotation in the microsoft word document is also proposed for generation of integrated authenticable quotations that survive copy-and-paste operations.quotation authentication','Multi-factor authentication'
'many web sites embed third-party content in frames, relying on the browser\'s security policy to protect them from malicious content. frames, however, are often insufficient isolation primitives because most browsers let framed content manipulate other frames through navigation. we evaluate existing frame navigation policies and advocate a stricter policy, which we deploy in the open-source browsers. in addition to preventing undesirable interactions, the browser\'s strict isolation policy also hinders communication between cooperating frames. we analyze two techniques for inter-frame communication. the first method, fragment identifier messaging, provides confidentiality without authentication, which we repair using concepts from a well-known network protocol. the second method, postmessage, provides authentication, but we discover an attack that breaches confidentiality. we modify the postmessage api to provide confidentiality and see our modifications standardized and adopted in browser implementations.securing frame communication in browsers','Multi-factor authentication'
'the ease with which a malicious third party can obtain a user\'s password when he or she logs into internet sites (such as bank or email accounts) from an insecure computer creates a substantial security risk to private information and transactions. for example, a malicious administrator at a cybercafe, or a malicious user with sufficient access to install key loggers at a kiosk, can obtain users\' passwords easily. even when users do not trust the machines they are using, many of them are faced with the prospect of accessing their accounts with a single level of privilege. to address this problem, we propose a system based on two modes of authentication--default and restricted. users can signal to the server whether they are in an untrusted environment so that the server can log them in under restricted privileges that allow them to perform basic actions that cause no serious damage if the session or their password is compromised.twokind authentication','Multi-factor authentication'
'bpel can automate orchestrations for crossorganizational web services; however, it meets a serious challenge from modeling human-intensive business activities, especially from addressing access control for human coordination considering complex interpersonal relationship in modern business. this paper analyzes the importance of human-intensive processes and introduces several additional types of bpel constructs, then discusses rbac model extracted from bpel process, finally uses improved foaf to enhance rbac model in bpel. the goal of our work is to enhance human coordination capability in bpel-based business processes by using rbac model and improved foaf.using improved foaf to enhance bpel-extracted rbac capability','Multi-factor authentication'
'traditionally, test cases are used to check whether a system conforms to its requirements. however, to achieve good quality and coverage, large amounts of test cases are needed, and thus huge efforts have to be put into test generation and maintenance. we propose a methodology, called abstract testing, in which test cases are replaced by verification scenarios. such verification scenarios are more abstract than test cases, thus fewer of them are needed and they are easier to create and maintain. checking verification scenarios against the source code is done automatically using a software model checker. in this paper we describe the general idea of abstract testing, and demonstrate its feasibility by a case study from the automotive systems domain.abstract testing','Penetration testing'
'this article describes the development of adaptive testing in response to the ever-growing need to dynamically and cost-effectively tailor ic testing to discriminately manage manufacturing process variations. various degrees of adoption are presented, together with benefits and examples of its use. finally, challenges for future development are discussed.adaptive testing','Penetration testing'
'unit and acceptance testing are central to agile software development, but is that all there is to agile testing? we build on previous work to provide a systematic mapping of agile testing publications at major agile conferences. the analysis presented in this paper allows us to answer research questions like: what is agile testing used for, what types of studies on agile testing have been published, what problems do people have when performing agile testing, and what benefits do these publications offer? we additionally explore topics such as: who are the major authors in this field, in which countries do these authors work, what tools are mentioned, and is the field driven by academics, practitioners, or collaborations? this paper presents our analysis of these topics in order to better structure future work in the field of agile testing and to provide a better understanding of what this field actually entails.agile testing','Penetration testing'
'a configuration of unit cubes in three dimensions with integer coordinates is called an &lt;em&gt;animal&lt;/em&gt; if the boundary of their union is homeomorphic to a sphere. shermer discovered several animals from which no single cube may be removed such that the resulting configurations are also animals [14]. here we obtain a dual result: we give an example of an animal to which no cube may be added within its minimal bounding box such that the resulting configuration is also an animal. we also present two &lt;em&gt;o&lt;/em&gt;(&lt;em&gt;n&lt;/em&gt;)-time algorithms for determining whether a given configuration of &lt;em&gt;n&lt;/em&gt; unit cubes is an animal.animal testing','Penetration testing'
'abstract: we discuss how conventional testing criteria such as branch coverage can be applied to the testing of member functions inside a class. to support such testing techniques we employ symbolic execution techniques and finite state machines (fsms). symbolic execution is performed on the code of a member function to identify states that are required to fulfil a given criterion. we use fsms to generate a sequence of member functions leading to the identified states. our technique is a mixture of code-based and specification-based testing techniques in the sense that it uses information derived from codes using symbolic execution together with information from specifications using fsms for testing activities.applying conventional testing techniques for class testing','Penetration testing'
'bit testing','Penetration testing'
'causal testing','Penetration testing'
'here is a presently operational plan to improve the quality of program testing. after all programs are tested alone, an independent quality control staff uses automated tools to certify that minimum testing criteria have been met.certification testing','Penetration testing'
'competitive testing','Penetration testing'
'component testing','Penetration testing'
'most information systems log events (e.g., transaction logs, audit trails) to audit and monitor the processes they support. at the same time, many of these processes have been explicitly modeled. for example, sap r/3 logs events in transaction logs and there are epcs (event-driven process chains) describing the so-called reference models. these reference models describe how the system should be used. the coexistence of event logs and process models raises an interesting question: &#8220;does the event log conform to the process model and vice versa?&#8221;. this paper demonstrates that there is not a simple answer to this question. to tackle the problem, we distinguish two dimensions of conformance: fitness (the event log may be the result of the process modeled) and appropriateness (the model is a likely candidate from a structural and behavioral point of view). different metrics have been defined and a conformance checker has been implemented within the prom framework.conformance testing','Penetration testing'
'consistency testing','Penetration testing'
'the traditionally wired interfaces of many electronic systems are in many applications being replaced by wireless interfaces. testing of electronic systems (both integrated circuits and printed circuit boards) still requires physical electrical contact through probe needles and/or sockets. this paper addresses the state-of-the-art, options, and hurdles-still-to-take of contactless testing, which would resolve many test challenges due to shrinking size and pitch of pads and pins and inaccessibility of advanced assembly techniques as system-in-package (sip) and 3d stacked ics.contactless testing','Penetration testing'
'traditional techniques to test a software application through the application\'s graphical user interface have a number of weaknesses. manual testing is slow, expensive, and does not scale well as the size and complexity of the application increases. software test automation which exercises an application through the application\'s ui using an api set can be difficult to maintain. we propose a software testing paradigm called declarative testing. in declarative testing, a test scenario focuses on what to accomplish rather than on the imperative details of how to manipulate the state of an application under test and verify the final application state against an expected state. declarative testing is a test design paradigm which separates test automation code into conceptual answer, executor, and verifier entities. preliminary experience with declarative testing suggests that the modular characteristics of the paradigm may significantly enhance the ability of a testing effort to keep pace with the evolution of a software application during the application\'s development process.declarative testing','Penetration testing'
'this paper presents a new approach to structural testing, called dependence testing. first we propose dependence oriented coverage criteria that extend conventional data flow oriented coverage criteria with control dependence. this allows one to capture the full dependence information of a program or specification systematically. we then describe a model checking-based approach to test generation for dependence testing. it is shown that dependence oriented coverage criteria can be characterized in the temporal logics ltl and ctl. this enables one to use any ltl and ctl model checkers as test generators. finally, we show that the temporal logic-based characterization can also be used for reducing the cost of dependence testing.dependence testing','Penetration testing'
'regression testing, as it\'s commonly practiced, is unsound due to inconsistent test repair and test addition. this paper presents a new technique, differential testing, that alleviates the test repair problem and detects more changes than regression testing alone. differential testing works by creating test suites for both the original system and the modified system and contrasting both versions of the system with these two suites. differential testing is made possible by recent advances in automated unit test generation. furthermore, it makes automated test generators more useful because it abstracts away the interpretation and management of large volumes of tests by focusing on the changes between test suites. in our preliminary empirical study of 3 subjects, differential testing discovered 21\%, 34\%, and 21\% more behavior changes than regression testing alone.differential testing','Penetration testing'
'in order to evaluate the usability of emotion-libras and the potential use of website resources for people who are deaf, a pilot test was conducted with five deaf volunteer participants. pre and post-test questionnaires were given, and a participant task with an observation form was used to gather the data. this double pilot testing was satisfactory, since the research questions were answered. with respect to emotion-libras evaluation, we found that a semantic differential scale format may facilitate understanding the relation between bipolar emotions, and new emotions may be included. regarding website resources, the preference for bilingual communication (signed and written language) was evidenced, also the desire for a dictionary that translates from sign to written language in websites because of the lack of fluency in written language.double testing','Penetration testing'
'context: complexity measures provide us some information about software artifacts. a measure of the difficulty of testing a piece of code could be very useful to take control about the test phase. objective: the aim in this paper is the definition of a new measure of the difficulty for a computer to generate test cases, we call it branch coverage expectation (bce). we also analyze the most common complexity measures and the most important features of a program. with this analysis we are trying to discover whether there exists a relationship between them and the code coverage of an automatically generated test suite. method: the definition of this measure is based on a markov model of the program. this model is used not only to compute the bce, but also to provide an estimation of the number of test cases needed to reach a given coverage level in the program. in order to check our proposal, we perform a theoretical validation and we carry out an empirical validation study using 2600 test programs. results: the results show that the previously existing measures are not so useful to estimate the difficulty of testing a program, because they are not highly correlated with the code coverage. our proposed measure is much more correlated with the code coverage than the existing complexity measures. conclusion: the high correlation of our measure with the code coverage suggests that the bce measure is a very promising way of measuring the difficulty to automatically test a program. our proposed measure is useful for predicting the behavior of an automatic test case generator.estimating software testing complexity','Penetration testing'
'the paper presents a case study of applying genetic algorithms (gas) to the automatic test data generation problem. we present the basic techniques implemented in our prototype test generation system, whose goal is to get branch coverage of the program under testing. we used our tool to experiment with simple programs, programs that have been used by others for test strategies benchmarking and the unix utility uniq. the effectiveness of ga-based testing system is compared with a random testing system. we found that for simple programs both testing systems work fine, but as the complexity of the program or the complexity of input domain grows, ga-based testing system significantly outperforms random testing.evolutionary testing','Penetration testing'
'this paper describes a new way of testing reactive systems as investigated by the rate-project at the tampere university of technology. we abandon the idea of systematically using a large library of predetermined test cases and instead use a \"live\" specification to generate test runs on- the-fly, as testing progresses. in order to do this, we assume that the behavior of the implementation under test is specified as a labelled transition system. this testing method is most applicable to testing concurrent, nondeterministic, and reactive behaviors rather than data-intensive computation.exploration testing','Penetration testing'
'fair testing through probabilistic testing','Penetration testing'
'fair testing','Penetration testing'
'an approach to functional testing is described in which the design of a program is viewed as an integrated collection of functions. the selection of test data depends on the functions used in the design and on the value spaces over which the functions are defined. the basic ideas in the method were developed during the study of a collection of scientific programs containing errors. the method was the most reliable testing technique for discovering the errors. it was found to be significantly more reliable than structural testing. the two techniques are compared and their relative advantages and limitations are discussed.functional program testing','Penetration testing'
'functional testing vs. structural testing of rams','Penetration testing'
'grammar testing is discussed in the context of grammar engineering (i.e., software engineering for grammars). we propose a generalisation of the known rule coverage for grammars, that is, context-dependent branch coverage. we investigate grammar testing, especially coverage analysis, test set generation, and integration of testing and grammar transformations. grammar recovery is chosen as a subfield of grammar engineering to illustrate the developed concepts. grammar recovery is concerned with the derivation of a language\'s grammar from some available resource such as a semi-formal language reference.grammar testing','Penetration testing'
'nonadaptive group testing involves grouping arbitrary subsets of  $n$ items into different pools. each pool is then tested and defective items are identified. a fundamental question involves minimizing the number of pools required to identify at most $d$  defective items. motivated by applications in network tomography, sensor networks and infection propagation, a variation of group testing problems on graphs is formulated. unlike conventional group testing problems, each group here must conform to the constraints imposed by a graph. for instance, items can be associated with vertices and each pool is any set of nodes that must be path connected. in this paper, a test is associated with a random walk. in this context, conventional group testing corresponds to the special case of a complete graph on $n$ vertices. for interesting classes of graphs a rather surprising result is obtained, namely, that the number of tests required to identify $d$ defective items is substantially similar to what is required in conventional group testing problems, where no such constraints on pooling is imposed. specifically, if $t(n)$  corresponds to the mixing time of the graph $g$ , it is shown that with $m=o(d^2t^2(n)\\log(n/d))$ nonadaptive tests, one can identify the defective items. consequently, for the erd&#x0151;s-r&#x00e9;nyi random graph  $g(n,p)$, as well as expander graphs with constant spectral gap, it follows that $m=o(d^2\\log^3n)$ nonadaptive tests are sufficient to identify $d$ defective items. next, a specific scenario is considered that arises in network tomography, for which it is shown that  $m=o(d^3\\log^3n)$ nonadaptive tests are sufficient to identify $d$ defective items. noisy counterparts of the graph constrained group testing problem are considered, for which parallel results are developed. we also briefly discuss extensions to compressive sensing on graphs.graph-constrained group testing','Penetration testing'
'graphical user interfaces (guis), due to their event-driven nature, present an enormous and potentially unbounded way for users to interact with software. during testing, it is important to &#8220;adequately cover&#8221; this interaction space. in this paper, we develop a new family of coverage criteria for gui testing grounded in combinatorial interaction testing. the key motivation of using combinatorial techniques is that they enable us to incorporate &#8220;context&#8221; into the criteria in terms of event combinations, sequence length, and by including all possible positions for each event. our new criteria range in both efficiency (measured by the size of the test suite) and effectiveness (the ability of the test suites to detect faults). in a case study on eight applications, we automatically generate test cases and systematically explore the impact of context, as captured by our new criteria. our study shows that by increasing the event combinations tested and by controlling the relative positions of events defined by the new criteria, we can detect a large number of faults that were undetectable by earlier techniques.gui interaction testing','Penetration testing'
'this paper introduces a new paradigm for mutation testing, which we call higher order mutation testing (hom testing). traditional mutation testing considers only first order mutants, created by the injection of a single fault. often these first order mutants denote trivial faults that are easily killed. higher order mutants are created by the insertion of two or more faults. the paper introduces the concept of a subsuming hom; one that is harder to kill than the first order mutants from which it is constructed. by definition, subsuming homs denote subtle fault combinations. the paper reports the results of an empirical study of hom testing using 10 programs, including several non-trivial real-world subjects for which test suites are available.higher order mutation testing','Penetration testing'
'in this paper a formal, quantitative approach to designing optimum hough transform (ht) algorithms is proposed. this approach takes the view that a ht is a hypothesis testing method. each sample in the ht array implements a test to determine whether a curve with the given parameters fits the edge point data. this view allows the performance of ht algorithms to be quantified. the power function, which gives the probability of rejection as a function of the underlying parametric distribution of data points, is shown to be the fundamentally important characteristic of ht behaviour. attempting to make the power function narrow is a formal approach to optimizing ht performance. to illustrate how this framework is useful the particular problem of line detection is discussed in detail. it is shown that the hypothesis testing framework leads to a redefinition of the ht in which the values are a measure of the distribution of points around a curve rather than the number of points on a curve. this change dramatically improves the sensitivity of the method to small structures. the solution to many ht design problems can be posed within the framework, including optimal quantizations and optimum sampling of the parameter space. in this paper the authors consider the design of optimum i-d filters, which can be used to sharpen the peak structure in parameter space. results on several real images illustrate the improvements obtained.hypothesis testing','Penetration testing'
'iddq testing has emerged from a company-specific cmos ic test technology in the 1960\'s and 1970\'s to a worldwide-accepted technique that is a requirement for low-defect ppm levels and failure rates. it is the single most sensitive test method to detect cmos ic defects and an abundance of studies have laid a solid foundation for why this is so.the iddq test uses the quiescent power supply current of logic states as an indication of defect presence. its major requirement for maximum efficiency is that the design implement nanowatt power levels (nanoamp supply current) in the quiescent portion of the power supply current. no direct connections are allowed between vdd and vss during the quiescent period.iddq testing has increased significantly since 1990, highlighting problems and driving solutions not addressed by the high reliability manufacturers of earlier technologies. faster iddq instrumentation and better software tools to generate and grade iddq test patterns are a result of this increased interest. two major issues confronting iddq testing are addressed: the yield loss issue and increased background current of deep submicron ic technologies projected by the sia/sematech road map. both issues are points of controversy.iddq testing','Penetration testing'
'the industry has accepted iddq testing to detect cmos ic defects. while iddt testing needs more research to be applicable in practice. however, it is noticed that observing the average transient current can lead to improvements in real defect coverage. this paper presents a formal procedure to identify iddt testable faults, and to generate input vector pairs to detect the faults based on boolean process. it is interesting to note that those faults may not be detected by iddq or other test methods, which shows the significance of iddt testing.iddt testing','Penetration testing'
'although desirable as an important activity for quality assurances and enhancing reliability, complete and exhaustive software testing is prohibitively impossible due to resources as well as timing constraints. while earlier work has indicated that uniform pairwise testing (i.e. based on 2-way interaction of variables) can be effective to detect most faults in a typical software system, a counter argument suggests such conclusion cannot be generalized to all software system faults. in some system, faults may also be non-uniform and caused by more than two parameters. considering these issues, this paper explores the issues pertaining to t-way testing from pairwise to variable strength interaction in order to highlight the state-of-the-art as well as the current state-of-practice.interaction testing','Penetration testing'
'a natural experiment is reported in which the performance of two cohorts of undergraduate students on identical tests of cognitive achievement is compared under differing conditions of proctoring and modes of administration, total n=302. between-subjects and within-subjects analyses reveal clear effects of test score inflation in the unproctored conditions compared with formal, proctored, examination conditions. these results caution against the use of unproctored cognitive testing in high-stakes situations, including on-line assessment.internet testing','Penetration testing'
'interoperability testing','Penetration testing'
'linear-consistency testing','Penetration testing'
'software is often tested with unit tests, in which each procedure is executed in isolation, and its result compared with an expected value. individual tests correspond to hoare triples used in program logics, with the pre-conditions encoded into the procedure initializations and the post-conditions encoded as assertions. unit tests for procedures that modify structures in-place or that may terminate unexpectedly require substantial programming effort to encode the postconditions, with the post-conditions themselves obscured by the test programming scaffolding. the correspondence between hoare logic and test specifications suggests directly using logical specifications for tests. the resulting tests then serve the dual purpose of a formal specification for the procedure. we show how logical test specifications can be embedded within java and how the resulting test specification language is compiled into java; this compilation automatically redirects mutations, as in software transactional memory, to support imperative procedures. we also insert monitors into the tested program for coverage analysis and error reporting. logical testing','Penetration testing'
'historically, ic testing and board testing have been considered two separate subjects. however, today\'s increasing complexityin both design and technology has given rise to a number of efforts to produce a consistent test strategy that smoothly couplesboth types of testing. this article describes one such effort by philips, a design for testability methodology for semicustomvlsi circuits. the methodology is based on the partitioning of a design into testable macros, hence the term ?macro testing.?the challenges in this approach are the partitioning itself, the selection of a test technique suited to the separate macrosand the chip\'s architecture, the execution of a macro test independent of its environment, and the assembly of macro testsinto a chip test.macro testing','Penetration testing'
'we show that fixed membership testing for many interesting subclasses of multi-pushdown machines is no harder than for pushdowns with single stack. the models we consider are mvpa, ovpa and mpda, which have all been defined and studied in the past. multi-stack pushdown automata, mpda, have ordered stacks with pop access restricted to the stack-top of the first non-empty stack. the membership for mpdas is known to be in nspace(&lt;em&gt;n&lt;/em&gt; ) and in p. we show that the p-time algorithm can be implemented in the complexity class logcfl; thus membership for mpdas is logcfl-complete. it follows that membership testing for ordered visibly pushdown automata ovpa is also in logcfl. the membership problem for multi-stack visibly pushdown automata, mvpa, is known to be np-complete. however, many applications focus on mvpa with &lt;em&gt;o&lt;/em&gt; (1) phases. we show that for mvpa with &lt;em&gt;o&lt;/em&gt; (1) phases, membership reduces to that in mpdas, and so is in logcfl. membership testing','Penetration testing'
'the oracle problem is very common in the testing of service-oriented systems. metamorphic testing has been proposed to alleviate the oracle problem in software testing. this talk aims at presenting the state of the art in metamorphic testing. it summaries what testing techniques have been successfully integrated with metamorphic testing and what application areas metamorphic testing have been applied to successfully alleviate the oracle problem. it will also introduce potential research projects using the metamorphic approach.metamorphic testing','Penetration testing'
'with the publication of ieee 1574.4 guide for design, operation, and integration of distributed resource island systems with electric power systems, there is an increasing amount of attention on not only the design and operations of microgrids, but also on the proper operation and testing of these systems. this standard provides alternative approaches and good practices for the design, operation, and integration of microgrids. this includes the ability to separate from and reconnect to part of the utility grid while providing power to the islanded power system. this presentation addresses the industry need to develop standardized testing and evaluation procedures for microgrids in order to assure quality operation in the grid connected and islanded modes of operation.microgrid testing','Penetration testing'
'microsystem testing','Penetration testing'
'microsystems testing','Penetration testing'
'first page of the articlemilitary testing','Penetration testing'
'in this paper we discuss a new automatic test scheduling system for architectures that use separate control and data-paths. multi-stage-combinational testing (mustc-testing) at the register-transfer level significantly eases test generation and can be used in lieu of or to complement sequential test generation at the gate level. we provide a system with eleven signal types to perform test scheduling at the rt level which allows module level pre-computed test sets to be directly used for testing. a test scheduler is then described along with the results obtained.mustc-testing','Penetration testing'
'computational scientists often encounter code-testing challenges not typically faced by software engineers who develop testing techniques. mutation sensitivity testing addresses these challenges, showing that a few well-designed tests can detect many code faults and that reducing error tolerances is often more effective than running additional tests.mutation sensitivity testing','Penetration testing'
'  object&dash;oriented software development is an evolutionary process, and hence the opportunities for integration are abundant. conceptually, classes are encapsulation of data attributes and their associated functions. software components are amalgamation of logically and/or physically related classes. a complete software system is also an aggregation of software components. all of these various integration levels warrant contemporary integration techniques. traditional integration techniques towards the end of software development process do not suffice any more. integration strategies are needed at class level, component level, sub&dash;system level, and system levels. classes require integration of methods. various types of class interaction mechanisms demand different testing strategies. integration of classes into components presses its own integration requirements. finally, the system integration demands different types of integration testing strategies. this paper discusses the various integration levels prevalent in object&dash;oriented software development. the integration requirements of each level are met by suggesting a solution for the same. an integration framework for integrating classes into a system is also proposed.object-oriented integration testing','Penetration testing'
'online testing','Penetration testing'
'the problem of deciding whether an observed behavior is acceptable is the oracle problem. when testing from a finite state machine (fsm), it is easy to solve the oracle problem and so it has received relatively little attention for fsms. however, if the system under test has physically distributed interfaces, called ports, then in distributed testing, we observe a local trace at each port and we compare the set of local traces with the set of allowed behaviors (global traces). this paper investigates the oracle problem for deterministic and nondeterministic fsms and for two alternative definitions of conformance for distributed testing. we show that the oracle problem can be solved in polynomial time for the weaker notion of conformance ({\\sqsubseteq_w}) but is np-hard for the stronger notion of conformance ({\\sqsubseteq_s}), even if the fsm is deterministic. however, when testing from a deterministic fsm with controllable input sequences, the oracle problem can be solved in polynomial time and similar results hold for nondeterministic fsms. thus, in some cases, the oracle problem can be efficiently solved when using \\sqsubseteq_s and where this is not the case, we can use the decision procedure for \\sqsubseteq_w as a sound approximation.oracles for distributed testing','Penetration testing'
'this paper compares partition testing and random testing on the assumption that program failure rates are not known with certainty before testing and are, therefore, modeled by random variables. it is shown that under uncertainty, partition testing compares more favorably to random testing than suggested by prior investigations concerning the deterministic case: the restriction to failure rates that are known with certainty systematically favors random testing. in particular, we generalize a result by weyuker and jeng stating equal fault detection probabilities for partition testing and random testing in the case where the failure rates in the subdomains defined by the partition are equal. it turns out that for independent random failure rates with equal expectation, the case above is a boundary case (the worst case for partition testing), and the fault detection probability of partition testing can be up to k times higher than that of random testing, where k is the number of subdomains. also in a related model for dependent failure rates, partition testing turns out to be consistently better than random testing. the dominance can also be verified for the expected (weighted) number of detected faults as an alternative comparison criterion.partition testing vs. random testing','Penetration testing'
'penetration testing is the art of finding an opendoor. it is not a science as science depends on falsifiablehypotheses. the most penetration testing can hope for isto be the science of insecurity - not the science of security- inasmuch as penetration testing can at most proveinsecurity by falsifying the hypothesis that any system,network, or application is secure. to be a science ofsecurity would require falsifiable hypotheses that anygiven system, network, or application was insecure,something that could only be done if the number ofpotential insecurities were known and enumerated suchthat the penetration tester could thereby falsify (test) aknown-to-be-complete list of vulnerabilities claimed tonot be present. because the list of potential insecurities isunknowable and hence unenumerable, no penetrationtester can prove security, just as no doctor can prove thatyou are without occult disease. putting it as picasso did,\"art is a lie that shows the truth\" and security bypenetration testing is a lie in that on a good day can showthe truth. these incompleteness and proof-by-demonstration characteristics of penetration testing ensurethat it remains an art so long as high rates of technicaladvance remains brisk and hence enumeration ofvulnerabilities an impossibility. brisk technical advanceequals productivity growth and thereby wealth creation,so it is forbidden to long for a day when penetrationtesting could achieve the status of science.penetration testing','Penetration testing'
'the dot com era ushered in a number of industry standard load testing tools. while there is no doubt that these tools have helped improve the quality of it systems, performance testing in the it industry is far from steady state. there are still severe gaps between performance test results and production systems performance in it projects. this paper proposes a number of areas where performance testing needs to improve radically, several of which can be incorporated in to load testing tools. examples are also provided of simple analytics during single user performance testing to demonstrate the effectiveness of this extra but necessary step in the testing process.performance testing','Penetration testing'
'measuring differences between near-synonyms constitutes a major challenge in the development of electronic dictionaries and natural language processing systems. this paper presents a pilot study on how population test method (ptm) may be used as an effective, empirical tool to define near-synonyms in a quantifiable manner. use of ptm presumes that all knowledge about lexical meaning in a language resides collectively in the mind(s) of its native speakers, and that this intersubjective understanding may be extracted via targeted surveys that encourage creative, thinking responses. in this paper we show (1) examples of such tests performed on a group of high school students in finland, (2) resulting data from the tests that is surprisingly quantifiable, and (3) a web-based visualization program we are developing to analyze and present the collected data.population testing','Penetration testing'
'testing with manually generated test cases often results in poor coverage and fails to discover many corner case bugs and security vulnerabilities. automated test generation techniques based on static or symbolic analysis usually do not scale beyond small program units. we propose predictive testing, a new method for amplifying the effectiveness of existing test cases using symbolic analysis. we assume that a software system has an associated test suite consisting of a set of test inputs and a set of program invariants, in the form of a set of assert statements that the software must always satisfy. predictive testing uses a combination of concrete and symbolic execution, similar to concolic execution, on the provided test inputs to discover if any of the assertions encountered along a test execution path could be violated for some closely related inputs. we extend predictive testing to catch bugs related to memory-safety violations, integer overflows, and string-related vulnerabilities. furthermore, we propose a novel technique that leverages the results of unit testing to hoist assertions located deep inside the body of a unit function to the beginning of the unit function. this enables predictive testing to encounter assertions more often in test executions and thereby significantly amplifies the effectiveness of testing.predictive testing','Penetration testing'
'nearly everyone in the computer business is concerned about quality. it\'s probably fair to say there\'ll be a pot of gold or its currency equivalent awaiting the first person to figure out how to package up some \"software quality\" and provide it in large quantities at substantial oem discounts!program testing','Penetration testing'
'there has been great interest in deciding whether a combinatorial structure satisfies some property, or in estimating the value of some numerical function associated with this combinatorial structure, by considering only a randomly chosen substructure of sufficiently large, but constant size. these problems are called property testing and parameter testing, where a property or parameter is said to be testable if it can be estimated accurately in this way. the algorithmic appeal is evident, as, conditional on sampling, this leads to reliable constant-time randomized estimators. our paper addresses property testing and parameter testing for permutations in a subpermutation perspective; more precisely, we investigate permutation properties and parameters that can be well-approximated based on randomly chosen subpermutations of much smaller size. in this context, we give a permutation counterpart of a famous result by alon and shapira [6] stating that every hereditary graph property is testable. moreover, we develop a theory of convergence of permutation sequences, which is used to characterize testable permutation parameters along the lines of the work of borgs et al. [12] in the case of graphs. this theory is interesting for its own sake, as it describes the closure of the set of all permutations as a special class of lebesgue measurable functions in [0, 1]2, which in turn may be used to define a new model of random permutations.property testing and parameter testing for permutations','Penetration testing'
'property testing [15,9] is the study of the following class of problems. given the ability to perform local queries concerning a particular object (e.g., a function, or a graph), the problem is to determine whether the object has a predetermined global property (e.g., linearity or bipartiteness), or differs significantly from any object that has the property. in the latter case we say it is far from (having) the property. the algorithm is allowed a probability of failure, and typically it inspects only a small part of the whole object.property testing','Penetration testing'
'recent research results have shown that the traditional structural testing for delay and signal integrity faults may result in overtesting due to the nontrivial number of such faults that are untestable in the functional mode although testable in the test mode. this paper presents a pseudofunctional-test methodology that attempts to minimize the overtesting problem of the scan-based circuits in automatic test pattern generation (atpg) and built-in self-test (bist) test generation approaches. the first pattern of a two-pattern test is still delivered by scan in the test mode but the pattern is generated in such a way that it does not violate the functional constraints extracted from the functional logic. the second pattern is then generated in a functional mode using the functional justification (also called broadside) test application scheme. the authors use a sequential boolean satisfiability solver to extract a set of functional constraints that consists of illegal states and internal signal correlation. the functional constraints are imposed upon an atpg tool to generate pseudofunctional tests and/or implemented as a monitor in the bist environment to allow only functional-like patterns generated from the random test pattern generator as tests. the experimental results for delay faults indicate that the percentage of functionally untestable delay faults is nontrivial for many circuits. this finding supports the hypothesis of the overtesting problem in delay testing. in addition, the results indicate the effectiveness of the proposed constraint extraction method and the proposed bist scheme.pseudofunctional testing','Penetration testing'
'automated random testing is an effective and predictable method for finding faults. while it was recently studied both in practice and in theory, no general laws were found that express the number of faults in function of the time or the number of tests performed. this article evaluates the michaelis-menten equation (max * t)/(k + t) as a law for representing the number of faults found by automated random testing. max is the number of faults it can uncover in the code, k is a constant dependent on the tested code and the strategy used and t is the number of tests. the evaluation relies on the testing of more than 6000 java classes from the qualitas corpus.random testing','Penetration testing'
'non-traditional environments often change rapidly without forewarning, are difficult or impossible to control, and have other environmental and operational constraints that cannot easily be modeled in a laboratory, partly because the necessary level of ecological validity is almost impossible to achieve in the artificial lab environment. current in situ field study evaluation techniques are insufficient in these environments. furthermore, it is often difficult or impossible to ascertain which behavioral data are needed to answer questions about user requirements, interface design, and user acceptance. in this workshop, we will use case studies to create and explore frameworks for future non-traditional field study evaluations.reality testing','Penetration testing'
'refusal testing','Penetration testing'
'one of the most important assumptions made by many classification algorithms is that the training and test sets are drawn from the same distribution, i.e., the so-called \"stationary distribution assumption\" that the future and the past data sets are identical from a probabilistic standpoint. in many domains of real-world applications, such as marketing solicitation, fraud detection, drug testing, loan approval, sub-population surveys, school enrollment among others, this is rarely the case. this is because the only labeled sample available for training is biased in different ways due to a variety of practical reasons and limitations. in these circumstances, traditional methods to evaluate the expected generalization error of classification algorithms, such as structural risk minimization, ten-fold cross-validation, and leave-one-out validation, usually return poor estimates of which classification algorithm, when trained on biased dataset, will be the most accurate for future unbiased dataset, among a number of competing candidates. sometimes, the estimated order of the learning algorithms\' accuracy could be so poor that it is not even better than random guessing. therefore,a method to determine the most accurate learner is needed for data mining under sample selection bias for many real-world applications. we present such an approach that can determine which learner will perform the best on an unbiased test set, given a possibly biased training set, in a fraction of the computational cost to use cross-validation based approaches.reverse testing','Penetration testing'
'recently, there has been much progress in the area of prepositional reasoning and search. current techniques can handle problem instances with thousands of variables and up to a million clauses. this has led to new applications in areas such as planning, scheduling, protocol verification, and software testing. much of the recent progress has resulted from a better understanding of the computational characteristics of the satisfiability problem. in particular, by exploiting connections between combinatorial problems and models from statistical physics, we now have methods that enable a much finer-grained characterization of computational complexity than the standard worst-case complexity measures. these findings provide insights into new algorithmic strategies based on randomization and distributed algorithm portfolios. i will survey the recent progress in this area and i will discuss the current state-of-the-art in propositional reasoning focusing on a series of challenge problems concerning propositional encodings, compilation techniques, approximate reasoning, robustness, and scalability.satisfiability testing','Penetration testing'
'this position paper proposes a research agenda for the field of security testing. it gives a critical account of the state of the art as seen by a practitioner and identifies questions that research failed to answer so far, or failed to answer in such a way that it would have had an impact in the real world. three categories of research problems are proposed: theory of vulnerabilities, theory of security testing, and tools and techniques. 1. about this paper the science of security testing is still in its infancy. this paper proposes a research agenda for this field. it does so from a very specific perspective: that of a tester who, being aware of the lack of a scientific basis of his work, has to and wants to assess the security level of software systems on the basis of testing. what such a tester needs is not research papers but useful tools that optimize the work that is already being done in various labs around the world. the key underlying assumption of this paper is therefore that research should take an approach similar to what a usability engineer would do when designing a tool: first understand the task, then design solutions and tools. hence the title, turning practice into theory. this paper contains no original research whatsoever. rather, it is a position paper and conveys the author\'s opinion on the subject. the author has a background in applied research and practical security testing, which may explain some of the views expressed here. primarily, the present paper collects problems the author encountered during several years of testing and evaluating systems for their security. secondarily it presents a number of observations how security testing is approached today, none of which should be taken for more than anecdotal evidence, though. the remainder of this paper is organized as follows: section 2 outlines the author\'s conception of securitysecurity testing','Penetration testing'
'the fraction of faults detected for a digital network is frequently high for the first few input combinations applied out of a set of test vectors. for on-line testing, there appears to be an advantage to splitting the test into segments which are applied at different times. it is shown that the expected time to error detection and the probability of an undetected double error can be reduced. the amount of reduction is dependent on the shape of the fault coverage curve. this approach may be applicable in fault-tolerant systems.segmented testing','Penetration testing'
'as developers move to build applications that spanservice-oriented architectures, many of them underestimate thetesting challenges associated with building and maintainingapplications that can comprise hundreds of different web services.developers need a robust set of testing tools and a systematicapproach to testing to prevent errors from being introduced or,worse yet, propagated throughout the system. wayne ariola, vicepresident of corporate development for parasoft, in a conversationwith queuecast host mike vizard, highlights some of the more commonmiscues associated with soa and discusses best practices forbuilding soa applications.soa testing','Penetration testing'
'an automated testing system that is well integrated into the software development process is considered. the knowledge-based interactive test script system (kitss) focuses on functional testing. it ignores a program\'s internal design and structure and directly uncovers discrepancies in the program\'s behavior as viewed from the outside world. kitss uses the same expressive input medium that is normally used and outputs test scripts in an automated test language. kitss checks the tests for consistency with its built-in domain model, which yields even better tests.software testing with kitss','Penetration testing'
'software testing','Penetration testing'
'automated random testing has shown to be an effective approach to finding faults but still faces a major unsolved issue: how to generate test inputs diverse enough to find many faults and find them quickly. stateful testing, the automated testing technique introduced in this article, generates new test cases that improve an existing test suite. the generated test cases are designed to violate the dynamically inferred contracts (invariants) characterizing the existing test suite. as a consequence, they are in a good position to detect new faults, and also to improve the accuracy of the inferred contracts by discovering those that are unsound. experiments on 13 data structure classes totalling over 28,000 lines of code demonstrate the effectiveness of stateful testing in improving over the results of long sessions of random testing: stateful testing found 68.4\% new faults and improved the accuracy of automatically inferred contracts to over 99\%, with just a 7\% time overhead.stateful testing','Penetration testing'
'  this paper presents a method for test case selection that allows a formal approach to testing software. the two main ideas are (1) that testers create stochastic models of software behavior instead of crafting individual test cases and (2) that specific test cases are generated from the stochastic models and applied to the software under test. this paper describes a method for creating a stochastic model in the context of a solved example. we concentrate on markov models and show how non&dash;markovian behavior can be embedded in such models without violating the markov property.stochastic software testing','Penetration testing'
'this paper uses a relation of fault coverage and defect level to determine the number of random patterns required to obtain a certain defect level. this technique has application to networks which use self-testing.sufficient testing in a self-testing environment','Penetration testing'
'system testing','Penetration testing'
'new transport protocols continue to appear as alternatives to the transmission control protocol (tcp). many of these are designed to address tcp&#8217;s inefficiency in operating over paths with a high bandwidth-delay product (bdp). to test these new protocols, especially comparatively, and to understand their interactions, extensions to the ns2 simulator allow real code from the linux kernel to be used within the ns2 simulations. however, how does the performance of such configurations compare to test-bed experiments of the same configuration? although, anecdotally, there are often comments within the research community about such issues, there are no studies that quantify the differences for a specific protocol suite. using a simple testbed, we assess four different transport protocols in a comparative study to examine how well ns2 matches reality. our tests are all conducted at 100mb/s over a wide range of delay and router buffer conditions: end-to-end delays from 25ms to 400ms, with end-to-end path buffering of 20\% to 100\% of the bdp. we find that in our simple configuration, there are significant differences in performance between ns2 and the testbed.tcp testing','Penetration testing'
'along with the evolution of the technique and use of aspect-oriented programming (aop), the difficulty of testing the aspect-oriented programs is now receiving much attention. in this position paper, we describe an aspectj program testing method based on fault model with the help of dependency model and interaction model.testing aspectj programs using fault-based testing','Penetration testing'
'in this paper, we present our experience of testing wireless embedded software. we used a wireless metering system in operation, and its software as a case study to demonstrate how a property-based testing technique, called metamorphic testing, can be used in detecting software failures of this wireless embedded system. our study shows that a careful design of test environments and selection of system properties will enable us to trace back the cause of failures and help in fault diagnosis and debugging.testing embedded software by metamorphic testing','Penetration testing'
'testing errors','Penetration testing'
'the complexity of languages like java and c++ can make introductory programming classes in these languages extremely challenging for many students. part of the complexity comes from the large number of concepts and language features that students are expected to learn while having little time for adequate practice or examples. a second source of difficulty is the emphasis that object-oriented programming places on abstraction. we believe that by placing a larger emphasis on testing in programming assignments in these introductory courses, students have an opportunity for extra practice with the language, and this affords them a gentler transition into the abstract thinking needed for programming. in this paper we describe how we emphasized testing in introductory programming assignments by requiring that students design and implement tests before starting on the program itself. we also provide some preliminary results and student reactions.testing first','Penetration testing'
'statistical tests are introduced for checking whether an image function f (x, y) defined on the unit disc d = {(x, y) : x2 + y2 &#8804; 1} is invariant under certain symmetry transformations of d, given that discrete and noisy data are observed. invariance under reflections or under rotations by rational angles is considered, as well as rotational invariance. these symmetry relations can be naturally expressed as restrictions for the zernike moments of f (x, y). therefore, the test statistics are based on the l2 distance between zernike series estimates of the image function itself and its version obtained after applying the symmetry transformation. the asymptotic distribution of the test statistics under both the hypothesis of symmetry as well as under fixed alternatives is derived. furthermore, the quality of the asymptotic approximations via simulation studies is investigated. the usefulness of our theory is verified by examining an important problem in confocal microscopy, i.e., possible imprecise alignments in the optical path of the microscope are investigated. for optical systems with rotational symmetry, the theoretical point-spread function (psf) is reflection symmetric with respect to two orthogonal axes, and rotationally invariant if the detector plane matches the optical plane of the microscope. the tests are used to investigate whether the required symmetries can indeed be detected in the empirical psf.testing for image symmetries','Penetration testing'
'this paper addresses the problem of testing whether a boolean-valued function $f$ is a halfspace, i.e., a function of the form $f(x)=\\mathrm{sgn}(w\\cdot x-\\theta)$. we consider halfspaces over the continuous domain $\\mathbf{r}^n$ (endowed with the standard multivariate gaussian distribution) as well as halfspaces over the boolean cube $\\{-1,1\\}^n$ (endowed with the uniform distribution). in both cases we give an algorithm that distinguishes halfspaces from functions that are $\\epsilon$-far from any halfspace using only $\\mathrm{poly}(\\frac{1}{\\epsilon})$ queries, independent of the dimension $n$. two simple structural results about halfspaces are at the heart of our approach for the gaussian distribution: the first gives an exact relationship between the expected value of a halfspace $f$ and the sum of the squares of $f$\'s degree-1 hermite coefficients, and the second shows that any function that approximately satisfies this relationship is close to a halfspace. we prove analogous results for the boolean cube $\\{-1,1\\}^n$ (with fourier coefficients in place of hermite coefficients) for balanced halfspaces in which all degree-1 fourier coefficients are small. dealing with general halfspaces over $\\{-1,1\\}^n$ poses significant additional complications and requires other ingredients. these include &#8220;cross-consistency&#8221; versions of the results mentioned above for pairs of halfspaces with the same weights but different thresholds; new structural results relating the largest degree-1 fourier coefficient and the largest weight in unbalanced halfspaces; and algorithmic techniques from recent work on testing juntas [e. fischer, g. kindler, d. ron, s. safra, and a. samorodnitsky, proceedings of the 43rd ieee symposium on foundations of computer science, 2002, pp. 103-112].testing halfspaces','Penetration testing'
'testing it','Penetration testing'
'we show that a boolean function over n boolean variables can be tested for the property of depending on only k of them, using a number of queries that depends only on k and the approximation parameter \\varepsilon. we present two tests, both non-adaptive, that require a number of queries that is polynomial k and linear in \\varepsilon ^{- 1}. the first test is stronger in that it has a 1-sided error, while the second test has a more compact analysis. we also present an adaptive version and a 2-sided error version of the first test, that have a somewhat better query complexity than the other algorithms.we then provide a lower bound of \\bar \\omega (\\sqrt k) on the number of queries required for the non-adaptive testing of the above property; a lower bound of \\omega (\\log (k + 1)) for adaptive algorithms naturally follows from this. in providing this we also prove a result about random walks on the group {\\rm z}_2^9 that may be interesting in its own right. we show that for some t(q) = \\bar 0(q^2) the distributions of the random walk at times t and t + 2 are close to each other, independently of the step distribution of the walk.we also discuss related questions. in particular, when given in advance a known k-junta function h, we show how to test a function f for the property of being identical to h up to a permutation of the variables, in a number of queries that is polynomial in k and \\varepsilon.testing juntas','Penetration testing'
'testing maintainability','Penetration testing'
'testing membership','Penetration testing'
'testing mems','Penetration testing'
'in this paper, we deal with errors in metamodels. metamodels define the abstract syntax of modeling languages. they play a central role in the model-driven architecture. other artifacts like models or tools are based on them and have to be changed if the metamodel is changed. consequently, correcting errors in a metamodel can be quite expensive as dependent artifacts have to be adapted to the corrected metamodel. we argue that metamodels should be tested systematically with automated tests. we present a corresponding approach that allows automated metamodel testing based on a test specification. from a test specification, multiple test models can be derived. each test model defines a potential instance of the metamodel under test. a positive test model defines a potential instance that should be an actual instance of the metamodel; a negative test model defines one that should not. we exemplify our approach with a metamodel for defining a company\'s structure. finally, we present &lt;em&gt;mmunit&lt;/em&gt;, an implementation of our approach that builds on the eclipse platform and integrates the junit framework. mmunit allows to test emf-based metamodels, which can contain additional constraints, e.g. constraints expressed in ocl.testing metamodels','Penetration testing'
'we present a (randomized) test for monotonicity of boolean functions. namely, given the ability to query an unknown function f : {0,1}^n -> {0,1} at arguments of its choice, the test always accepts a monotone f, and rejects f with high probability if it is epsilon-far from being monotone (i.e., every monotone function differs from f on more than an epsilon fraction of the domain). the complexity of the test is poly(n/epsilon).the analysis of our algorithm relates two natural combinatorial quantities that can be measured with respect to a boolean function; one being global to the function and the other being local to it.we also consider the problem of testing monotonicity based only on random examples labeled by the function. we show an omega(\\sqrt{2^n/epsilon}) lower bound on the number of required examples, and provide a matching upper bound (via an algorithm).testing monotonicity','Penetration testing'
'a string &#945;&#8712;&#931;n is called p-periodic, if for every i,j &#8712; {1,...,n}, such that $i\\equiv j \\bmod p$, &#945;i = &#945;j, where &#945;i is the i-th place of &#945;. a string &#945;&#8712;&#931;n is said to be period(&#8804; g), if there exists p&#8712; {1,...,g} such that &#945; is p-periodic. an &#949;-property tester for period(&#8804; g) is a randomized algorithm, that for an input &#945; distinguishes between the case that &#945; is in period(&#8804; g) and the case that one needs to change at least &#949;-fraction of the letters of &#945;, so that it will become period(&#8804; g). the complexity of the tester is the number of letter-queries it makes to the input. we study here the complexity of &#949;-testers for period(&#8804; g) when g varies in the range $1,\\dots,\\frac{n}{2}$. we show that there exists a surprising exponential phase transition in the query complexity around g=log n. that is, for every &#948; &gt; 0 and for each g, such that g&#8805; (logn)1+&#948;, the number of queries required and sufficient for testing period(&#8804; g) is polynomial in g. on the other hand, for each $g\\leq \\frac{log{n}}{4}$, the number of queries required and sufficient for testing period(&#8804; g) is only poly-logarithmic in g. we also prove an exact asymptotic bound for testing general periodicity. namely, that 1-sided error, non adaptive &#949;-testing of periodicity ($period(\\leq \\frac{n}{2})$) is $\\theta(\\sqrt{n\\log{n}})$ queries.testing periodicity','Penetration testing'
'let s @? @?^n be a finite set(@a\"1 ..., @a\"k) of exponents. we construct explicitly a testing set t\"s @? @?^n with k elements t\"1 ..., t\"k (namely t\"i = (2^@a^\"^t^^^i,..., 2^@a^\"^t^^^n)), such that if p=@?@a@?sa\"@ax^@a@?@?[x\"1...,x\"n] then there exists i (1 @? i @? k) such that p(t\"i)  0.testing polynomials','Penetration testing'
'a process is an important concept in modern software development. only when the activities are organized in process descriptions, can these be communicated, followed, observed, and improved. the basis for understanding what testing is, is therefore the understanding of the testing process. this paper presents the general concept of a process, and expands on this to present a suggestion for a generic testing process. for each of the activities in this generic process this paper presents suggestions for detailed activities, meant to serve as inspiration for further work on a truly generic testing process.testing processes','Penetration testing'
'let &#934; be a set of general boolean functions on n variables, such that each function depends on exactly k variables, and each variable can take a value from [1,d]. we say that &#934; is &#949;-far from satisfiable, if one must remove at least &#949;nk functions in order to make the set of remaining functions satisfiable. our main result is that if &#934; is &#949;-far from satisfiable, then most of the induced sets of functions, on sets of variables of size c(k,d)&#949;2, are not satisfiable, where c(k,d) depends only on k and d. using the above claim, we obtain similar results for k-sat and k-naeq-sat.assume we relax the decision problem of whether an instance of one of the above mentioned problems is satisfiable or not, to the problem of deciding whether an instance is satisfiable or &#949;-far from satisfiable. while the above decision problems are np-hard, our result implies that we can solve their relaxed versions, that is, distinguishing between satisfiable and &#949;-far from satisfiable instances, in randomized constant time.from the above result we obtain as a special case, previous results of alon and krivelevich [3] and of czumaj and sohler [8], concerning testing of graphs and hypergraphs colorability. we also discuss the problem of testing whether a graph g can be d-colored, such that it does not contain any copy of a colored graph from a fixed, given set of colored graphs.testing satisfiability','Penetration testing'
'the automated testing system (ats) is an open source, python-based tool for automating the testing of applications, especially scientific simulations. it\'s especially designed to support the work of a team of subject-matter experts.testing scientific programs','Penetration testing'
'we propose a methodology based on testing as a framework to capture the interactions of a machine represented in a denotational model and the data it manipulates. using a connection that models machines on the one hand, and the data they manipulate on the other, testing is used to capture the interactions of each with the objects on the other side: just as the data that are input into a machine can be viewed as tests that the machine can be subjected to, the machine can be viewed as a test that can be used to distinguish data. this approach is based on generalizing from duality theories that now are common in semantics to logical connections, which are simply contravariant adjunctions. in the process, it accomplishes much more than simply moving from one side of a duality to the other; it faithfully represents the interactions that embody what is happening as the computation proceeds. our basic philosophy is that tests can be used as a basis for modeling interactions, as well as processes and the data on which they operate. in more abstract terms, tests can be viewed as formulas of process logics, and testing semantics connects processes and process logics, and assigns computational meanings to both.testing semantics','Penetration testing'
'in testing cloud environment testing tasks requested by different tenants have many uncertainties. the arriving time, deadline and the number of tasks are unknown in advance. especially, the relationships between testing tasks and testing environments are very complex. how to efficiently manage these tasks is really a challenging problem. this paper studies the special features of testing tasks and presents a task management framework. we analyze the dependencies and conflicts associated with testing tasks and their related runtime environments, using rule matching mechanism to derive the relationships supported by domain knowledge. based on these analyses, improved algorithms are introduced to cluster and dynamically schedule testing tasks to minimize the make-span or meet deadlines with the consideration of testing task resource requirements and cloud resource utilization balance at the same time. a fault tolerance mechanism is built to cope with testing errors, whose results are studied to ameliorate clustering and scheduling algorithms. a suite of experiments compares the effectiveness of the proposed approach with other algorithms.testing tasks management in testing cloud environment','Penetration testing'
'we present the debug and test strategies used in the thumbpod system for embedded fingerprint authentication. thumbpod uses multiple levels of programming (java, c and hardware) with a hierarchy of programmable architectures (kvm on top of a sparc core on top of an fpga). the thumbpod project teamed up seven graduate students in the concurrent development and verification of all these programming layers. we pay special attention to the strengths and weaknesses of our bottom-up testing approach.testing thumbpod','Penetration testing'
'after a brief overview, separate presentations are given on tools that support the testing process in a variety of ways. some tools simulate the final execution environment as a way of expediting test execution, others automate the development of test plans, and still others collect performance data during execution. the tools address three aspects of the testing process. they provide a controlled environment in which testing can take place as well as test-data control, and some tools actually perform the tests, capturing and organizing the resulting output. the tools covered are: rute (real-time unix text environment); xray/dx; tdc (testing and debugging concurrent) ada; t; mothra; specification analyzer; and test inc.testing tools','Penetration testing'
'conducting usability tests is a time and resource intensive process. the ability to do remote testing significantly reduces the cost associated with testing while still providing rich feedback. however, mobile touch interfaces provide a unique challenge for remote testers. this paper compares the results of testing an application on a mobile touch device (the ipod touch) and an emulator using a between subject test. in it, we examine the differences in time on task, usability issues discovered, and task completion. results suggest that emulators can be used to detect some but not all usability issues and that they may provide some false positives as well. further research is required to separate issues with the emulator vs. device from the remote vs. in-person test environment.testing touch','Penetration testing'
'as a direct result of the awe-inspiring changes in lsi technology, an 8-bit microcomputer today has an equivalent of 8000 transistors-which for testing purposes are quite inaccessible, said tudor finch of bell labs in his introduction to session 8. finch, who chaired the session, went on to point out that this level of integration forces one to resort to more indirect techniques-e.g., techniques applying stimuli to input pins and observing the corresponding responses. the stimuliresponse behavior for a fault-free chip then characterizes its fault-free signaturere -iations indicate a failure condition. these deviations long with the stimuli test patterns can then be exhaustively catalogued for each and every specific fault condition to yield what is referred to as a \"fault-dictionary.\" the problem, observed finch, is that the sheer combinatorics of the situation yield literally astronomical numbers, thus excluding exhaustive characterizing techniques. this circumstance has motivated the development of algorithmic as well as heuristic techniques, which for specific fault-models given the correct behavior of a circuit automatically generates all fault-signatures. the relationship of test vectors or patterns required per gate versus circuit complexity was illustrated trated by t. finch as shown in the figure.testing, testing','Penetration testing'
testing,'Penetration testing'
'by measuring the temperature at some points of an ic during a test procedure, defects can be detected (fault testing). with a simple processing of the measured temperature waveform, the distance between the temperature monitoring point and the activated defect can be derived. this paper presents four temperature-measuring strategies to determine this distance for diagnosis purposes.thermal testing','Penetration testing'
'yields for digital very-large-scale-integration chips have been declining in the recent years, and the decline is accelerating as the technology moves deep into nanoscale. recently, we have proposed the notion of error tolerance to improve yields for a wide range of high-performance digital applications, including audio, speech, video, graphics, visualization, games, and wireless communication. error tolerance systematically codifies the fact that chips used in such applications can be acceptable despite having defects that produce erroneous outputs, provided that the errors are guaranteed to be of certain types and have severities within thresholds specified by the application. in this paper, we propose a new testing approach called threshold testing to practically exploit the notion of error tolerance for applications where errors with absolute numerical magnitudes lower than an application-specified threshold are acceptable. we propose a new automatic test pattern generator (atpg) for threshold testing for single stuck-at faults. this test generator embodies several completely new techniques, including new approaches for directing the search for a test vector, new types of objectives, new types of necessary conditions, and new approaches to identify and exploit these conditions. we demonstrate that threshold testing can enhance yield and that it is practical in terms of test generation effort and test application costs. we also propose threshold fault simulators and atpg for bridging and transition delay faults. we use these tools to show that the stuck-at-fault model is indeed a suitable model for threshold testing. this opens the way for developing low-cost tools for threshold testing that will provide high threshold coverage for realistic faults and defects and hence help provide higher yields in future nanoscale processes at low costs.threshold testing','Penetration testing'
'testing is recognized as a fundamental part of software development. tools for unit testing and coverage testing are becoming widely used; they are necessary, but not sufficient, to demonstrate that a piece of software works correctly. while various forms of path coverage are reported by coverage tools, a tester cannot rely only on this. two simple functions are used to illustrate this with junit and clover. these examples should be useful in a classroom setting when discussing software testing techniques.tools for coverage testing','Penetration testing'
'unit testing versus integration testing','Penetration testing'
'unit testing is used by programmers to discover bugs with least cost. static unit testing offers the minimum path coverage while dynamic unit testing detects control and data flow problems. in this paper, we develop a method to translate a program from \"character\" mode to \"graph\" mode, so that the tester can picture the existing code without losing the ability to generate tests. in static analysis phrase, this method can help testers to detect grammar errors, extra code, non-initialized variables, search paths to cover the code, and generate tests. in dynamic phrase, testers can get benefit from this method to check internal logic, exception and boundary conditions.unit testing','Penetration testing'
'usability testing','Penetration testing'
'the web is constantly evolving and as it does web designers and developers need a way of being able to interact with people with disabilities and users of at and observe them interacting with their web interfaces. user testing is an excellent bridge between developers and users of at but this paper asks if user testing is currently an elitist exercise undertaken by developers who are already well disposed towards best practice and how this can be changed.user testing','Penetration testing'
'the author warns that, even though companies are going all out to ensure their software satisfies y2k tests, passing grades may not guarantee a secure futurey2k testing','Penetration testing'
'how to protect sensible resources is an important issue in the development of web service applications. this paper presents a formal model for resource protections, aiming at statically analyzing and verifying that the applications use these resources in a valid manner, i.e., obeying all the protection policies. the policies are logical properties of resource usage behaviors. the usage behaviors are extracted from the execution of web services by a type and effect system, and represented as concurrent regular expressions. after a suitable transformation, the expressions can be checked for validity by model-checking tools. web service applications use the resources correctly if their concurrent regular expressions are verified valid. the analysis result shows our approach can improve system performances in comparison with runtime checkers, e.g., execution monitors.a formal model for resource protections in web service applications','Privacy protections'
'key to the efficient operation of turbo codes is the interleaver design. with proper design, the pairing of low-weight sequences to the input of the encoders can be avoided. circular shift interleavers can be used to ensure that the minimum distance due to weight-2 input sequences grows roughly as \\math , where n is the block length. in this paper, we generalized the mapping function of the circular shift interleaver to be suitable for both equal and unequal error protections. in addition, we present the results of a unique examination of the behavior of the general circular shift interleaver for equal and unequal error protections using turbo code with short frames.a general interleaver for equal and unequal error protections of turbo codes with short frames','Privacy protections'
'on the 21st december, 2001 australia introduced a new privacy law titled the privacy amendment private sector act, which extended the privacy act (1988) to apply to private sector companies. previous literature has examined the gap between self-regulation and legislation, but no research had investigated australia\'s transformation from self-regulation to legislation. the methodology used in this paper employs a triangulation method, which includes literature research into the background of privacy in australia and research into the privacy amendment (private sector) act 2000 itself, a longitudinal web site assessment and questionnaire. the study also outlines whether the new law brought into force at the end of 2001 meets government and consumer expectations in australia, and how this method of privacy protection compares with different privacy protection methods such as (i)self-regulation, found within the us, and (ii)legislation found within the eu.business compliance to changing privacy protections','Privacy protections'
'a summary is presented of a series of critical experiments that led to recent discoveries of large improvement factors for digital radio performance by antenna pattern diversity, antenna angle diversity, frequency diversity, and space diversity. the measured diversity improvement factors for digital radio against multipath dispersive fading are larger than those predicted by the existing analog radio model by at least one order of magnitude. applications of these findings will lead to substantial savings in the cost for diversity protections for digital radio routes. these discoveries stimulated the development of new models of diversity improvement factors for digital radio and the development of the drdiv computer program for engineering digital radio routes. background information is given on multipath fading and diversity concepts, and a glossary of terms is includeddiversity protections for digital radio-summary of ten-year experiments and studies','Privacy protections'
'the darpa information assurance program has the aim of developing and executing experiments that test specific hypotheses about defense in depth and dynamic defense capabilities. this paper describes the development and execution of an experiment in layering. the basic hypothesis was that layers of defense, when added in a careful and systematic way to a base system lead to increased protection against attacks on the system. for the particular experiment, a mission and broad policy were defined and a base system was developed to support the mission and the policy. the boundary controller for the system was designed and developed as a series of layers; these elements became the main focus of experimentation on layering. the results tended to confirm the experimental hypothesis that layers have a cumulative effect on protection against outside attacks. however, there are often other opportunities for attackers to go around the layers or avoid them altogether. a broader methodological result was that the entire process of developing experiments needs to be carefully thought through. in addition, the experimental data resulting from this experiment provide only a limited corroboration for the given experimental hypothesis.layering boundary protections','Privacy protections'
'first page of the articlelegal battle looming for internet protections acts','Privacy protections'
'this paper introduces yarra, a conservative extension to c to protect applications from non-control data attacks. yarra programmers specify their data integrity requirements by declaring critical data types and ascribing these critical types to important data structures. yarra guarantees that such critical data is only written through pointers with the given static type. any attempt to write to critical data through a pointer with an invalid type (perhaps because of a buffer overrun) is detected dynamically. we formalize yarra\'s semantics and prove the soundness of a program logic designed for use with the language. a key contribution is to show that yarra\'s semantics are strong enough to support sound local reasoning and the use of a frame rule, even across calls to unknown, unverified code. we evaluate a prototype implementation of a compiler and runtime system for yarra by using it to harden four common server applications against known non-control data vulnerabilities. we show that yarra defends against these attacks with only a negligible impact on their end-to-end performance.modular protections against non-control data attacks','Privacy protections'
'we present a recursive algorithm to update a kripke model so as to satisfy a formula of the computation-tree logic (ctl). recursive algorithms for model update face a difficulty: deleting (adding) transitions from (to) a kripke model to satisfy a universal (an existential) subformula may dissatisfy some existential (universal) subformulas. our method employs protected models to overcome this difficulty. we demonstrate our algorithm with a classical example of automatic synthesis described by emerson and clarke in 1982. from a dummy model, where the accessibility relation is the identity relation, our algorithm can efficiently generate a model to satisfy a specification of mutual exclusion in a variant of ctl. such a variant extends ctl with an operator that limits the out-degree of states. we compare our method with a generateand-test algorithm and outline a proof of soundness and completeness for our method.nondeterministic update of ctl models by preserving satisfaction through protections','Privacy protections'
'we propose several new methods to protect the scalar multiplication on an elliptic curve against differential analysis. the basic idea consists in transforming the curve through various random morphisms to provide a non-deterministic execution of the algorithm. the solutions we suggest complement and improve the state-of-the-art, but also provide a practical toolbox of efficient countermeasures. these should suit most of the needs for protecting implementations of crypto-algorithms based on elliptic curves.protections against differential analysis for elliptic curve cryptography','Privacy protections'
'the paper reveals the solution for the protection of inputs and outputs of embedded systems and their mathematical description. these are the problems of galvanic separation, restrictions disturbing voltage, limit reduction of signals and signal verification of loaded value. they dealt galvanic isolators and limiters limit on another principle. the work compares different methods of input and output. another part is a summary of methods for operations related to the evaluation of the accuracy of the capture inputs before further processing. other parts of the thesis is a mathematical description of the behavior of protection of inputs and outputs. to find the causes of nonlinearity limit limiters and analog galvanic isolators. the linear optocouplers are designed to the partial non-linearity caused by the method used and participation components. the research work was performed to financial support of grant reg. no iga/32/fai/11/d.protections of embededded system inputs','Privacy protections'
'coarse-grained reconfigurable architectures or cgras are drawing significant attention since they promise both performance with parallelism and flexibility with reconfiguration. soft errors or transient faults are becoming a serious design concern in embedded systems including cgras since soft error rate is increasing exponentially as technology scaling. a recently proposed software-based technique with tmr (triple modular redundancy) implemented on cgras incurs extreme performance overhead mainly due to expensive voting mechanisms for outputs from triplication of every operation. in this paper, we propose selective validation mechanisms for efficient modular redundancy techniques in the datapaths on cgras. our techniques selectively validate results at synchronous operations rather than every operation in order to reduce the expensive performance overhead from the validation mechanism. our experimental results demonstrate that our selective validation based tmr technique can improve the performance by 38.3\% on average over benchmarks as compared to the recently proposed software-based tmr technique with the full validation.selective validations for efficient protections on coarse-grained reconfigurable architectures','Privacy protections'
'the number of web threats increased in large measure in the last few years. it is not related to the pc based threats only, new operating systems of handy devices are in danger as well. in this paper testing methods of web threat protections are discussed. a unique anti-malware testing procedure has been developed under the aegis of checkvir lab. this testing procedure can provide actual comparative test results of anti-malware solutions automatically for the it user community on the web and in addition other manually or semi-automatically executed tests can provide more details about the knowledge of the tested products as well. these methods can provide results soon after the new version of a particular version of an anti-malware solution is released. the real-time automatic testing is based on a set of dedicated pcs continuously checking the possible updates and they are dealing with executing the predefined testing jobs.testing protections against web threats','Privacy protections'
'the sensitivities of the bits of low weight codewords inturbo codes to noises are discussed firstly and theresult is that the lower weight the higher sensitivity.theoretical analysis shows that through the protectionson such key-bits which have high sensitivities; theminimal weight of the codes can be improvedapparently. the simulations also give out the supportedresults: the ber could be highly improved at higheb/n0 for code rates both 1/3 and 1/2.the key-bits in turbo codes and their protections','Privacy protections'
'quantum cryptography offers a promising unbreakable cryptographic solution as it ensures perfect secrecy in applications such as quantum key distribution and bit commitment. the focus of this paper is to trace the development of quantum key distribution protocols and discuss the state of the art and open issues in this field. although the protocols have mathematically proven to be totally secure, it must be emphasized that any real world implementation suffers the limitations of physical devices which can serve as vulnerability for an eavesdropper to exploit. in this paper, we survey the quantum key distribution protocols which exist in literature and explore the attacks to which they are vulnerable. we highlight the main implementation bottlenecks related with each implementation and the solutions proposed thereof.a survey of quantum key distribution protocols','Privacy-preserving protocols'
'private information retrieval (pir) protocols allow users to learn data items stored at a server which is not fully trusted, without disclosing to the server the particular data element retrieved. several pir protocols have been proposed, which provide strong guarantees on user privacy. nevertheless, in many application scenarios it is important to protect the database as well. in this paper, we investigate the amount of data disclosed by the the most prominent pir protocols during a single run. we show that a malicious user can stage attacks that allow an excessive amount of data to be retrieved from the server. furthermore, this vulnerability can be exploited even if the client follows the legitimate steps of the pir protocol, hence the malicious request can not be detected and rejected by the server. we devise mechanisms that limit the pir disclosure to a single data item.controlling data disclosure in computational pir protocols','Privacy-preserving protocols'
'privacy-preserving reconciliation protocols on ordered sets are protocols that solve a particular subproblem of secure multiparty computation. here, each party holds a private input set of equal size in which the elements are ordered according to the party\'s preferences. the goal of a reconciliation protocol on these ordered sets is then to find all common elements in the parties\' input sets that maximize the joint preferences of the parties. in this paper, we present two main contributions that improve on the current state of the art. first, we propose two new protocols for privacy-preserving reconciliation and prove their correctness and security properties. we implement and evaluate our protocols as well as two previously published multi-party reconciliation protocols. our implementation is the first practical solution to reconciliation problems in the multi-party setting. our comparison shows that our new protocols outperform the original protocols. the basic optimization idea is to reduce the highest degree polynomial in the protocol design. second, we generalize privacy-preserving reconciliation protocols, i. e., relaxing the input constraint from totally ordered input sets of equal size to pre-ordered input sets of arbitrary size.design and implementation of privacy-preserving reconciliation protocols','Privacy-preserving protocols'
'it is often the case in applications of cryptographic protocols that one party would like to determine a practical upper-bound on the physical distance to the other party. for instance, when a person conducts a cryptographic identification protocol at an entrance to a building, the access control computer in the building would like to be ensured that the person giving the responses is no more than a few meters away. the \"distance bounding\" technique we introduce solves this problem by timing the delay between sending out a challenge bit and receiving back the corresponding response bit. it can be integrated into common identification protocols. the technique can also be applied in the three-party setting of \"wallets with observers\" in such a way that the intermediary party can prevent the other two from exchanging information, or even developing common coinflips.distance-bounding protocols','Privacy-preserving protocols'
'in the article problem of securing distributed biometric authentication system was discussed. after introduction to biometric domain, the key issues connected with secure communication in distributed authentication environments were briefly described. in this work a concept of enhancement of biometric authentication system by application of quantum protocols was formulated. two quantum protocols were analyzed. obtained experimental results enable further development of research work in chosen area.distributed authentication systems enhanced by quantum protocols','Privacy-preserving protocols'
'ad hoc networks are dynamic networks formed \"on the fly\" by a set of nodes. achieving secure routing in such networks is a big challenge. asymmetric signature schemes provide mechanisms for authentication, but may result in inefficient implementations, specially when a large number of nodes is expected. some of these efficiency problems can be mitigated with the use of aggregate signatures, which reduce the space and computations required for managing many different signatures. in this work we formalize a new concept, aggregate designated verifier signature schemes, which is suitable for authentication of routes in reactive protocols. we propose a specific and efficient scheme with provable security in the random oracle model.efficient authentication for reactive routing protocols','Privacy-preserving protocols'
'a tool for automated validation of attacks on authentication protocols has been used to find several flaws and ambiguities in the list of attacks described in the well known report by clark and jacob. in this paper the errors are presented and classified. corrected descriptions of the incorrect attacks are given for the attacks that can be easily repaired.errors in attacks on authentication protocols','Privacy-preserving protocols'
'wireless sensor networks are increasingly being integrated into the modern automobile with the intention of improving safety and reducing costs. however, it has been shown that the first widely-deployed and mandatory in-car sensor networks --tire pressure monitoring systems (tpmss) -- employ no cryptographic algorithms for protecting their wireless communication, incurring serious security and privacy risks for consumers. in this paper, we design and evaluate cost-effective secure communication protocols that can resolve the privacy and security vulnerabilities of existing tpmss as well as other forthcoming in-car wireless sensor networks. our implementation on arduino platforms shows that the proposed protocol incurs modest overhead and is applicable to resource-constrained embedded systems.lightweight secure communication protocols for in-vehicle sensor networks','Privacy-preserving protocols'
'we address the problem of data linkage and data extraction across database tables of sensitive information about individuals, in an environment of constraints on organisations\' ability to share data and a need to protect individuals\' privacy and confidentiality. we propose several privacy-preserving data linkage and data extraction protocols. our first protocol enables data linkage across separate database tables, without requiring any identifying information to be revealed to any party outside the originating data source. our second protocol enables the extraction of a cohort of individuals\' data from a data source, without revealing the membership of any individual in that cohort to the data source. we describe a variation of the first protocol which enables data sources to generate common pseudonyms without revealing any identifying information to any party, and show how the protocols are applicable for any number of data sources.privacy-preserving data linkage protocols','Privacy-preserving protocols'
'a new data link layer protocol, named the twin-bus-controller (tbc) protocol, is proposed for a fibre optic network with unidirectional bus topology. the tbc protocol operates on a contention-based, time-division multiplexing scheme, and is managed by two centralized bus controllers. these controllers, which also function as network managers cooperate with each other to control and coordinate the activities on the twin bus. the ibc protocol has the capability to perform at a very high network utilization, and uses simple hardware at all stations except the two bus controllers. this arrangement provides a relatively inexpensive means to accommodate a large number of stations. heterogeneous data consisting of real-time sensor and control signals, voice and video data, and non-real-time data such as those due to accounting and administration, can be simultaneously handled by the tbc protocol. the tbc protocol maintains global queues for all different types of data, and each class of data has a bounded delay. furthermore, any new type of data can be added easily to the network without shutting it down or affecting those stations that are unrelated to the new data. a finite-state-machine model has been used to describe the tbc protocol. performance of the tbc protocol has been evaluated by statistical analysis as well as via simulation for multiple classes of data traffic. performance of the tbc protocol has been compared with that of buzznet and fasnet. the tbc protocol can be directly applied to diverse computer communication systems, e.g. office, manufacturing, and banking environments.protocols','Privacy-preserving protocols'
'reliable and atomic group multicast have been proposed as fundamental communication paradigms to support secure distributed computing in systems in which processes may behave maliciously. these protocols enable messages to be multicast to a group of processes, while ensuring that all honest group members deliver the same messages and, in the case of atomic multicast, deliver these messages in the same order. we present new reliable and atomic group multicast protocols for asynchronous distributed systems. we also describe their implementation as part of rampart, a toolkit for building high-integrity distributed services, i.e., services that remain correct and available despite the corruption of some component servers by an attacker. to our knowledge, rampart is the  first system to demonstrate reliable and atomic group multicast in asynchronous systems subject to process corruptions.secure agreement protocols','Privacy-preserving protocols'
'we relate two models of security protocols, namely the linear logic or multiset rewriting model, and the classical logic, horn clause representation of protocols. more specifically, we show that the latter model is an abstraction of the former, in which the number of repetitions of each fact is forgotten. this result formally characterizes the approximations made by the classical logic model.security protocols','Privacy-preserving protocols'
'attacks against encrypted protocols have become increasingly popular and sophisticated. such attacks are often undetectable by the traditional intrusion detection systems (idss). additionally, the encrypted attack-traffic makes tracing the source of the attack substantially more difficult. in this paper, we address these issues and devise a mechanism to trace back attackers against encrypted protocols. in our efforts to combat attacks against cryptographic protocols, we have integrated a traceback mechanism at the monitoring stubs (mss), which were introduced in one of our previous works. while we previously focused on strategically placing monitoring stubs to detect attacks against encrypted protocols, in this work we aim at equipping mss with a traceback feature. in our approach, when a given ms detects an attack, it starts tracing back to the root of the attack. the traceback mechanism relies on monitoring the extracted features at different mss, i.e., in different points of the target network. at each ms, the monitored features over time provide a pattern which is compared or correlated with the monitored patterns at the neighboring mss. a high correlation value in the patterns observed by two adjacent mss indicates that the attack traffic propagated through the network elements covered by these mss. based on these correlation values and a prior knowledge of the network topology, the system can then construct a path back to the attacking hosts. the effectiveness of the proposed traceback scheme is verified by simulations.tracing back attacks against encrypted protocols','Privacy-preserving protocols'
'secure multi-party computation (smc) balances the use and confidentiality of distributed data. this is especially important for privacy-preserving data mining (ppdm). most secure multi-party computation protocols are only proven secure under the semi-honest model, providing insufficient security for many ppdm applications. smc protocols under the malicious adversary model generally have impractically high complexities for ppdm. we propose an accountable computing (ac) framework that enables liability for privacy compromise to be assigned to the responsible party without the complexity and cost of an smc-protocol under the malicious model. we show how to transform a circuitbased semi-honest two-party protocol into a simple and efficient protocol satisfying the ac-framework.transforming semi-honest protocols to ensure accountability','Privacy-preserving protocols'
'security protocols are indispensable in secure communication. we give an operational semantics of security protocols in terms of a prolog-like language. with this semantics, we can uncover attacks on a security protocol that are possible with no more than a given number of rounds. though our approach is exhaustive testing, the majority of fruitless search is cut off by selecting a small number of representative values that could be sent by an attacker. hence, the number of scenarios is relatively small and our method is quite practical. furthermore, our method not only reports possible attacks but also describes the attacks in great detail. this description would be very helpful to protocol designers and analyzers.uncovering attacks on security protocols','Privacy-preserving protocols'
'access control concerns in manets are very serious and considered as a crucial challenge for operators who prospects to employ unrivaled capabilities of such networks for different applications. we propose a novel hierarchical distributed aaa architecture for proactive link state routing protocols notably olsr [1]. this proposal contains a lightweight and secure design of an overlay authentication and authorization paradigm for mobile nodes as well as a reliable accounting system to enable operators to charge nodes based on their connection duration time. we also suggest a hierarchical distributed aaa (authentication, authorization, and accounting) server architecture with resource and location aware election mechanism. moreover, this proposal mitigates the olsr security issues [2] noticeably and eventually defines a node priority-based quality of service.watchman','Privacy-preserving protocols'
'a field experiment was conducted within wausau insurance companies (wausau) to determine the effects of gss anonymity and status on group productivity and satisfaction. professionals and managers within wausau participated in gss sessions to discuss ways to solve the problem of insurance fraud within their industry. groups of four and five members interacted either with or without anonymity and either under equal status or unequal status. anonymous groups generated more total comments, more unique ideas, and more ideas of higher rarity than did identified groups. equal status group members were more satisfied than were unequal status group members.a field experiment on gss anonymity and group member status','Pseudonymity, anonymity and untraceability'
'anonymity is the property of keeping secret the identity of user accessing a certain resource or service, and it is a main approach to protect user&#8217;s privacy. at present, many solutions of anonymity have been proposed, but they seldom concern how to evaluate the security of an anonymity mechanism. as user&#8217;s actions are often dynamic and random in the network, this paper proposes a novel conceptual anonymity analysis model based on the fuzzy relation theory in fuzzy mathematics. theoretic analysis shows that the proposed model can effectively analyze the security of existing anonymity systems in respect of the type of mobility, and can help design a new anonymity protection scheme.a fuzzy anonymity analysis model for mobility in anonymity system','Pseudonymity, anonymity and untraceability'
'a mix-mediated anonymity service and its payment','Pseudonymity, anonymity and untraceability'
'in secure data management the inference problem occurs when data classified at a high security level becomes inferrible from data classified at lower levels. we present a model-theoretic approach to this problem that captures the epistemic state of the database user as a set of possible worlds or models. privacy is enforced by requiring the existence of k &gt; 1 models assigning distinct values to sensitive attributes, and implemented via model counting. we provide an algorithm mechanizing this process and show that it is sound and complete for a large class of queries.a model-theoretic approach to data anonymity and inference control','Pseudonymity, anonymity and untraceability'
'positioning capabilities offered in modern mobile devices enable usage of location-based services. privacy and security is of great importance for related applications. we present a framework that allows conducting research on anonymity techniques in a real-life environment using smartphones. the proposed solution also includes logging mechanisms that facilitate positioning research dataset development in open format. to present the capabilities of the solution, we deliver the concept of k-anonymity to protect mobile users that issue queries to location-based services. experimental evaluation of the solution includes development of real-life logging dataset using smartphones by volunteers. different flavours of anonymity algorithms are easy to be included and tested. we present flavours of k-anonymity techniques and experimentally validate them. the solution has received encouraging feedback and successfully assists the researchers of location based services to experiment, validate and develop their techniques in real life environment.a novel mobile framework for anonymity techniques and services research','Pseudonymity, anonymity and untraceability'
'trusted computing (tc) as envisioned by the trusted computing group promises a solution to the problem of establishing a trust relationship between otherwise unrelated platforms. in order to achieve this goal the platform has to be equipped with a trusted platform module (tpm), which is true for millions of contemporary personal computers. the tpm provides solutions for measuring the state of a platform and reporting it in an authentic way to another entity. the same cryptographic means that ensure the authenticity also allow unique identification of the platform and therefore pose a privacy problem. to circumvent this problem the tcg proposed a trusted third party, the privacy certification authority (privacyca). unfortunately, currently no privacyca is generally available. in this paper we introduce our freely available implementation of a privacyca. in addition, our privacyca is itself a trusted service. it is capable of reporting its state to clients. furthermore, we use a novel way to minimize the trusted computing base of java-based applications in conjunction with hardware-supported virtualization. we automatically generate the service interface from a structural specification. thus, to the best of our knowledge, we were not only first to make this crucial service publicly available, but now also provide a trustworthy service whose privacy policy can be attested to its users by employing tc mechanisms. a privacyca for anonymity and trust','Pseudonymity, anonymity and untraceability'
'many smart card based remote authentication schemes have been proposed to preserve user privacy against eavesdropper. however, none of the exiting scheme provides both users\' anonymity to server and traceability to the malicious user. in this paper, we present a scheme that preserve user anonymity not only against outside attackers, but also against the remote server. when a malicious user was found, server can trace the user with the help of the trusted third party.a remote user authentication scheme preserving user anonymity and traceability','Pseudonymity, anonymity and untraceability'
'user authentication plays an important role to protect resources or services from being accessed by unauthorized users. in a recent paper, das et al. proposed a secure and efficient uniqueness-and-anonymity-preserving remote user authentication scheme for connected health care. this scheme uses three factors, e.g. biometrics, password, and smart card, to protect the security. it protects user privacy and is believed to have many abilities to resist a range of network attacks, even if the secret information stored in the smart card is compromised. in this paper, we analyze the security of das et al.\'s scheme, and show that the scheme is in fact insecure against the replay attack, user impersonation attacks and off-line guessing attacks. then, we also propose a robust uniqueness-and-anonymity-preserving remote user authentication scheme for connected health care. compared with the existing schemes, our protocol uses a different user authentication mechanism to resist replay attack. we show that our proposed scheme can provide stronger security than previous protocols. furthermore, we demonstrate the validity of the proposed scheme through the ban (burrows, abadi, and needham) logic.a robust uniqueness-and-anonymity-preserving remote user authentication scheme for connected health care','Pseudonymity, anonymity and untraceability'
'in 2004, shao pointed out that both sun et al.&#39;s and hsu et al.&#39;s threshold proxy signature schemes are vulnerable to coalition attack. for enhancing the security, shao proposed an improved scheme with anonymity to outsiders. this signature scheme is meaningless to any outsider because there is no way for him to prove each individual proxy signing public key validity. we further propose a practical threshold proxy signature scheme to remedy this drawback. the new scheme has the following advantages: (l) any verifier can check whether the authors of the proxy signature belong to the designated proxy group by the original signer, while outsiders cannot find the actual signers; (2) the original signer can identify the actual proxy signers; (3) the verification of the proxy signing public keys of the proxy signers and the threshold proxy signature can be accomplished within a signature verification procedure simultaneously.a threshold proxy signature scheme with nonrepudiation and anonymity','Pseudonymity, anonymity and untraceability'
'forward secrecy and user anonymity are provided in the wtls for the wireless internet communications. forward secrecy can be built by using diffie-hellman key agreement with random numbers in the hello messages, and user anonymity by employing the signcryption scheme to the clientkeyexchange and certificateverify procedure. in addition to these features, the modified wtls handshake protocol has mutual authentication and key agreement against man-in-the-middle attacks. the resulting security features, bandwidth efficiency, and computational load are analyzed in comparison with the existing wtls.a wtls handshake protocol with user anonymity and forward secrecy','Pseudonymity, anonymity and untraceability'
'while several well-developed anonymous services have been made available on-line to protect internet user\'s privacy, we have seen that due to the lack of accountability, they have often been exploited by criminals to conduct various illegal activities (e.g., sharing child pornography materials, sending threatening emails, etc.) against innocent people in both cyberspace and physical world. there is an immediate need for building accountability within anonymous systems. in this paper, we show that although accountability appears to be contradictory to and may impair anonymity, it is technically feasible to combine anonymity and accountability into one framework, namely accountable anonymity. to our knowledge, this is one of the first attempts trying to achieve both accountability and anonymity. we propose an efficient proxy reencryption based design that allows normal activities to be done in anonymous manner, while keeping malicious/criminal operations accountable at the same time. formal security analysis and prototype implementation demonstrate that our scheme is not only immune to well-known attacks against anonymity and accountability, but also able to provide non-frameability and confidentiality along with accountable anonymity.accountable anonymity','Pseudonymity, anonymity and untraceability'
'mobility and anonymity are two essential properties desirable in ip-based networks. in this paper, we aim to address the issue on how to achieve mobility and anonymity concurrently. at a glance, these two properties seem to be contradictory. this is partly due to the fact that there exists no single definition that clearly defines these notions. we approach this problem by firstly define these properties formally and address the problem of achieving these properties at the same time. then, we proceed with a concrete construction based on an existing ip-based network, which is tor. without losing generality, our method can be applied to any other existing network, such as morphmix or tarzan. we highlight the difficulty of achieving mobility and anonymity concurrently although it seems trivial to merge these two properties altogether. finally, we evaluate our proposed construction based on the definition that we have developed. our work can be seen as the first attempt towards formalizing the notions of mobility, anonymity and location privacy.achieving mobility and anonymity in ip-based networks','Pseudonymity, anonymity and untraceability'
'in 2009, sun et al. proposed an improved anonymous scheme on juang et al.\'s password-authenticated key exchange (pake) sheme using smart cards. although, this scheme overcomes some weaknesses of juang et al.\'s scheme and achieves user anonymity, it is still lack of the untraceability property such that the third party over the communication channel can recognize whether or not the same user in different sessions. in this paper, we propose a new robust anonymous pake scheme using smart cards which not only can strengthen the security of sun et al.\'s scheme by addressing untraceability, but also can achieve greater communication efficiency. moreover, we present a strict security proof for our scheme.an efficient and provable secure pake scheme with robust anonymity','Pseudonymity, anonymity and untraceability'
'an attacker that can identify messages as coming from the same source, can use this information to build up a picture of targets&#8217; behaviour, and so, threaten their privacy. in response to this danger, unlinkable protocols aim to make it impossible for a third party to identify two runs of a protocol as coming from the same device. we present a framework for analysing unlinkability and anonymity in the applied pi calculus. we show that unlinkability and anonymity are complementary properties; one does not imply the other. using our framework we show that the french rfid e-passport preserves anonymity but it is linkable therefore anyone carrying a french e-passport can be physically traced.analysing unlinkability and anonymity using the applied pi calculus','Pseudonymity, anonymity and untraceability'
'this paper presents clouds, a peer-to-peer protocol that guarantees both anonymity and censorship resistance in semantic overlay networks. the design of such a protocol needs to meet a number of challenging goals: enabling the exchange of encrypted messages without assuming previously shared secrets, avoiding centralised infrastructures, like trusted servers or gateways, and guaranteeing efficiency without establishing direct connections between peers. anonymity is achieved by cloaking the identity of protocol participants behind groups of semantically close peers. censorship resistance is guaranteed by a cryptographic protocol securing the anonymous communication between the querying peer and the resource provider. although we instantiate our technique on semantic overlay networks to exploit their retrieval capabilities, our framework is general and can be applied to any unstructured overlay network. experimental results demonstrate the security properties of clouds under different attacks and show the message overhead and retrieval effectiveness of the protocol. anonymity and censorship resistance in unstructured overlay networks','Pseudonymity, anonymity and untraceability'
'awareness of legal constraints regarding the use and provision of electronic systems, leads us to question the feasibility and applicability of technical solutions that take into account security and privacy regulations. we discuss the issue with reference to directives of the european community and italian legislation. in particular we study the case of e-mail, proposing a protocol that retains as many characteristics of the e-mail as possible, yet allowing for complete anonymity and proofs of correct deploy. we discuss the implications of taking anonymity to the extremes and evaluate the limits of the protocol.anonymity and certification','Pseudonymity, anonymity and untraceability'
'traditional methods for evaluating the amount of anonymity afforded by various mix configurations have depended on either measuring the size of the set of possible senders of a particular message (the anonymity set size), or by measuring the entropy associated with the probability distribution of the messages possible senders. this paper explores further an alternative way of assessing the anonymity of a mix system by considering the capacity of a covert channel from a sender behind the mix to an observer of the mix&#39;s output. initial work considered a simple model, with an observer (eve) restricted to counting the number of messages leaving a mix configured as a firewall guarding an enclave with one malicious sender (alice) and some other naive senders (cluelessi&#39;s). here, we consider the case where eve can distinguish between multiple destinations, and the senders can select to which destination their message (if any) is sent each clock tick.anonymity and covert channels in simple timed mix-firewalls','Pseudonymity, anonymity and untraceability'
'human artists typically have a personal signature, by which their individual authorship can be recognized. modernist artists tried to avoid such idiosyncracies, focussing on abstract structure instead-and welcomed computers, accordingly. but even those computer artists who have deliberately tried to lose their signature have not managed to do so. perhaps evolutionary methods might help? reasons are discussed both for believing and for doubting that evolutionary art could be wholly free from personal signatures.anonymity and evolutionary art','Pseudonymity, anonymity and untraceability'
'the tor network is a widely deployed anonymity system on the internet used by thousands of users every day. a basic monitoring system has been designed and implemented to allow long-term statistics, provide feedback to the interested user, and detect certain attacks on the network. the implementation has been added to torstatus, a project to display the current state of the tor network. during a period of six months, this monitoring system collected data, where information and patterns have been extracted and analyzed. interestingly, the tor network is very stable with more than half of all the servers located in germany and the united states. the data also shows a sinusoidal pattern every 24 h in the total number of servers.anonymity and monitoring','Pseudonymity, anonymity and untraceability'
'key establishment is a crucial cryptographic primitive for building secure communication channels between two parties in a network. it has been studied extensively in theory and widely deployed in practice. in the research literature a typical protocol in the public-key setting aims for key secrecy and mutual authentication. however, there are many important practical scenarios where mutual authentication is undesirable, such as in anonymity networks like tor, or is difficult to achieve due to insufficient public-key infrastructure at the user level, as is the case on the internet today. in this work we are concerned with the scenario where two parties establish a private shared session key, but only one party authenticates to the other; in fact, the unauthenticated party may wish to have strong anonymity guarantees. we present a desirable set of security, authentication, and anonymity goals for this setting and develop a model which captures these properties. our approach allows for clients to choose among different levels of authentication. we also describe an attack on a previous protocol of &#216;verlier and syverson, and present a new, efficient key exchange protocol that provides one-way authentication and anonymity.anonymity and one-way authentication in key exchange protocols','Pseudonymity, anonymity and untraceability'
'as the internet continues to emerge as a critical information infrastructure, it early warning systems (it-ews) have taken on greater importance in protecting both its endpoints and the infrastructure itself. although it is generally accepted that open sharing of cyber data and warnings between the independent (but mutually vulnerable) endpoints promotes broader situational awareness, such openness introduces new privacy challenges. in this paper, we present a high-level model for security information sharing between autonomous operators on the internet that enables meaningful collaboration while addressing the enduring privacy and infrastructure needs of those individual collaborators. our concept for a collaborative and decentralised it-ews is based on a novel combination of existing techniques, including peer-to-peer networking and traceable anonymous certificates. we concentrate on the security and confidentiality of the data exchange platform rather than of the data itself, a separate area of research.anonymity and privacy in distributed early warning systems','Pseudonymity, anonymity and untraceability'
'although operating as groups is required by many of ad hoc applications, there is only a sparse research done on anonymous multicast routing in manets. network privacy policies deal with protecting confidential information of the network such as nodes\' identities, venues or relationships. a potential eavesdropper can launch traffic analysis to infer such information. in this paper, we propose an anonymous multicast routing protocol for mobile ad hoc networks. our protocol extends anonymous routing from unicast communication to multicast scenarios and also provides additional privacy mechanisms.anonymity and privacy in multicast mobile ad hoc networks','Pseudonymity, anonymity and untraceability'
'in mobile adhoc networks, generating and maintaining anonymity for any adhoc node is challenging because of the node mobility, dynamic network topology, cooperative nature of the network and broadcast nature of the communication media. anonymity is provided to protect the communication, by hiding the participants as well as the message contents. existing techniques based on cryptosystem and broadcasting cannot be easily adapted to manets because of their extensive cryptographic computations and/or large communication overheads. in this paper, we first propose an unconditionally secure privacy preserving message authentication scheme (ppmas) which uses modified new variant elgamal signature scheme (mnes). this scheme enables a sender to transmit messages, providing authentication along with anonymity, without relying on any trusted third parties. the anonymous message uses privacy preserving communication protocol for manet, which is capable of anonymous end to end connections. it also allows the untraceability of the link between the identifier of a node and its location. the experimental analysis of the proposed system is presented.anonymity and security in mobile ad hoc networks','Pseudonymity, anonymity and untraceability'
'software agents that play a role in e-commerce and e-government applications involving the internet often contain information about the identity of their human user such as credit cards and bank accounts. this paper discusses whether this is necessary: whether human users and software agents are allowed to be anonymous under the relevant legal regimes and whether an adequate interaction and balance between law and anonymity can be realised from both the perspective of computer systems and the perspective of law.anonymity and software agents','Pseudonymity, anonymity and untraceability'
'the advances in electronic commerce are replacing many of the paper processes involved in commercial transactions, from the contracting process to the electronic payment process. when we are dealing with payments of high value, bank transfers are preferred. but this kind of payments does not allow the anonymity of the parties involved in the commercial transaction. electronic checks is a useful payment system for this kind of scenario, but the proposed solutions do not allow an anonymous transference as in paper-based checks. in this paper, we propose a new electronic bearer bank check scheme. the system allows the e-check to be transferred to any person by its owner without depositing it, and remaining anonymous in front of the bank.anonymity and transferability for an electronic bank check scheme','Pseudonymity, anonymity and untraceability'
'in this paper, we propose a taxonomy of privacy-related information-hiding/disclosure properties in terms of the modal logic of knowledge for multiagent systems. the properties considered here are anonymity, privacy, onymity, and identity. intuitively, anonymity means the property of hiding who performed a certain specific action, privacy hiding what was performed by a certain specific agent, onymity disclosing who performed a certain specific action, and identity disclosing what was performed by a certain specific agent. building on halpern and o\'neill\'s work, we provide formal definitions of these properties and study the logical structure underlying them. in particular, we show that some weak forms of anonymity and privacy are compatible with some weak forms of onymity and identity, respectively. we also discuss relationships between our definitions and existing standard terminology, in particular pfitzmann and hansen\'s consolidated proposal.anonymity, privacy, onymity, and identity','Pseudonymity, anonymity and untraceability'
'based on the nomenclature of the early papers in the field, we propose a set of terminology which is both expressive and precise. more particularly, we define anonymity, unlinkability, unobservability, and pseudonymity (pseudonyms and digital pseudonyms, and their attributes).  we hope that the adoption of this terminology might help to achieve better progress in the field by avoiding that each researcher invents a language of his/her own from scratch. of course, each paper will need additional vocabulary, which might be added consistently to the terms defined here.anonymity, unobservability, and pseudeonymity &#8212; a proposal for terminology','Pseudonymity, anonymity and untraceability'
'anonymity is a form of nonidentifiability which i define as noncoordinatability of traits in a given respect. this definitionbroadens the concept, freeing it from its primary association withnaming. i analyze different ways anonymity can be realized. i alsodiscuss some ethical issues, such as privacy, accountability andother values which anonymity may serve or undermine. my theorycan also conceptualize anonymity in information systems where, forexample, privacy and accountability are at issue.anonymity','Pseudonymity, anonymity and untraceability'
'in this paper we propose three smartcard-based variants of anonymous authentication using unique one-time pseudonyms. the first variant can be used to authenticate a user. however, his identity cannot be revealed and linked to other pseudonyms unless solving the computational diffie-hellman problem. in the second variant a set r of revocation centers is able to revoke the anonymity in collaboration with a trust center t but they are not able to link the revealed identity to other pseudonyms of the same user. using the third variant additionally provides linkability if r and t cooperate. some selected applications for the proposed protocols include physical access control, secure auctions, ecoins and online gambling.anonymous authentication with optional shared anonymity revocation and linkability','Pseudonymity, anonymity and untraceability'
'a method of integrating user authentication with anonymity and untraceability is presented based on the secret-key certificate and the algebraic structure of error-correcting codes. authentication protocol proposed here provides a means for the authentication server to avoid the requirement of maintaining a secure database of user secrets. especially, since the proposed protocol uses a symmetric-key cryptography and eliminates the key management problem, it is efficient and convenient for both the hardware-limited users and the authentication server.authentication protocol providing user anonymity and untraceability in wireless mobile communication systems','Pseudonymity, anonymity and untraceability'
'attacking anonymous communication networks is very tempting, and many types of attacks have already been observed. in the case for tor, a widely used anonymous overlay network is considered. despite the deployment of several protection mechanisms, an attack originated by just one rogue exit node is proposed. the attack is composed of two elements. the first is an active tag injection scheme. the malicious exit node injects image tags into all http replies, which will be cached for upcoming requests and allow different users to be distinguished. the second element is an inference attack that leverages a semi-supervised learning algorithm to reconstruct browsing sessions. captured traffic flows are clustered into sessions, such that one session is most probably associated to a specific user. the clustering algorithm uses http headers and logical dependencies encountered in a browsing session. a prototype has been implemented and its performance evaluated on the tor network. the article also describes several countermeasures and advanced attacks, modeled in a game theoretical framework, and their effectiveness assessed with reference to the nash equilibrium. copyright &#x00a9; 2011 john wiley & sons, ltd.breaking tor anonymity with game theory and data mining','Pseudonymity, anonymity and untraceability'
'okubo et al. proposed a hash-chain based authentication protocol which protects users\' location privacy and anonymity with strong forward security. however, li et al. claimed that the hash-chain calculation must be a burden on low-cost rfid tags and gives back-end servers heavy calculation loads since the numbers for hash computation are required in a hash chaining technique. thus, the non-group cellular automata chaining technique is adopted composed only of logical bitwise operations, in order to maintain both security guarantees and a low-cost construction.ca based rfid authentication protocol for privacy and anonymity','Pseudonymity, anonymity and untraceability'
'the participating nodes exchange information without knowing who is the original sender in p2p networks of basic form packets are relayed through the adjacent nodes and do not contain identity information about the sender since these packets are passed through a dynamically-formed path and since the final destination is not known until the last time, it is impossible to know who has sent it in the beginning and who will be the final recipient the anonymity, however, breaks down at download/upload time because the ip address of the host from which the data is downloaded can be known to the outside we propose a technique to provide anonymity for both the client and the server node in unstructured/structured p2p network a random node along the path between the client and the server node is selected as an agent node and works as a proxy: the client will see it as the server and the server looks at it as the client, hence protecting the identity of the client and the server from each other.client and server anonymity preserving in p2p networks','Pseudonymity, anonymity and untraceability'
'csp and anonymity','Pseudonymity, anonymity and untraceability'
'in this paper, we define practical schemes to protect the cloud consumer\'s identity (id) during message exchanges (connection anonymity) in saas. we describe the typical/target scenario for service consumption and provide a detailed privacy assessment. this is used to identify different levels of interactions between consumers and providers, as well as to evaluate how privacy is affected. we propose a multi-layered anonymity framework, where different anonymity techniques are employed together to protect id, location, behavior and data privacy, during each level of consumer-provider interaction. we also define two schemes for generating and managing anonymous credentials, which are used to implement the proposed framework. these schemes provide two options of connection anonymity: traceable (anonymity can be disclosed, if required) and untraceable (anonymity cannot be disclosed). the consumer and provider will be able to choose which is more suitable to their needs and regulatory environments.defining and implementing connection anonymity for saas web services','Pseudonymity, anonymity and untraceability'
'effects of anonymity and evaluative tone on idea generation in computer-mediated groups','Pseudonymity, anonymity and untraceability'
'as internet media has become widely used over time, public opinions formed by internet discussions affect political and social issues more critically. while internet space guarantees equal status and opportunity based on anonymity, privacy invasion and cyber bully have been causing serious troubles recently. in order to prevent this undesirable effect, the south korean government implemented the real name verification law in 2007 by which discussion participants must pass a verification process to post their opinions in most websites. this study examines the effect of this policy. our findings suggest that the enhanced identification process shows significant effects on reducing uninhibited behaviors at the aggregate level, but there is no significant impact regarding a particular user\'s behavioral shift. in addition, the law is not associated with users\' participation. this study contributes to online anonymity and privacy research area by using real world data and can shed light on useful implications to policy makers.empirical analysis of online anonymity and user behaviors','Pseudonymity, anonymity and untraceability'
'there are numerous works on the privacy and the security problems for rfid systems. however, many of them have failed due to the lack of formal security proof. in the literature, there are a few formal models that consider forward untraceability. in asiacrypt 2007, vaudenay presented an new security and privacy model for rfid that combines early models to more understandable one. in this paper, we revisit vaudenay\'s model and modify it by considering the notion of forward untraceability. our modification considers all message flows between rfid reader and tags before and after compromising secrets of tag. we analyze some rfid schemes claiming to provide forward untraceability and resistance to server impersonation. for each scheme, we exhibit attacks in which a strong adversary can trace the future interactions of the tag and impersonate the valid server to the tag. further, we show that a previously proposed attack claiming to violate forward untraceability of an existing rfid scheme does not violate forward untraceability.extending an rfid security and privacy model by considering forward untraceability','Pseudonymity, anonymity and untraceability'
'existing approaches on privacy-preserving data publishing rely on the assumption that data can be divided into quasi-identifier attributes (qi) and sensitive attribute (sa). this assumption does not hold when an attribute has both sensitive values and identifying values, which is typically the case. in this paper, we study how such attributes would impact the privacy model and data anonymization. we identify a new form of attacks, called \"freeform attacks\", that occur on such data without explicit qi attributes and sa attributes. we present a framework for modeling identifying/sensitive information at the value level, define a problem to eliminate freeform attacks, and outline an efficient solution.ff-anonymity','Pseudonymity, anonymity and untraceability'
'we introduce the concept of a group principal and present a number of different classes of group principals, including thresholdgroup-principals. these appear to naturally useful concepts for looking at security. we provide an associated epistemic language and logic and use it to reason about anonymity protocols and anonymity services, where protection properties are formulated from the intruder\'s knowledge of group principals. using our language, we give an epistemic characterization of anonymity properties. we also present a specification of a simple anonymizing system using our theory.group principals and the formalization of anonymity','Pseudonymity, anonymity and untraceability'
'as p2p multimedia streaming service is becoming more popular, it is important for p2p-vod content providers to protect their servers identity. in this paper, we first show that it is possible to launch an \"identity attack\": exposing and identifying servers of peer-to-peer video-on-demand (p2p-vod) systems. the conventional wisdom of the p2p-vod providers is that identity attack is very difficult because peers cannot distinguish between regular peers and servers in the p2p streaming process. we are the first to show that it is otherwise, and present an efficient and systematic methodology to perform p2p-vod servers detection. furthermore, we present an analytical framework to quantify the probability that an endhost is indeed a p2p-vod server. in the second part of this paper, we present a novel architecture that can hide the identity and provide anonymity protection for servers in p2p-vod systems. to quantify the protective capability of this architecture, we use the \"fundamental matrix theory\" to show the high complexity of discovering all protective nodes so as to disrupt the p2p-vod service. we not only validate the model via extensive simulation, but also implement this protective architecture on planetlab and carry out measurements to reveal its robustness against identity attack.identity attack and anonymity protection for p2p-vod systems','Pseudonymity, anonymity and untraceability'
'low-latency anonymous communication networks require padding to resist timing analysis attacks, and dependent link padding has been proven to prevent these attacks with minimal overhead. in this paper we consider low-latency anonymity networks that implement dependent link padding, and examine various network topologies. we find that the choice of the topology has an important influence on the padding overhead and the level of anonymity provided, and that stratified networks offer the best trade-off between them. we show that fully connected network topologies (free routes) are impractical when dependent link padding is used, as they suffer from feedback effects that induce disproportionate amounts of padding; and that cascade topologies have the lowest padding overhead at the cost of poor scalability with respect to anonymity. furthermore, we propose an variant of dependent link padding that considerably reduces the overhead at no loss in anonymity with respect to external adversaries. finally, we discuss how tor, a deployed large-scale anonymity network, would need to be adapted to support dependent link padding.impact of network topology on anonymity and overhead in low-latency anonymity networks','Pseudonymity, anonymity and untraceability'
'we propose a new specification framework for information hiding properties such as anonymity and privacy. the framework is based on the concept of a function view, which is a concise representation of the attacker\'s partial knowledge about a function. we describe system behavior as a set of functions, and formalize different information hiding properties in terms of views of these functions. we present an extensive case study, in which we use the function view framework to systematically classify and rigorously define a rich domain of identity-related properties, and to demonstrate that privacy and anonymity are independent. the key feature of our approach is its modularity. it yields precise, formal specifications of information hiding properties for any protocol formalism and any choice of the attacker model as long as the latter induce an observational equivalence relation on protocol instances. in particular, specifications based on function views are suitable for any cryptographic process calculus that defines some form of indistinguishability between processes. our definitions of information hiding properties take into account any feature of the security model, including probabilities, random number generation, timing, etc., to the extent that it is accounted for by the formalism in which the system is specified. partially supported by onr grants n00014-02-1-0109 and n00014-01-1-0837 and darpa contract n66001-00-c-8015.information hiding, anonymity and privacy: a modular approach','Pseudonymity, anonymity and untraceability'
'internet anonymity, advantages and disadvantages for healthcare','Pseudonymity, anonymity and untraceability'
'internet anonymity','Pseudonymity, anonymity and untraceability'
'many people provide their personal information for services via internet. therefore, people would encounter potential danger of exposing personal information and difficulties of managing id/pw. idms is a service helps a user manage id and pw efficiently and maintain personal information securely. for the development of idms, interoperability between idms and providing anonymity are necessary. in this paper, we introduce and analysis idms and propose the method both authentication and anonymity are provided in idms.interoperability and anonymity for id management systems','Pseudonymity, anonymity and untraceability'
'a proxy signature enables the original signer to delegate her signing capability to a proxy entity, who signs a message on behalf of the original signer. in this paper, we discuss the necessity of a secure channel in proxy signatures. though establishing a secure channel has much influence on the efficiency of the scheme, to the best of our knowledge, this topic has not been discussed before. all known proxy signatures used a secure channel to deliver a signed warrant except one which used a 3-pass weak blind signature. however, the kpw scheme [2] appeared to be secure without the secure channel. we think that our result can contribute to designing more efficient proxy signature scheme.invisibility and anonymity of undeniable and confirmer signatures','Pseudonymity, anonymity and untraceability'
'it is well known that despite all of its advantages the digital revolution also leads to large variety of new risks. one principal issue in this context is the growing dependence of our modern information society from the availability and correct (proved) function of modern communication services. first, i\'ll give a short overview on threats in communication networks (grids, clouds, etc), protocols and secure personal devices. then i\'ll discuss current network security approaches based on anonymous message exchanges within communicating systems. cryptography was first used to ensure data confidentiality, it has been &#8220;democratized&#8221; by ensuring the safety of telecommunications services, thereby extending its scope to authentication of a person or device, or a message, non-repudiation, integrity but also the anonymity of transactions. the anonymity is sometimes quite important in the new telecommunication and mobile networks services, much more than just message confidentiality. the talk will focus on some examples and new approaches developed in our research laboratory to deal with anonymity in routing protocols for mobile communicating systems.keynote lecture anonymity and privacy in communicating critical systems','Pseudonymity, anonymity and untraceability'
'we propose a knowledge aware bisimulation equivalence relation on the calculus of applied pi calculus. applied pi is well-known for discribing and analyzing security protocols. our equivalence relation is especially useful in analyzing the property of anonymity. we give an analysis of ikp anonymity as a running example to show the effectiveness of this approach. knowledge aware bisimulation and anonymity','Pseudonymity, anonymity and untraceability'
'with many location-based services, it is implicitly assumed that the location server receives actual users locations to respond to their spatial queries. consequently, information customized to their locations, such as nearest points of interest can be provided. however, there is a major privacy concern over sharing such sensitive information with potentially malicious servers, jeopardizing users&#x2019; private information. the anonymity- and cloaking-based approaches proposed to address this problem cannot provide stringent privacy guarantees without incurring costly computation and communication overhead. furthermore, they require a trusted intermediate anonymizer to protect user locations during query processing. this paper proposes a fundamental approach based on private information retrieval to process range and k-nearest neighbor queries, the prevalent queries used in many location-based services, with stronger privacy guarantees compared to those of the cloaking and anonymity approaches. we performed extensive experiments on both real-world and synthetic datasets to confirm the effectiveness of our approaches.location privacy: going beyond k-anonymity, cloaking and anonymizers','Pseudonymity, anonymity and untraceability'
'we present several protocols to achieve mutual communication anonymity between an information requester and a provider in a p2p information-sharing environment, such that neither the requester nor the provider can identify each other, and no other peers can identify the two communicating parties with certainty. most existing solutions achieve mutual anonymity in pure p2p systems without any trusted central controls. compared with two such representative ones, our protocols improve efficiency in two different ways. first, utilizing trusted third parties and aiming at both reliability and low-cost, we propose a group of mutual anonymity protocols. we show that with some limited central support, our protocols can accomplish the goals of anonymity, efficiency, and reliability. second, we propose a mutual anonymity protocol which relies solely on self-organizations among peers without any trusted central controls. in this protocol, the returning path can be shorter than the requesting path. this protocol does not need to broadcast the requested file back to the requester so that the bandwidth is saved and efficiency is improved. in addition, this protocol does not need special nodes to keep indices of sharing files, thus eliminating the index maintenance overhead and the potential for inconsistency between index records and peer file contents. we have evaluated our techniques in a browser-sharing environment. we show that the average increase in response time caused by our protocols is negligible, and these protocols show advantages over existing protocols in a p2p system.low-cost and reliable mutual anonymity protocols in peer-to-peer networks','Pseudonymity, anonymity and untraceability'
'lurking, anonymity and participation in computer conferencing','Pseudonymity, anonymity and untraceability'
'some systems offer probabilistic anonymity. the degree of anonymity is considered and defined by reiter and rubin [1]. in this paper metrics are proposed to measure anonymity of probabilistic systems. the metric induces a topology on probabilistic applied &lt;em&gt;***&lt;/em&gt; processes, which are used to model anonymous systems. the degree of anonymity is formally defined, and as an illustrating example, crowds --- an anonymous system for web transaction --- is analyzed. measuring anonymity','Pseudonymity, anonymity and untraceability'
'in this paper we explore the tradeoffs between security and performance in anonymity networks such as tor. using probability of path compromise as a measure of security, we explore the behaviour of various path selection algorithms with a tor path simulator. we demonstrate that assumptions about the relative expense of ip addresses and cheapness of bandwidth break down if attackers are allowed to purchase access to botnets, giving plentiful ip addresses, but each with relatively poor symmetric bandwidth. we further propose that the expected latency of data sent through a network is a useful performance metric, show how it may be calculated, and demonstrate the counter-intuitive result that tor\'s current path selection scheme, designed for performance, both performs well and is good for anonymity in the presence of a botnet-based adversary.metrics for security and performance in low-latency anonymity systems','Pseudonymity, anonymity and untraceability'
'authentication and key agreement protocols are the essential guardians of the distributed applications. they help the servers and users establish mutual trust and create secure communication channels. in this paper, we propose an authentication and key agreement scheme that is secure and has low communication and computation costs. besides its efficiency, the most significant feature of the scheme is to provide initiator untraceability which completely conceals the users\' identities from all eavesdropping adversaries. the scheme is suitable to be used in mobile services and e-commerce applications due to its low costs on communication and computation as well as its untraceability feature.mobile friendly and highly efficient remote user authenticated key agreement protocol featuring untraceability','Pseudonymity, anonymity and untraceability'
'the challenge of privacy-preserving data mining lies in respecting privacy requirements while discovering the original interesting patterns or structures. existing methods loose the correlations among attributes by transforming the different attributes independently, or cannot guarantee the minimum abstraction level required by legal policies. in this paper, we propose a novel privacy-preserving transformation framework for distance-based mining operations based on the concept of privacy-preserving microclusters that satisfy a privacy constraint as well as a significance constraint. our framework well extends the robustness of the state-of-the-art k-anonymity model by introducing a privacy constraint (minimum radius) while keeping its effectiveness by a significance constraint (minimum number of corresponding data records). the privacy-preserving microclusters are made public for data mining purposes, but the original data records are kept private. we present efficient methods for generating and maintaining privacy-preserving microclusters and show that data mining operations such as clustering can easily be adapted to the public data represented by microclusters instead of the private data records. the experiment demonstrates that the proposed methods achieve accurate clusterings results while preserving the privacy.on robust and effective k-anonymity in large databases','Pseudonymity, anonymity and untraceability'
'in 2003, chang and chang proposed an efficient and anonymous auction protocol with freewheeling bids. they claimed that their scheme could ensure the bidders\' anonymity and secure confidentiality. later, jiang et al. pointed out that their scheme could not withstand the man-in-the-middle attacks; during this time, jiang et al. proposed an improved scheme based on the scheme by chang and chang. in 2006, chang and chang proposed an anonymous auction scheme that was much more efficient than the abovementioned schemes. however, all of these three schemes were unable to achieve strong anonymity, bidding privacy, and secret bidding prices for sealed-bids. this article proposes a new scheme to resolve the above problems in which two managers and zero knowledge proof are used. the proposed scheme can be widely applied in any sensitive auction (e.g., auctions of cosmetics, medical services, etc.). moreover, the proposed scheme also satisfies the following requirements: (1) bidding privacy, (2) strong anonymity, (3) secret bidding prices, (4) unforgeability, (5) verifiability, (6) non-repudiation, (7) traceability, (8) one-time registration and (9) easy revocation.practical electronic auction scheme with strong anonymity and bidding privacy','Pseudonymity, anonymity and untraceability'
'a set of group-oriented blind (t, n) threshold signature schemes is proposed based on the discrete logarithm problem. using these schemes, any t out of n signers in a group can represent the group in signing blind threshold signatures. a threshold signature in the proposed schemes is the same size as an individual signature, and the signature verification process is simplified by means of a group public key. the schemes are suitable for single-authority applications in privacy protection, secure voting systems, and anonymous payment systems for distributing the power of a single authority. the assistance of a mutually trusted authority is not required. in addition, individual signers can choose their own private keys, and all the members together decide on the group public key.privacy and anonymity protection with blind threshold signatures','Pseudonymity, anonymity and untraceability'
'in this talk, i will address three aspects of user privacy in advertiser-supported, online services. first, i present the design of a novel browser plug-in that enables anonymous search. next, i consider economic aspects of user privacy from the point of view of the operator of an advertiser-supported website. finally, i present recent work on \"accountability\" in online activity, where the goal is to hold website operators responsible for appropriate handling of users\' sensitive information rather than to prevent users from ever providing information that might be misused.privacy, anonymity, and accountability in ad-supported services','Pseudonymity, anonymity and untraceability'
'in this paper we are concerned with the privacy, traceability and anonymity for content distribution and protection applications. we believe for many content protection applications, privacy friendly anonymous trust is needed. we argue broadcast encryption technologies have advantages over public key encryption systems in these applications. when coming to combat piracy, we provided some business scenarios where it is essential to offer traceability in order to protect the revenues drawn from the provided services. however, in some scenarios, privacy concern needs to be carefully addressed in order to have a viable business. we shed insights on different technologies that can be used to offer solutions in each category.privacy, traceability, and anonymity for content protection','Pseudonymity, anonymity and untraceability'
'anonymity means that the identity of the user performing a certain action is maintained secret. the protocols for ensuring anonymity often use random mechanisms which can be described probabilistically. the user, on the other hand, may be selected either nondeterministically or probabilistically. we investigate various notions of anonymity, at different levels of strength, for both the cases of probabilistic and nondeterministic users.probabilistic and nondeterministic aspects of anonymity','Pseudonymity, anonymity and untraceability'
'in this age of globalization, organizations need to publish their microdata owing to legal directives or share it with business associates in order to remain competitive. this puts personal privacy at risk. to surmount this risk, attributes that clearly identify individuals, such as name, social security number, and driving license number, are generally removed or replaced by random values. but this may not be enough because such de-identified databases can sometimes be joined with other public databases on attributes such as gender, date of birth, and zipcode to re-identify individuals who were supposed to remain anonymous. in the literature, such an identity-leaking attribute combination is called as a quasi-identifier. it is always critical to be able to recognize quasi-identifiers and to apply to them appropriate protective measures to mitigate the identity disclosure risk posed by join attacks. in this paper, we start out by providing the first formal characterization and a practical technique to identify quasi-identifiers. we show an interesting connection between whether a set of columns forms a quasi-identifier and the number of distinct values assumed by the combination of the columns.we then use this characterization to come up with a probabilistic notion of anonymity. again we show an interesting connection between the number of distinct values taken by a combination of columns and the anonymity it can offer. this allows us to find an ideal amount of generalization or suppression to apply to different columns in order to achieve probabilistic anonymity. we work through many examples and show that our analysis can be used to make a published database conform to privacy rules like hipaa. in order to achieve probabilistic anonymity, we observe that one needs to solve multiple 1-dimensional k-anonymity problems. we propose many efficient and scalable algorithms for achieving 1-dimensional anonymity. our algorithms are optimal in a sense that they minimally distort data and retain much of its utility.probabilistic anonymity','Pseudonymity, anonymity and untraceability'
'  in many internet commerce applications buyers can easily achieve anonymity, limiting what a seller can learn about any buyer individually. however, because sellers need to keep a fixed web address, buyers can probe them repeatedly or pool their information about sellers with the information obtained by other buyers&semi; hence, sellers&#8217; strategies become public knowledge. under assumptions of buyer anonymity, publicly-known seller strategies, and no negotiation transaction costs for buyers, we find that take-it-or-leave-it offers will yield at least as much seller profit as any attempt at price discrimination could yield. as we relax those assumptions, however, we find that sellers, and in some cases buyers as well, may benefit from a more general bargaining protocol.protocols for automated negotiations with buyer anonymity and seller  reputations','Pseudonymity, anonymity and untraceability'
'many authentication and key agreement protocols were proposed for protecting communicated messages. in previous protocols, if the user@?s identity is transmitted in plaintext, an adversary can tap the communications and employ it to launch some attacks. in most protocols with user anonymity, they focus on satisfaction of several security requirements. from a client@?s point of view, those protocols are not admired since the cost of storage, computation and communication is high. in pervasive computing, a client usually uses a limited-resource device to access multiple servers. the storage and computation are very important issues especially in this kind of environments. also, for a convenience of designing protocol, most protocols use timestamps to prevent the replay attack. as we know, the serious time synchronization problem exists in timestamp-based protocols. finally, most protocols do not have formal proofs for the security. in this paper, we propose a secure and efficient identification and key agreement protocol with user anonymity based on the difficulty of cracking the elliptic curve diffie-hellman assumption. in addition, we also propose an augmented protocol for providing the explicit mutual authentication. compared with the related protocols, the proposed protocols@? computation cost is lower and the key length is shorter. therefore, our protocols are suitable even for applications in low power computing environments. finally, we formally prove the security of the proposed protocols by employing the random oracle model.provably secure and efficient identification and key agreement protocol with user anonymity','Pseudonymity, anonymity and untraceability'
'proxy authorization signature is essential to electronic commerce and other electronic transaction. this paper analyzed the security threats of present proxy authorization signatures and the basic principles of proxy signature together with its superiority in electronic transaction application. we present a conditionally anonymous proxy authorization signature scheme with forward security. the identity blinding and identity tracing algorithm enable the proxy signer to be conditionally anonymous for secrecy protection and authorization supervision; the scheme renders effective supervision on signature generating and authorization, avoids the misuse of proxy entrusting and identity tracing, prevents generalized signature forgery, coalition attack and intruder-in-middle attack. we also present an application case of the proxy authorization signature scheme in electronic commerce, it further justify the scheme&#8217;s brevity, security, high efficiency regarding its application in such circumstances as with restricted computation ability, integrated space and limited bandwidth yet requiring for high-speed operation.proxy authorization signature with conditional anonymity and its application','Pseudonymity, anonymity and untraceability'
'crowds, which is a classical p2p anonymous communication protocol, adopts random forwarding to effectively provide a privacy preserving way of accessing the web with good expansibility, without web sites being able to recognize who is browsing. however, it does not provide anonymity against global eavesdroppers. furthermore, recipient anonymity and relationship anonymity are not implemented by crowds. this paper utilizes the characteristic that ipv6 protocol allows user-defined ipv6 options to improve crowds anonymous communication protocol. the contributions of our improved crowds protocol (crowds6) are as follows: 1. it makes other relays of the re-routing path correctly get last-hop&#8217;s address by adding a self-defined option in sender&#8217;s packet to store last-hop&#8217;s address; 2. when sender sends packets, it uses last-hop&#8217;s public key to encrypt symmetry key which is used to decode the message content, so only the last-hop can get recipient&#8217;s address, resolving the key sharing problem between sender and last-hop, and achieving recipient anonymity to some extent. the theoretical analysis and simulation results show that our improved crowds protocol effectively resolves the problem that other relays of the re-routing path can not correctly get last-hop&#8217;s address, and performances better than traditional anonymous protocol on defending predecessor attack by utilizing the key sharing technology, at the same time, recipient anonymity is also realized while the communication delay is reduced.recipient anonymity','Pseudonymity, anonymity and untraceability'
'semantic security and anonymity are the two main properties that an identity-based encryption scheme can satisfy. such properties can be defined in either an adaptive or a selective scenario, which differ on the moment where the attacker chooses the identity/ies that are the target of the attack. there are well-known separations between selective and adaptive semantic security on the one hand, and between selective and adaptive anonymity on the other hand. in this paper we investigate the relations between these selective and adaptive notions, for identity-based encryption schemes enjoying at the same time some security and anonymity properties. on the negative side, we prove that there is a separation between selective and adaptive anonymity even for schemes which enjoy adaptive semantic security. on the positive side, we prove that selective semantic security and adaptive anonymity imply adaptive semantic security.relations between semantic security and anonymity in identity-based encryption','Pseudonymity, anonymity and untraceability'
'responder anonymity and anonymous peer-to-peer file sharing','Pseudonymity, anonymity and untraceability'
'anonymity services in the eu may be forced by the new eu data retention directive to collect connection data and deanonymise some of their users in case of serious crimes. for this purpose, we propose a new privacy-friendly solution for incorporating revocation in an anonymous communication system. in contrast to other known methods, our scheme does not reveal the identity of a user to any other entity involved in the revocation procedure but the law enforcement agency. another advantage is, that no user will need to provide more identifying information than his connection (ip) address, that is what he needs to communicate with the system anyway. the proposed scheme is based mainly on threshold group signatures and threshold atomic proxy re-encryption.revocable anonymity','Pseudonymity, anonymity and untraceability'
'we propose a new information-hiding property called role interchangeability for the verification of the anonymity and privacy of security protocols. first, we formally specify the new property in multi-agent systems, and describe its relationship with known anonymity properties that are also defined in multi-agent systems. moreover, we define privacy in a way that is symmetric with anonymity, and show that exploiting this symmetry is useful for deriving anonymity and privacy from role interchangeability. next, we show a way of verifying the new property. we show that role interchangeability in a multiagent system is characterized by the existence of role-interchange functions on the set of traces corresponding to the system. in addition, a simulation proof method is presented to prove the existence of the functions for a protocol described as an automaton. finally, as a case study, we apply our method to the formal verification of the foo electronic voting protocol.role interchange for anonymity and privacy of voting','Pseudonymity, anonymity and untraceability'
'in this paper, we present previously unproposed schemes for encryption with anonymity and ring signature by applying two techniques. that is, we construct a key-privacy encryption scheme by using n-ary representation, and a ring signature scheme by using the repetition of evaluation of functions. we analyze precisely the properties of these schemes and show their advantage and disadvantage.schemes for encryption with anonymity and ring signature','Pseudonymity, anonymity and untraceability'
'we study the problem of anonymizing social networks to prevent individual identifications which use both structural (node degrees) and textual (edge labels) information in social networks. we introduce the concept of structural and textual (st)-equivalence of individuals at two levels (strict and loose), and formally define the problem as structure and text aware k-anonymity of social networks (stk-anonymity). in an stk-anonymized network, each individual is st-equivalent to at least k-1 other nodes. the major challenge in achieving stk-anonymity comes from the correlation of edge labels, which causes the propagation of edge anonymization. to address the challenge, we present a two-phase approach. in particular, a set-enumeration tree based approach and three pruning strategies are introduced in the second phase to avoid the propagation problem during anonymization. experimental results on both real and synthetic datasets are presented to show the effectiveness and efficiency of our approaches.stk-anonymity','Pseudonymity, anonymity and untraceability'
'the increasing number of elderly patients in the world has lead to various new appliances and technologies in the modem tele-healthcare platform. one such application is the medical sensor network (msn). in this application, patients are deployed with certain medical sensors and wearable devices and are remotely monitored by professionals. thus, seeing a doctor in person is no longer the only option for those in need of medical care. since it is also an economical way to reduce healthcare costs and save medical resources, we expect a robust, reliable, and scalable msn in the near future. however, the time signal and temporal history in the current msn are vulnerable due to unsecured infrastructure and transmission strategies. meanwhile, the msn may leak patients\' identifications or other sensitive information that violates personal privacy. to make sure that the critical time signal is accountable, we propose a new architecture for the msn that is capable of temporal accountability. in addition, it also preserves privacy ability via a crowds anonymous system. the analysis results clearly indicate the advantages of being our proposed methods in terms of low-cost and reliable and having scalable features.temporal accountability and anonymity in medical sensor networks','Pseudonymity, anonymity and untraceability'
'in this paper we consider how timing channels (from high assurance computing) arise in the study of certain anonymity systems. we then discuss how the same type of shannon analysis for these timing channels applies to spikes (action potentials in the field of neuroscience).timing channels, anonymity, mixes, and spikes','Pseudonymity, anonymity and untraceability'
'we discuss problems and trade-offs with systems providing anonymity for web browsing (or more generally any communication system that requires low latency interaction). we focus on two main systems: the freedom network [12] and pipenet [8]. although freedom is efficient and reasonably secure against denial of service attacks, it is vulnerable to some generic traffic analysis attacks, which we describe. on the other hand, we look at pipenet, a simple theoretical model which protects against the traffic analysis attacks we point out, but is vulnerable to denial of services attacks and has efficiency problems. in light of these observations, we discuss the trade-offs that one faces when trying to construct an efficient low latency communication system that protects users anonymity.traffic analysis attacks and trade-offs in anonymity providing systems','Pseudonymity, anonymity and untraceability'
'because the privacyca defined in tcg specification may leak user&#8217;s privacy and trace his transactions, two privacyca schemes with higher untraceability are proposed. the agents-based one provides higher untraceability by introducing agents and vk certificates to eliminate the linkage between ek and aik. by adopting partially blind signature algorithm in the generation of aik triples, pbs-based one provides absolute untraceability to user&#8217;s transactions because the public aik is blinded while being signed by the privacyca. new schemes keep good compatibility to tcg specification and provide different levels of untraceability to satisfy particular applications.two schemes of privacyca with higher untraceability','Pseudonymity, anonymity and untraceability'
'in this work, we study the problem of anonymity-preserving data publishing in moving objects databases. in particular, the trajectory of a mobile user on the plane is no longer a polyline in a two-dimensional space, instead it is a two-dimensional surface: we know that the trajectory of the mobile user is within this surface, but we do not know exactly where. we transform the surface\'s boundary poly-lines to dual points and we focus on the information distortion introduced by this space translation. we develop a set of efficient spatio-temporal access methods and we experimentally measure the impact of information distortion by comparing the performance results of the same spatio-temporal range queries executed on the original database and on the anonymized one.uncertainty for anonymity and 2-dimensional range query distortion','Pseudonymity, anonymity and untraceability'
'unconditional sender and recipient untraceability in spite of active attacks','Pseudonymity, anonymity and untraceability'
'in this paper, we study the concept of privacy-preserving multi-service subscription systems. with such system, service providers can propose to their customers, by the way of a subscription, several distinct services that users can access while being anonymous. we moreover study how users can be untraceable w.r.t. the service provider during the subscription process, in such a way that it is additionally possible to make profiling on the users\' customs. this permits the service provider to propose some advertisements to users while protecting the privacy of the latter, even this may be seen as contradictory. we also propose concrete instantiations, based on signature schemes with extensions from camenisch and lysyanskaya.untraceability and profiling are not mutually exclusive','Pseudonymity, anonymity and untraceability'
'the traceability is one of the core requirements for group signature schemes. recently, group signature schemes based on the bilinear mapping were proposed. the bilinearity of a bilinear mapping allows an efficient signature scheme verification for signature schemes based on the discrete logarithm type problem. but the bilinearity of a bilinear mapping can be an attack point with respect to the traceability for group signature schemes when it is designed in such a way that the linearity can be preserved. we show that cheng-zhu-qiu-wang\'s group signature schemes [8] based on bilinear mapping have no traceability property due to their improper use of a bilinear mapping in their signature generation and verification step. we also propose security enhanced group signature schemes for both of mini group signature scheme and improved group signature scheme of cheng-zhu-qiu-wang\'s.untraceability of group signature schemes based on bilinear mapping and their improvement','Pseudonymity, anonymity and untraceability'
'web privacy and anonymity','Pseudonymity, anonymity and untraceability'
'this paper presents a generic and simple transformation that adds traceability to an anonymous encryption scheme. we focus on the case of honest senders, which finds applications in many real-life scenarios. advantageously, our transformation can be applied to already deployed public-key infrastructures. two concrete implementations are provided.a simple construction for public-key encryption with revocable anonymity','Public key encryption'
'one of needham and schroeder\'s proposed signature authentication protocols is shown to fail when there is a possibility of compromised keys: this invalidates one of the applications of their technique. a more elaborate mechanism is proposed which does not require a network clock, but does require a third party to the transaction. the latter approach is shown to be reliable in a fairly strong sense.authentication of signatures using public key encryption','Public key encryption'
'in certificateless public key encryption (cl-pke), the private key generator (pkg) keeps a master secret key to generate a partial private key corresponding to a user\'s identity. together with a secret value generated by the user, a full private key can be constructed for decryption. traditional security model for cl-pke assumes that (i) both the master secret key of the pkg and the full private key of the user under attack are in absolute secrecy; and (ii) the attacker can only obtain either the target user\'s secret value without any partial knowledge of the partial private key or vice versa. however, the advancement of practical side-channel attacks enable attackers to obtain partial information of both keys easily, making the above assumption invalid. in this paper, we give the first leakage-resilient cl-pke. we consider different leakage conditions for type i (third party attackers) and type ii (honest-but-curious pkg) attackers, following the classification in traditional cl-pke. we give a concrete construction in the composite order bilinear group. we prove the security of our scheme in the standard model, overcoming some technical difficulties in the security proofs for both type i and type ii attackers of cl-pke.leakage-resilient certificateless public key encryption','Public key encryption'
'lossy trapdoor functions (ltfs) was introduced by peikert and waters in 2008. the importance of the ltfs was justified by their numerous cryptographic applications, like the construction of injective one-way trapdoor functions, cca-secure public-key encryption, etc. however, little research on application of ltfs to key-leakage resilient public-key encryption was done. in this article we introduce a new variant of ltfs featuring leakage-resilience, namely lrltfs and give a realization of lrltfs with leakage rate 1/&#920;(&#954;) (where &#954; is the security parameter) under the decisional diffie-hellman (ddh) assumption. we further improve the leakage rate to 1-o(1) over a composite-order group in which the decisional composite residuosity (dcr) assumption holds. we also introduce a new notion of key-leakage attacks, which we call weak key-leakage attacks, for bridging the adaptive and non-adaptive key-leakage attacks in the setting of public-key cryptosystem. in this model, the leakage adversary only gets a part of public key before accessing to a leakage oracle. we show that lrltfs imply public-key encryption schemes secure against chosen-ciphertext weak key-leakage attacks in a black-box sense.leakage-resilient lossy trapdoor functions and public-key encryption','Public key encryption'
'luc public key encryption','Public key encryption'
'in this talk, we discuss properties of public key encryption schemes which are derived from group signatures. abdalla and warinschi (icics 2004) and ohtake et al. (africacrypt 2009) already showed that it is possible to construct a chosen-ciphertext secure public key encryption from an arbitrary group signature scheme in a black-box manner. by extending these results, we further show that if the underlying group signature scheme has some special property, then its converted public key encryption scheme also yields a special property which is inherited from the underlying group signature.on the properties of public key encryption from group signatures','Public key encryption'
'a timed-release cryptosystem allows a sender to encrypt a message so that only the intended recipient can read it only after a specified time. we formalize the concept of a secure timed-release public-key cryptosystem and show that, if a third party is relied upon to guarantee decryption after the specified date, this concept is equivalent to identity-based encryption; this explains the observation that all known constructions use identity-based encryption to achieve timed-release security. we then give several provably-secure constructions of timed-release encryption: a generic scheme based on any identity-based encryption scheme, and two more efficient schemes based on the existence of cryptographically admissible bilinear mappings. the first of these is essentially as efficient as the boneh-franklin identity-based encryption scheme, and is provably secure and authenticated in the random oracle model; the final scheme is not authenticated but is provably secure in the standard model (i.e., without random oracles).provably secure timed-release public key encryption','Public key encryption'
'the rsa method is used for the interchange of secret messages via insecure channels. it is elegant in theory and fast and reliable in practice. applications are in the field of communication networks.the method is initialized by choosing some suitable large prime numbers. encrypting and decrypting of a message are done by modular arithmetic. the modulus is a large integer (e.g. 200 decimal digits). any attempt to break the system amounts to factoring the modulus.the basic operations that are needed for an implementation are fundamental for any computer algebra package, e.g. sac-2 offers them in an suitable way. this project therefore requires only little programming, but some insight into theory and the ability to find and put together existing components. an optional part of the project deals with the security of the method.public key encryption','Public key encryption'
'this paper describes some recently successful results in the cmos vlsi implementation of public-key data encryption algorithms. architectural details, circuits, and prototype test results are presented for rsa encryption and multiplication in the finite field gf(2m). these designs emphasize high throughput and modularity. an asynchronous modulo multiplier is described which permits a significant improvement in rsa encryption throughput relative to previously described synchronous implementations.vlsi implementation of public-key encryption algorithms','Public key encryption'
'in the article problem of securing distributed biometric authentication system was discussed. after introduction to biometric domain, the key issues connected with secure communication in distributed authentication environments were briefly described. in this work a concept of enhancement of biometric authentication system by application of quantum protocols was formulated. two quantum protocols were analyzed. obtained experimental results enable further development of research work in chosen area.distributed authentication systems enhanced by quantum protocols','Security protocols'
'reliable and atomic group multicast have been proposed as fundamental communication paradigms to support secure distributed computing in systems in which processes may behave maliciously. these protocols enable messages to be multicast to a group of processes, while ensuring that all honest group members deliver the same messages and, in the case of atomic multicast, deliver these messages in the same order. we present new reliable and atomic group multicast protocols for asynchronous distributed systems. we also describe their implementation as part of rampart, a toolkit for building high-integrity distributed services, i.e., services that remain correct and available despite the corruption of some component servers by an attacker. to our knowledge, rampart is the  first system to demonstrate reliable and atomic group multicast in asynchronous systems subject to process corruptions.secure agreement protocols','Security protocols'
'attacks against encrypted protocols have become increasingly popular and sophisticated. such attacks are often undetectable by the traditional intrusion detection systems (idss). additionally, the encrypted attack-traffic makes tracing the source of the attack substantially more difficult. in this paper, we address these issues and devise a mechanism to trace back attackers against encrypted protocols. in our efforts to combat attacks against cryptographic protocols, we have integrated a traceback mechanism at the monitoring stubs (mss), which were introduced in one of our previous works. while we previously focused on strategically placing monitoring stubs to detect attacks against encrypted protocols, in this work we aim at equipping mss with a traceback feature. in our approach, when a given ms detects an attack, it starts tracing back to the root of the attack. the traceback mechanism relies on monitoring the extracted features at different mss, i.e., in different points of the target network. at each ms, the monitored features over time provide a pattern which is compared or correlated with the monitored patterns at the neighboring mss. a high correlation value in the patterns observed by two adjacent mss indicates that the attack traffic propagated through the network elements covered by these mss. based on these correlation values and a prior knowledge of the network topology, the system can then construct a path back to the attacking hosts. the effectiveness of the proposed traceback scheme is verified by simulations.tracing back attacks against encrypted protocols','Security protocols'
'\"requirements engineering','Security requirements'
'we present a detailed examination of the access constraints for a small real-world health information system with the aim of achieving minimal access rights for each of the involved principals. we show that, even for such a relatively simple system, the resulting constraints are very complex and cannot be expressed easily or clearly using the static per-method access control lists generally supported by component-based software. we derive general requirements for the expressiveness of access constraints and propose criteria for a more suitable access control mechanism in the context of component-based systems. we describe a two-level mechanism which can fulfil these criteria.a case study in access control requirements for a health information system','Security requirements'
'in order to develop security critical information systems, specifying security quality requirements is vitally important, although it is a very difficult task. fortunately, there are several security standards, like the common criteria (iso/iec 15408), which help us handle security requirements. this article will present a common criteria centred and reuse-based process that deals with security requirements at the early stages of software development in a systematic and intuitive way, by providing a security resources repository as well as integrating the common criteria into the software lifecycle, so that it unifies the concepts of requirements engineering and security engineering.a common criteria based security requirements engineering process for the development of secure information systems','Security requirements'
'nowadays, security solutions are focused mainly on providing security defences, instead of solving one of the main reasons for security problems that refers to an appropriate information systems (is) design. fortunately there are several standards, like the common criteria, which help to deal with the security requirements along all the is development cycle. in this paper a comparative analysis of eight different relevant technical proposals, which place great importance on the establishing of security requirements in the development of is, is carried out. and they provide some significant contributions in aspects related to security. nevertheless, they only satisfy partly the necessary criteria for the establishment of security requirements, with guarantees and integration in the development of is. thus we conclude that they are not specific enough for dealing with security requirements in the first stages of is development in a systematic and intuitive way.a comparison of the common criteria with proposals of information systems security requirements','Security requirements'
'many protocols are designed in recent years to guarantee fairness in electronic exchange that is widely used in e-commerce and e-government systems. in spite of the cryptographic view that has overwhelmed the most recent work, we tend to view the fairness problem in a more complete vision. first we present a renewed set of requirement definitions for fair protocols. second we study the encrypted item validation problem and present a simple and effective solution. with these preliminary discussions, we present our new item-item exchange protocol that is the first one to satisfy those newly introduced requirements.a fair item-item exchange protocol satisfying newly introduced requirements','Security requirements'
'in this paper we present a formal language for specifying and reasoning about cryptographic protocol requirements. we give sets of requirements for key distribution protocols and for key agreement protocols in that language. we look at a key agreement protocol due to aziz and diffie that might meet those requirements and show how to specify it in the language of the nrl protocol analyzer. we also show how to map our formal requirements to the language of the nrl protocol analyzer and use the analyzer to show that the protocol meets those requirements. in other words, we use the analyzer to assess the validity of the formulae that make up the requirements in models of the protocol. our analysis reveals an implicit assumption about implementations of the protocol and reveals subtleties in the kinds of requirements one might specify for similar protocols.a formal language for cryptographic protocol requirements','Security requirements'
'a formal model to aid documenting and harmonizing of information security requirements','Security requirements'
'a framework for security requirements','Security requirements'
'security and efficiency of rekeying are crucialrequirements for multicast key management. however,the two requirements pull in different directions andbalancing them to meet the application needs is still anopen issue. in this paper we introduce a hybrid keytree scheme to balance security, namely the resistanceto collusion, and the efficiency. the resistance tocollusion is measured by an integer parameter. thecommunication and the storage requirements for thecontroller depend on this parameter too, and theydecrease as the resistance to collusion is relaxed. weanalytically evaluate the efficiency of our scheme andcompare with the previous work. the results show that ourscheme allows a fine-tuning of security requirements versusefficiency requirements at run-time, that is not possible withthe previous key management schemes.a hybrid key tree scheme for multicast to balance security and efficiency requirements','Security requirements'
'we present a pattern system for security requirements engineering, consisting of security problem frames and concretized security problem frames. these are special kinds of problem frames that serve to structure, characterize, analyze, and finally solve software development problems in the area of software and system security. we equip each frame with formal preconditions and postconditions. the analysis of these conditions results in a pattern system that explicitly shows the dependencies between the different frames. moreover, we indicate related frames, which are commonly used together with the considered frame. hence, our approach helps security engineers to avoid omissions and to cover all security requirements that are relevant for a given problem.a pattern system for security requirements engineering','Security requirements'
'the classic requirement engineering provides limited technology and inefficient methods for requirements evolution, which leads to great restriction on finish the task of requirements evolution with high-efficiency and high-quality. for resolving above problem, this paper presents a reflective requirements specification for requirements evolution. the reflective requirements specification divides into two layers: base-level and meta-level. the base-level is the requirements defined by users, which are described by owl-s; the meta-level are composed of meta-information reified from base-level requirements. for supporting requirements specification evolution, on the basis of the above research, we define a group of operation set used for evolution of reflective requirements specification. under the domain requirements model, requirements evolution can be completed through the evolution operations.a reflective requirements specification for requirements evolving','Security requirements'
'the increasing use of personal information on web-based applications can result in unexpected disclosures. consumers often have only the stated web site policies as a guide to how their information is used, and thus on which to base their browsing and transaction decisions. however, each policy is different, and it is difficult&#x2014;if not impossible&#x2014;for the average user to compare and comprehend these policies. this paper presents a taxonomy of privacy requirements for web sites. using goal-mining, the extraction of pre-requirements goals from post-requirements text artefacts, we analysed an initial set of internet privacy policies to develop the taxonomy. this taxonomy was then validated during a second goal extraction exercise, involving privacy policies from a range of health care related web sites. this validation effort enabled further refinement to the taxonomy, culminating in two classes of privacy requirements: protection goals and vulnerabilities. protection goals express the desired protection of consumer privacy rights, whereas vulnerabilities describe requirements that potentially threaten consumer privacy. the identified taxonomy categories are useful for analysing implicit internal conflicts within privacy policies, the corresponding web sites, and their manner of operation. these categories can be used by web site designers to reduce web site privacy vulnerabilities and ensure that their stated and actual policies are consistent with each other. the same categories can be used by customers to evaluate and understand policies and their limitations. additionally, the policies have potential use by third-party evaluators of site policies and conflicts.a requirements taxonomy for reducing web site privacy vulnerabilities','Security requirements'
'in healthcare, role-based access control systems are often extended with exception mechanisms to ensure access to needed information even when the needs don\'t follow the expected patterns. exception mechanisms increase the threats to patient privacy, and therefore their use should be limited and subject to auditing. we have studied access logs from a hospital epr system with extensive use of exception-based access control. we found that the uses of the exception mechanisms were too frequent and widespread to be considered exceptions. the huge size of the log and the use of pre-defined or uninformative reasons for access make it infeasible to audit the log for misuse. the informative reasons that were given provided starting points for requirements on how the usage needs should be accomplished without exception-based access. with more structured and fine-grained logging, analysis of access logs could be a very useful tool for learning how to reduce the need for exception-based access.a study of access control requirements for healthcare systems based on audit trails from access logs','Security requirements'
'we present an approach to modeling and enforcing usage control requirements on remote clients in service-oriented architectures. technically, this is done by leveraging a trusted software stack relying on a hardware-based root of trust and a trusted java virtual machine to create a measurable and hence trust worthy client-side application environment. we define a model-driven approach to specifying remote policies that makes the technical intricacies of the target platform transparent to the policy modeler.a technical architecture for enforcing usage control requirements in service-oriented architectures','Security requirements'
'a technique to include computer security, safety, and resilience requirements as part of the requirements specification','Security requirements'
'an abstract requirements specification states system requirements precisely without describing a real or a paradigm implementation. although such specifications have important advantages, they are difficult to produce for complex systems and hence are seldom seen in the \"real\" programming world. this paper introduces an approach to producing abstract requirements specifications that applies to a significant class of real-world systems, including any system that must reconstruct data that have undergone a sequence of transformations. tions. it also describes how the approach was used to produce a requirements document for scp, a small, but nontrivial navy communications system. the specification techniques used in the scp requirements document are introduced and illustrated with examples.abstract requirements specification','Security requirements'
'access control requirements for environmental information knowledge base systems','Security requirements'
'in this paper, we discuss how diverging privacy requirements in a multiuser ubiquitous environment can be satisfied. we are investigating how to ensure that the requirements of concerned as well as open&#45;minded users can be satisfied, simultaneously. in order to reduce overhead and increase granularity of the objects to be managed, we propose to cluster all available sensors, creating objects we denote as virtual sensors. all further operations are done on the virtual sensors. we will discuss our management architecture and present our simulation environment showing how the pervasive environment automatically adapts to user requirements and user locations.adapting pervasive systems to multiuser privacy requirements','Security requirements'
'a major challenge in the field of software engineering is to make users trust the software that they use in their every day activities for professional or recreational reasons. trusting software depends on various elements, one of which is the protection of user privacy. protecting privacy is about complying with user&#x2019;s desires when it comes to handling personal information. users&#x2019; privacy can also be defined as the right to determine when, how and to what extend information about them is communicated to others. current research stresses the need for addressing privacy issues during the system design rather than during the system implementation phase. to this end, this paper describes pris, a security requirements engineering method, which incorporates privacy requirements early in the system development process. pris considers privacy requirements as organisational goals that need to be satisfied and adopts the use of privacy-process patterns as a way to: (1) describe the effect of privacy requirements on business processes; and (2) facilitate the identification of the system architecture that best supports the privacy-related business processes. in this way, pris provides a holistic approach from &#x2018;high-level&#x2019; goals to &#x2018;privacy-compliant&#x2019; it systems. the pris way-of-working is formally defined thus, enabling the development of automated tools for assisting its application.addressing privacy requirements in system design: the pris method','Security requirements'
'adoption of requirements engineering','Security requirements'
'[context &#38; motivation] requirements (re)prioritization is an essential mechanism of agile development approaches to maximize the value for the clients and to accommodate changing requirements. yet, in the agile requirements engineering (re) literature, very little is known about how agile (re)prioritization happens in practice. [question/problem] to gain better understanding of prioritization practices, we analyzed the real-life processes as well as the guidance that the literature provides. we compare the results of a literature research with the results of a multiple case study that we used to create a conceptual model of the prioritization process. we set out to answer the research question: \"which concepts of agile prioritization are shared in practice and in literature and how they are used to provide guidance for prioritization.\" [results] the case study yielded a conceptual model of the inter-iteration prioritization process. further, we achieved a mapping between the concepts from the model and the existing prioritization techniques, described by several authors. [contribution] the model contributes to the body of knowledge in agile re. it makes explicit the concepts that practitioners tacitly use in the agile prioritization process. we use this for structuring the mapping study with the literature and plan to use it for analyzing, supporting, and improving the process in agile projects. the mapping gives us a clear understanding of the \'deviation\' between the existing methods as prescribed in literature and the processes we observe in real life. it helps to identify which of the concepts are used explicitly by other authors/ methods.agile requirements prioritization','Security requirements'
'although some agile development advocates suggest that requirements aren\'t really necessary in an agile environment, the department this issue discusses how every system of consequence needs good requirements. additionally, it attempts to clarify what \"doing requirements\" really means.agile requirements','Security requirements'
'this paper presents an aspect-oriented approach to integrated elicitation of functional and security requirements based on use case-driven development. we identify security threats with respect to use cases and adopt threat mitigations for preventing or reducing security threats. to capture crosscutting nature of threats and mitigations, we specify them as aspects that encapsulate pointcuts and advice. a threat (mitigation) pointcut is a collection of join points in use cases at which the use cases are threatened (secured); whereas threat/mitigation advice describes how a threat can become an attack (can be mitigated). eliciting threats and mitigations as aspects provides a structured way for separating functional and security concerns.an aspect-oriented approach to security requirements analysis','Security requirements'
'a formal model of security requirements for enterprise information technology protection is developed. the model is based on set theory and represented using an entity-relationship diagram. components of the model include high level business objectives and their criticality, business requirements and their utilization, resources and their characterization as protector or protected resources, controls and their effectiveness, threats, vulnerabilities, potential exploits, and the resulting impact. an example representation of a formal relationship is provided. the model provides a canonical representation of enterprise security, enables automation and hence rigorous analysis of the security cost and effectiveness, provides for completeness and consistency checking, and offers a means for what-if as well as comparative analysis of security readiness.an enterprise level security requirements specification model','Security requirements'
'web browsers, web servers, java application servers and osgi frameworks are all instances of java execution environments that run more or less untrusted java applications. in all these environments, java applications can come from different sources. consequently, application developers rarely know which other applications exist in the target java execution environment. this paper investigates the requirements that need to be imposed on such a container from a security point of view and how the requirements have been implemented by different java application containers. more specifically, we show a general risk analysis considering assets, threats and vulnerabilities of a java container. this risk analysis exposes generic java security problems and leads to a set of security requirements. these security requirements are then used to evaluate the security architecture of existing java containers for java applications, applets, servlets, osgi bundles, and enterprise java beans. for comparison, the requirements are also examined for a c++ application.an evaluation of java application containers according to security requirements','Security requirements'
'popular definitions of privacy and security have often been given as: privacy-- the rights of &lt;u&gt;individuals&lt;/u&gt; regarding the collection, storage, use and dissemination of personal information pertaining to them. security-- the protection of data from accidental or intentional but unauthorized access, modification, destruction or disclosure. these popular definitons are largely a result of privacy protection legislation, including 1) the fair credit reporting act of 1969, 2) the family educational rights and privacy act of 1974, 3) the privacy act of 1974, 4) state privacy protection laws, and 5) the pending right of privacy act (hr1984) which is the most comprehensive, and potentially the most costly of all. as pertains to data base systems the following generalized definitions are more appropriate: privacy-- the confidentiality of data regarding individuals, financial conditions of an enterprise, criteria for business decisions, design specifications of products, customers of an enterprise or other data which may be thought of as a corporate asset. security-- the control of the computer information environment to accommodate information privacy through assurances that all authorized modifications are reflected and unauthorized modifications are prohibited, and accidental or intentional but unauthorized access is prohibited. this paper summarizes the methods available for insuring privacy and security in data bases and develops: 1) guidelines for developing a security action plan, 2) techniques for evaluating the security features of a data base system, and 3) methods for implementing effective procedural and administrative controls. more detail concerning these guidelines and techniques is included in appendix a of these proceedings.an overview of privacy and security requirements for data bases','Security requirements'
'we present a process to develop secure software with an extensive pattern-based security requirements engineering phase. it supports identifying and analyzing conflicts between different security requirements. in the design phase, we proceed by selecting security software components that achieve security requirements. the process enables software developers to systematically identify, analyze, and finally realize security requirements using security software components. we illustrate our approach by a lawyer agency software example.analysis and component-based realization of security requirements','Security requirements'
'electronic voting refers to the use of computers or computerized voting equipment to cast ballots in an election and it is not an easy task due to the need of achieving electronic voting security requirements. the cryptographic voting protocols use advanced cryptography to make electronic voting secure and applicable.in this paper, formal definitions of security requirements for cryptographic voting protocols (privacy, eligibility, uniqueness, fairness, uncoercibility, receipt-freeness, accuracy, and individual verifiability) are provided, and elaborate checklists for each requirement are presented. the voting problem is clearly defined in terms of security requirements. the voting problem arises from the trade-off between receipt-freeness and individual verifiability. this paper suggests the predefined fake vote (prefote) scheme as an applicable solution to overcome the voting problem. the prefote scheme is not a voting protocol; however, it is a building block that can be used by any voting protocol.analysis of security requirements for cryptographic voting protocols (extended abstract)','Security requirements'
'an intrusion detection system (ids) is a collection of sensors (often in the form of mobile agents) that collect data (security related events), classify them and trigger an alarm when unwanted manipulations to regular network behaviour is detected. activities of attackers and network are time dependent. in the paper, fault trees with time dependencies (fttd) are used to describe intrusions with emphasis put on timing properties. in fttd, events and gates are characterized by time parameters. fttd are used in verification whether the ids reacts sufficiently quick on the intrusions. as an example, \"the victim trusts the intruder&#148; attack is analysed.analysis of timing requirements for intrusion detection system','Security requirements'
'currently security features are implemented andvalidated during the last phases of the softwaredevelopment life cycle. this practice results in lesssecure software systems and higher cost of fixingdefects software vulnerability. to achieve more securesystems, security features must be considered duringthe early phases of the software development process.this paper presents a high-level methodology thatanalyzes the information flow requirements andensures the proper enforcement of information flowcontrol policies. the methodology uses requirementsspecified in the unified modeling language (uml) asits input and stratified logic programming language asthe analysis language. the methodology improvessecurity by detecting unsafe information flows beforeproceeding to latter stages of the life cycle.analyzing information flow control policies in requirements engineering','Security requirements'
'information practices that use personal, financial and health-related information are governed by u.s. laws and regulations to prevent unauthorized use and disclosure. to ensure compliance under the law, the security and privacy requirements of relevant software systems must be properly aligned with these regulations. however, these regulations describe stakeholder rules, called rights and obligations, in complex and sometimes ambiguous legal language. these \"rules\" are often precursors to software requirements that must undergo considerable refinement and analysis before they are implementable. to support the software engineering effort to derive security requirements from regulations, we present a methodology to extract access rights and obligations directly from regulation texts. the methodology provides statement-level coverage for an entire regulatory document to consistently identify and infer six types of data access constraints, handle complex cross-references, resolve ambiguities, and assign required priorities between access rights and obligations to avoid unlawful information disclosures. we present results from applying this methodology to the entire regulation text of the u.s. health insurance portability and accountability act (hipaa) privacy rule.analyzing regulatory rules for privacy and security requirements','Security requirements'
'nowadays the importance of a dedicated information security management (ism) is undisputedly. one essential task in realizing a company\'s ism is to implement a compulsory operational risk management (orm) aiming also at ensuring the compliance with certain standards. the risks addressed by orm prevalently result from information systems. a promising approach is to focus on business processes to combine the technical system focused perspective of security management with the more centralized perspective of operational risk management. within this paper first we will deliver an introduction an integrated it risk management and its corresponding decisions. afterwards we will derive requirements for application systems in order to supporting decisions in it-risk management. for this purpose a catalogue of requirements will be developed. based on this catalogue software systems for it security management and operational risk management were examined with regard to their adequacy for decision support in it-risk management.applications for it-risk management - requirements and practical evaluation','Security requirements'
'the task of detecting, repulsing and preventing attacks in wireless environments is becoming more and more difficult as a result of several factors including defective design and improper requirements specification of wireless intrusion detection systems. systems specifications provide the basics and framework to design an effective system, and for wireless intrusion detection systems, this assumes greater importance due to the complexity of wireless environments, and the architectural and functional constraints created by such environments. in the light of this, this paper discusses the architectural and functional issues involved in intrusion systems design for mobile and adhoc networks (wireless environments); and also proposes a new way of making systems requirements specification more effective and responsive to the needs of wireless implementations.architectural and functional issues in systems requirements specifications for wireless intrusion detection systems implementation','Security requirements'
'this paper presents an aspect&#45;oriented approach to integrated specification of functional and security requirements based on use&#45;case&#45;driven software development. it relies on explicit identification of security threats and threat mitigations. we first identify security threats with respect to use&#45;case based functional requirements in terms of security goals and the stride category. then, we suggest threat mitigations for preventing or reducing security threats. to capture the crosscutting nature of threats and mitigations, we specify them as aspects that encapsulate pointcuts and advice. this provides a structured way for separating functional and security concerns and for analysing the interaction between them.aspect&#45;oriented specification of threat&#45;driven security requirements','Security requirements'
'aspect-oriented requirements engineering (aore) techniques provide new composition mechanisms to specify and reason about dependencies that crosscut elements of a requirements specification. this paper introduces the basic concepts of aspect-oriented requirements engineering and its support for compositional reasoning--reasoning about dependencies and interactions--over a requirements specification. typical applications of aspect-oriented requirements engineering techniques are also highlighted. the paper concludes with an annotated bibliography of key tools, techniques and application studies.aspect-oriented requirements engineering','Security requirements'
'the requirements traceability matrix (rtm) supports many software engineering and software verification and validation (v&v) activities such as change impact analysis, reverse engineering, reuse, and regression testing. the generation of rtms is tedious and error-prone, though, thus rtms are often not generated or maintained. automated techniques have been developed to generate candidate rtms with some success. when using rtms to support the v&v of mission-or safety-critical systems, however, a human analyst must vet the candidate rtms. the focus thus becomes the quality of the final rtm. this paper investigate show human analysts perform when vetting candidate rtms. specifically, a study was undertaken at two universities and had 26 participants analyze rtms of varying accuracy for a java code formatter program. the study found that humans tend to move their candidate rtm toward the line that represents recall = precision. participants who examined rtms with low recall and low precision drastically improved both.automated requirements traceability','Security requirements'
'using autonomy requirements engineering, software developers can determine what autonomic features to develop for a particular system as well as what artifacts that process might generate.autonomy requirements engineering','Security requirements'
'quality is meeting requirements - or is it? the authors challenge this familiar metaphor for requirements and introduce a new one based on their experience in an industry that seems far from software development or is it?.beyond requirements','Security requirements'
'a model for requirements engineering is described which uses a taxonomy of goal-types to guide further analysis. goals are classified according to the desired system state described in requirements statements. heuristics then prompt further description of functions according to each goal class. other analyses encourage expansion of goal statements into specification of objects agents, activity and information processes. these link functional decomposition of requirements to object oriented modelling. implications of the model and supporting tools are briefly reviewed.bridging the requirements gap','Security requirements'
'calculating with requirements','Security requirements'
'requirements engineering is a creative process in which stakeholders work together to create ideas for new software systems that are eventually expressed as requirements. this paper reports a workshop that integrated creativity techniques with different types of use case and system context modeling to discover stakeholder requirements for easm, a future air space management software system to enable the more effective, longer-term planning of uk and european airspace use. the workshop was successful in that it provided a range of outputs that were later assessed for their novelty and usefulness in the final specification of the easm software. the paper describes the workshop structure, gives examples of outputs from it, and uses these results to answer 2 research questions about the utility of creativity techniques and workshops that had not been answered in previous research.can requirements be creative? experiences with an enhanced air space management system','Security requirements'
'ensuring compliance to laws, regulations, and standards in a constantly changing business environment is a major challenge for companies. so, organizations have an increasing need for systematic approaches to manage compliance throughout the business process (bp) life cycle. a new pattern-based approach, including a toolset, captures and manages bp compliance requirements. this approach is a first step toward comprehensive management of bp compliance and acts as a springboard to fully automate and continuously audit bps.capturing compliance requirements','Security requirements'
'we discuss a number of essential problems of software requirements engineering, related to management, organisations, users, stakeholders, methodology, tools, and education. most of the problems seem to have their roots in how requirements engineering is appreciated at the business management and it management levels.challenges in requirements engineering','Security requirements'
'planning of requirements changes is often inaccurateand implementation of changes is time consuming anderror prone. one reason for these problems is impreciseand inefficient approaches to analyze the impact ofchanges. this thesis proposes a precise and efficientimpact analysis approach that focuses on functional systemrequirements changes of embedded control systems. itconsists of three parts: (1) a fine-grained conceptual tracemodel, (2) process descriptions of how to establish tracesand how to analyze the impact of changes, and (3)supporting tools. empirical investigation shows that theapproach has a beneficial effect on the effectiveness andefficiency of impact analyses and that it supports a moreconsistent implementation of changes.change-oriented requirements traceability','Security requirements'
'evolving model of software requirements is very important to the success of software engineering, but we need theoretical principles to lead us to do it. according to the nonlinear dynamic requirements decomposition equations developed by us based on the nonlinear dynamic system, we discuss the chaotic phenomena of software requirements through simulations mainly in the paper. it is impossible for us to get all sub-requirements through requirements decomposition when the decomposition is in chaos. the occurrence of chaos is determined by requirements decomposition rate parameter (rdrp). the change of initial values of requirements may cause a large change of decomposed results, but will not affect the occurrence of chaos. when rdrp is too large, a small change may cause the decomposition into an unstable state from a stable state. the decomposition may also be in chaos even if it is nearly completed when rdrp is unsuitable. therefore, it is better for us to let the decomposition outside chaos to get full sub-requirements.chaos of software requirements','Security requirements'
'this article presents a framework for characterizing architecturally significant requirements (asrs) on the basis of an empirical study using grounded theory. the study involved interviews with 90 practitioners with an accumulated 1,448 years of software development experiences in more than 500 organizations of various sizes and domains. these findings could provide researchers with a framework for discussing and conducting further research on asrs and can inform researchers\' development of technologies for dealing with asrs. the findings also enrich understanding of requirements and architecture interactions, allowing the twin peaks to move from aspiration to reality.characterizing architecturally significant requirements','Security requirements'
'it is sometimes necessary to collaborate with individuals and organizations which should not be fully trusted. collaborators must be authorized to access information systems some of the data in which, typically, should be withheld. new collaborations require dynamic alterations to security provisions. solutions based on extending access control to deal with collaborations are either awkward and costly, or unreliable. an alternative approach, complementing basic access control, is results filtering. content filtering is also costly, but provides a number of benefits not obtainable with access control alone. the most important is that the complexity of setting up and maintaining isolating information cells for every combination of access rights is avoided. new classes of collaborators can be added without requiring a reorganization of the entire information structure. there is no overhead for internal use. since content of documents, not their labels, is checked, misfiling will not cause inappropriate release. the approach used in the tihi/saw projects at stanford uses simple rules to drive filtering primitives. the filters run on a modest, but dedicated computer managed by a security officer. the rules implement the security policy and balance manual effort and complexity. the functional allocation of responsibilities is good. result filtering can also be used to implement pure intrusion detection, since it is invisible. the intruder can be given an impression of success, while becoming a target for monitoring or cover storiescollaboration requirements','Security requirements'
'we present a study of on the goal-oriented modeling of re processes executed by a practicing systems development team. the research combines an empirical case study of re practices with the evaluation and simulation capability of i* modeling. our analysis focuses on a system implementation project at a mid-size u.s. university and applies the theory of distributed cognition to generate a range of design insights for goal identification and process enhancement.computing requirements','Security requirements'
'the invisibility of the individuals and groups that gave rise to requirements artifacts has been identified as a primary reason for the persistence of requirements traceability problems. the paper presents an approach based on modelling the dynamic contribution structures underlying requirements artifacts, which addresses this issue. it shows how these structures can be defined, using information about the agents who have contributed to artifact production, in conjunction with details of the numerous traceability relations that hold within and between artifacts themselves. it further outlines how the approach can be implemented, demonstrates the potential it provides for \"personnel-based\" requirements traceability, and discusses issues pertinent to its uptake.contribution structures [requirements artifacts]','Security requirements'
'control requirements for control systems','Security requirements'
'uscreates, a behavior change consultancy, designs creative spaces that can facilitate conversations to collect project requirements. these spaces and the conversations they house are explained through a project that aims to improve the health and wellbeing on migrant workers in mid essex.creative requirements conversations','Security requirements'
'evidence is mounting that aspect-oriented programming is useful for (re-)structuring the many concerns that software is designed to address. many of these concerns often arise in the problem domain, and, therefore, there is a growing effort to examine \'early aspects\' - to identify and represent concerns that arise during software requirements engineering and design, and to determine how these concerns interact. but can one seek to identify aspects too early? while identifying concerns during requirements elicitation may indeed be profitable, the notion of crosscutting concerns, indeed of crosscutting requirements, may only make sense when elements of a solution also begin to be explored. there are two consequences of this: a case for more interleaving of the processes of requirements engineering and design, and a case for the explicit development of specifications that map the problem and solution structures. we elaborate and discuss this thesis, and offer an alternative research agenda for aspect-oriented requirements engineering.crosscutting requirements','Security requirements'
'the key to mobile payment acceptance is in the hands ofcustomers. in this paper we use the results of the mobilepayment survey mp1 in order to identify and roughlyweigh the most relevant acceptance criteria. the outcomeof the paper is an evaluation scheme containing the coveredpayment scenarios, important main criteria (security,costs and convenience) and additional functionalityrequirements for each mp procedure. the scheme isbased on empirical results and can assess a given mpprocedure with regard to customer acceptance as well asto compare different procedures. the operational mpprocedures paybox, i-mode and vodafone m-pay areexamined and compared according to the scheme. finally,a prospect is given to possible further developmentof mobile payment procedures in the direction of an integrativeuniversal mobile payment system (umps).current mobile payment procedures on the german market from the view of customer requirements','Security requirements'
'customer requirements for security in relational database management','Security requirements'
'in the event of a release of toxic contaminant, either accidental or intentional, it would be useful to have an evolvable sensor network for tracking the toxic plume. in such an event, homeland security or dod personnel are responsible for modeling the transport and dispersion of the plume. to do this requires specific source and meteorological data. such data may not be available; however, it might be recoverable from concentration data monitored by a mobile sensor network. to be useful for assimilating the monitored data into a dispersion model, the sensor network must be sited strategically and should be evolvable to follow the plume of toxic contaminant. this paper discusses the requirements of such a network from the point of view of data needs for assimilating the sensor data into the transport and dispersion models.data requirements from evolvable sensor networks for homeland security problems','Security requirements'
'one early definition of requirements engineering has been the \"establishment of a vision in context\". as software technologies transcend all aspects of life, technology, and business, the contexts to be considered are becoming ever more rich and more dynamic--cyberphysical systems and social software mark just two of the important trends.design requirements','Security requirements'
'over the past years of developing software i\'ve increased efforts to understand what makes the development process successful and what makes it fail. this paper describes how starting to develop using agile methodologies solved many but not all problems. and how subsequently discovering constantine and lockwood\'s usage-centered design and incorporating it into an agile development process increased our likelihood of success. in addition to being more likely to meet end-user expectations, uc-d helped our team do that sooner, guess right more often, and achieve our goal of releasing usable software earlier. u-cd represents a repeatable, collaborative approach to interaction design that can be incorporated into an agile software development process.designing requirements','Security requirements'
'the iso/iec common criteria for information technology security evaluation could benefit from focusing on development, rather than evaluation, to provide assurance.developer-focused assurance requirements','Security requirements'
'quantum mechanical calculations based on density functional theory (dft) are used to study dynamic behavior of shocked polymeric nitrogen, a novel energetic material. we report results on system sizes in excess of 3,000 atoms. such calculations on system sizes within the 1,000 atom range remain problematic using standard implementations of dft. we evaluate the feasibility of using several available dft codes for this work through comparison of scalability and resource requirements. in this study, we utilize a recently developed highly-scalable localized orbital dft code, cp2k, designed to treat large systems. scaling and performance benchmarks of the cp2k on several department of defense (dod) high performance computing (hpc) computers are presented for a variety of system sizes and shapes. additionally, we report preliminary calculations on the conventional explosive nitromethane. in those calculations in excess of 3,500, atoms are treated.direct quantum mechanical simulations of shocked energetic materials supporting future force insensitive munitions requirements','Security requirements'
'sustainability has become one of the \"grand challenges\" of our civilization. because of their pervasiveness, the way we design, and consequently use, software-intensive systems has a significant impact on sustainability. this gives software requirements engineering an important role to play in society. however, there is currently no specific support for handling sustainability requirements, while such support exists and has proved useful for other quality requirements like security or usability. this paper reports on a software project in which sustainability requirements were treated as first class quality requirements, and as such systematically elicited, analysed and documented. the authors intended to assess how current techniques support these activities. beyond raising awareness on the importance of sustainability concerns in requirements engineering, this experience report suggests that, while a lot of work remains to be done, small and easy steps may already lead us to more sustainable systems. it also contributes to the agenda of requirements engineering researchers concerned with sustainability.discovering sustainability requirements','Security requirements'
'distributed software systems are the basis for innovative applications. the key for achieving survivable and maintainable distributed systems is agility because the nondeterministic nature of distribution would otherwise leave the system uncontrollable, especially in emerging mobile ad-hoc networks. a mobile ad-hoc network (manet) is based on a self-organizing and rapidly deployed network of mobile services to collaborate without using any pre-existing fixed network infrastructure. survivability is defined as the capability of a service to fulfill its mission in a timely manner, even in the presence of attacks, failures, or accidents. there are four key survivability properties: resistance, recognition, recovery and adaptation. recovery, a hallmark of survivability, is the capability to maintain critical components and resource during attack, limit the extent of damage, and restore full services following attack. exception handling is a way to deals with the recovery aspect of survivability. resistance can be viewed as the process of limiting access to critical and vulnerable resources only to authorized users, programs, processes, or other systems. this paper bridges the analysis of secure business process and its recovery aspect in terms of exception handling in the context of access control requirements. we propose an integrated approach to engineer a survivable distributed system through dynamic regeneration of workflow specifications in the context of business process execution language for web services (bpel) and extensible access control markup language (xacml).dynamic regeneration of workflow specification with access control requirements in manet','Security requirements'
'caring for security at requirements engineering time is amessage that has finally received some attention recently.however, it is not yet very clear how to achieve thissystematically through the various stages of therequirements engineering process.the paper presents a constructive approach to themodeling, specification and analysis of application-specificsecurity requirements. the method is based on agoal-oriented framework for generating and resolvingobstacles to goal satisfaction. the extended frameworkaddresses malicious obstacles (called anti-goals) set up byattackers to threaten security goals. threat trees are builtsystematically through anti-goal refinement until leafnodes are derived that are either software vulnerabilitiesobservable by the attacker or anti-requirementsimplementable by this attacker. new security requirementsare then obtained as countermeasures by application ofthreat resolution operators to the specification of the anti-requirementsand vulnerabilities revealed by the analysis.the paper also introduces formal epistemic specificationconstructs and patterns that may be used to support aformal derivation and analysis process. the method isillustrated on a web-based banking system for whichsubtle attacks have been reported recently.elaborating security requirements by construction of intentional anti-models','Security requirements'
'dependability is gradually being recognized as an important issue for critical software systems. so far, there still lacks an effective approach for dependability requirements elicitation. this paper presents our work-in progress on an hazop-based approach (called dre-hazop) to eliciting the dependability requirements in a systematical way.elicitation of dependability requirements','Security requirements'
'confidentiality, the protection of unauthorized disclosure of information, plays an important role in information security of software systems. security researchers have developed numerous approaches on how to implement confidentiality, typically based on cryptographic algorithms and tight access control. however, less work has been done on defining systematic methods on how to elicit and define confidentiality requirements in the first place. moreover, most of these approaches are illustrated with simulated examples that do not capture the richness of real world experience. this paper reports on our experiences eliciting confidentiality requirements in a real world project in the health care area. the method applied originates from the m.sc. thesis of one of the authors and is still considered work in progress. still, valuable insight into issues of confidentiality requirements engineering can be gained from this case study and we expect that its publication will become a basis for discussion and the definition of a further research agenda in this area.eliciting confidentiality requirements in practice','Security requirements'
'eliciting requirements','Security requirements'
'use cases have become increasingly common during requirements engineering, but they offer limited support for eliciting security threats and requirements. at the same time, the importance of security is growing with the rise of phenomena such as e-commerce and nomadic and geographically distributed work. this paper presents a systematic approach to eliciting security requirements based on use cases, with emphasis on description and method guidelines. the approach extends traditional use cases to also cover misuse, and is potentially useful for several other types of extra-functional requirements beyond security.eliciting security requirements with misuse cases','Security requirements'
'this mini tutorial reviews application of psychological theories in requirements engineering. theories from psychology of emotion and motivation are introduced and applied in a scenario-based process to analyse affective situations which might be produced by user-oriented re. use of agent technology in storyboards and scenario analysis of affective situations is described and illustrated with case studies in health informatics for persuasive technology applications.emotional requirements engineering','Security requirements'
'unobtrusive user authentication is more convenient than explicit interaction and can also increase system security because it can be performed frequently, unlike the current \'\'once explicitly and for a long time\'\' practice. existing unobtrusive biometrics (e.g., face, voice, gait) do not perform sufficiently well for high-security applications, however, while reliable biometric authentication (e.g., fingerprint or iris) requires explicit user interaction. this work presents experiments with a cascaded multimodal biometric system, which first performs unobtrusive user authentication and requires explicit interaction only when the unobtrusive authentication fails. experimental results obtained for a database of 150 users show that even with a fairly low performance of unobtrusive modalities (equal error rate above 10\%), the cascaded system is capable of satisfying a security requirement of a false acceptance rate less than 0.1\% with an overall false rejection rate of less than 0.2\%, while authenticating unobtrusively in 65\% of cases.empirical evaluation of combining unobtrusiveness and security requirements in multimodal biometric systems','Security requirements'
'text-based passwords are still the most commonly used authentication mechanism in information systems. we took advantage of a unique opportunity presented by a significant change in the carnegie mellon university (cmu) computing services password policy that required users to change their passwords. through our survey of 470 cmu computer users, we collected data about behaviors and practices related to the use and creation of passwords. we also captured users\' opinions about the new, stronger policy requirements. our analysis shows that, although most of the users were annoyed by the need to create a complex password, they believe that they are now more secure. furthermore, we perform an entropy analysis and discuss how our findings relate to nist recommendations for creating a password policy. we also examine how users answer specific questions related to their passwords. our results can be helpful in designing better password policies that consider not only technical aspects of specific policy rules, but also users\' behavior in response to those rules.encountering stronger password requirements','Security requirements'
'encryption: needs, requirements and solutions in banking networks','Security requirements'
' applications that continuously gather and disclose personal information about users are increasingly common. while disclosing this information may be essential for these applications to function, it may also raise privacy concerns. partly, this is due to frequently changing context that introduces new privacy threats, and makes it difficult to continuously satisfy privacy requirements. to address this problem, applications may need to adapt in order to manage changing privacy concerns. thus, we propose a framework that exploits the notion of privacy awareness requirements to identify runtime privacy properties to satisfy. these properties are used to support disclosure decision making by applications. our evaluations suggest that applications that fail to satisfy privacy awareness requirements cannot regulate users information disclosure. we also observe that the satisfaction of privacy awareness requirements is useful to users aiming to minimise exposure to privacy threats, and to users aiming to maximise functional benefits amidst increasing threat severity. engineering adaptive privacy: on the role of privacy awareness requirements','Security requirements'
'  z is a declarative, non&dash;executable specification language&semi; its diffusion in the field of requirements engineering outside academia is slow but growing. in this paper we focus on some methods for analyzing and testing z specification documents, with special emphasis on non&dash;sequential systems specifications. we describe two techniques we have adopted: the former allows the specifier to add to the requirements document a number of properties that then can be checked using a formal semantics&semi; the latter makes it possible to build directly from the requirements specification document a distributed prototype which can be executed and tested over a network of workstations.engineering formal requirements','Security requirements'
'many software-intensive systems have significant safety and security ramifications and need to have their associated safety- and security-related requirements properly engineered. it has been observed by several consultants, researchers, and authors that inadequate requirements are a major cause of accidents involving software-intensives systems, and poor security requirements prevent the early incorporation of security concerns into the architecture. yet in practice, there is very little interaction between the requirements, safety, and security disciplines and little collaboration between their respective communities. most requirements engineers, safety engineers, and security engineers know little about their respective disciplines. also, safety and security engineering typically concentrates on architectures and designs rather than requirements because hazard and threat analysis typically depends on the identification of hardware and software components, the failure of which can cause accidents and vulnerabilities which can enable successful attacks. this leads to safety- and security-related requirements that are often ambiguous, incomplete, unverifiable, and even missing. this tutorial begins with a single common realistic example of a safety- and security-critical system that will be used throughout to provide good examples of safety- and security-related requirements. the tutorial provides a consistent ontology of safety, security, and requirements concepts and terminology, provides clear definitions and descriptions of the different kinds of safety- and security-related requirements, and finishes with a practical consistent combined process for engineering them.engineering safety and security related requirements for software intensive systems','Security requirements'
'specifying correct and complete access control policies is essential to secure data and ensure privacy in information systems. traditionally, policy specification has not been an explicit part of the software development process. this isolation of policy specification from software development often results in policies that are not in compliance with system requirements and/or organizational security and privacy policies, leaving the system vulnerable to data breaches. this paper presents the results and lessons learned from a case study that employs the requirements-based access control analysis and policy specification (recaps) method to specify access control policies for a web-based event registration system. the recaps method aids software and security engineers in specifying access control policies derived from requirements specifications and other available sources. our case study revealed that the recaps method helps identify inconsistencies across various software artifacts, such as requirements specification, database design, and organizational security and privacy policies. had these problems not been identified and resolved, they would have crippled later phases of software development, resulted in missing or incomplete system functionality, and compromised the system\'s security and privacy. this case study reinforces, validates, and extends our previous recommendations that access control policy specification should be an integral part of the software development process for information systems to achieve information assurance and improve the quality of the information system.ensuring compliance between policies, requirements and software design','Security requirements'
'one of the highest priorities of system requirements needed in software development industry is security requirements. however, to identify the complete and correct software security requirements are a challenging task especially creating enterprise assets security requirements. enterprise assets security requirements are to identify security basic needs, to assess risks, to establish security approach and service, and to specify external enterprise consideration including confidentiality, integrity,availability, and accountabilityconcerns. moreover, these may be applied to other security requirements such as identification and authentication, access control, firewall architecture, etc. security patterns may be used to create this security requirements but understanding, analyzing and transforming from security patterns to security requirements are difficult to accomplish. we proposed a grammar, called esrmg (enterprise security and risk management grammar), and a prototyping tool based on security patterns in a scope of enterprise asset identification and risk managements which are the fundamental of enterprise security requirements. the proposed grammar and tool are beneficial for any organization to construct enterprise security requirements and may help reduce cost and time in overall of system development.enterprise assets security requirements construction from esrmg grammar based on security patterns','Security requirements'
'erroneous requirements','Security requirements'
'the goal of this paper is to determine bounds for estimating minimum sample size requirement for reliable biometric identification. a new approach for the reliable estimation of the minimum sample size is proposed for arbitrary ensemble of subjects. a bound on number of acquisitions/samples per subject is arrived through an iterative procedure that tests sequences for user-specific sequences. the approach proposed in this paper is supported by information theoretic measures. these results are fundamental to the integration of concepts from statistics, complexity and probabilistic (borel) measure spaces. we evolve a novel concept of information equivalence in comparing random sequences for its information content. furthermore, the problem of missing or lost/corrupted matching scores is also investigated. the solution for these missing biometric matching scores is based on completeness of certain typical space and these scores can be estimated using proposed iterative algorithm.estimating sample size requirements for reliable personal authentication using user-specific samples','Security requirements'
'abstract: evolutionary prototyping focuses on gathering a correct and consistent set of requirements. the process lends particular strength to building quality software by means of the ongoing clarification of existing requirements and the discovery of previously missing or unknown requirements. traditionally, the iterative reexamination of a system \'s requirements has not been the panacea that practitioners sought, due to the predisposition for requirements creep and the difficulty i n managing it. this paper proposes the combination of evolutionary prototyping and an aggressive risk- mitigation strategy. together, these techniques support successful requirements discovery and clarification, and they guard against the negative effects of requirements creep. we embody these techniques in a comprehensive software development model, which we call the epram (evolutionary prototyping with risk analysis and mitigation) model. the model was intentionally designed to comply with the level 2 key process area of the software engineering institute \'s capability maturity model. validation is currently underway on several software development efforts that employ the model to support the rapid development of electronic commerce applications.evolving beyond requirements creep','Security requirements'
'in a top-down, language-based design methodology, requirements can be specified in an executable format, reducing the ambiguity typically encountered with written requirements, and serving as the starting point for the evolution of more detailed requirements and design specifications. as part of the rapid prototyping of application specific signal processors (rassp) program, a vhdl executable requirement was constructed to capture the interface timing and functional requirements for an embedded processor intended to form images in real-time for a synthetic aperture radar. this paper includes a brief description of the application, then describes the implementation strategy and issues associated with the development of the vhdl executable requirement, emphasizing the importance of the vhdl test bench concept. anticipated benefits of widespread utilization of vhdl executable requirements are discussed along with potential impediments to adoption. areas for additional research and development are identified.executable requirements','Security requirements'
'although encipherment has often been discussed as a means to protect computer data, its costs are not well established. five experiments were conducted to measure the cpu time on a cdc 6400 required by additive ciphers programmed both in assembly language and in fortran: a &#8220;null transformation&#8221; to measure the time to move data without encipherment; encipherment with a one-word key; encipherment with a 125-word key; double key encipherment; and encipherment using a pseudo random key. the results were analyzed for consistency over 100 runs, and the effects of constant and intermittent errors were considered.timing rates for assembly language encipherment ranged from 498,800 characters per second for a pseudo random key cipher to 2,092,000 characters per second for a constant one-word key cipher. the latter is almost equivalent to the rate required simply to move data without encipherment. fortran tests required over four times as much cpu time. this paper introduces the idea of enciphering time coefficient the ratio of enciphering time to the time taken to fetch and store data without encipherment.execution time requirements for encipherment programs','Security requirements'
'experiencing requirements','Security requirements'
'contribution structures offer a way to model the network of people who have participated in the requirements engineering process. they further provide the opportunity to extend conventional forms of artifact-based requirements traceability with the traceability of contributing personnel. in this paper, we describe a case study that investigated the modeling and use of contribution structures in an industrial project. in particular, we demonstrate how they made it possible to answer previously unanswerable questions about the human source(s) of requirements. in so doing, we argue that this information addresses problems currently attributed to inadequate requirements traceability.extended requirements traceability','Security requirements'
'managing requirements modeling and prototyping is risky. if things go awry, projects can spiral out of control. over the years, with the help of colleagues from industry and academia, the author has identified a requirements modeling and prototyping process that is fast, powerful, cost-effective, sane, and objective. the main lesson learnt is that throwaway prototyping (sometimes called exploratory prototyping) is always cost-effective and always improves specifications. the process has nine steps: elicit initial requirements; model requirements; identify constraints; prioritize initial requirements; design; evaluate designs; specification; interactive prototyping; and requirements validation.fast, cheap requirements','Security requirements'
'our goal is to develop a method for creating models of functional software requirements in which features are explicit, with a focus on the automotive software domain. in the current state of our proposed method, feature requirements are modelled using state machines that describe changes to a shared domain model.feature-oriented requirements modelling','Security requirements'
'although there is a substantial amount of work on formal requirements for two and three-party key distribution protocols, very little has been done on requirements for group protocols. however, since the latter have security requirements that can differ in important but subtle ways, we believe that a rigorous expression of these requirements can be useful in determining whether a given protocol can satisfy an application\'s needs. in this paper we make a first step in providing a formal understanding of security requirements for group key distribution by using the npatrl language, a temporal requirement specification language for use with the nrl protocol analyzer. we specify the requirements for gdoi, a protocol being proposed as an ietf standard, which we are formally specifying and verifying in cooperation with the msec working group.formalizing gdoi group key management requirements in npatrl','Security requirements'
'legal prescriptions are increasingly impacting on infor- mation systems and on organisations that must comply with them in order to avoid to be prosecuted or fined. addressing law compliance in early phases of the requirements analy- sis helps in improving the alignment of information systems with the law. in this paper, we point out ontological dif- ferences between legal concepts and requirements and set the basis for a systematic process able to support decision making about requirements for law compliant systems.from laws to requirements','Security requirements'
'requirements analysis phase of information system development is still predominantly human activity. software requirements are commonly written in natural language, at least during the early stages of the development process. in this paper we present a simple method for automated analysis of requirements specifications for data-driven applications. our approach is rule-based and uses dependency syntax parsing for the extraction of domain entities, attributes, and relationships. the results obtained from several test cases show that hand-crafted rules applied on the dependency parse of the requirements sentences might offer a feasible approach for the task. finally, we discuss applicability and limitations of the presented approach.from requirements to code','Security requirements'
'from requirements to services','Security requirements'
'the broadening of the bandwidth of the telecommunications network brings with it the need for faster higher-capacity switching systems. to meet this need, asynchronous transfer mode (atm) switching, optical switching, and other technologies are being pursued. the influence of this environment on networks is discussed from two aspects: the business administration aspect,which involves the issues of competition and cooperation, reduction in labor, and new services, and the technological aspect, which involves the issues of open and global networks and the fusion of telecommunications and information processing technologies. on that basis, there is clarification of what is required of the switching system in terms of architecture, connectivity, reliability, and security. the form of the expected switching system in terms of architecture, and the enhancement of call control, operations, and the user-network interface is describedfuture switching system requirements','Security requirements'
'with the increasing popularity of internet based businesses, more and more e-commerce web sites are being constructed. these sites are often constructed using robust, reliable, scalable and high performing middleware. however, in order to build an enterprise wide e-commerce system that can stand the test of millions of \'hits\' a day, and provide fast response times to internet clients, it is critical to correctly define the middleware requirement of the organisation, and hence making the right purchasing decision. this paper presents a systematic approach to gathering and analysing middleware requirements, documentation process, and the corresponding requirement evaluation techniques.gathering middleware requirements','Security requirements'
'use case modeling is a commonly used technique to describe functional requirements in requirements engineering. typically, use cases are captured from textual requirements documents describing the functionalities the system should meet. requirements elicitation, analysis and modeling is a time consuming and error-prone activity, which it is not usually supported by automated tools. this paper tackles this problem by taking free-form textual requirements and offering a semi-automatic process for generation of domain models, such as use cases. our goal is twofold: (i) reduce the time spent to produce requirements artifacts; and (ii) enable future application of model-driven engineering techniques to maintain traceability information and consistency between textual and requirements visual models artifacts.generating requirements analysis models from textual requirements','Security requirements'
'current user-centred software engineering (ucse) approaches provide many techniques to combine know-how available in multidisciplinary teams. although the involvement of various disciplines is beneficial for the user experience of the future application, the transition from a user needs analysis to a structured interaction analysis and ui design is not always straightforward. we propose storyboards, enriched by metadata, to specify functional and non-functional requirements. accompanying tool support should facilitate the creation and use of storyboards. we used a meta-storyboard for the verification of storyboarding approaches. get your requirements straight','Security requirements'
'research has been actively proposed into how to specify requirements in the upper stream of software development. for example, the main research issues regarding structured analysis and object oriented analysis methodologies include requirements elicitation, modeling, and validation of specifications to give a starting point for software development. at the same time, another area of research has emerged that recognizes the importance of guaranteeing requirements quality by goals. as the impact of it penetrates to mobile devices, information appliances and automobiles, goal oriented requirements engineering (gore) approaches for performance and safety in embedded systems have been proposed. non-functional requirements (nfrs) such as business strategy, security and privacy, are now being formalized by requirements engineering (re) technologies, because enterprise business is now heavily influenced by it, for example in e-business. as it is fast becoming ubiquitous in society, the importance of goal orientation will increase as socio-technology enables visualization of the role of software in social systems. in this paper, we discuss the current states and trends of gore from the viewpoints of both academia and industry.goal oriented requirements engineering','Security requirements'
'goals are a logical mechanism for identifying, organizing and justifying software requirements. strategies are needed for the initial identification and construction of goals. in this paper we discuss goals from the perspective of two themes: goal analysis and goal evolution. we begin with an overview of the goal-based method we have developed and summarize our experiences in applying our method to a relatively large example. we illustrate some of the issues that practitioners face when using a goal-based approach to specify the requirements for a system and close the paper with a discussion of needed future research on goal-based requirements analysis and evolution. keywords: goal identification, goal elaboration, goal refinement, scenario analysis, requirements engineering, requirements methodsgoal-based requirements analysis','Security requirements'
'abstract: goals capture, at different levels of abstraction, the various objectives the system under consideration should achieve. goal-oriented requirements engineering is concerned with the use of goals for eliciting, elaborating, structuring, specifying, analyzing, negotiating, documenting, and modifying requirements. this area has received increasing attention over the past few years. the paper reviews various research efforts undertaken along this line of research. the arguments in favor of goal orientation are first briefly discussed. the paper then com-pares the main approaches to goal modeling, goal specification and goal-based reasoning in the many activities of the requirements engineering process. to make the discussion more concrete, a real case study is used to suggest what a goal-oriented requirements engineering method may look like. experience with such approaches and tool support are briefly discussed as well.goal-oriented requirements engineering','Security requirements'
'this paper proposes a requirements-driven security engineering approach for analyzing application-specific security requirements that are formally derived into security design preserving security requirements properties. the approach adopts the kaos framework to formally construct a complete and consistent security requirements model that is extended using the b method to produce security design and further implementation while preserving requirements properties. this unique treatment of secure software engineering is systematic, constructive and considers security early in development.goal-oriented, b-based formal derivation of security design specifications from security requirements','Security requirements'
'pervasive grid adoption is predicated on the availability of widely deployed usable software and a user community willing to use it. currently, widespread adoption of grids, even within technically sophisticated communities, is limited, and determining and eliminating these barriers to adoption are essential in order for grids to becoming widely capitalized. through a series of face-to-face interviews conducted during the summer of 2004, we have identified issues relating to job submission, file transfer, usability, and systems management that must be resolved in order to improve the usability of grid infrastructures. the background to these issues and some possible solutions are described in this paper.grid user requirements--2004','Security requirements'
'groupware-assisted requirements assessment','Security requirements'
'microstrip patch antennas provide good compatibility with communication system\'s printed circuitry. here a circular microstrip patch antenna with circular configuration of ground plane and substrate has been designed for gps using a cavity line model. rhcp is obtained by placing coaxial pins quadraturly and symmetrically along the two main axes and pins are fed through a hybrid feed arrangement. circular ground plane effect is investigated for return loss and vswr, which are typical parameter used to study the behavior of antennas. the results obtained provide a workable antenna design for incorporation in gps receivers.high precision antenna design with hybrid feeds for gps requirements','Security requirements'
'the development of a common criteria protection profile for high-robustness separation kernels requires explicit modifications of several common criteria requirements as well as extrapolation from existing (e.g., medium robustness) guidance and decisions. the draft u.s. government protection profile for separation kernels in environments requiring high robustness (skpp) is intended to be applicable to a class of products (the target of evaluation, or toe) that includes, but is not limited to, real time and embedded systems. this paper describes certain skpp concepts and requirements and provides underlying motivations and rationale for their inclusion in the skpp. primary areas of focus are the security requirements regarding information flow, dynamic configuration, and the application of the principle of least privilege to restrict actions of active entities.high robustness requirements in a common criteria protection profile','Security requirements'
'[context &#38; motivation] large contractual projects often have to comply against government regulations and standards. [question/problem] in such a context, the contractual document can be voluminous, and there can be a large number of standards and regulations to follow. these documents typically form a complex interrelationship network. this means that in the requirements engineering (re) process, this network needs to be analysed for deriving project requirements to be implemented. a key activity of this re process is to demonstrate compliance by showing, through appropriate traces, that all relevant requirements have been elicited from the regulatory documents. [principal ideas/results] [contribution] in this problem-statement paper, we describe some key impediments to achieving requirements-compliance that we have identified in a large systems engineering project.impediments to requirements-compliance','Security requirements'
'many standards that mandate requirements traceability as well as current literature do not provide a comprehensive model of what information should be captured and used as a part of a traceability scheme. therefore, the practices and usefulness of traceability vary considerably across systems development efforts, ranging from very simplistic practices just aimed at satisfying the mandates to very comprehensive traceability schemes used as an important tool for managing the systems development process. we present a case study of a systems development organization, employing a comprehensive view of traceability. a model describing the traceability practice in the organization, perceived benefits of such a scheme and lessons learnt from implementing it are presented.implementing requirements traceability','Security requirements'
'requirements play an important role in the automotive business, as most components (like electronic control units) are developed by suppliers basing on specification documents. while a decade ago mainly sketches had been handed over to the supplier, now fully elaborated specification documents are written. the presentation gives an impression on the several stages of improvement of requirements engineering processes at mercedes-benz passenger car development along with some typical improvement patterns and lessons learned.improving requirements engineering processes','Security requirements'
'in this article, we present a logical framework for reasoning about inconsistent requirements in the context of multi-viewpoint requirements engineering process. in order to analyse the sources of inconsistencies and to reason with inconsistent requirements, we present an argumentation view of the requirements. intuitively, argumentation is a tool for reasoning with inconsistent knowledge: requirements are defined in terms of arguments (a conclusion with its support); then, a class of acceptable arguments is built (arguments with no counterarguments). we propose to characterize different classes of requirements which are ordered: from weakly confident to strongly confident (i.e. consistent). in the paper, we present inference rules to build intra and inter-viewpoint reasoning. inference rules are issued from the classes of requirements. we show how this work is useful for the requirements engineers to analyse inconsistent fragments of requirements.a multi-agent system to the common management of a renewable resource: application to water sharing m. le bars a,b and j.m. attonaty b a lamsade laboratory universit&#233; paris-dauphine 75775 paris cedex 16. france. binra station d\'&#233;conomie rurale, bp 01 78850 grignon france. {lebarsm@aol.com;attonaty@grigon.inra.fr} abstract. water sharing has become an important problem in france. a lot of negotiations are taking place at a local level between farmers, water suppliers, public services and environmentalists to allocate water resources between users. the problem is to share water with respect of different criteria like economic (global output) ethical (disparities between actors) environmental (water savings). different approaches have been already taken using linear programming or game theory, but they are always based on the hypothesis that decision-makers are completely rational, take into account few players and are often monoperiodic. we suggest that an agent-based modelling (abm) built with a multi-agent approach could help negotiations between different players by showing the consequences of water allocation rules and taking in consideration the players \'respective attitudes and their ability to change their behaviour. in this paper we will first present the model structure with the different types of agents modelled, how the model runs over a number of years and the first results of simulations. keywords. distributed artificial intelligence; multi-agent systems title: a new hybrid method for solving constraint optimization problems in anytime contexts authors: samir loudni and patrice boizumault affiliation: ecole des mines de nantes abstract: in this paper, we present a new hybrid method for solving constraint optimization problems in anytime contexts. we use the valued constraint satisfaction problem (vcsp) framework to model numerous discrete optimization problems. our method (vns/lds+cp) combines a variable neighborhood search (vns) scheme with limited discrepancy search (lds) using constraint propagation (cp) to evaluate cost and legality of moves made by vns. our experimental results on real-word problem instances demonstrate that our method clearly outperforms both lns/cp/gr (another hybrid method which also relies on the vcsp framework) and other standard local search methods as simulated-annealing. this confirm the benefit of the use, in a local search, of the lds partial search with constraint propagation. keywords: anytime problems, constraint-satisfaction, constraint-optimization, local search methods, hybrid methods. using software agents to avoid collisions among multiple robots markus j&#228;ger corporate technology, information and communications siemens ag 81739 munich, germany markus.jaeger@mchp.siemens.de ai algorithms collaborative software agents cooperating robots this paper describes a method where collaborative software agents are used to coordinate the independently planned trajectories of multiple mobile robots to avoid collisions and deadlocks among them. whenever the distance between two robots drops below a certain value, the agents exchange information about the planned trajectories of the robots and determine whether they are in danger of a collision. if a possible collision is detected, the agents monitor the robots movements and, if necessary, insert idle times between certain segments of the trajectories in order to avoid the collision. deadlocks among two or more robots occur if a number of robots block each other in a way such that none of them is able to continue along its trajectory without causing a collision. these deadlocks are reliably detected by the agents. after a deadlock is detected, alternative trajectories for each of the involved robots are successively planned until the deadlock is resolved. the agents use a combination of three fully distributed algorithms to reliably solve the task. they do not use any global synchronization. ----------- authors affiliations: - cooperating software agents - cooperating robots - collision avoidance among multiple robots - area partitioning - area coverage the title : successive search method for valued constraint satisfaction and optimization problems. authors names : mohamed tounsi and philippe david email adresses : mohamed.tounsi@emn.fr and philippe.david@emn.fr affiliation : computer science department, ecole des mines de nantes , 4 rue alfred kastler 44307 nantes, france. abstract : in this paper we introduce a new method based on russian doll search (rds) for solving optimization problems expressed as valued constraint satisfaction problems (vcsps). the rds method solves problems of size n (where n is the number of variables) by replacing one search by n successive searches on nested subproblems using the results of each search to produce a better lower bound. the main idea of our method is to introduce the variables through the successive searches not one by one but by sets of k variables. we present two variants of our method: the first one where the number k is fixed, noted kfrds; the second one, kvrds, where k can be variable. finally, we show that our method improves rds on daily management of an earth observation satellite. keywords : constraint satisfaction, vcsp, optimization problems. b-course: a web service for bayesian data analysis petri myllymaki, tomi silander, henry tirri, pekka uronen complex systems computation group (cosco) p.o.box 26, department of computer science fin-00014 university of helsinki, finland url: http://www.cs.helsinki.fi/research/cosco/ b-course (http://b-course.cs.helsinki.fi) is a free web-based online data analysis tool, which allows the users to analyze their data for multivariate probabilistic dependencies. these dependencies are represented as bayesian network models. in addition to this, b-course also offers facilities for inferring certain type of causal dependencies from the data. the software uses a novel \"tutorial style\" user-friendly interface which intertwines the steps in the data analysis with support material that gives an informal introduction to the bayesian approach adopted. although the analysis methods, modeling assumptions and restrictions are totally transparent to the user, this transparency is not achieved at the expense of analysis power: with the restrictions stated in the support material, b-course is a powerful analysis tool exploiting several theoretically elaborate results developed recently in the fields of bayesian and causal modeling. b-course can be used with most web-browsers (even lynx), and the facilities include features such as automatic missing data handling and discretization, a flexible graphical interface for probabilistic inference on the constructed bayesian network models (for java enabled browsers), automatic pretty-printed layout for the networks, exportation of the models, and analysis of the importance of the derived dependencies. in this paper we discuss both the theoretical design principles underlying the b-course tool, and the pragmatic methods adopted in the implementation of the software. artificial neural networks in hydrological watershed modeling: surface flow contribution from the ungaged parts of a catchment richard chibanga1, jean berlamont2 and joos vandewalle3 1phd student in the civil eng. dept. and 2prof. civil eng. dept., and head of hydraulics laboratory, 3 prof. head of electrical engineering dept - sista/cosic, katholieke universiteit, kasteelpark arenberg 40, 3001 heverlee (leuven), belgium abstract watershed modeling is often faced with the difficulty of determining the flow contribution from the ungaged sections of the catchment. where the main concern is making accurate streamflow forecasts at specific watershed locations, it is cost-effective and efficient to implement a simple system theoretic model. in this paper artificial neural networks (anns) are used as system theoretic models to model the ungaged flows. using data from the kafue river sub-catchment in zambia and a simple reservoir routing model, an estimate of the flow contribution from the ungaged sections is derived. inputs: rainfall, evaporation, previous-time-step flow are fed to a series of feedforward-backpropagation anns with target-output the current derived flow. selected best- performing anns are compared with autoregressive moving average models with exogenous inputs (armax) and they give accurate and more robust forecasts over long term than the best performing armaxs thereby making anns a viable alternative in time-series forecasting. keywords: semi conceptual-system theoretic; artificial neural networks; subsystem; tributary-runoff; forecasting; mapping. title: generation of propagation rules for intentionally defined constraints authors: slim abdennadher computer science department, university of munich oettingenstr. 67, 80538 munich, germany slim.abdennadher@informatik.uni-muenchen.de christophe rigotti laboratoire d\'ingenierie des systemes d\'information batiment 501, insa lyon, 69621 villeurbanne cedex, france christophe.rigotti@insa-lyon.fr abstract: a general approach to implement propagation and simplification of constraints consists of applying rules over these constraints. however, a difficulty that arises frequently when writing a constraint solver is to determine the constraint propagation algorithm. in previous work, different methods for automatic generation of propagation rules for constraints defined over finite domains have been proposed. in this paper, we present a method for generating propagation rules for constraint predicates defined by means of a constraint logic program. keywords: constraint solving, machine learning, rule-based programming title : \"data flow coherence criteria in ilp tools\" authors : smaranda muresan department of computer science, columbia university, new york, usa smara@cs.columbia.edu tudor muresan department of computer science, technical univ. of cluj-napoca, cluj-napoca, romania tmuresan@cs.utcluj.ro rodica potolea department of computer science, technical univ. of cluj-napoca, cluj-napoca, romania potolea@cs.utcluj.ro keywords : inductive logic programming, automatic program generation, data flow coherence criteria, pruning the search space abstract: in this paper we present a new method that uses data-flow coherence criteria in definite logic program generation. we outline three main advantages of these criteria supported by our results: i) drastically pruning the search space (around 90\%), ii) reducing the set of positive examples and reducing or even removing the need for the set of negative examples, and iii) allowing the induction of predicates that are difficult or even impossible to generate by other methods. besides these criteria, the approach takes into consideration the program termination condition for recursive predicates. the paper outlines some theoretical issues and implementation aspects of our system for automatic logic program induction. title: an expert recommendation system using concept-based relevance discernment authors: takashi yukawa ntt corporation ntt communication science laboratories 2-4 hikaridai, seika-cho, kyoto, japan yukawa@cslab.kecl.ntt.co.jp kaname kasahara ntt corporation ntt communication science laboratories 2-4 hikaridai, seika-cho, kyoto, japan kaname@cslab.kecl.ntt.co.jp tsuneaki kato the university of tokyo graduate school of arts and science dept. of language and information science 3-1-8 komaba, meguro-ku, tokyo, japan kato@boz.c.u-tokyo.ac.jp toshiro kita ntt communications corporation solution business division yamato seimei bldg. 1-1-7, uchisaiwai-cho chiyoda-ku, tokyo, japan toshiro.kita@ntt.com keywords: information retrieval, recommendation system, vector space model, concept base, knowledge management abstract: an expert recommendation system using concept-based relevance discernment is proposed. this system processes the description of a technical topic as input and then finds engineers who have a high level of expertise in that area. the technique employed is an extended vector space model that locates both technical topics and engineers in the same multi-dimensional space, and then calculates their relevance. this system can also retrieve engineers or documents that are related to a field matching a given engineer\'s technical interests. such a system can be expected to play the role of a person\'s professional network, and be a valuable tool for knowledge management among several organizations. paper title: combinatorial optimization through statistical instance-based learning keywords: constructive search, heuristics, optimization, instance-based learning authors: orestis telelis, panagiotis stamatopoulos department of informatics and telecommunications university of athens 157 84 athens, greece {telelis,takis}@di.uoa.gr abstract: different successful heuristic approaches have been proposed for solving combinatorial optimization problems. commonly, each of them is specialized to serve a different purpose or address specific difficulties. however, most combinatorial problems that model real world applications have a priori well known measurable properties. embedded machine learning methods may aid towards the recognition and utilization of these properties for the achievement of satisfactory solutions. in this paper, we present a heuristic methodology which employs the instance-based machine learning paradigm. this methodology can be adequately configured for several types of optimization problems which are known to have certain properties. experimental results are discussed concerning two well known problems, namely the knapsack problem and the set partitioning problem. these results show that the proposed approach is able to find significantly better solutions compared to intuitive search methods based on heuristics which are usually applied to the specific problems. title: interleaved backtracking in distributed constraint networks > author: youssef hamadi > affiliation: hewlett packards labs filton road, stoke gifford, > bristol bs34 8qz, united kingdom > email: yh@hplb.hpl.hp.com > > > abstract > the adaptation of software technology to distributed > environments is an important challenge today. in this > work we combine parallel and distributed search. by > this way we add the potential speed up of a parallel > exploration in the processing of distributed problems. > this paper extends dibt, a distributed search proce > dure operating in distributed constraint networks [6]. > the extension is twofold. first the procedure is up > dated to face delayed information problems upcoming > in heterogeneous systems. second, the search is ex > tended to simultaneously explore independent parts of > a distributed search tree. by this way we introduce > parallelism into distributed search, which brings to in > terleaved distributed intelligent backtracking (idibt). > our results show that 1) insoluble problems do not > greatly degrade performance over dibt and 2) super > linear speed up can be achieved when the distribution > of solution is nonuniform. > > keywords: distributed constraint satisfaction, distributed ai, > collaborative software agents, searchinconsistent requirements','Security requirements'
'this paper provides a roadmap for developing security-critical projects using rational unified process as a framework for development. the security quality requirements engineering (square) methodology provides a way to address security issues early in the development lifecycle. square can be more effective when it fits into an organization&#8217;s existing development process. hence this paper describes a way to fit the square methodology into the rational unified process.incorporating security requirements engineering into the rational unified process','Security requirements'
'a communication protocol is a fundamental component of a multi-agent system. the security requirements for a communication protocol should be articulated during the early stages of software development. however, there is no formal way provided for software developers to find out what makes a communication protocol secure and what are secure designs. in this paper we propose a method that defines security requirements, bridges security requirement analysis with security design, and integrates the security techniques into a communication protocol to fulfill the security requirements.incorporating security requirements into communication protocols in multi-agent software systems','Security requirements'
'misuse case modeling is a viable option to depict the security requirements together with functional requirements. to investigate the interplay between functional and security requirements, this paper presents an approach to decomposing use cases, misuse cases, and mitigation use cases. we identify relationships among decomposed cases for each case type (use, misuse, mitigation use) and ensure consistency among the cases as decomposition occurs by properly modeling shared and optional cases. we also assign applicable actors to the decomposed cases. decomposition is conducted for each case type independently and then integrated with the \"threatens\" and \"mitigates\" relationships. we provide processes for the proper use of the \"threatens\" relationship between misuse cases and use cases and the \"mitigates\" relationship between mitigation use cases and misuse cases at different levels of abstraction. thus, a complete set of security-centric requirements can be specified from the project outset to guide subsequent software development phases.integrating functional and security requirements with use case decomposition','Security requirements'
'behavior trees were invented by geoff dromey as a graphical modelling notation. their design was driven by the desire to ease the task of capturing functional system requirements and to bridge the gap between an informal language description and a formal model. vital to dromey\'s intention is the idea of incrementally building the model out of its building blocks, the functional requirements. this is done by graphically representing each requirement as its own behavior tree and incrementally merging the trees to form a more complete model of the system. in this paper we investigate the essence of this constructive approach to creating a model in general notation-independent terms and discuss its advantages and disadvantages. the result can be seen as a framework of rules and provides us with a semantic underpinning of requirements integration. integration points are identified by examining the (implicit or explicit) preconditions of each requirement. we use behavior trees as an example of how this framework can be put into practise.integrating requirements','Security requirements'
'database systems for real-time applications must satisfy timing constraints associated with transactions in addition to maintaining data consistency. in addition to real-time requirements, security is usually required in many applications. multilevel security requirements introduce a new dimension to transaction processing in real-time database systems. in this paper, we argue that, due to the conflicting goals of each requirement, trade-offs need to be made between security and timeliness. we first define mutual information, a measure of the degree to which security is being satisfied by a system. a secure two-phase locking protocol is then described and a scheme is proposed to allow partial violations of security for improved timeliness. analytical expressions for the mutual information of the resultant covert channel are derived and a feedback control scheme is proposed that does not allow the mutual information to exceed a specified upper bound. results showing the efficacy of the scheme obtained through simulation experiments are also discussed.integrating security and real-time requirements using covert channel capacity','Security requirements'
'when two or more distinct organizations interconnect their internal computer networks they form an inter-organization network(ion). ions support the exchange of cad/cam data between manufacturers and subcontractors, software distribution from vendors to users, customer input to suppliers\' order-entry systems, and the shared use of expensive computational resources by research laboratories, as examples. this paper analyzes the technical implications of interconnecting networks across organization boundaries.after analyzing the organization context in which ions are used, we demonstrate that such interconnections are not satisfied by traditional network design criteria of connectivity and transparency. to the contrary, a primary high-level requirement is access control, and participating organizations must be able to limit connectivity and make network boundaries visible. we describe a scheme based on non-discretionary control which allows interconnecting organizations to combine gateway, network, and system-level mechanisms to enforce cross-boundary control over invocation and information flow, while minimizing interference with internal operations.access control requirements such as these impose new requirements on the underlying interconnection protocols. we demonstrate such alternative interconnection protocols that support loose coupling across administrative boundaries and that accommodate the necessary control mechanisms. message-based gateways that support non-real-time invocation of services (e.g., file and print servers, financial transactions, vlsi design tools, etc.) are a promising basis for such loose couplings.inter-organization networks: implications of access control: requirements for interconnection protocol','Security requirements'
'we are developing an approach using jackson\'s problem frames to analyse security problems in order to determine security vulnerabilities. we introduce the notion of an anti-requirement as the requirement of a malicious user that can subvert an existing requirement. we incorporate anti-requirements into so-called abuse frames to represent the notion of a security threat imposed by malicious users in a particular problem context. we suggest how abuse frames can provide a means for bounding the scope of security problems in order to analyse security threats and derive security requirements.introducing abuse frames for analysing security requirements','Security requirements'
'this paper proposes an approach to the prioritisation of system changes that takes account of the relative costs and benefits of those changes and the risks that they reduce or introduce. this is part of the serum methodology (software engineering risk: understanding and management), which is being developed to help identify effective ways of using risk analysis and control in software production. serum introduces risk management at the initial business analysis stage of requirements investigation, and assumes an evolutionary approach to software delivery. prioritisation is determined from five factors: benefits, costs and risk exposure in the current system, target system, and development process. the relative importance of these factors is adjustable. results from a case study at nec illustrate the prioritisation process and a supporting software tool is also described.introducing measurable quality requirements','Security requirements'
'introducing requirements engineering appears to involve a cultural change in organizations. such a cultural change requires that requirements are defined and managed systematically, not only from a technical point of view, but also from the customers\' and users\' points of view. this paper describes experiences gained from four finnish organizations that have started tointroduce requirements engineering to their product development. the goal of this study was to evaluate which factors support, and which prevent, a cultural change. linking business goals to technical requirements via user needs and user requirements was one of the keyimprovement actions that supported cultural change. eliciting needs directly from real users and representing user requirements in the form of use cases were also key activities. however, bringing about a change of culture was challenging because both managers and product development engineers held beliefs that prevented active user need elicitation and systematic user requirement documentation.introducing requirements engineering','Security requirements'
'service-centric software systems offer new opportunities for requirements processes. this paper reports a new tool designed to increase the completeness of system requirements using information about designs and implementations of web services. it presents an algorithm for retrieving web services in domains that are analogical to a current requirements problem, to support creative thinking about requirements for that problem. it describes how the algorithm parses and analogically matches natural language descriptions of system requirements and web service descriptions. the paper also reports 2 evaluations of the tool that demonstrate improvements to specifications of requirements for a system in the automotive domain.inventing requirements from software','Security requirements'
'this paper reports a workshop that integrated creativity techniques with extended use case diagrams and storyboard representations of use cases to discover stakeholder requirements for vantage, a new system designed to reduce environmental impact at airports. the workshop revised the boundaries of the system and generated 200 new requirements-based ideas and storyboards for vantage. the paper describes the workshop structure, gives examples of outputs from it, and uses these outputs to answer 3 research questions about the usefulness of ideas generated and creativity techniques employed.inventing requirements','Security requirements'
'legal requirements facing new signature technology','Security requirements'
'this paper examines the american federal wiretap act and its application to the use of keystroke loggers as forensic tools and by private individuals. the paper concludes that for purposes of the wiretap act, a keystroke logger intercepts electronic communications if the keystrokes that the logger records are being transmitted over telephone lines or the internet. under the wiretap act, law enforcement personnel must obtain a wiretap order in order to use a keystroke logger to intercept any electronic communications.legal requirements for the use of keystroke loggers','Security requirements'
'information technologies misuse has increased the vulnerability of personal data, which has lead to growing concern about issues of personal privacy among political leaders, it managers, information security consultants and the millions of people currently online. manycountries have developed, or are preparing, laws and regulations to combat the related threats and to guarantee personal data protection. despite efforts to construct secure systems, few papers have, as yet, focused on security from the very outset of the system development life-cycle. this paper presents a pragmatic proposal to incorporate the legal and regulatory measures to guarantee personal data protection as a part of the requirements engineering process, instead of an addendum to system deployment. the authors investigatehow recent efforts in the requirements engineering field can contribute to improving security issues in information systems, in particular those dealing with personal data. a reusable collection of security requirements and, as a novelty, personal data protection requirements(including information on related software components links) are provided. the pre-defined requirements, together with a simple process model based on requirements reuse, provide a strategy that organizations can use to become privacy-compliant.legal requirements reuse','Security requirements'
'making requirements measurable','Security requirements'
'the importance of tacit knowledge in requirements engineering (re) is widely acknowledged. while valuable work has developed techniques to expose sources of tacit knowledge during requirements elicitation, such techniques are not universally applied and tacit knowledge, continues to negatively affect the quality of the requirements. in this position paper we present a brief review and interpretation of the literature on tacit knowledge that, we believe, is useful for re. we describe a number of techniques that offer analysts the means to reason about the effect of tacit knowledge and improve the quality of requirements and their management.making tacit requirements explicit','Security requirements'
'management information requirements assessment','Security requirements'
'streaming video over ip networks has become increasingly popular; however, compared to traditional data traffic, video streaming places different demands on quality of service (qos) in a network, particularly in terms of delay, delay variation, and data loss. in response to the qos demands of video applications, network techniques have been proposed to provide qos within a network. unfortunately, while efficient from a network perspective, most existing solutions have not provided end-to-end qos that is satisfactory to users. in this paper, packet scheduling and end-to-end qos distribution schemes are proposed to address this issue. the design and implementation of the two schemes are based on the active networking paradigm. in active networks, routers can perform user-driven computation when forwarding packets, rather than just simple storing and forwarding packets, as in traditional networks. both schemes thus take advantage of the capability of active networks enabling routers to adapt to the content of transmitted data and the qos requirements of video users. in other words, packet scheduling at routers considers the correlation between video characteristics, available local resources and the resulting visual quality. the proposed qos distribution scheme performs inter-node adaptation, dynamically adjusting local loss constraints in response to network conditions in order to satisfy the end-to-end loss requirements. an active network-based simulation shows that using qos distribution and packet scheduling together increases the probability of meeting end-to-end qos requirements of networked video. copyright &#169; 2005 john wiley & sons, ltd.managing qos requirements for video streaming: from intra-node to inter-node','Security requirements'
'managing the development of software requirements can be a complex and difficult task. the environment is often chaotic. as analysts and customers leave the project, they are replaced by others who drive development in new directions. as a result, inconsistencies arise. newer requirements introduce inconsistencies with older requirements. the introduction of such requirements inconsistencies may violate stated goals of development. in this article, techniques are presented that manage requirements document inconsistency by managing inconsistencies that arise between requirement development goals and requirements development enactment. a specialized development model, called a requirements dialog meta-model, is presented. this meta-model defines a conceptual framework for dialog goal definition, monitoring, and in the case of goal failure, dialog goal reestablishment. the requirements dialog meta-model is supported in an automated multiuser world wide web environment, called dealscribe. an exploratory case study of its use is reported. this research supports the conclusions that: 1) an automated tool that supports the dialog meta-model can automate the monitoring and reestablishment of formal development goals, 2) development goal monitoring can be used to determine statements of a development dialog that fail to satisfy development goals, and 3) development goal monitoring can be used to manage inconsistencies in a developing requirements document. the application of dealscribe demonstrates that a dialog meta-model can enable a powerful environment for managing development and document inconsistencies.managing requirements inconsistency with development goal monitors','Security requirements'
'mark_08 focuses on potentials and benefits of lightweight knowledge management approaches, such as ontology-based annotation, semantic wikis and rationale management techniques, applied to requirements engineering. methodologies, processes and tools for capturing, externalizing, sharing and reusing of knowledge in (distributed) requirements engineering processes are discussed. furthermore, the workshop is an interactive exchange platform between the knowledge management community, requirements engineering community and industrial practitioners. this proceeding includes selected and refereed contributions.managing requirements knowledge (mark_08)','Security requirements'
'm-health promises a great vision for future medical relevant activities. security and privacy are prime measures to strengthen trust in this ecosystem. federated identity management (fidm) is to identify people, applications, platforms or devices in an m-health federation. this paper describes scenarios of m-health, generalizes requirements of fidm, and maps some standards of web services to provide such an infrastructure.mapping web services standards to federated identity management requirements for m-health','Security requirements'
'matching user requirements','Security requirements'
'metrics for requirements engineering','Security requirements'
'[context &#38; motivation] obtaining traceability among requirements and between requirements and other artifacts is an extremely important activity in practice, an interesting area for theoretical study, and a major hurdle in common industrial experience. substantial effort is spent on establishing and updating such links in any large project - even more so when requirements refer to a product family. [question/problem]while most research is concerned with ways to reduce the effort needed to establish and maintain traceability links, a different question can also be asked: how is it possible to harness the vast amount of implicit (and tacit) knowledge embedded in already-established links? is there something to be learned about a specific problem or domain, or about the humans who establish traces, by studying such traces? [principal ideas/results] in this paper, we present preliminary results from a study applying different machine learning techniques to an industrial case study, and test to what degree common hypothesis hold in our case. [contribution] reshaping traceability data into knowledge can contribute to more effective automatic tools to suggest candidates for linking, to inform improvements in writing style, and at the same time provide some insight into both the domain of interest and the actual implementation techniques.mining requirements links','Security requirements'
'modeling database security requirements','Security requirements'
'modeling security requirements for applications','Security requirements'
'security requirements engineering is emerging as a branch of software engineering, spurred by the realization that security must be dealt with early on during the requirements phase. methodologies in this ?eld are challenging, as they must take into account subtle notions such as trust (or lack thereof), delegation, and permission; they must also model entire organizations and not only systems-to-be. in our previous work we introduced secure tropos, a formal framework for modeling and analyzing security requirements. secure tropos is founded on three main notions: ownership, trust, and delegation. in this paper we re?ne secure tropos introducing the notions of at-least delegation and trust of execution; also, at-most delegation and trust of permission. we also propose monitoring as a security design pattern intended to overcome the problem of lack of trust between actors. the paper presents a semantics for these notions, and describes an implemented formal reasoning tool based on datalog.modeling security requirements through ownership, permission and delegation','Security requirements'
'it service requirements offer a seemingly classic requirements engineering (re) problem. but, when attempting to solve it with re methods, we are faced with difficulties. re methods encourage us to identify the functional and non-functional requirements of a service. industrial service-management frameworks, however, use a different vocabulary. itil, one of the most prominent service-management frameworks, refers to service utilities and service warranties. in this paper, we propose a method for modeling warranties as a function of the service constancy expected by stakeholders and the threats to this constancy. we identify four kinds of warranties: express, implied, tacit and pending. we thereby seek to bridge the gap between service-management frameworks and re methods and to improve the practice of service management in organizations.modeling service-level requirements','Security requirements'
'requirements engineering has attained an important role in software development over the last few years as developers and other stakeholders have realized the importance of adequate requirement analysis and design in software development processes. however, the specification and analysis of functional requirements is better established compared to non-functional requirements. this could be attributed to the fact that nonfunctional requirements, such as reliability, accuracy, performance, usability and security are often subjective. security requirements are often incorporated in an ad hoc manner or considered at post-requirement phase. it is believed that addressing these requirements during the early phase of system development will improve the quality of developed applications. confidentiality is an aspect of a system\'s security requirements aimed at preventing unauthorized use of personal or corporate data. concerns from the different stakeholders, which can be diverging, have to be addressed in realizing confidentiality requirements. these concerns are also usually influenced by proposed system functions. this research is aimed at precisely defining confidentiality requirements and applying this for modelling and reasoning in confidentiality requirements engineering.modelling and reasoning for confidentiality requirements in software development','Security requirements'
'most people think of requirements as things to manipulate at the start of a project. others, more enlightened, recognize that requirements also have a role toward the end of projects to test compliance. but few people have recognized an active role for requirements during their system\'s use&#x2014;to monitor whether the system continues to comply with its requirements during its lifetime.monitoring our requirements','Security requirements'
'in our study of composite systems [fickas&helm, 92], we found a class of requirements that could not be guaranteed to hold. specifically, these requirements required the environment of the overall system to behave in ways that could not be controlled. the best we could do in such cases was to note the assumptions placed on the environment for the requirements to be met, and then monitor the environment at runtime to detect deviations from our assumptions about its behavior [fickas&feather, 1995]. this paper discusses a short example of carrying out this type of monitoring. it introduces three tools to support requirements monitoring: (1) a tool to capture a requirement formally, (2) a tool to translate that requirement into a runtime specification, and (3) a tool to actually do the runtime monitoring.monitoring requirements','Security requirements'
'modern systems engineering mandates the integration of heterogeneous models in systems design and analysis. analysis of modern, mixed technology systems requires the horizontal integration of heterogeneous models for predictive analysis. the ever increasing role of performance constraints such as power, cost and throughput requires vertical integration of heterogeneous component models describing different requirements facets. the rosetta [1, 2] specification language has been developed to address the issue of specification and analysis of heterogeneous models. in this paper we describe the semantics of rosetta\'s specificationcomposition in the context of systems level requirements modeling.multi-faceted requirements modeling','Security requirements'
'using hypervisors or virtual machine monitors for security has become very popular in recent years, and a number of proposals have been made for supporting multi-level security on secure hypervisors, including pr/sm, nettop, shype, and others. this paper looks at the requirements that users of mls systems will have and discusses their implications on the design of multi-level secure hypervisors. it contrasts the new directions for secure hypervisors with the earlier efforts of kvm/370 and digital\'s a1-secure vmm kernel.multi-level security requirements for hypervisors','Security requirements'
'context: coping with rapid requirements change is crucial for staying competitive in the software business. frequently changing customer needs and fierce competition are typical drivers of rapid requirements evolution resulting in requirements obsolescence even before project completion. objective: although the obsolete requirements phenomenon and the implications of not addressing them are known, there is a lack of empirical research dedicated to understanding the nature of obsolete software requirements and their role in requirements management. method: in this paper, we report results from an empirical investigation with 219 respondents aimed at investigating the phenomenon of obsolete software requirements. results: our results contain, but are not limited to, defining the phenomenon of obsolete software requirements, investigating how they are handled in industry today and their potential impact. conclusion: we conclude that obsolete software requirements constitute a significant challenge for companies developing software intensive products, in particular in large projects, and that companies rarely have processes for handling obsolete software requirements. further, our results call for future research in creating automated methods for obsolete software requirements identification and management, methods that could enable efficient obsolete software requirements management in large projects.obsolete software requirements','Security requirements'
'tacit knowledge in requirements documents can lead to miscommunication between software engineers andon presuppositions in requirements','Security requirements'
'on the key storage requirements for secure terminals','Security requirements'
'computer modelling is a viable method for aiding requirements elicitation of future systems as it provides a mechanism for understanding and consolidating ideas which people can readily relate to. however a major drawback with a computer model is the capturing of its requirements for future contractual use. in this paper we put forward a method by which requirement statements can be attributed within models that allows for post compilation extraction and analysis. this approach is based on adding a parasitic language to a modelling language which strongly couples requirements to items of code, but which is transparent to the dynamic execution of the model. the paper illustrates the approach on a simple language and discusses how the approach has been incorporated within a large system modelling language. the ability to provide direct links to requirement management repositories is shown.parasitic languages for requirements','Security requirements'
'requirements prioritization is recognized as an important but challenging activity in software product development. for a product to be successful, it is crucial to find the right balance among competing quality requirements. although literature offers many methods for requirements prioritization, the research on prioritization of quality requirements is limited. this study identifies how quality requirements are prioritized in practice at 11 successful companies developing software intensive systems. we found that ad-hoc prioritization and priority grouping of requirements are the dominant methods for prioritizing quality requirements. the results also show that it is common to use customer input as criteria for prioritization but absence of any criteria was also common. the results suggests that quality requirements by default have a lower priority than functional requirements, and that they only get attention in the prioritizing process if decision-makers are dedicated to invest specific time and resources on qr prioritization. the results of this study may help future research on quality requirements to focus investigations on industry-relevant issues.prioritization of quality requirements','Security requirements'
'requirements prioritization is used in the early phases of software development to determine the order in which requirements should be implemented. requirements are not all equally important to the final software system because time constraints, expense, and design can each raise the urgency of implementing some requirements before others. laws and regulations can make requirements prioritization particularly challenging due to the high costs of noncompliance and the substantial amount of domain knowledge needed to make prioritization decisions. in the context of legal requirements, implementation order ideally should be influenced by the laws and regulations governing a given software system. in this paper, we present a prioritization technique for legal requirements. we apply our technique on a set of 63 functional requirements for an open-source electronic health records system that must comply with the u.s. health insurance portability and accountability act.prioritizing legal requirements','Security requirements'
'privacy is extremely important in healthcare systems. unfortunately, most of the solutions already deployed are developed empirically. after discussing some of such existing solutions, this paper describes an analytic and generic approach to protect personal data by anonymization. this approach is then applied to some representative scenarios. the architecture and its implementation with a javacard are finally presented. our analysis, solution and implementation are generic enough to be adapted to various collaborative systems that process sensitive data such as ecommerce, e-government, social applications, etc.privacy requirements implemented with a javacard','Security requirements'
'there is a growing concern to ensure personal information is protected in the emerging information society, and this can be attributed to the increasing incident of identity theft and confidentiality breach. there are also potential risks associated with mishandling personal information in the healthcare sector, for example, medical conditions, which should remain confidential, can be disclosed to unauthorized persons, subsequently leading to negative social and psychological effects on the affected individuals. many governments and international agencies have developed legislations and guidelines to prevent misuse of personal information by organizations in their jurisdictions. however, there is a challenge in properly integrating the complex nature and interaction of confidentiality concerns in many information systems. this is because the concerns involve multiple interests - the data owner, the data custodian, potential users of the system, as well as government agencies, and they can be conflicting. in addition, the requirements are usually specified in free text form, which can be ambiguous and difficult to translate to software systems. a better understanding of confidentiality requirement properties will assist information system designers and developers in specifying and analyzing the requirements, and ultimately result in good \"confidentiality-aware\" systems. this research is aimed at developing an approach for improved specification, modelling and analysis of confidentiality requirements. in this paper, we describe the study to identify key confidentiality properties, which will enable precise specification of confidentiality requirements.properties of confidentiality requirements','Security requirements'
'quality in requirements engineering','Security requirements'
'defining quality requirements completely and correctly is more difficult than defining functional requirements because stakeholders do not state most of quality requirements explicitly. we thus propose a method to measure a requirements specification for identifying the amount of quality requirements in the specification. we also propose another method to recommend quality requirements to be defined in such a specification. we expect stakeholders can identify missing and unnecessary quality requirements when measured quality requirements are different from recommended ones. we use a semi-formal language called x-jrdl to represent requirements specifications because it is suitable for analyzing quality requirements. we applied our methods to a requirements specification, and found our methods contribute to define quality requirements more completely and correctly.quality requirements analysis using requirements frames','Security requirements'
'[context and motivation] in market-driven software development it is crucial, but challenging, to find the right balance among competing quality requirements (qr). [problem] in order to identify the unique challenges associated with the selection, trade-off, and management of quality requirements an interview study is performed. [results] this paper describes how qr are handled in practice. data is collected through interviews with five product managers and five project leaders from five software companies. [contribution] the contribution of this study is threefold: firstly, it includes an examination of the interdependencies among quality requirements perceived as most important by the practitioners. secondly, it compares the perceptions and priorities of quality requirements by product management and project management respectively. thirdly, it characterizes the selection and management of quality requirements in down-stream development activities. quality requirements in practice','Security requirements'
'context: the environment in which the system operates, its context, is variable. the autonomous ability of a software to adapt to context has to be planned since the requirements analysis stage as a strong mutual influence between requirements and context does exist. on the one hand, context is a main factor to decide whether to activate a requirement, the applicable alternatives to meet an activated requirement as well as their qualities. on the other hand, the system actions to reach requirements could cause changes in the context. objectives: modelling the relationship between requirements and context is a complex task and developing error-free models is hard to achieve without an automated support. the main objective of this paper is to develop a set of automated analysis mechanisms to support the requirements engineers to detect and analyze modelling errors in contextual requirements models. method: we study the analysis of the contextual goal model which is a requirements model that weaves together the variability of both context and requirements. goal models are used during the early stages of software development and, thus, our analysis detects errors early in the development process. we develop two analysis mechanisms to detect two kinds of modelling errors. the first mechanism concerns the detection of inconsistent specification of contexts in a goal model. the second concerns the detection of conflicting context changes that arise as a consequence of the actions performed by the system to meet different requirements simultaneously. we support our analysis with a case tool and provide a systematic process that guides the construction and analysis of contextual goal models. we illustrate and evaluate our framework via a case study on a smart-home system for supporting the life of people having dementia problems. results: the evaluation showed a significant ability of our analysis mechanisms to detect errors which were not notable by requirements engineers. moreover, the evaluation showed acceptable performance of these mechanisms when processing up to medium-sized contextual goal models. the modelling constructs which we proposed as an input to enable the analysis were found easy to understand and capture. conclusions: our developed analysis for the detection of inconsistency and conflicts in contextual goal models is an essential step for the entire system correctness. it avoids us developing unusable and unwanted functionalities and functionalities which lead to conflicts when they operate together. further research to improve our analysis to scale with large-sized models and to consider other kinds of errors is still needed.reasoning with contextual requirements','Security requirements'
'  this paper outlines a method, called reconciliation, for managing interference between partial specifications or viewpoints. the method supports the detection, verification and tracking of ontological overlaps. the paper describes the heuristics on which the method is based and illustrates the application of the method using a scenario.reconciling requirements','Security requirements'
'this paper addresses secure broadcast in wireless sensor networks (wsn). we introduce new set of security protocols for broadcast operation, providing enhancements over the current set of security protocols. we introduce two protocols as an enhancement for &#236;tesla protocol. in these protocols we focuses on reducing the memory requirements for the broadcast session while maintain the same security level. the first protocol, low buffer &#236;tesla (lb- &#236;tesla), achieves 88\% reduction in the node buffer size comparing to the original &#236;tesla protocol. the second protocol, reversed mac &#236;tesla (rm- &#236;tesla), achieves 92\% reduction in the node buffer size comparing to the original &#236;tesla protocol. this reduction enables longer session\'s duration and reduces the memory requirement.reducing &#305;tesla memory requirements','Security requirements'
'reducing tlb power requirements','Security requirements'
'representation and utilization of non-functional requirements for  information system design','Security requirements'
'software requirements arrive in different shapes and forms to development organizations. this is particularly the case in market-driven requirements engineering, where the requirements are on products rather than directed towards projects. this results in challenges related to making different requirements comparable. in particular, this situation was identified in a collaborative effort between academia and industry. a model, with four abstraction levels, was developed as a response to the industrial need. the model allows for placement of requirements on different levels and supports abstraction or break down of requirements to make them comparable to each other. the model was successfully validated in several steps at a company. the results from the industrial validation point to the usefulness of the model. the model will allow companies to ensure comparability between requirements, and hence it generates important input to activities such as prioritization and packaging of requirements before launching a development project.requirements abstraction model','Security requirements'
'requirements analysis and maintenance','Security requirements'
'as result of the process of evolution driven by the law of synergy, emergence endows the dynamics of composite systems with properties unidentifiable in their individual parts. the phenomenon of emergence involves: -- self-organization of the dynamical systems such that the synergetic effects can occur; -- interaction with other systems from which the synergetic properties can evolve in a new context. multi-agent systems enable cloning of real-life systems into autonomous software entities with a \'life\' of their own in the dynamic information environment offered by today\'s cyberspace. after introducing the concept of holonic enterprise (he) as a paradigm for the networked world that enables virtual representation of real-life organizations as multi-agent systems i will present a fuzzy-evolutionary approach which mimics emergence in cyberspace as follows: -- it induces self-organizing properties by minimizing the entropy measuring the information spread across the virtual system/organization such that equilibrium is reached in an optimal interaction between the system\'s parts to reach the system\'s objectives most efficiently; -- it enables system\'s evolution into a better one by enabling interaction with external systems found via genetic search strategies (mimicking mating with most fit partners in natural evolution) such that the new system\'s optimal organizational structure (reached by minimizing the entropy) is better then the one before evolution. the holonic enterprise paradigm provides a framework for information and resource management in global virtual organizations by modeling enterprise entities as software agents linked through the internet. applying the proposed fuzzy-evolutionary approach to the virtual societies \'living\' on the dynamic web endows them with behavioral properties characteristic to natural systems. in this parallel universe of information, enterprises enabled with the proposed emergence mechanism can evolve towards better and better structures while at the same time self-organizing their resources to optimally accomplish the desired objectives.requirements analysis in tropos','Security requirements'
'we present a tool, called the &lt;em&gt;requirements analysis tool &lt;/em&gt; that performs a wide range of best practice analyses on software requirements documents. the novelty of our approach is the use of user-defined glossaries to extract structured content, and thus support a broad range of syntactic and semantic analyses, while allowing users to write requirements in the stylized natural language advocated by expert requirements writers. semantic web technologies are then leveraged for deeper semantic analysis of the extracted structured content to find various kinds of problems in requirements documents. requirements analysis tool','Security requirements'
'requirements analysis using sadt','Security requirements'
'requirements engineering, the first phase of any software development project, is the achilles&#39; heel of the whole development process, as requirements documents are often inconsistent and incomplete. in industrial requirements documents, natural language is the main presentation means. this results in the fact that the requirements documents are imprecise, incomplete, and inconsistent. a viable way to detect inconsistencies and omissions in documents is to extract system models from them. in our previous work we developed approaches translating textual scenarios to message sequence charts (mscs) and textual descriptions of automata to automata themselves. it turned out that these approaches are highly sensitive to proper definition of terms (communicating objects for mscs, states for automata). the goal of the presented paper is a systematic comparison of different term extraction heuristics, as a preliminary stage of msc or automata extraction. the extracted terms were declared to communicating objects (in the case of mscs) or to states (in the case of automata). the heuristics were compared on the basis of correctness of resulting mscs and automata. we came to the conclusion that named entity recognition is the best performing technique for term extraction from requirements documents.requirements analysis','Security requirements'
'requirements people rarely talk about aesthetics. this column draws on the 10 principles of good design from dieter rams to examine the important roles of aesthetics in requirements practices. it proposes the use of meaning carriers to think about new types of requirements.requirements and aesthetics','Security requirements'
'requirements and issues for automatic focused overload control','Security requirements'
'requirements archaeology','Security requirements'
'nasa spends millions designing and building spacecraft for its missions. the dependence on software is growing as spacecraft become more complex. with the increasing dependence on software comes the risk that bugs can lead to the loss of a mission. at nasa&#8217;s jet propulsion laboratory new tools are being developed to address this problem. logic model checking and runtime verification can increase the confidence in a design or an implementation. a barrier to the application of such property-based checks is the difficulty in mastering the requirements notations that are currently available. for these techniques to be easily usable, a simple but expressive requirement specification method is essential. this paper describes a requirements capture notation and supporting tool that graphically captures formal requirements and converts them into automata that can be used in model checking and for runtime verification.requirements capture with rcat','Security requirements'
'numerous studies have shown that a software project&#8217;s cost, schedule and defect density escalate as the rate of requirements change increases. yet none of these studies have explored the effects of not making requirements changes in response to changes in user needs. this paper explains why a project incurs just as much, if not more, risk when requirements changes are suppressed.requirements change','Security requirements'
'requirements classification and reuse','Security requirements'
'aspect-oriented requirements engineering (aore) introduced an artifact called requirements composition table (rct). rct presents a holistic view of an application\'s functionality structured by core features and crosscutting concerns. this artifact can effectively support various project tasks and serve as a common frame of reference for all parties on a project team. as aore remains little-known to most practitioners in the software development field, the purpose of this paper is to explain the rct concept to practitioners and discuss its benefits. the rct technique has been implemented for a number of wall street applications at various investment banks. rct can help us perform important project tasks and has proven to be one of the most valuable artifacts of a software project. this paper discusses the steps to develop an rct, provides an example of how to use it to perform change impact analysis for releases, describes experiences using rcts in practice, and discusses lessons learned on projects implementing the rct technique.requirements composition table explained','Security requirements'
'we believe that the requirements at the leaf-node level of the requirements tree structure cannot be viewed in isolation and that dependencies between them exist. we pursued this notion in order to find a coherent set of requirement dependencies that would facilitate the formation of a requirements network at the leaf-node level. this network could well be aristocratic or egalitarian. an aristocratic network will highlight a vital few nodes (requirements) with a high degree of connectivity. such vital few requirements work as &#38;#39;hubs&#38;#39; and are expected to have a decisive influence on the success of the product&#38;#47;system, to which the requirements structure points. such critical information may provide a more effective resolution to issues such as release planning, prioritisation, reuse, change management and so on, enhance systems engineering implementation and thereby make system development more effective. in this paper, we discuss the formulation of a mechanism to identify the dependencies between requirements and we discuss the process to connect them using a series of steps in order to form a network of requirements at the leaf-node level of the requirement tree structure.requirements dependencies','Security requirements'
'the paper describes the quotations gathered during interviews and focus groups during a consulting engagement to help the client improve its requirements engineering (re) process. the paper describes also a model of the software lifecycle derived from a michael jackson quotation, a model that explains about 95\% of the quotations that we gathered. in particular, it explains why basic requirements determination is unstoppable and how management attempts to stop re lead to the phenomena that are described by the quotations and less than optimal requirements specifications.requirements determination is unstoppable','Security requirements'
'requirements development','Security requirements'
'unless you have a precise description of your product\'s requirements, it is very unlikely that you will satisfy them. only if the implementer already has a deep understanding of the problem can you hope to come close without documenting the requirements. even in that case, potential disagreements make producing a requirements document worthwhile.requirements documentation','Security requirements'
'requirements engineering - industry needs','Security requirements'
'a number of recent proposals aim to incorporate security engineering into mainstream software engineering. yet, capturing trust and security requirements at an organizational level, as opposed to an it system level, and mapping these into security and trust management policies is still an open problem. this paper proposes a set of concepts founded on the notions of ownership, permission, and trust and intended for requirements modeling. it also extends tropos, an agent-oriented software engineering methodology, to support security requirements engineering. these concepts are formalized and are shown to support the automatic verification of security and trust requirements using datalog. to make the discussion more concrete, we illustrate the proposal with a health care case study.requirements engineering for trust management: model, methodology, and reasoning','Security requirements'
'[context and motivation] for the requirements engineering (re) community it is clear that requirements engineering is a specific activity and role within software development. [question/problem] however: what about practice? is re seen there as a separate role? what qualifications do practitioners see as critical for this task? [principal ideas/results] 141 job advertisements from 2009 and 67 from 2012 were analysed statistically in order to find out how practice perceives and staffs re: which official job title do those persons have who do re? which further responsibilities do these persons have? which qualifications are demanded? [contribution] the study&#180;s main results are: the position \"requirements engineer\" hardly exists. re instead is done by consultants, software engineers, architects, developers and project managers, who additionally have an average of 3 further tasks. re is no task for job beginners: 73\% of the job advertisements wish or demand previous job experience. further important qualifications are: 94\% soft skills (the top 3 soft skills are: capacity for teamwork, english language and communication skills), 76\% demand knowledge with respect to the technology used, while only 34\% mention re knowledge. re is most often combined with solution design (77\% respectively 61\%).requirements engineering in practice','Security requirements'
'[context and motivation] in the last years motion-based games have achieved an increasing success. these games have great potential to support physiotherapeutic programs, as they can guide the patients in performing the right movements for their rehabilitation. [question/problem] however, on the one hand, existing games performed on commercial systems (e.g., wii, kinect) are not suitable for people affected by motor pathologies. on the other hand, the design of games for physiotherapy is hard, as they should meet the \"physiotherapy requirements\" of the medical staff, provide an enjoyable experience to the patients, and overcome the technical limitations of the systems that support their execution. [principal ideas/results] these limitations can be addressed by defining a standard process, independent from the considered pathology and that starts from the requirements collection and representation, to support the development of motion-based games for physiotherapy [contribution] for this reason, this paper proposes re-fit, a methodology to elicit and model the re-fit extends existing requirements elicitation (brainstorming, surveys, and direct observation) and modeling techniques (flags goal model). re-fit was developed in collaboration with the spinal unit of niguarda hospital and the respiratory medicine section of policlinico in milan. our experience demonstrated that re-fit is not only suitable to develop new physiotherapeutic games, but also to evaluate the adequacy of existing games for people affected by a specific pathology.requirements engineering meets physiotherapy','Security requirements'
'[context and motivation] this paper reports the results and lessons learned of a requirements engineering improvement project conducted in a siemens business unit. [question/problem] in particular, the project addressed the following major problems: (i) communication gap between marketing and development, resulting in misbalance between technology-driven and market-driven requirements; (ii) limited value of monolithic requirements specifications, resulting in inconsistencies across product versions; (iii) requirements overloading, resulting in cumbersome and time consuming descoping; (iv) insufficient traceability, resulting in poor or missing impact analysis, regression testing and other traceability errors; (v) intransparent mapping between a nonhierarchical topology of problem space artifacts to hierarchically structured solution space artifacts; (vi) missing support for platform variant management and reuse, resulting in long release cycles; (vii) waterfall process, resulting in inability to effectively handle change in requirements or design. [principal ideas/results] the paper describes the situation at the business unit before the process improvement project, gives a short overview on how the project was implemented and the techniques applied to solve the various problems the organization was facing. the paper wraps up with a comparison between the initial and the final state of the requirements engineering process in the organization and finally, a lessons learned section discusses some of the highlights and pitfalls encountered during the project. [contribution] the paper can be used as an initial point of reference to other practitioners and organizations facing similar problems and/or involved in similar improvement projects.requirements engineering process improvement','Security requirements'
'requirements engineering (re) tools are increasingly used to ease the re processes and allow for more systematic and formalized handling of requirements, change management and traceability. for developers and companies evaluating the use of re tools it is thus essential to know which re processes are supported by tools and how they fit to their own priorities. the answer isn\'t easy because many sales prospects highlight numerous features&#8212;yet leave out to which degree they\'re supported and whether all features really matter. to gain insight into how current re tools adapt to re activities, we ran a 146-item survey based on the features covered by the iso/iec tr 24766:2009, a new framework for assessing re tool capabilities. we received responses from 37 participants, covering all relevant tools. in addition to the tools\' score in each activity, we assessed their performance in three concrete use scenarios. our findings can help practitioners select an re tool as well as provide areas for improvement for re tools developers.requirements engineering tools','Security requirements'
'requirements engineering visualization is a rapidly growing field of research; however, the specific characteristics of what makes for effective visualizations during a particular engineering phase have not yet been distinguished. visualizations, when coupled with traditional practices, augment the ability of resulting requirements artifacts to reach a wide range of stakeholders and provide for a rapid and shared understanding of complex information. this paper represents a survey of the research papers presented during the rev workshops from 2006 to 2008 in order to ascertain how the research trends have evolved over the past few years. by examining approaches to requirements engineering visualization that have been proposed, in retrospect, we hope to show the areas of recent focus, as well as to discover those areas that may hold opportunities for further research with respect to the most commonly understood re lifecycle phases and activities. in the process, we offer a preliminary classification scheme through which to categorize the various research efforts. where none existed before, the resulted categorization enables a constructive discussion about the coverage of previous rev contributions from various perspectives, while discovering the gaps, and provides opportunities for further research with the understanding of the trends of applying visualization in requirements engineering research and practice.requirements engineering visualization','Security requirements'
'requirements engineering','Security requirements'
'the documentation of customer needs from the source specifications in a modeling environment for allocating them to architectural elements needs efficient tools and techniques in requirement engineering. once requirements are present in models, enhancement with suitable properties, classification, prioritization. and allocation on system architecture are then possible. a downside is that the customer needs are likely to evolve over time, and then, we would need to manually redo the modeling of the requirements. then, what we want to avoid is the manual definition of the customer needs from the source documents as a requirements model in the target environment. we propose in this paper a solution to import and export in papyrus mdt, a uml modeling tool, the customers\' needs from microsoft documents using the requirement interchange format, reqif.requirements exchange','Security requirements'
'location-based access control (lbac) takes a mobile user\'s current position into account when making the decision if he should be allowed to access a particular resource like a file or service. for example using lbac we can enforce that a nurse is only allowed to view a patient\'s data using a pda when she stays at the premises of the hospital. there are a couple of research papers that propose data models for lbac; almost all of them are extensions of role-based access control (rbac). in the paper at hand we first motivate the employment of lbac by some application scenarios before we review the most important lbac-models. despite the body of research in the field of lbac we could identify requirements that cannot be covered with the available models; these requirements are discussed in detail.requirements for a location-based access control model','Security requirements'
'requirements for access control','Security requirements'
'requirements for cryptographic hash functions','Security requirements'
'abstract: in many software application domains, constant evolution is the dominant problem, shaping both software design and the software process. telecommunication software is the prototypical example of such an application domain. this paper examines how requirements engineering, formal description techniques, and formal methods should be adapted to work well in these application domains.requirements for evolving systems','Security requirements'
'requirements for programming languages in safety and security software standards','Security requirements'
'nowadays, automobiles have a role as a sensor aggregation which is used to grasp the circumstance and its locational information. the value from sensors on each vehicle is called probe data. organically-consolidated probe data which make social seminal information is deserving of societal expectation. these probe vehicle systems also known as floating car data system (fcd) are developed under active promotion of cooperation among industry, academia, and government. in addition, for providing global social services over the internet, lots of companies and general persons who use such services have drew attention to privacy, especially the management of personal data. in this paper, after providing a brief overview of general probe vehicle system, we sew up practical difficulties and threat analysis. base on its analysis, we consider the requirements for envision a design and deployment of secure probe vehicle system, and inform the related activities at iso.requirements for protection methods of personal information in vehicle probing system','Security requirements'
'the first argument presented in this paper is that if we have a clear understanding of the objectives of the requirements engineering (re) process then we can identify what techniques we need. no single method or technique will be sufficient. the second argument is that a key objective of the re process is to specify a system which will ultimately be successful. three common types of failure are examined, process failure, expectation failure and interaction failure. the third argument is that if the causes of each type of failure can be described then we will be able to identify what techniques are needed to help us avoid failure. the five common causes are described: the requirements engineering process itself, human communication within requirements, knowledge development, documentation of requirements and management. a discussion of each cause is followed a list of the re techniques needed. the paper concludes with a `wish list\' of seventy requirements for re techniques, and a brief discussion of the strengths and weaknesses of the re community in meeting those requirements.requirements for requirements engineering technique','Security requirements'
'        physical human&#226;  robot interaction and cooperation has become a topic of          increasing importance and of major focus in robotics research. an essential          requirement of a robot designed for high mobility and direct interaction with          human users or uncertain environments is that it must in no case pose a threat          to the human. until recently, quite a few attempts were made to investigate          real-world threats via collision tests and use the outcome to considerably          improve safety during physical human&#226;  robot interaction. in this paper, we give          an overview of our systematic evaluation of safety in human&#226;  robot interaction,          covering various aspects of the most significant injury mechanisms. in order to          quantify the potential injury risk emanating from such a manipulator, impact          tests with the dlr-lightweight robot iii were carried out using standard          automobile crash test facilities at the german automobile club (adac). based on          these tests, several industrial robots of different weight have been evaluated          and the influence of the robot mass and velocity have been investigated. the          evaluated non-constrained impacts would only partially capture the nature of          human&#226;  robot safety. a possibly constrained environment and its effect on the          resulting human injuries are discussed and evaluated from different          perspectives. as well as such impact tests and simulations, we have analyzed the          problem of the quasi-static constrained impact, which could pose a serious          threat to the human even for low-inertia robots under certain circumstances.          finally, possible injuries relevant in robotics are summarized and          systematically classified.      requirements for safe robots','Security requirements'
'maximizing local autonomy by delegating functionality to end nodes when possible (the end-to-end design principle) has led to a scalable internet. scalability and the capacity for distributed control have unfortunately not extended well to resource access-control policies and mechanisms. yet management of security is becoming an increasingly challenging problem in no small part due to scaling up of measures such as number of users, protocols, applications, network elements, topological constraints, and functionality expectations. in this article, we discuss scalability challenges for traditional access-control mechanisms at the architectural level and present a set of fundamental requirements for authorization services in large-scale networks. we show why existing mechanisms fail to meet these requirements and investigate the current design options for a scalable access-control architecture. we argue that the key design options to achieve scalability are the choice of the representation of access control policy, the distribution mechanism for policy, and the choice of the access-rights revocation scheme. although these ideas have been considered in the past, current access-control systems in use continue to use simpler but restrictive architectural models. with this article, we hope to influence the design of future access-control systems towards more decentralized and scalable mechanisms.requirements for scalable access control and security management architectures','Security requirements'
'in engineering the requirements for a telecommunications system, the greatest obstacle to be overcome is the sheer complexity of the required behavior. we present several ways of managing and minimizing this complexity, all of proven effectiveness. most of the specification techniques result from specific application of general requirements principles to the telecommunications domain.requirements for telecommunications services','Security requirements'
'&lt;bold&gt;large information systems often span various functional areas of an organization and are used by different groups across several levels of the organization. when fulfillment of requirements in an&lt;/bold&gt; information system &lt;bold&gt;is deficient for a functional area or an organizational unit, lower levels of acceptance among certain user groups may occur. this article explores the relationship between user acceptance of an&lt;/bold&gt; information system &lt;bold&gt;and the degree of information requirements fulfillment for various user groups.&lt;/bold&gt;requirements fulfillment','Security requirements'
'there are significant interactions between video game stakeholder emotional requirements and security require- ments. counter-intuitively, some traditional security requirements are not necessarily met by the game implementation  some forms of security breaches are condoned by the stakeholders (if not actually demanded by them) and the requirements engineering process must support these contradictions. we present an overview of security requirements for video games and show how stakeholder diversity introduces significant complexities to the requirements negotiation process. our analysis of certain security threats, and their emotional motivations, shows that these motivations form an important element of the emotional requirements and that significant context is necessary for properly capturing the emotional requirements related to security. finally, we show how emotional requirements can be used to guide security goal development for this domain and pro- pose the use of in-game justice systems to allow players to address security violations in realtime. keywords: non-functional requirements, emotion, emo- tional requirements, security, security requirements, video game.requirements in conflict','Security requirements'
'requirements management at nasa','Security requirements'
'requirements management','Security requirements'
'even if all the real needs are covered in the requirements and also implemented, errors may be induced by human-computer interaction through a bad interaction design and its resulting user interface. such a system may even not be used at all. alternatively, a great user interface of a system with features that are not required will not be very useful as well. so, the main topics of this tutorial are requirements and interaction design, as well as their joint modeling through discourse models and ontologies. our discourse models are derived from results of human communication theories, cognitive science and sociology (even without employing speech or natural language). while these models were originally devised for capturing interaction design, it turned out that they can be also viewed as specifying classes of scenarios, i.e., use cases. in this sense, they can also be utilized for specifying requirements. ontologies are used to define domain models and the domains of discourse for the interactions with software systems. user interfaces for these software systems can be generated semi-automatically from our discourse models, domain-of-discourse models and specifications of the requirements. this is especially useful when user interfaces for different devices are needed. so, requirements meet interaction design to make applications both more useful and usable.requirements meet interaction design','Security requirements'
'requirements metrics - value added','Security requirements'
'requirements modeling','Security requirements'
'the field of requirements engineering emerges out of tradition of research and engineering practice that stresses the importance of generalizations and abstractions. although abstraction is essential to design it also has its dark side. by abstracting away from the context of an investigation, the designer too easily lapses into modeling only those things that are easy to model. the subtleties, special cases, interpretations and concrete features of the context of use are smoothed over in the rush to capture the essence of the requirements. often, however, what is left out is essential to understanding stakeholders\' needs. in contrast, approaches that stress context at the expense of abstraction may lead to floundering or to short-term customer satisfaction at the expense of long-term fragility of the system. what is needed is a synthesis of these two approaches: a synthesis that recognizes the complementary values of abstraction and context in requirements engineering and that does not relegate either one to a background role. such a synthesis requires us not only to adopt new methods in practice but also to rethink our underlying assumptions about what requirements models are models of and what it means to validate them.requirements models in context','Security requirements'
'with the emergence of an effective infrastructure supporting grid computing and web services, service-oriented computing has been growing over the last few years, and service-oriented architectures are becoming an important computing paradigm. when different trust domains control different component services, trust management plays a critical role to smooth the collaboration among component services. the federation of these component services makes new demands for managing trust-related behavior. although many extant trust management systems deal with intradomain trust behaviors, there is a growing need for effective strategies for managing inter-domain behaviors. in this paper we explore requirements for a federated trust management system. the purpose of this paper is not to suggest a single type of system covering all necessary features; instead, its purpose is to initiate a discussion of the requirements arising from inter-domain federation, to offer a context in which to evaluate current and future solutions, and to encourage the development of proper models and systems for federated trust management. our discussion addresses issues arising from trust representation, trust exchange, trust establishment, trust enforcement, and trust storage.requirements of federated trust management for service-oriented architectures','Security requirements'
'requirements of role-based access control for collaborative systems','Security requirements'
'this study presents an analysis of the impact of mitigation on computer worm propagation in mobile ad-hoc networks (manets). according to the recent darpa baa - defense against cyber attacks on manets [4], \"one of the most severe cyber threats is expected to be worms with arbitrary payload that can infect and saturate manetbased networks on the order of seconds\". critical to the design of effective worm counter measures in manet environments is an understanding of the propagation mechanisms and the performance of the mitigation technologies. this work aims to advance the security of these critical systems through increased knowledge of propagation mechanisms, performance and the effect of mitigation technologies. we present both analytic and simulation analysis of mitigation effectiveness. the ultimate goal of these studies is to develop an accurate set of performance requirements on mitigation techniques to minimize worm propagation in tactical, battlefield manets.requirements on worm mitigation technologies in manets','Security requirements'
'designing product lines with substitutable components and subassemblies permits companies to offer a broader variety of products while continuing to exploit economies of scale in production and inventory costs. past research on models incorporating component substitutions focuses on the benefits from reduced safety-stock requirements. this paper addresses a dynamic requirements-planning problem for two-stage multi product manufacturing systems with bill-of-materials flexibility, i.e., with options to use substitute components or subassemblies produced by an upstream stage to meet demand in each period at the downstream stage. we model the problem as an integer program, and describe a dynamic-programming solution method to find the production and substitution quantities that satisfy given multi period downstream demands at minimum total setup, production, conversion, and holding cost. this methodology can serve as a module in requirements-planning systems to plan opportunistic component substitutions based on relative future demands and production costs. computational results using real data from an aluminum-tube manufacturer show that substitution can save, on average, 8.7\% of manufacturing cost. we also apply the model to random problems with a simple product structure to develop insights regarding substitution behavior and impacts.requirements planning with substitutions','Security requirements'
'requirements engineering is an essential activity in creating embedded real-time systems. companies that produce a number of partially similar products can reduce development time and cost, improve quality and simplify software maintenance by applying reuse practices. requirements reuse is an essential enabler to achieve effective software reuse. this study describes two different approaches for requirements reuse at danfoss. the first approach reuses those requirements that are envisioned to be common between two consecutive projects and allows changing and parameterization of parts of the requirements. the second approach organizes all requirements into a common model and explicitly manages variability and different requirement variants in this common model. the results show that both approaches can result in significant savings in reduced effort by reusing common requirements. the first approach was found to be effective when the domain maturity is low and the significant set of requirements were changed from project to project. the second approach allows high reuse potential and significant savings for stable domains, where most requirements tend to be small additions or minor changes of existing requirements.requirements reuse at danfoss','Security requirements'
'for several decades, software reuse has been a recognized solution to improving efficiency of software development. however, implementing reuse in practice remains challenging, and the it community has little visibility into the state of the practice specifically as it pertains to reusing software requirements. this paper presents the results of a survey conducted in the global it industry in 2010 and discusses the state of the practice for software requirements reuse. the survey studies reuse adoption in two different contexts, i.e., software product lines and software maintenance. the analysis of the survey data focuses on the latter context as a more common case in practice and investigates the impact of various factors on reuse adoption and effectiveness.requirements reuse','Security requirements'
'requirements tools','Security requirements'
'one of the hurdles in the enforcement of access control remains the translation of the organization\'s high level policy, that drives the access control decisions, down to technology specific deployment descriptors, configuration files and code. this huge gap between the high level policy and the access logic has as a consequence that it is hard to trace implementation fragments to the actual requirement they contribute to, and to support evolution. the notion of an access interface is introduced as a contract between the authorization engine and the various applications using its services. a so-called view connector makes sure that the application behaves consistently with this contract. the implementation is based on aspect orientation, rendering the whole design more robust in the light of unanticipated changes.requirements traceability to support evolution of access control','Security requirements'
'requirements traceability','Security requirements'
'requirements analysts need a new toolbox with both the right tools and the instructions to use them including agile development and user-centered design for techniques such as analysis of web analytics, wire-framing, and user stories. we can also look to the creativity literature and take techniques such as constraint removal, storytelling, and other worlds.requirements tracery','Security requirements'
'requirements traceability is an important undertaking as part of ensuring the quality of software in the early stages of the software development life cycle. this paper demonstrates the applicability of swarm intelligence to the requirements tracing problem using pheromone communication and a focus on the common text around linking terms or words in order to find related textual documents. through the actions and contributions of each individual member of the swarm, the swarm as a whole exposes relationships between documents in a collective manner. two techniques have been examined, simple swarm and pheromone swarm. the techniques have been validated using two real-world datasets from two problem domains.requirements tracing','Security requirements'
'requirements trawling','Security requirements'
'in recent years, global software development (gsd) increasingly spread across the globe due to benefits it provides like low cost, availability of resources and access to wider and cheap human resource market. however, it is evident from literature that software development teams face many challenges in this new overwhelming paradigm like culture difference, coordination, communication and loss of teamness and so on. further, it is also evident that requirements engineering (re) is the most complicated and sturdy phase of software development even in collocated, but increasing pace of gsd has made it more diverse and complicated. consequently in such fashion, cross functional stakeholders must specify, analyze and manage requirements across diverse cultures, time zones and organizational boundaries; thereby requirements understanding (ru) is necessary. in this paper, the authors identified different factors of gsd challenges and their impact on ru and solutions used to cater these challenges. for this purpose, several industrial surveys have been conducted in well reputed software companies in kingdom of saudi arabia (ksa) involved in gsd, with the intent to get views and perceptions regarding the impact of challenges on ru and solution used to overcome those challenges.requirements understanding','Security requirements'
'one of the most important factors of success in the development of a software product line is the elicitation, management, and representation of variability. in this context, this article explores the possible advantages of adoption of the model driven engineering (mde) paradigm in the variability specification. feature graphs and goal models can be considered special models in the context of mde. the global picture is a sequence of models from requirements to features, and from both of these to architecture (a uml model). the transformation process is based on the respective meta-models. the conclusion is positive as the introduction of mde raises the abstraction level in the instantiation process of the product line. more effort is needed to further evaluate some of the ideas related to automated transformations: in particular, the traceability register is essential if we want to exploit their possible benefits.requirements variability models','Security requirements'
'modern software development processes, like the rational unified process, prescribe iterative approach to software development. one of the fundamental assumptions of an iterative process is that system requirements don\'t have to be completely understood tocommence development.at first glance the assumption that one can start developing a system without completely understanding its requirements seems paradoxical. however, upon closer inspection requirements can be divide into many categories one of them being the \"architecturally-significant requirements\". it is the understanding of these requirements, the associated development risks, and the system architecture that drive the early iterations ofsystem development.requirements, architectures and risks','Security requirements'
'requirements are sensitive to the context in which the system-to-be must operate. where such context is well understood and is static or evolves slowly, existing re techniques can be made to work well. increasingly, however, development projects are being challenged to build systems to operate in contexts that are volatile over short periods in ways that are imperfectly understood. such systems need to be able to adapt to new environmental contexts dynamically, but the contextual uncertainty that demands this self-adaptive ability makes it hard to formulate, validate and manage their requirements. different contexts may demand different requirements trade-offs. unanticipated contexts may even lead to entirely new requirements. to help counter this uncertainty, we argue that requirements for self-adaptive systems should be run-time entities that can be reasoned over in order to understand the extent to which they are being satisfied and to support adaptation decisions that can take advantage of the systems&#8217; self-adaptive machinery. we take our inspiration from the fact that explicit, abstract representations of software architectures used to be considered design-time-only entities but computational reflection showed that architectural concerns could be represented at run-time too, helping systems to dynamically reconfigure themselves according to changing context. we propose to use analogous mechanisms to achieve requirements reflection. in this paper we discuss the ideas that support requirements reflection as a means to articulate some of the outstanding research challenges.requirements-aware systems','Security requirements'
'requirements-based uml','Security requirements'
'security is primarily concerned with protecting assets from harm. identifying and evaluating assets are therefore key activities in any security engineering process &#8212; from modeling threats and attacks, discovering existing vulnerabilities, to selecting appropriate countermeasures. however, despite their crucial role, assets are often neglected during the development of secure software systems. indeed, many systems are designed with fixed security boundaries and assumptions, without the possibility to adapt when assets change unexpectedly, new threats arise, or undiscovered vulnerabilities are revealed. to handle such changes, systems must be capable of dynamically enabling different security countermeasures. this paper promotes assets as first-class entities in engineering secure software systems. an asset model is related to requirements, expressed through a goal model, and the objectives of an attacker, expressed through a threat model. these models are then used as input to build a causal network to analyze system security in different situations, and to enable, when necessary, a set of countermeasures to mitigate security threats. the causal network is conceived as a runtime entity that tracks relevant changes that may arise at runtime, and enables a new set of countermeasures. we illustrate and evaluate our proposed approach by applying it to a substantive example concerned with security of mobile phones.requirements-driven adaptive security','Security requirements'
'in this paper, i try to identify the strategies that experienced is project managers say they use to cope with requirements uncertainty on development projects for external clients. i show that project managers say they use different strategies for coping with the different dimensions of requirement uncertainty, as this construct has been formulated in the literature. i then argue that requirement uncertainty should be formulated as a profile construct and not as a latent or aggregate construct as at present, if it is to have pragmatic validity as a guide to action for project managers.requirements-uncertainty','Security requirements'
'rethinking requirements','Security requirements'
'retrofitting the edp auditor&#8212;edp security skill need and requirements','Security requirements'
'- robert darimont and jeanine souqui&#232;resthis paper advocates for a process-oriented approach to reuse operational requirements. in process-oriented approaches, a development (and reuse is just a particular case of development) keeps track of the intermediate states and steps leading to the final artifacts. the paper shows that it is worth recording the reuse process for developing operational requirements and not only the final product artifacts generated by the reuse process. the motivation behind is that syntactical constructs of the specification languages are generally not sufficient to trace the reuse process. such traces are important for documentation purpose, for maintenance, replay and software evolution.reusing operational requirements','Security requirements'
'the paper describes a product-line-oriented approach to reusing requirements for systems with highly complex variability. software product lines are a powerful means to manage comprehensively of all artifacts produced during system development for reuse. hence, classical product line approaches provide mechanisms to handle requirements for reuse. but especially in the context of automotive systems, we face the challenge of creating reusable requirements specifications that each contain variability; reuse for requirements specifications of this kind means handling variability of variability models. this paper describes techniques for generating requirements specifications with variability from a so-called requirements library. the research results described originate from a process improvement initiative at daimlerchrysler. the presented approaches are therefore pragmatic and aimed at current industrial practice but are formally based on a category-theoretical notation. driven by practical issues, the paper comes up with extended means for variability modeling and a new notion of variability, broadening the scope of what can be managed by product lines.reusing requirements','Security requirements'
'due to the spreading of sms services and appearing of new business models, value-added sms services have been introduced. according to the research results about wide distribution of security incidents on ict systems worldwide, in spite of known security solutions, there is a necessity for organizational approach to implement security. this paper presents research and development efforts in building process model securup for security requirements engineering conformed to rup framework. the model consists of processes, artifacts, activities and according roles for successful elicitation, analysis and specification of recognized security requirements and is validated on presented case study. the model validation results have shown significant process improvement, especially on roles and activities identification in securup elaboration process, but only further case studies in industry can be best indicators for usefulness of such models.rup-based process model for security requirements engineering in value-added service development','Security requirements'
'in this paper, we study a problem of privacy protection in large survey rating data. the rating data usually contains both ratings of sensitive and non-sensitive issues, and the ratings of sensitive issues include personal information. even when survey participants do not reveal any of their ratings, their survey records are potentially identifiable by using information from other public sources. we propose a new (k,&#949;,l)-anonymity model, in which each record is required to be similar with at least k&#8722;1 others based on the non-sensitive ratings, where the similarity is controlled by &#949;, and the standard deviation of sensitive ratings is at least l. we study an interesting yet nontrivial satisfaction problem of the (k,&#949;,l)-anonymity, which is to decide whether a survey rating data set satisfies the privacy requirements given by users. we develop a slice technique for the satisfaction problem and the experimental results show that the slicing technique is fast, scalable and much more efficient in terms of execution time than the heuristic pairwise method.satisfying privacy requirements','Security requirements'
'this paper describes a heuristic approach for solving the problem of dynamically scheduling tasks in a real-time system where tasks have deadlines and general resource requirements. the crux of our approach lies in the heuristic function used to select the task to be scheduled next. the heuristic function is composed of three weighted factors. these factors explicitly consider information about real-time constraints of tasks and their utilization of resources. simulation studies show that the weights for the various factors in the heuristic function have to be fine-tuned in order to obtain a degree of success in the range of 75-88 percent of that obtained via exhaustive search. however, modifying the approach to use limited backtracking improves the degree of success substantially to as high as 99.5 percent. this improvement is observed even when the initial set of weights are not tailored for a particular set of tasks. simulation studies also show that in most cases the schedule determined by the heuristic algorithm is optimal or close to optimal.scheduling tasks with resource requirements in hard real-time systems','Security requirements'
'in this position paper, we argue that search based software engineering techniques can be applied to the optimisation problem during the requirements analysis phase. search based techniques offer significant advantages; they can be used to seek robust, scalable solutions, to perform sensitivity analysis, to yield insight and provide feedback explaining choices to the decision maker. this position paper overviews existing achievements and sets out future challenges.search based requirements optimisation','Security requirements'
'software security is an increasingly important aspect of computing; however, it is still addressed as an after thought in too many development efforts. while a variety of approaches have been proposed for security requirements engineering, we find many still lacking with respect to their usability. in this proposal i describe my work in the area of security requirements engineering. sure, secure and usable requirements engineering, is a new approach that supports non-security experts in order to specify security requirements from which testing artifacts can be derived. in addition, assure, automated support for secure and usable requirements engineering, a system that implements the sure technique is presented.secure and usable requirements engineering','Security requirements'
'secure computer network requirements','Security requirements'
'security issues for software systems ultimately concern relationships among social actors - stakeholders, system users, potential attackers - and the software acting on their behalf. this paper proposes a methodological framework for dealing with security and privacy requirements based on i*, an agent-oriented requirements modeling language. the framework supports a set of analysis techniques. in particular, attacker analysis helps identify potential system abusers and their malicious intents. dependency vulnerability analysis helps detect vulnerabilities in terms of organizational relationships amongstakeholders. countermeasure analysis supports the dynamic decision-making process of defensive system players in addressing vulnerabilities and threats. finally, access control analysis bridges the gap between security requirement models and security implementation models. the framework is illustrated with an example involving security and privacy concerns in the design of agent-based health information systems. in addition, we discuss model evaluation techniques, including qualitative goal model analysis and property verification techniques based on model checking.security and privacy requirements analysis within a social setting','Security requirements'
'security and privacy requirements in computing','Security requirements'
'security requirements and solutions in distributed electronic health records','Security requirements'
'software is required to comply with the laws and standards of software security. however, stakeholders with less concern regarding security can neither de- scribe the behaviour of the system with regard to secu- rity nor validate the system\'s behaviour when the secu- rity function conflicts with usability. scenarios or use- case specifications are common in requirements elici- tation and are useful to analyse the usability of the system from a behavioural point of view. in this paper, the authors propose a method to weave scenario frag- ments based on security evaluation criteria into sce- narios. the experiments showed that the weaving method led to a better scenario than the method in- volving writing or modifying the scenario with refer- ence to security evaluation criteria. keywords: requirements elicitation, security require- ments, scenario analysis, aspect-oriented software de- velopmentsecurity requirements elicitation via weaving scenarios based on security evaluation criteria','Security requirements'
'software engineering curricula too often neglect the development of security requirements for software systems. as a consequence, programmers often produce buggy code with weak security measures. this report focuses on three case studies in which graduate students applied a novel security requirements engineering methodology to real-world software development projects. the experiences showed promise for curriculum integration in educating students about the importance of security requirements in software engineering, as well as how to develop such requirements.security requirements engineering for software systems','Security requirements'
'the majority of the current product line practices in requirements engineering do not adequately address security requirements engineering despite the fact that security requirements engineering is both a central task and a critical success factor in product line development due to the complexity and extensive nature of product lines. therefore, our contribution is to present and to demonstrate the applicability of our proposed security quality requirements engineering process (sreppline), which is based on a security requirements decision model driven by security standards along with a security variability model. we shall demonstrate our proposal by describing part of a real case study as a preliminary validation of these models. the final aim of this approach is to deal with security requirements variability from the early stages of the product line development in a systematic way, in order to facilitate conformance of the products with the most relevant security standards with regard to the management of security requirements, such as iso/iec 27001 and iso/iec 15408security requirements engineering process for software product lines','Security requirements'
'everyone agrees that security is a problem, ranging from microsoft to the banks that have been recent victims of rogue traders. what is paradoxical is that there does not seem to be a wholehearted commitment by both academics and industry to treat this topic systematically at the top level of requirements engineering. our vision is of a future in which we inform the security requirements engineering process by organisational theory. this would act as thebridge between the well-ordered world of the software project informed by conventional requirements and the unexpected world of anti-requirements associated with the malicious user. we frame a vision for the requirements engineering community that would involve the community solving six difficult problems.security requirements engineering','Security requirements'
'service-oriented architectures (soas) are a commonly used paradigm for it infrastructures in various fields. due to their flexibility and the easy accessibility of their underlying web services, soas are the architecture of choice for more and more service providers. semantic soas (ssoas) are going one step further and are enhancing the common soa with semantic components. however, a major success criterion of any soa is the existence of a reliant security infrastructure. therefore, this paper identifies security requirements for an egovernment ssoa focusing on communication security, trust, privacy and access control. our work is based on the architecture designed within the scope of the european research project access-egov, which envisions the development of a ssoabased egovernment platform.security requirements for a semantic service-oriented architecture','Security requirements'
'security requirements for network management data','Security requirements'
'information security requirements are important in all software engineering projects, not only to ensure the correct level of security in the end product but also to avoid implementing security solutions that turn out to be a bad fit. this article compares methods for eliciting and describing security requirements in software development projects, from the viewpoint of developers without extensive security skills. as the authors argue, all software projects need a well-balanced amount of security awareness from the beginning. this article is part of a special issue on security of the rest of us.security requirements for the rest of us','Security requirements'
'software product line engineering has proven to be one of the most successful paradigms for developing a diversity of similar software applications and software-intensive systems at low costs, in short time, and with high quality, by exploiting commonalities and variabilities among products to achieve high levels of reuse. at the same time, due to the complexity and extensive nature of product line development, security and requirements engineering are critical success factors in the development of a software product line. however, most of the current product line practices in requirements engineering do not adequately address the security requirements engineering. therefore, in this paper we will propose a security requirements decision model driven by security standards along with a security variability model to manage the variability of the security requirements related artefacts. the aim of this approach is to deal with security requirements from the early stages of the product line development in a systematic way, in order to facilitate the conformance to the most relevant security standards with regard to the management of security requirements, such as iso/iec 27001 and iso/iec 15408.security requirements variability for software product lines','Security requirements'
'first page of the articleservices and revenue requirements','Security requirements'
'this paper describes an approach to modeling the evolution of non-secure applications into secure applications in terms of the software requirements model and software architecture model. the requirements for security services are captured separately from application requirements, and the security services are encapsulated in connectors in the software architecture, separately from the components providing functional services. the enterprise architecture is described in terms of use case models, static models, and dynamic models. the software architecture is described in terms of components and connectors, which can be deployed to distributed configurations. by separating application concerns from security concerns, the evolution from a non-secure application to a secure application can be achieved with less impact on the application. an electronic commerce system is described to illustrate the approach.software requirements and architecture modeling for evolving non-secure applications into secure applications','Security requirements'
'although a number of requirements change classifications have been proposed in the literature, there is no empirical assessment of their practical value in terms of their capacity to inform change monitoring and management. this paper describes an investigation of the informative efficacy of a taxonomy of requirements change sources which distinguishes between changes arising from `market\', `organisation\', `project vision\', `specification\' and `solution\'. this investigation was effected through a case study where change data was recorded over a 16 month period covering the development lifecycle of a government sector software application. while insufficiency of data precluded an investigation of changes arising due to the change source of `market\', for the remainder of the change sources, results indicate a significant difference in cost, value to the customer and management considerations. findings show that higher cost and value changes arose more often from `organisation\' and `vision\' sources; these changes also generally involved the co-operation of more stakeholder groups and were considered to be less controllable than changes arising from the `specification\' or `solution\' sources. overall, the results suggest that monitoring and measuring change using this classification is a practical means to support change management, understanding and risk visibility.software requirements change taxonomy','Security requirements'
'software requirements negotiation','Security requirements'
'the importance of candidate software requirements can vary by orders of magnitude, yet most software providers do not have accurate and efficient means for selecting among them. this papers describes a case study at ericsson radio systems ab of two techniques for software requirements prioritizing as a means for determining the importance of candidate requirements, a pair-wise comparison technique and a numeral assignment technique. the results from the case study indicate that the pair-wise comparison technique is an efficient, informative and accurate means for finding the candidate requirements importance. we therefore recommend the pair-wise comparison technique for software requirements prioritizing. at ericsson we have extended its use in other development projects.software requirements prioritizing','Security requirements'
'requirements in security area are not same with other research areas. security-related requirements are listed into protection profile (pp). a protection profile defines an implementation-independent set of security requirements for a category of target of evaluations. generally, pp contains functional requirements and security assurance requirements about the security of development environment for it product or system and pp can applied to development site. this paper proposes some security-related check points for development site can be included into pp by analyzing iso/iec 15408 and iso/iec 21827.special checklist for security requirements in software development site','Security requirements'
'we present, in this paper, a role-based model for programming distributed cscw systems. this model supports specification of dynamic security and coordination requirements in such systems. we also present here a model-checking methodology for verifying the security properties of a design expressed in this model. the verification methodology presented here is used to ensure correctness and consistency of a design specification. it is also used to ensure that sensitive security requirements cannot be violated when policy enforcement functions are distributed among the participants. several aspect-specific verification models are developed to check security properties, such as task-flow constraints, information flow, confidentiality, and assignment of administrative privileges.specification and verification of security requirements in a programming model for decentralized cscw systems','Security requirements'
'a powerful methodology for specifying scenario-based requirements of reactive systems is described, in which behavioral requirements are \"played in\" directly from the system\'s gui or some abstract version thereof, and full behavior can then be \"played out\" freely, just as if a conventional system model were present. the approach is supported and illustrated by a tool we have built, which we call the play-engine. the ideas appear to be relevant to many stages of system development, including requirements engineering, specification, testing, and implementation.specifying and executing requirements','Security requirements'
'stakeholders frequently use speculative language when they need to convey their requirements with some degree of uncertainty. due to the intrinsic vagueness of speculative language, speculative requirements risk being misunderstood, and related uncertainty overlooked, and may benefit from careful treatment in the requirements engineering process. in this paper, we present a linguistically-oriented approach to automatic detection of uncertainty in natural language (nl) requirements. our approach comprises two stages. first we identify speculative sentences by applying a machine learning algorithm called conditional random fields (crfs) to identify uncertainty cues. the algorithm exploits a rich set of lexical and syntactic features extracted from requirements sentences. second, we try to determine the scope of uncertainty. we use a rule-based approach that draws on a set of hand-crafted linguistic heuristics to determine the uncertainty scope with the help of dependency structures present in the sentence parse tree. we report on a series of experiments we conducted to evaluate the performance and usefulness of our system.speculative requirements','Security requirements'
'in this paper, we present static verification of security requirements for cscw systems using finite-state techniques, i.e., model checking. the coordination and security constraints of cscw systems are specified using a role based collaboration model. the verification ensures completeness and consistency of the specification given global requirements. we have developed several verification models to check security properties, such as task-flow constraints, information flow or confidentiality, and assignment of administrative privileges. the primary contribution of this paper is a methodology for verification of security requirements during designing collaboration systems.static verification of security requirements in role based cscw systems','Security requirements'
'this paper introduces and develops the concept of tolerable sets for analyzing general security requirements. tolerable sets, and corresponding purging functions and invisibility based on the sets, are used to state and test such requirements. the approach used in this paper resulted from our attempt to apply the noninterference ideas of goguen and meseguer to the problem of stating special security requirements in the case of so-called trusted subjects. it turns out that the conditional purging function defined by goguen and meseguer is only one example, though an important one, of a conditional purging function. this paper provides a definition and characterization of a general class of purging functions similar to the purging function of goguen and meseguer. furthermore, it relates purging and invisibility to security requirements. some particular applications are described toward the end of the paper. at the end there are some critical remarks about purging functions.stating security requirements with tolerable sets','Security requirements'
'techniques for requirements acquisition must find new ways to gather information about brands and emotional responses to them. consumers will also likely have new types of service requirements that must be captured, documented, and easily traceable via new multidisciplinary techniques. in this installment of the requirements department, the authors describe a way to use storyboards to capture the interplay between human interaction and service design and so improve the quality of service design delivery.storyboarding requirements','Security requirements'
'computational e-mail systems, which allow mail messages to contain command scripts that automatically execute upon receipt, can be used as a basis for building a variety of collaborative applications. however, their use also presents a serious security problem because a command script from a sender may access/modify receiver\'s private files or execute applications on receiver\'s behalf. existing solutions to the problem either severely restrict i/o capability of scripts, limiting the range of applications that can be supported over computational e-mail, or permit all i/o to scripts, potentially compromising the security of the receiver\'s files. our model, called the intersection model of security, permits i/o for e-mail from trusted senders but without compromising the security of private files. we describe two implementations of our security model: an interpreter-level implementation and an operating systems-level implementation. we discuss the tradeoffs between the two implementations and suggest directions for future work.support for the file system security requirements of computational e-mail systems','Security requirements'
'system requirements','Security requirements'
'systematic documentation of requirements','Security requirements'
'security experts use their knowledge to attempt attacks on an application in an exploratory and opportunistic way in a process known as penetration testing. however, building security into a product is the responsibility of the whole team, not just the security experts who are often only involved in the final phases of testing. through the development of a black box security test plan, software testers who are not necessarily security experts can work proactively with the developers early in the software development lifecycle. the team can then establish how security will be evaluated such that the product can be designed and implemented with security in mind. the goal of this research is to improve the security of applications by introducing a methodology that uses the software system\'s requirements specification statements to systematically generate a set of black box security tests. we used our methodology on a public requirements specification to create 137 tests and executed these tests on five electronic health record systems. the tests revealed 253 successful attacks on these five systems, which are used to manage the clinical records for approximately 59 million patients, collectively. if non-expert testers can surface the more common vulnerabilities present in an application, security experts can attempt more devious, novel attacks.systematizing security test case planning using functional requirements phrases','Security requirements'
'although agile software development methods such as scrum and dsdm are gaining popularity, the consequences of applying agile principles to software product management have received little attention until now. in this paper, this gap is filled by the introduction of a method for the application of scrum principles to software product management.for this purpose, the \'agile requirements refinery\' is presented, an extension to the scrum process that enables product managers to cope with large requirements in an agile development environment. a real-life case study is presented to illustrate how agile methods can be applied to software product management. the experiences of the case study company are provided as a set of lessons learned that will help others to apply agile principles to their software product management process.the agile requirements refinery','Security requirements'
'the cost of reducing key-storage requirements in secure networks','Security requirements'
'assumptions are frequently made during requirements analysis of a system-to-be about the trustworthiness of its various components (including human components). these trust assumptions can affect the scope of the analysis, derivation of security requirements, and in some cases how functionality is realized. this paper presents trust assumptions in the context of analysis of security requirements. a running example shows how trust assumptions can be used by a requirements engineer to help define and limit the scope of analysis and to document the decisions made during the process. the paper concludes with a case study examining the impact of trust assumptions on software that uses the secure electronic transaction (set) specification.the effect of trust assumptions on the elaboration of security requirements','Security requirements'
'the good old days versus today: the changed hardware and software security requirements with automation','Security requirements'
'  we use the term requirements to denote what are often called functional requirements. requirements are located in the environment, which is distinguished from the machine to be built. a requirement is a condition over phenomena of the environment. a specification is a restricted form of requirement, providing enough information for the implementer to build the machine (by programming it) without further environment knowledge. to describe requirements appropriately we must fit our descriptions into an appropriate structure. this structure must respect the distinction between the machine and the environment, and the distinction between those environment properties that are given (indicative descriptions) and those that must be achieved by the machine (optative descriptions). formalisation is a fundamental problem of requirements engineering. since most environments are parts of the physical world, and therefore informal, the formalisation task is inescapable. some techniques are discussed for tackling this task. in particular, the use of designations is explained, and the distinction between definition and assertion. by using the smallest possible set of designated terms, augmented by appropriate definitions, the developer can create a narrow bridge between the environment and its descriptions in the requirements. in this way a sufficiently faithful approximation to the informal reality can be obtained.the meaning of requirements','Security requirements'
'the specification and implementation of &#8220;commercial&#8221; security requirements including dynamic segregation of duties','Security requirements'
'the purpose of this paper is to show the potential of umts long-term evolution using ofdm modulation by adopting a combined perspective on feedback channel design and resource allocation for ofdma multiuser downlink channel. first, we provide an efficient feedback scheme that we call mobility-dependent successive refinement that enormously reduces the necessary feedback capacity demand. the main idea is not to report the complete frequency response all at once but in subsequent parts. subsequent parts will be further refined in this process. after a predefined number of time slots, outdated parts are updated depending on the reported mobility class of the users. it is shown that this scheme requires very low feedback capacity and works even within the strict feedback capacity requirements of standard hsdpa. then, by using this feedback scheme, we present a scheduling strategy which solves a weighted sum rate maximization problem for given rate requirements. this is a discrete optimization problem with nondifferentiable nonconvex objective due to the discrete properties of practical systems. in order to efficiently solve this problem, we present an algorithm which is motivated by a weight matching strategy stemming from a lagrangian approach. we evaluate this algorithm and show that it outperforms a standard algorithm which is based on the well-known hungarian algorithm both in achieved throughput, delay, and computational complexity.throughput maximization under rate requirements for the ofdma downlink channel with limited feedback','Security requirements'
'requirements prioritization aims at identifying the most important requirements for a software system, a crucial step when planning for system releases and deciding which requirements to implement in each release. several prioritization methods and supporting tools have been proposed so far. how to evaluate their properties, with the aim of supporting the selection of the most appropriate method for a specific project, is considered a relevant question. in this paper, we present an empirical study aiming at evaluating two state-of-the art tool-supported requirements prioritization methods, ahp and cbrank. we focus on three measures: the ease of use, the time-consumption and the accuracy. the experiment has been conducted with 23 experienced subjects on a set of 20 requirements from a real project. results indicate that for the first two characteristics cbrank overcomes ahp, while for the accuracy ahp performs better than cbrank, even if the resulting ranks from the two methods are very similar. the majority of the users found cbrank the \'\'overall best\'\' method.tool-supported requirements prioritization','Security requirements'
'social recommendations have been rapidly adopted as important components in social network sites. however, they assume a cooperative relationship between parties involved. this assumption can lead to the creation of privacy issues and new opportunities for privacy infringements. traditional recommendation techniques fail to address these issues, and as a consequence the development of privacy-aware cooperative social recommender systems give rise to an important research gap. in this paper we identify key problems that arise from the privacy dimension of social recommendations and propose a comprehensive requirements architecture for building privacy-aware cooperative social recommender systems.towards a comprehensive requirements architecture for privacy-aware social recommender systems','Security requirements'
'security certification activities for software systems rely heavily on requirements mandated by regulatory documents and their compliance evidences to support accreditation decisions. therefore, the design of a workbench to support these activities should be grounded in a thorough understanding of the characteristics of certification requirements and their relationships with certification activities. to this end, we utilize our findings from the case study of a certification process of the united states department of defense (dod) to identify the design objectives of a requirements-driven workbench for supporting certification analysts. the primary contributions of this paper are: identifying key areas of automation and tool support for requirements-driven certification activities; an ontology-driven dynamic and flexible workbench architecture to address process variability; and a prototype implementation.towards a requirements-driven workbench for supporting software certification and accreditation','Security requirements'
'security and requirements engineering are one of the most important factors of success in the development of a software product line due to the complexity and extensive nature of them, given that a weakness in security can cause problems throughout the products of a product line. the main contribution of this work is that of providing a security standard-based process for software product line development, which is an add-in of activities in the domain engineering. this process deals with security requirements from the early stages of the product line lifecycle in a systematic and intuitive way especially adapted for product line based development. it is based on the use of the latest security requirements techniques, together with the integration of the common criteria (iso/iec 15408) and the iso/iec 17799 controls into the product line lifecycle. additionally, it deals with security artefacts variability and traceability, providing us with a security core assets repository. moreover, it facilitates the conformance to the most relevant security standards with regard to the management of security requirements, such as iso/iec 27001 and iso/iec 17799. finally, we will illustrate our proposed process by describing part of a real case study, as a preliminary validation of it.towards security requirements management for software product lines','Security requirements'
'this thesis describes a decision-centric traceability framework that supports software engineering activities such as architectural preservation, impact analysis, and visualization of design intent. we present a set of traceability patterns, derived from studying real-world architectural designs in high-assurance and high-performance systems. we further present a trace-retrieval approach that reverse engineers design decisions and their associated traceability links by training a classifier to recognize fragments of design decisions and then using the traceability patterns to reconstitute the decisions from their individual parts.tracing architecturally significant requirements','Security requirements'
'trade-off analysis between security policies for java mobile codes and requirements for java application','Security requirements'
'automation is a very promising technique to reduce the chances of flaws happening downstream the software production line. in this context, a very challenging problem is the transformation of requirements to software architectures. the challenge is even more crucial for quality requirements, as they represent the main driver of an architecture. this paper is an initial attempt to provide an approach that supports the transition from requirements to architecture for software security: a quality of ever growing importance in today&#8217;s world.transforming security requirements into architecture','Security requirements'
'identity management refers to the process of representing and recognising entities as digital identities in computer networks. authentication, which is an integral part of identity management, serves to verify claims about holding specific identities. identity management is therefore fundamental to, and sometimes include, other security constructs such as authorisation and access control. different identity management models will have different trust requirements. since there are costs associated with establishing trust, it will be an advantage to have identity management models with simple trust requirements. the purpose of this paper is to describe trust problems in current approaches to identity management, and to propose some solutions.trust requirements in identity management','Security requirements'
'information retrieval (ir) approaches have proven useful in recovering traceability links between free-text documentation and source code. ir-based traceability recovery approaches produce ranked lists of traceability links between pieces of documentation and source code. these traceability links are then pruned using various strategies and, finally, validated by human experts. in this paper we propose two contributions to improve the precision and recall of traceability links and, thus, reduces the required human experts\' manual validation effort. first, we propose a novel approach, trust race, inspired by web trust models to improve the precision and recall of traceability links: trust race uses intractability recovery approach to obtain a set of traceability links, which rankings are then re-evaluated using a set of other traceability recovery approaches. second, we propose a novel traceability recovery approach, his trace, to identify traceability links between requirements and source code through cvs/svn change logs using a vector space model (vsm). we combine a traditional recovery traceability approach with his trace to build trust race in which we use histraceas one expert adding knowledge to the traceability links extracttedfrom cvs/svn change logs. we apply trustrace on two case studies to compare its traceability links with those recovered using only the vsm-based approach, in terms of precision and recall. we show that trustrace improves with statistical significance the precision of the traceability links while also improving recall but without statistical significance.trust-based requirements traceability','Security requirements'
'security and privacy, accountability and anonymity, transparency and unobservability: these terms and more are vital elements for defining the overall security requirements---and, thus, security measurability criteria---of systems. however, these distinct yet related concepts are often substituted for one another in our discussions on securing trustworthy systems and services. this is damaging since it leads to imprecise security and trust requirements. consequently, this results in poorly defined metrics for evaluating system security. this paper proposes a trust-terms ontology, which maps out and defines the various components and concepts that comprise ict security and trust. we can use this ontology tool to gain a better understanding of their trust and security requirements and, hence, to identify more precise measurability criteria.trust-terms ontology for defining security requirements and metrics','Security requirements'
'good computer security depends upon knowing who is using the system and managing how each individual is able to access the information on the system. the next generation of computer should build in the kind of controls which are often added on to today\'s systems. this session will discuss the objectives for positively identifying system users, controlling their use of computerized resources, and providing accountability for users who are authorized and for those who attempt to exceed their authority. traditionally, user identification depends upon an identification code (id) and a password. password security has proven to be weak: where users are allowed to select their own passwords, they tend to select ones that are easily guessed, and where passwords are assigned, the users tend to write them down. future user identification systems should expand into something a user has (e.g. a badge) or something a user is (e.g. fingerprints). signature verification is a technology which is worth improving since it offers good security within existing legal and social contexts. ideally, user identification and authentication should be based upon a combination of two or more technologies.user identification, access control, and audit requirements','Security requirements'
'authentication is a security service that consists of verifying that someone\'s identity is as claimed. there are a number of challenges to presenting information from the authentication process to the user in a way that is meaningful and ensures security. we show examples where authentication requirements are not met, due to user behaviour and properties of existing user interfaces, and suggest some solutions to these problems.user interface requirements for authentication of communication','Security requirements'
'changed priorities and shifts in the nation\'s economic and social environment have uncovered in the private sector a chronic and crippling shortcoming in the linkage between the developers and the consumers of technology. if major dislocations and unaffordable inefficiences in our economic system are to be avoided, this gap must be closed.user requirements','Security requirements'
'we introduce the property of vacuity for requirements. a requirement is vacuous in a set of requirements if it is equivalent to a simpler requirement in the context of the other requirements. for example, the requirement \"if a then b\" is vacuous together with the requirement \"not a\". the existence of a vacuous requirement is likely to indicate an error. we give an algorithm that proves the absence of this kind of error for real-time requirements. a case study in an industrial context demonstrates the practical potential of the algorithm.vacuous real-time requirements','Security requirements'
'this paper is concerned with the correct specification and validation of quality attribute requirements (qars) that crosscut through a diverse set of complex system functions. these requirements act as modifiers of system level functional requirements and thus have substantial influence on the eventual architectural selection. because system designers traditionally address these requirements one quality attribute at a time, the process frequently results in qars that contain subtle conflicting behaviors. this paper presents an approach to qar-induced behavior validation and conflict detection via execution-based model checking early in the software development process. it explores the concept of conflicts between requirements with temporal and sequencing behaviors and presents an automated approach for discovering such conflicts. published 2012. this article is a us government work and is in the public domain in the usa. &#169; 2012 wiley periodicals, inc. (the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the us government. the us government is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright annotations thereon.)validating quality attribute requirements','Security requirements'
'validating requirements','Security requirements'
'the waterfall-based software development methodology usually requires significant efforts and lengthy intervals to develop, review, inspect, and test very large and complex real-time software systems. although the methodology itself is stable and mature, the quality of software products still largely depends on development efforts expended in testing, which is usually very costly and time consuming. the author describes the implementation of an integrated approach of software development, review, inspection, and testing based on well-identified software requirements. due to the huge size and the complex feature interactions of the 5ess switch software, the process of review, inspection, and testing has been very important to the project. to enhance the quality of the products, it is necessary to ensure the feature requirements requested by customers, external and internal, are developed correctly. the requirement traceability procedure described in the paper is a systematic method to assist 5ess switch scientists and engineers to achieve the goal. the results of the implementation of the requirement traceability procedure and its radix tool are significantverifying software requirements','Security requirements'
'the paper reports on an analysis technology based on the tracing approach to test trustworthy requirements of a distributed system. the system under test is instrumented suchthat it generates events at runtime to enable reasoning about the implementation of these requirements in a later step. specifically, an event log collected during a system run is converted into a specification of the system. the (trustworthy) requirements of the system must be formally specified by an expert who has sufficient knowledge about the behavior of the system. the reengineered model of the system and the requirement descriptions are then processed by an off-the-shelf model checker. the model checker generates scenarios that visualize fulfillments or violations of the requirements. a complex example of a concurrent system serves as a case study.verifying trustworthiness requirements in distributed systems with formal log-file analysis','Security requirements'
'we argue for the creation and use of a very lightweight requirements modeling language as an alternative to textual and pictorial requirements specifications.very lightweight requirements modeling','Security requirements'
'applications from the embedded consumer domain put challenging requirements on soc infrastructures, i.e. interconnect and memory. specifically, video applications demand large storage capacity and high bandwidth while data accesses can be irregular. the soc architectures used for implementing these applications typically contain a heterogeneous collection of processing elements and use a single interface to off-chip dram in order to provide the required storage capacity at a low cost. proper integration of interconnect and memory architecture is required to achieve the required bandwidths and latencies for accessing memory. the application requirements as well as the characteristics and constraints for accessing memory are key inputs for noc design. future memory technologies may cause a paradigm shift by offering high-bandwidth memory access, possibly via multiple memory interfaces.video processing requirements on soc infrastructures','Security requirements'
'this paper introduces an approach to multiperspective requirements engineering (preview) which has been designed for industrial use and discusses our practical experience in applying preview. we have developed a flexible model of viewpoints and, using examples from an industrial application, show how this can be used to organise system requirements derived from radically different sources. we show how \'concerns\', which are key business drivers of the requirements elicitation process, may be used to elicit and validate system requirements. they are decomposed into questions which must be answered by system stakeholders. we briefly describe the process of using preview which has been designed to allow incremental requirements elicitation. finally, we discuss some practical considerations which emerged when the approach was applied in industry.viewpoints for requirements elicitation','Security requirements'
'current developments in it point towards the formation of loosely coupled enterprises, often referred to as virtual enterprises. these enterprises require both secure and flexible collaboration between unrelated information systems. web services technology can be used as an ideal platform for realising virtual enterprises throughh their ease of integration, flexibility, and support of xml vocabularies. to ensure the successful implementation of web services within virtual enterprises, new approaches to security are required. together with authentication, access control has been seen as a pillar of it security approaches. the focus of this paper will be to determine requirements that could play a role when the access control policies of such enterprises are defined.virtual enterprise access control requirements','Security requirements'
'we present an approach to requirements validation in which the formal specification of the requirements is directly interpreted and the results are visually presented to the customer through a graphical user interface, relying on the customer to visually validate the specified requirements. the communication between the user interface and the specification interpreter is accomplished through corba. the approach supports the cooperation of customers and developers in eliciting and validating the requirements. we present a case study of the application of the technique to the validation of a generic access control component. the use of corba has the advantage that any corba-compliant language can be used for the user interface, independently of the implementation of the specification interpreter. the contributions of the paper are 1) the presentation of a case study of the visual requirementvalidation technique, 2) the revision and improvement of a previously presented visual validation technique, and 3) the application of requirements validation to a reusable component.visual requirements validation','Security requirements'
'emotional requirements capture the game de- signer\'s vision for the player\'s emotional experi- ence and are used to facilitate communication be- tween pre-production and production teams. how- ever, production-phase deficiencies in emotional re- quirements have been identified. in this work, we ex- tend the definition of emotional requirements to include emotion prototypes and emotion markers and present improved techniques for eliciting, capturing and visu- alizing emotional requirements. a detailed investiga- tion of one gameplay scenario is presented, with a focus on evaluating visualization techniques for emotional re- quirements. the solutions developed in this work met the needs of all development team members and appear to be general solutions for the domain. keywords: requirements visualization, non- functional requirements, emotion, emotional require- ments, video game.visualizing emotional requirements','Security requirements'
'requirements engineering has gained growing attention in both academia and industry, as today&#8217;s software intensive systems are expected to provide highly user-centric functions and qualities. thus, it is important to understand under what situations existing requirements engineering practice is not working well. continuing our probe into the industrial practices status quo, this paper reports the results from a recent survey of requirements practices in china in 2009. the web-based survey of requirements engineering practices focuses on requirements elicitation techniques, requirements representation techniques. although purporting to report on the state-of-the-art of requirements engineering in china, it is likely to portray the state-of-the-art of re worldwide as well.why requirements engineering fails','Security requirements'
'the answers to many important performance relatedquestions with multiclass queueing models depends uponhaving estimates for the service times of different classesof jobs. we present a general approach to infer the per-classservice times at different servers in an environmentwhere only server throughput, utilization and per-class responsetime measurements are available. the per-classservice times are solutions to an optimization problemwith queueing-theoretic formulas in the objective and constraints.we further study the impact of the variance of servicetimes on the variance of response times. a few casestudies are presented to demonstrate the power of our approach.workload service requirements analysis','Security requirements'
'this paper presents suggestions in incorporating essays in a computer security course. the first approach is a short essay that presents the student\'s personal view on computer security within the campus. in the paper, s/he uses information from the university\'s it website and documents, as well as security disclosures of websites and software applications. this assignment is presented at the beginning of the course and serves as a basis of discussion both for the development of writing skills and for the identification of vulnerabilities faced by the campus community on the intranet and internet illustrating the need for clear written regulations in it security. the second approach requires an extended essay that describes the work performed in a term project. the essay assignment is developed in several stages that include a summary proposal, partial and complete drafts and final paper. the writing requires the student to go through all the steps needed to develop a significant scientific paper. the work must be original, supported by extensive literature review and follow a topic relevant to computer security. the essay development is complemented by ongoing class discussion of scientific papers, and by an in class presentation. the course was already piloted with the extended essay version and has received significant positive feedback from the students. it was recently approved to fit the graduation writing requirements at our institution.writing requirements in computer security','Security requirements'
'a threat analysis framework and methodology was developed by the authors to catalogue threats, vulnerabilities, attacks and countermeasures for smart cards (contact and contactless) and wireless sensor network node technologies. the goal of this research was to determine \"security lessons\" learned from the world of smart cards that may be applied to wireless sensor network nodes and vice versa.a comparative analysis of common threats, vulnerabilities, attacks and countermeasures within smart card and wireless sensor network node technologies','Side-channel analysis and countermeasures'
'active hardware attacks succeed in deriving cryptographic secrets from target devices. they were originally proposed for systems implementing rsa, fiat-shamir scheme, and schnorr\'s scheme.common targets for these attacks are systems used for client authentication in order to access services, e.g., pay-per view tv, video distribution and cellular telephony. these client systems hold secrets, typically cryptographic keys, owned by the service provider, and often implement the fiat-shamir identification scheme. given the strength of active attacks and the increasingly wide deployment of client systems, it is desirable to design proactive countermeasures for them.in this paper we focus on the fiat-shamir scheme. we prove that the conventional active attack can be easily avoided through appropriate system and protocol configuration; we denote this configuration as the precautious fiat-shamir scheme. we argue that proactive countermeasures against active attacks are feasible and lead to systems that are inherently resistant to active attacks by careful protocol design, rather than ad hoc solutions.active hardware attacks and proactive countermeasures','Side-channel analysis and countermeasures'
'to explore mission-critical information, an adversary using active traffic analysis attacks injects probing traffic into the victim network and analyzes the status of underlying payload traffic. active traffic analysis attacks are easy to deploy and hence become a serious threat to mission critical applications. this paper suggests statistical pattern recognition as a fundamental technology to evaluate effectiveness of active traffic analysis attacks and corresponding countermeasures. our evaluation shows that sample entropy of ping packets\' round trip time is an effective feature statistic to discover the payload traffic rate. we proposesimple countermeasures that can significantly reduce the effectiveness of ping-based active traffic analysis attacks. our experiments validate the effectiveness of this scheme, whichcan also be used in other scenarios.active traffic analysis attacks and countermeasures','Side-channel analysis and countermeasures'
'network violence can be divided into network symbol violence, the network trial, and group violence, the cause of which includes the hidden characteristics of networks, collective behavior and entertainment desire, the dislocation of freedom, and the deficiency of law and moral restriction. the prevention and cure strategies of network violence include intensifying network management, implementing network real-name, strengthening the construction of network legal system and educating the youth.an analysis on causes and countermeasures of network violence','Side-channel analysis and countermeasures'
'implementation attacks pose a serious threat to the security of cryptographic algorithms and protocols. in such attacks, not the abstract descriptions of cryptographic methods are attacked but their practical realizations in cryptographic devices. this opens up a wide range of powerful attacks, which are introduced in this article. also the main approaches to counteract implementation attacks are discussed.an introduction to implementation attacks and countermeasures','Side-channel analysis and countermeasures'
'hotlinking is a web behavior that links web resources on a hosting site into a webpage belonging to another site. however, unauthorized hotlinking is unethical, because it not only violates the interests of hosting sites by consuming bandwidth and detracting site visiting traffic but also violates the copyrights of protected materials. to fully understand the nature of hotlinking, we conduct a large-scale measurement study and observe that hotlinking widely exists over the internet and is severe in certain categories of websites. moreover, we perform a detailed postmortem analysis on a real hotlink-victim site. after analyzing a group of commonly used hotlinking attacks and the weakness of current defense methods, we present an anti-hotlinking framework for protecting materials on hosting servers based on existing network security techniques. the framework can be easily deployed at the server-side with moderate modifications, and is highly customizable with different granularities of protection. we implement a prototype of the framework and evaluate its effectiveness against hotlinking attacks.an investigation of hotlinking and its countermeasures','Side-channel analysis and countermeasures'
'with the rapid development of the internet, ec is growing rapidly. the use of information technology to transmit and process business information has inevitably brought the defects and shortcomings of its own into the field of ec.the security issue is becoming the primary factor restricting the development of electronic commerce, and is the most important issues related to ec system can be run successfully. ec security issues can be basically divided into two parts, namely, computer network security and business transaction security. the paper discusess these two parts and gives the countermeasures in the last.analyse on ec security problems and countermeasures','Side-channel analysis and countermeasures'
'npo is one of the most important sectors in national economy, which has made great contribution to economic development. perfect incentive mechanism on npo performance management is the important guarantee of improving performance management level. however, present npo incentive mechanism has the problems of npo material incentive to employees is insufficient, npo inner motivation to employees is inadequate and npo tax incentive is imperfect, which hidden the healthy development of npo in a certain degree. put forward suggestions of trying hard to improve npo income, increasing inner incentive to npo employees and perfecting tax incentive to npo on the basis of theoretical and npo incentive mechanism analysis.analysis and countermeasures of incentive mechanism on npo performance management','Side-channel analysis and countermeasures'
'the present study is designed to explore the reasons for pragmatic failures occurred in chinese learners\' oral english with the guidance of error analysis and to figure out effective countermeasures to remove them. furthermore, we try to provide a tentative verification of the role pragmatic competence plays in oral english teaching and learning, which is supposed to be helpful for successful communication, especially in cross-cultural context.analysis and countermeasures of pragmatic failures in chinese learners\' oral english','Side-channel analysis and countermeasures'
'recently, major portal sites are suffering from a number of attacks and it is growing exponentially. july 2009, there has been system failure on government sites and some of the major portal sites due to the ddos (distributed denial of service) attack. moreover, portal sites are exploited by a cross-site scripting vulnerability in 2010. to solve these problems, each portal site made an effort to eliminate the security vulnerability of the website and to protect personal information such as id and password. however, portal sites still have the security vulnerabilities against arp (address resolution protocol) poisoning attack and the certificate spoofing attack. in this paper, we show the results of our penetration test and present the countermeasures on the arp (address resolution protocol) poisoning attack and the certificate spoofing attack.analysis and countermeasures of security vulnerability on portal sites','Side-channel analysis and countermeasures'
'with the rapid development of chinese economy, flight delay becomes more and more serious and brings a lot of complaints and emergency situations. the present situation of flight delay in our country and various influencing factors of flight delay is analyzed. related statistic characteristics of flight delay are given by mathematical analysis and computation. some measures to reduce flight delay are put forward from an integrated and systematic viewpoint, which is related to air traffic control departments, airlines and airports.analysis and countermeasures to flight delay based on statistical data','Side-channel analysis and countermeasures'
'traditional development model of coal industry has many serious problems, such as environmental pollution, low output ratio of coal, low utilization ratio and low recovery ratio, etc. based on the analysis of the advantages of the coal circular economy industry, the development models of coal circulatory economy industry chain is studied, and then the industry chain model of circulatory economy for huainan mining group is analyzed. lastly some suggestions are put forward, which is helpful to carry out coal circulatory economy industry chain development mode and provide reference for sustain development of coal enterprises of china.analysis of and countermeasures for coal circular economy developing model in china','Side-channel analysis and countermeasures'
'environmental logistics is the new mode of cycle logistics, which promotes the healthy development of the economy and consumer life. it changes the original one-way function relationship between economic development and logistics or consumption life and logistics. it also reduces the damage logistics brings to the environment. it is an important aspect in the economic development of low-carbon cycle. at present, environmental logistics is regarded as the development focus of modern logistics industry throughout the world. therefore the corresponding environmental logistics policies and regulations are introduced to lay the foundation for sustainable development of environmental logistics. in china, it has also gradually aroused the wide concern of the industry, and is considered the important trend of china\'s future logistics industry development, but it is still in its infancy and faces many bottlenecks in the development constraints. this article analyzes the promotion difficulties of environmental logistics and tries to find out the way to break through.analysis of environmental logistics promotion difficulties and their countermeasures','Side-channel analysis and countermeasures'
'in recent research it turned out that boolean verification of digital signatures in the context of ws-security is likely to fail: if parts of a soap message are signed and the signature verification applied to the whole document returns true, then nevertheless the document may have been significantly altered.in this paper, we provide a detailed analysis on the possible scenarios that enable these signature wrapping attacks. derived from this analysis, we propose a new solution that uses a subset of xpath instead of id attributes to point to the signed subtree, and show that this solution is both efficient and secure.analysis of signature wrapping attacks and countermeasures','Side-channel analysis and countermeasures'
'high energy-consuming enterprises consumption account for a large proportion in the total social consumption in china, the key problems which are commonly cared by both sides of power suppliers and consumers is discovered by analyzing consuming characteristics and behaviors, the electricity consumption of high energy-consuming enterprises is studied, the connection is analyzed between electricity price and electricity demand. it has guiding significance for making electricity price of the high energy-consuming and the decision-making power. large industry consumers to can have a powerful consuming wind power by increasing the appropriate device input and taking appropriate management and technical measures, the peak-regulation of system can be improved.analysis of the bearing capacity and countermeasures of electricity price in high-energy consuming enterprises','Side-channel analysis and countermeasures'
'the present study is designed to explore the reasons for pragmatic failures occurred in chinese learners\' oral english with the guidance of error analysis and to figure out effective countermeasures to remove them. furthermore, we try to provide a tentative verification of the role pragmatic competence plays in oral english teaching and learning, which is supposed to be helpful for successful communication, especially in cross-cultural context.analysis on and countermeasures of pragmatic failures in chinese learners\' oral english','Side-channel analysis and countermeasures'
'since china entered wto, the policy restrictions in many industries have been gradually loosened or eliminated. the foreign companies have obtained sufficient understanding on china\'s markets through the long-term grope, as a result, many foreign enterprises begin to step towards sole proprietorship and they gradually expedited their speed. influences from such a change and trend upon china\'s economy are presenting themselves gradually. this article is to analyze the evolution of the phenomenon, the features and the unfavorable impacts upon china&#8217;s economy in order to find out the countermeasures.analysis on features and countermeasures of foreign-founded enterprises\' sole proprietorship in china','Side-channel analysis and countermeasures'
'different species of malicious software (malware) have been around for quite a while. add a command and control structure--and here you are: a \"cyber army\" of hijacked machines waiting for the commands of the so-called \"bot herder\" ready to serve the master\'s will. botnets may be used for distributing spam, for installing additional malware, for information harvesting, for distributed denial of service attacks and for other actions initiated and controlled by the bot herder. today, thousands of botnets are well understood. their actions are observed and in some cases controlled/limited. in addition, experts active in this field argue that there is a very large number of botnets escaping tracking efforts by mechanisms such as frequent reconfiguration and frequent migration of command-and-control structures. in his keynote, peter martini will comment on the challenge of detecting botnets, on aggregation and clustering of similar species of malicious software and on countermeasures used today. he will comment on the relevance of botnet size and the problem of measuring the current size of well-known botnets. finally, he will comment on legal issues and missing pieces in the fight against botnets: botnets have come to stay.botnets  --  detection, classification and countermeasures','Side-channel analysis and countermeasures'
'we describe several software side-channel attacks based on inter-process leakage through the state of the cpu&#8217;s memory cache. this leakage reveals memory access patterns, which can be used for cryptanalysis of cryptographic primitives that employ data-dependent table lookups. the attacks allow an unprivileged process to attack other processes running in parallel on the same processor, despite partitioning methods such as memory protection, sandboxing and virtualization. some of our methods require only the ability to trigger services that perform encryption or mac using the unknown key, such as encrypted disk partitions or secure network links. moreover, we demonstrate an extremely strong type of attack, which requires knowledge of neither the specific plaintexts nor ciphertexts, and works by merely monitoring the effect of the cryptographic process on the cache. we discuss in detail several such attacks on aes, and experimentally demonstrate their applicability to real systems, such as openssl and linux&#8217;s dm-crypt encrypted partitions (in the latter case, the full key can be recovered after just 800 writes to the partition, taking 65 milliseconds). finally, we describe several countermeasures for mitigating such attacks.cache attacks and countermeasures','Side-channel analysis and countermeasures'
'logical attacks on smart cards have been used for many years, but their attack potential is hindered by the processes used by issuers to verify the validity of code, in particular bytecode verification. more recently, the idea has emerged to combine logical attacks with a physical attack, in order to evade bytecode verification. we present practical work done recently on this topic, as well as some countermeasures that can be put in place against such attacks, and how they can be evaluated by security laboratories.combined attacks and countermeasures','Side-channel analysis and countermeasures'
'the design of ka-band satellite fade countermeasure (fcm) systems is conditioned by the detection/prediction algorithm to be included within practical dsp-based fcm controllers. it depends upon the ability of systems to efficiently integrate the dynamic and stochastic nature of the ka-band fading process which is dominated by rain attenuation and amplitude scintillation. the paper analyzes the modeling and statistical performance of two predictive fade detection algorithms. prediction is introduced as a way to minimize the impact of the finite response time on the ber/throughput of practical fcm systems. both fixed (fdm) and variable (vdm) detection margin strategies are introduced and compared in terms of their margin requirements, fcm utilization factor, and channel capacity utilization. the vdm is shown to be more efficient than its fixed counterpart. the long-term ber availability and average user data throughput of a vdm/fixed-fec/adaptive transmission rate fcm are then evaluated for a typical low-power low-rate ka-band in-bound vsat linkcomparative analysis and performance of two predictive fade detection schemes for ka-band fade countermeasures','Side-channel analysis and countermeasures'
'modern embedded systems manage sensitive data increasingly often through cryptographic primitives. in this context, side-channel attacks, such as power analysis, represent a concrete threat, regardless of the mathematical strength of a cipher. evaluating the resistance against power analysis of cryptographic implementations and preventing it, are tasks usually ascribed to the expertise of the system designer. this paper introduces a new security-oriented data-flow analysis assessing the vulnerability level of a cipher with bit-level accuracy. a general and extensible compiler-based tool was implemented to assess the instruction resistance against power-based side-channels. the tool automatically instantiates the essential masking countermeasures, yielding a x2.5 performance speedup w.r.t. protecting the entire code.compiler-based side channel vulnerability analysis and optimized countermeasures application','Side-channel analysis and countermeasures'
'the present paper addresses a number of theoretical issues related to haddon\'s countermeasure strategies. an attempt is made to formalize the strategies using a model of causation that generalizes haddon\'s energy transfer model of unwanted phenomena.important problems associated with the term \"barrier\" are outlined. by means of applying the causal model to the level of countermeasure a clarification of the terminology is proposed.finally, an attempt is made to extend the scope of countermeasures from causal actions to communicative actions, i.e. actions based on signs produced in order to influence the action of the agent interpreting them.countermeasures and barriers','Side-channel analysis and countermeasures'
'computer security has become vital for protecting users, applications and data, yet the field still faces severe shortages in skilled professionals. typical methods to teach security from textbooks and academic papers are not engaging and take considerable time. our hypothesis is that a security game that closely emulates real-world systems can improve learning about computer security above and beyond just reading technical documents. our security game, countermeasures, provides a game-type environment for learning and practicing security skills through a series of guided objectives. countermeasures uses a real, interactive shell for input and targets a real server for exploits to provide an environment resembling security systems currently deployed. evaluation with 20 test subjects illustrates the merits and shows the potential of our approach.countermeasures','Side-channel analysis and countermeasures'
'it is a complicated systemic work since the informatization is carried into chinese construction enterprises for its huge investments,long periods and high risks. it is necessary to supervise them in order to reduce and disperse the project risks of construction enterprises&#8217; informatization. the thesis puts forward several effective solutions after analyzing the necessities, statuscurrent status analysis and countermeasures on supervising informatization of chinese construction enterprises','Side-channel analysis and countermeasures'
'this paper describes a dfa attack on the aes key schedule. this fault model assumes that the attacker can induce a single byte fault on the round key. it efficiently finds the key of aes-128 with feasible computation and less than thirty pairs of correct and faulty ciphertexts. several countermeasures are also proposed. this weakness can be resolved without modifying the structure of the aes algorithm and without decreasing the efficiency.differential fault analysis on aes key schedule and some countermeasures','Side-channel analysis and countermeasures'
'the hmac algorithm is widely used to provide authentication and message integrity to digital communications. however, if the hmac algorithm is implemented in embedded hardware, it is vulnerable to side-channel attacks. in this paper, we describe a dpa attack strategy for the hmac algorithm, based on the sha-2 hash function family. using an implementation on a commercial fpga board, we show that such attacks are practical in reality. in addition, we present a masked implementation of the algorithm, which is designed to counteract first-order dpa attacks.differential power analysis of hmac based on sha-2, and countermeasures','Side-channel analysis and countermeasures'
'the simplicity of smtp mail can be combined with the robustness of the sendmail mta program and misused in numerous ways to create extraordinary and powerful e-mail bombs. these e-mail bombs can be launched in many different attack scenarios which can easily flood and shut down chains of smtp mail servers. sendmail-based smtp mail relays also can be used covertly to distribute messages and files that could be very damaging to the integrity and brands of victims. this article discusses mail-bombing techniques, automated attack tools, and countermeasures. also discussed is an actual internet-based attack that was launched in 1997 on the langley afb smtp e-mail infrastructure. the authors also present an analysis of the cyber attack, graphs illustrating the attack volume, and a statistical e-mail bomb early warning systeme-mail bombs and countermeasures','Side-channel analysis and countermeasures'
'we describe several software side-channel attacks based on inter-process leakage through the state of the cpu&#x2019;s memory cache. this leakage reveals memory access patterns, which can be used for cryptanalysis of cryptographic primitives that employ data-dependent table lookups. the attacks allow an unprivileged process to attack other processes running in parallel on the same processor, despite partitioning methods such as memory protection, sandboxing, and virtualization. some of our methods require only the ability to trigger services that perform encryption or mac using the unknown key, such as encrypted disk partitions or secure network links. moreover, we demonstrate an extremely strong type of attack, which requires knowledge of neither the specific plaintexts nor ciphertexts and works by merely monitoring the effect of the cryptographic process on the cache. we discuss in detail several attacks on aes and experimentally demonstrate their applicability to real systems, such as openssl and linux&#x2019;s dm-crypt encrypted partitions (in the latter case, the full key was recovered after just 800 writes to the partition, taking 65 milliseconds). finally, we discuss a variety of countermeasures which can be used to mitigate such attacks.efficient cache attacks on aes, and countermeasures','Side-channel analysis and countermeasures'
'as the dishonesty violations of the listed companies occur from time to time, the credit risk of listed companies has become an issue related to the financial system stability. by understanding the concepts of credit and credit risk, a quantitative research has been carried out on the credit taking listed company in anhui province as an example on the premise of considering the availability of data. according to analysis of the company\'s credit system, combined with of 46 financial statements of listed companies in anhui province, 12 indicators are selected for analysis and ranks with the company credit. then through the cluster analysis of listed companies, these companies will be divided into three types. as results, some conclusions have been drawn from the empirical analysis that being work along both effective regulation and institutional innovation is an effective solution to the problem for the way of enhancing the company credit.empirical analysis and countermeasures research on corporate credit taking listed companies of anhui province as an example','Side-channel analysis and countermeasures'
'web services are increasingly becoming an integral part of next-generation web applications. a web service is defined as a software system designed to support interoperable machine-to-machine interaction over a network based on a set of xml standards. this new architecture and set of protocols brings new security challenges such as confidentiality, integrity, anonymity, authentication, authorization and availability of requested services. vulnerabilities in web services are very dangerous since they can be used by attackers to damage the company\'s information system and steal confidential data. in this paper, we carry out an experimental analysis of attacks against web services. we demonstrate experimentally three types of attacks and we reveal dangerous techniques and tools used by attackers that administrators have to prevent. moreover, we study the effects of these attacks by observing their impact on information system data and resources. finally, we propose general countermeasures to prevent and mitigate such attacks.experimental analysis of attacks against web services and countermeasures','Side-channel analysis and countermeasures'
'security is a concern in the design of smartcards. it is possible to leak much side channel information related to secret key when cryptographic algorithm runs on smartcards. power analysis attacks are a very strong cryptanalysis by monitoring and analyzing power consumption traces. in this paper, we experiment exclusive or operation. we also analyze the tendency of state-of-the-art regarding hardware countermeasures and experiments of hamming-weights on power attacks. it can be useful to evaluate a cryptosystem related with hardware security technology.experiments and hardware countermeasures on power analysis attacks','Side-channel analysis and countermeasures'
'smes ??small and medium enterprises?? have become an important force in promoting the national economy, but in the business of crisis and instability in the international situation as a whole case of stage financing has become the most important issues to smes. for this purpose, the authors analyzed the status of financing for the sustainable development of smes, and gave the corresponding countermeasures and suggestions.financing situation of smes and countermeasures analysis','Side-channel analysis and countermeasures'
'the dns bandwidth amplification attack (baa) is a distributed denial-of-service attack in which a network of computers floods a dns server with responses to requests that have never been made. amplification enters into the attack by virtue of the fact that a small 60-byte request can be answered by a substantially larger response of 4,000 bytes or more in size. we use the prism probabilistic model checker to introduce a continuous time markov chain model of the dns baa and three recently proposed countermeasures, and to perform an extensive cost-benefit analysis of the countermeasures. our analysis, which is applicable to both dns and dnssec (a security extension of dns), is based on objective metrics that weigh the benefits for a server in terms of the percentage increase in the processing of legitimate packets against the cost incurred by incorrectly dropping legitimate traffic. the results we obtain, gleaned from more than 450 prism runs, demonstrate significant differences between the countermeasures as reflected by their respective net benefits. our results also reveal that dnssec is more vulnerable than dns to a baa attack, and, relatedly, dnssec derives significantly less benefit from the countermeasures.formal analysis of the dns bandwidth amplification attack and its countermeasures using probabilistic model checking','Side-channel analysis and countermeasures'
'two-player pursuit-evasion games in the literature typically either assume both players have perfect knowledge of the opponent\'s positions or use primitive sensing models. this unrealistically skews the problem in favor of the pursuer who needs only maintain a faster velocity at all turning radii. in real life, an evader usually escapes when the pursuer no longer knows the evader\'s position. in our previous work, we modeled pursuit evasion without perfect information as a two-player bimatrix game by using a realistic sensor model and information theory to compute game-theoretic payoff matrices. that game has a saddle point when the evader uses strategies that exploit sensor limitations, whereas the pursuer relies on strategies that ignore the sensing limitations. in this paper, we consider, for the first time, the effect of many types of electronic countermeasures (ecm) on pursuit-evasion games. the evader\'s decision to initiate its ecm is modeled as a function of the distance between the players. simulations show how to find optimal strategies for ecm use when initial conditions are known. we also discuss the effectiveness of different ecm technologies in pursuit-evasion games.game and information theory analysis of electronic countermeasures in pursuit-evasion games','Side-channel analysis and countermeasures'
'in this paper we investigate implementations of aria on an 8-bit smartcard. our investigation focuses on the resistance against different types of differential power analysis (dpa) attacks. we show that an unprotected implementation of aria allows to deduce the secret key with a low number of measurements. in order to thwart these simple dpa attacks, we mask and randomize the aria implementation on the smartcard. it turns out that due to the structure of aria, a masked implementation requires significantly more resources than an unprotected implementation. however, the masked and randomized implementation provides a high resistance against power analysis attacks.investigations of power analysis attacks and countermeasures for aria','Side-channel analysis and countermeasures'
'the dawning ubiquitous computing age demands a new attacker model for the myriads of pervasive computing devices used: since a potentially malicious user is in full control over the pervasive device, additionally to the cryptographic attacks the whole field of physical attacks has to be considered. most notably are here so-called side channel attacks, such as differential power analysis (dpa) attacks. at the same time, the deployment of pervasive devices is strongly cost-driven, which prohibits expensive countermeasures. in this article we survey a broad range of countermeasures and discuss their suitability for ultra-constrained devices, such as passive rfid-tags. we conclude that adiabatic logic countermeasures, such as 2n-2n2p and sal, seem to be promising candidates, because they increase the resistance against dpa attacks while at the same time lowering the power consumption of the pervasive device.lightweight cryptography and dpa countermeasures','Side-channel analysis and countermeasures'
'security within the cloud is of paramount importance as the interest and indeed utilization of cloud computing increase. multitenancy in particular introduces unique security risks to cloud computing as a result of more than one tenant utilizing the same physical computer hardware and sharing the same software and data. the purpose of this paper is to explore the specific risks in cloud computing due to multitenancy and the measures that can be taken to mitigate those risks.multitenancy - security risks and countermeasures','Side-channel analysis and countermeasures'
'with the rapid development of shanghai railway system and the surrounding commercial development, deep excavation near the underground railway tunnel becomes inevitable. based on the design proposal of one deep excavation project near shanghai subway line no.7 in bao qing road, this article aims to predict the dynamic effects of deep excavation on the existing nearby underground railway tunnel by numerical analysis using finite element package abaqus. in order to prevent excessive displacement of continuous wall due to the deep excavation, triaxial concrete pile will be used to strengthen the trench wall and the the bottom of the excavation will also be strengthened. after comparison, it is found that these strengthening measures can prevent the deformation of nearby underground tunnel effectively, but special attention is required to choose the optimized reinforcing strength.numerical analysis and countermeasures of influence of excavation on adjacent tunnels','Side-channel analysis and countermeasures'
'occupation burnout has recently been a fascinating topic in research fields like management and psychology. due to work pressure and other reasons, college teachers\' occupation burnout catches more concerns than ever. as teachers\' occupation burnout appears generically and yet not being optimistic in zhejiang province, we conduct recently a questionnaire survey on teachers\' occupation burnout at a local college, and then manipulate the spss software to carry out the statistical analysis. the final results reveal that teachers\' occupation burnout in this college is quite negative and alarming. furthermore, we would propose several corresponding management countermeasures for occupation confidence reconstruction.on statistical analysis and management countermeasures of occupation burnout for college teachers','Side-channel analysis and countermeasures'
'because of their shorter key sizes, cryptosystems based on elliptic curves are being increasingly used in practical applications. a special class of elliptic curves, namely, koblitz curves, offers an additional but crucial advantage of considerably reduced processing time. in this article, power analysis attacks are applied to cryptosystems that use scalar multiplication on koblitz curves. both the simple and the differential power analysis attacks are considered and a number of countermeasures are suggested. while the proposed countermeasures against the simple power analysis attacks rely on making the power consumption for the elliptic curve scalar multiplication independent of the secret key, those for the differential power analysis attacks depend on randomizing the secret key prior to each execution of the scalar multiplication.power analysis attacks and algorithmic approaches to their countermeasures for koblitz curve cryptosystems','Side-channel analysis and countermeasures'
'one of the biggest challenges of designers of cryptographic devices is to protect the devices against implementation attacks. power analysis attacks are among the strongest of these attacks. this article provides an overview of power analysis attacks and discusses countermeasures against them. in particular, this article summarizes recent results with countermeasures that can be implemented at the cell level. many countermeasures of this kind have been proposed, but several limitations of these countermeasures have been identified.power analysis attacks and countermeasures','Side-channel analysis and countermeasures'
'in the engineering implementation of ason control plane, rsvp_te protocol is widely used. based on the specific analysis of reliability mechanisms of rsvp_te protocol, security problems that rsvp_te protocol may confront with are discussed, and the corresponding countermeasures are proposed in the paper.security analysis and countermeasures of rsvp-te signaling protocol','Side-channel analysis and countermeasures'
'nowadays, the music industry is moving from analog to digital. most of the big portal sites provide commercial online music streaming services. in this paper, we analyzed the security of the korean commercial online music streaming services which are provided by the korea\'s major portal sites(dosirak, cyworld, and naver). we will show attacks on commercial online music streaming services that lead to an infringement of the copyright and we will also propose countermeasures for online commercial music streaming services.security analysis on commercial online music streaming service and countermeasures','Side-channel analysis and countermeasures'
'in this paper, we address issues related to defending against widespreading worms on the internet. we study a new class of worms called the selfadaptive worms. these worms dynamically adapt their propagation patterns to defensive countermeasures, in order to avoid or postpone detection, and to eventually infect more computers. we show that existing worm detection schemes cannot effectively defend against these self-adaptive worms. to counteract these worms, we introduce a game-theoretic formulation to model the interaction between worm propagator and defender. we show that the effective integration of multiple defensive schemes (e.g., worm detection, forensics analysis) is critical for defending against self-adaptive worms. we propose different combinations of defensive schemes for different kinds of self-adaptive worms, and evaluate the performance of defensive schemes based on real-world traffic traces.self-adaptive worms and countermeasures','Side-channel analysis and countermeasures'
'in this paper, we address issues related to the modeling, analysis, and countermeasures of worm attacks on the internet. most previous work assumed that a worm always propagates itself at the highest possible speed. some newly developed worms (e.g., &#8220;atak&#8221; worm) contradict this assumption by deliberately reducing the propagation speed in order to avoid detection. as such, we study a new class of worms, referred to as self-disciplinary worms. these worms adapt their propagation patterns in order to reduce the probability of detection, and eventually, to infect more computers. we demonstrate that existing worm detection schemes based on traffic volume and variance cannot effectively defend against these self-disciplinary worms. to develop proper countermeasures, we introduce a game-theoretic formulation to model the interaction between the worm propagator and the defender. we show that an effective integration of multiple countermeasure schemes (e.g., worm detection and forensics analysis) is critical for defending against self-disciplinary worms. we propose different integrated schemes for fighting different self-disciplinary worms, and evaluate their performance via real-world traffic data.self-disciplinary worms and countermeasures','Side-channel analysis and countermeasures'
'simple power analysis (spa), first introduced by kocher et al. in [1], is a technique that involves directly interpreting power consumption measurements collected during cryptographic operations. although the possibility of attacking elliptic curve cryptosystems (ecc) by spa repeatedly appears in research papers, all accessible references evade the essence of reporting conclusive experiments where actual elliptic curve cryptosystems were successfully attacked and prevented. in this paper we describe power analysis experiments conducted on 3 implementations of elliptic curve cryptosystems. they are respectively binary, double-and-add-always and montgomery methods of point multiplication. the experimental results indicate that using spa analysis, the complete key material could be successfully retrieved from binary method, but not from double-and-add-always and montgomery method.simple power analysis on elliptic curve cryptosystems and countermeasures','Side-channel analysis and countermeasures'
'virtual enterprise knowledge sharing is important activities in its life cycle. according to the characteristics of virtual enterprises in the evolution of non-symmetry dynamic game, the thesis analyzes the role in situation of affecting factors to the knowledge sharing, and discusses the inner mechanism of virtual enterprise\'s activities of knowledge sharing. finally, the thesis proposes four countermeasures-selecting the knowledge-sharing partners, and building advanced knowledge technology, improving cross-cultural soft power, protecting intellectual property and optimizing allocation mechanism.the analysis and countermeasures research of virtual enterprise asymmetric knowledge sharing','Side-channel analysis and countermeasures'
'the current status of c2c developing and the difficulties exist in logistics distribution have been discussed in this paper. it made an analysis on the elements which affect the logistics distribution of c2c and then put forward the countermeasures to improve the logistics distribution under c2c.the analysis of logistic bottleneck of taobao.com in c2c model and its countermeasures','Side-channel analysis and countermeasures'
'the financing problem of medium and small sized high-tech enterprises is the key point to enterprises\' survival and development. firstly it is pointed out that the difficulty in financing is due to without enough emphasis on medium and small sized high-tech enterprises, without sound targeted laws and regulations, the imperfect credit guarantee system, and corporate governance structure problem of the enterprise then, according to the problems existing in the financing of china\'s medium and small sized high-tech enterprises, the assay put forward several countermeasures which have referential value.the research and analysis to the financing dilemma and countermeasures of china\'s medium and small sized high-tech enterprises','Side-channel analysis and countermeasures'
'credit system, based on the elective system as units of measurement to calculate the amount of learning and flexibility of learning process, is a new teaching management system well adapting to the needs of social and educational development. in this paper, xi\'an university of technology is the research object; the teaching and management of its credit system and the recognition of students during the past four years are studied. it is shown that more than 80\% of students approve of the credit system and satisfy with its teaching and learning performance; and more than 50\% choose the teachers mainly on the basis of their teaching styles and titles. in line with the analysis of research results, problems arising in the implementation process of the credit system are summarized and feasible countermeasures combining with the practice are ultimately put forward.the status of the management and the countermeasures analysis of higher education credit system','Side-channel analysis and countermeasures'
'in order to improve the present situation of university intellectual property rights operating management and to enhance the scientific research innovation ability, this paper analyzes the present situation and the existing problems of university intellectual property rights operating management. based on it some measures are put forward such as improving the leaderships\' understanding of the work of intellectual property rights, doing university technology transfer works well, strengthening system construction of intellectual property management and establishing patent special funds. these measures\'s implementations ensure the high efficiency of the university intellectual property operating management.university intellectual property management situations and countermeasures analysis','Side-channel analysis and countermeasures'
'social web has changed the concept of leisure time. as a result street neighbors have been replaced by e-neighbors and walls have become e-walls to share ideas and gossips. despite so many advantages we cannot ignore potential threats to user privacy and security. in order to be extremely usable, such systems should have strict security and privacy policies in place. in this paper the authors focus on \"facebook\" to understand privacy and security problems by carrying out a web based survey. based on the findings from empirical data the authors propose different enhancements for the improvement of user privacy and potential threats to user account security.a study on privacy and security aspects of facebook','Social aspects of security and privacy'
'most of information technology and information systems courses teach students about possible invasion of privacy as a result of poor information system security, not about privacy as an essential principle in information systems. moreover, students must know not only policy and compliance aspects of privacy, but also technical safeguards, so they can develop or configure systems to avoid privacy violations. laboratory exercises are crucial in understanding technical aspects of privacy. this paper describes a virtual environment we have developed for \"hands-on\" laboratory exercises that allow students to recognize what happens \"behind the scene\" when they interact with information systems. two example modules are developed and presented.a virtual environment for teaching technical aspects of privacy','Social aspects of security and privacy'
'the real and growing threats to critical infrastructure demand that it professionals remain vigilant.addressing new security and privacy challenges','Social aspects of security and privacy'
'internet standards development requires consideration of security issues in the protocols. but what does &amp;#x201c;security&amp;#x201d; mean in this context? we often conflate several different aspects into the blanket term &amp;#x201c;security.&amp;#x201d; here, the author looks at some of these aspects separately.aspects of internet security','Social aspects of security and privacy'
'the lack of trust is identified as the key concern for consumers in the ecommerce environment. service providers attempt to address this concern by implementing public key infrastructure (pki) systems for online security and privacy and to enhance user confidence. much research has focused on the technical implementation of online security and privacy systems. this paper discusses social and cultural influence as critical elements of a trusted online service environment. it suggests a mechanism for enhancing trust in e-commerce that takes account of these influences.cultural and social aspects of security and privacy','Social aspects of security and privacy'
'the globalisation of biotechnology brings not only new economic prospects but also new risks. the development of international bio-safety guidelines is essential.ethical and social aspects of biotechnology ethical and social aspects of biotechnology','Social aspects of security and privacy'
'user interaction for interactive tv (itv) services becomes a critical aspect in the design of new itv and internet protocol tv (iptv) offers. new services like social tv, direct image and data up- and download from the set-top box, connectivity of the itv system to the pc and other mobile media devices, pose the question on how to support security, privacy, and personalization. to investigate the (sometimes na&#239;ve) concepts of users of what changes an interactive tv system will bring in terms of security and privacy for their living room behaviors, and how to best support these aspects in the user interaction, an ethnographic study with 40 households (126 participants) was conducted. we investigated users\' assumptions and ideas on the concepts of security, privacy and personalization using a combination of playful probing and an interview at the start and the end of the study. findings show that households are not aware of possible problems in terms of security and privacy, before the introduction of interactive tv. this work presents user generated ideas on how to improve security aspects on the tv, user\'s perception on identification mechanisms for tv services, and summarizes ethnographic insights found in the study.ethnographic insights on security, privacy, and personalization aspects of user interaction in interactive tv','Social aspects of security and privacy'
'expectations of security and privacy','Social aspects of security and privacy'
'methodological and practical aspects of data mining','Social aspects of security and privacy'
'privacy and security','Social aspects of security and privacy'
'evolution of web 2.0 applications has changed the outlook of business models and companies. organizations need to rethink their communication, marketing and sale channels and how their employees and customers interact together internally and externally. following this new trend, they also need to adopt their it infrastructure and enhance their online presence and services in order to stay competitive in their businesses. through this technological transition to web 2.0 paradigm new security and privacy issues arise which should be taken into consideration to protect the whole rich internet application (ria) components. web 2.0 has also introduced new possibilities for a better human computer interaction via rich internet applications such as mashups that provide a user-driven micro-integration of web-accessible data. at the moment mashups are mainly used for less important tasks such as customized queries and map-based visualizations; however they have the potential to be used for more fundamental, complex and sophisticated tasks in combination with business processes. in this paper, the security and privacy aspects of mashup architecture and some existing challenges will be discussed in more details.privacy aspects of mashup architecture','Social aspects of security and privacy'
'radio frequency identification &#40;rfid&#41; has recently received a lot of attention as an augmentation technology in manufacturing, scm, and retail inventory control. however, widespread deployment of rfid tags may create new threats to security and privacy of individuals and organisations. this paper gives an overview of all types of rfid privacy and security problems and its countermeasures.rfid tags&#58; privacy and security aspects','Social aspects of security and privacy'
'this paper presents an implementation of a delay tolerant network (dtn) on the android platform on mobile smartphones that we named \"bytewalla\". moreover, we present an implementation of a security solution for bytewalla based on the irtf dtn bundle security protocol specification to protect the bundle content while in transit to the destination. then, we make a throughout analysis of the data transmission overhead of the security protected bundles, which has direct consequences on the power consumption and system performance. our overall, conclusion is that the integration of dtn links in the general ip-network architecture is feasible and will make it easier to integrate dtn applications into communication-challenged areas. to our knowledge our implementation of the bundle protocol is the first for the android platform.security and performance aspects of bytewalla','Social aspects of security and privacy'
'smart cards are replacing traditional magnetic cards for payment transactions. one of the main reasons is enhanced security capabilities that can be built in a smart card. with the high popularity of web technology, there is a trend towards smart cards being used as an electronic wallet for payment transactions on internet. most of the related work of smart card payment transactions concentrates only on the security aspects of hardware/firmware, encryption method and key management, or they only propose the online shopping protocol for uni-directional payment transaction based on the scenery of exact payment from the customer to merchant during business activity.we developed a prototype system called \"smartflow\" to demonstrate these kinds of business activities on internet by smart card. the main focus of this paper is to present the framework of \"smartflow\" and some important security and privacy issues for bi-directional payment transaction with change among more than two parties involved business activity. further, we present the application of downloading software license key from internet into smart card in smartflow environment. we have already implemented a prototype \"smartflow\" system with these functionalities.security and privacy aspects of smartflow internet payment system','Social aspects of security and privacy'
'death is an uncomfortable subject for many people, and digital systems are rarely designed to deal with this event. in particular, the wide array of existing digital authentication infrastructure rarely deals with gracefully retiring credentials in a uniform fashion. this research paper highlights an emerging paradigm: gracefully dealing with expired digital identities in a secure, privacy-preserving fashion. it examines the confluence of modern browser technology, cloud services, and human factors involved in managing a person\'s digital footprint while they live and retiring it when they die. we contemplate a potential approach to dealing with credentials after death by using cloud computing. we consider the reasons that such an approach may actually provide an opportunity for enhancing authentication security by frustrating identity stealing attacks. we note that this paper is not aimed at trivializing the real grief and loss that people feel, but rather an attempt to understand how security and privacy concerns are shaped by the end of life, with the ultimate goal of easing this transition for friends and family.security and privacy considerations in digital death','Social aspects of security and privacy'
'security and privacy subscription information','Social aspects of security and privacy'
'we show id cards at every juncture. is this necessary? is it helpful? or is it actually harmful, not just to our privacy but to security as well?security and privacy','Social aspects of security and privacy'
'voting via the internet has become a feasible option for political as well as non-political ballots. however, there are many obstacles which have to be overcome, especially legal restrictions have to be transformed into technical and security solutions. the article starts with a briefpresentation of advantages and disadvantages of internet ballots and presents application fields and pilot schemes. then, technological security aspects are derived due to democratic basic principles. especially the applied voting procedures are critical in security terms. hence, the most relevant cryptographic protocols are presented and their drawbacks and shortcomings are identified. however, this article does not propose a new voting protocol. beyond fixing cryptographic procedures for ballots, more elements are to be specified, e.g. responsibilities and rights of involved authorities or security precautions regarding hardware and software. for this reason, a structural security framework for electronic voting systems is presented which can be used for their composition and analysis.security aspects of internet voting','Social aspects of security and privacy'
'scada (supervisory control and data acquisition) systems play an important role in industrial process. in the past, these used to be stand-alone models, with closed architecture, proprietary protocols and no external connectivity. nowadays, scada rely on wide connectivity and open systems and are connected to corporate intranets and to the internet for improve efficiency and productivity. scada networks connected to corporate networks brought some new seecurity related challenges. this paper presents an overview of the security aspects of this interconnection.security aspects of scada and corporate network interconnection','Social aspects of security and privacy'
'in this paper, we are concerned with how a real-world social situation shapes the interaction with a novel technology that combines collocated mobile phone and public display use for groups of people. we present a user study of a system that allows collaborative creation and sharing of comic strips on public displays in a social setting such as a pub or caf&#233;. the system utilizes mobile phones and public displays for shared collaborative expression between collocated users. a user study spanning three sessions was conducted in real-world settings: one during the social event following a seminar on games research and two in a bar on a regular weekday evening. we present and discuss our findings with respect to how the larger social situation and location influenced the interaction with the system, the collaboration between participants of a team, how people moved between different roles (i.e., actor, spectator and bystander), and the privacy issues it evoked from participants.social and privacy aspects of a system for collaborative public expression','Social aspects of security and privacy'
'some security aspects of decision support systems','Social aspects of security and privacy'
'privacy is a concept which received relatively little attention during the rapid growth and spread of information technology through the 1980\'s and 1990\'s. design to make information easily accessible, without particular attention to issues such as whether an individual had a desire or right to control access to and use of particular information was seen as the more pressing goal. we believe that there will be an increasing awareness of a fundamental need to address privacy concerns in information technology, and that doing so will require an understanding of policies that govern information use as well as the development of technologies that can implement such policies. the research reported here describes our efforts to design a privacy management workbench which facilitates privacy policy authoring, implementation, and compliance monitoring. this case study highlights the work of identifying organizational privacy requirements, analyzing existing technology, on-going research to identify approaches that address these requirements, and iteratively designing and validating a prototype with target users for flexible privacy technologies.usable security and privacy','Social aspects of security and privacy'
'the real and growing threats to critical infrastructure demand that it professionals remain vigilant.addressing new security and privacy challenges','Social network security and privacy'
'expectations of security and privacy','Social network security and privacy'
'network security','Social network security and privacy'
'privacy and security','Social network security and privacy'
'we present a practical scheme for internet-scale collaborative analysis of information security threats which provides strong privacy guarantees to contributors of alerts. wide-area analysis centers are proving a valuable early warning service against worms, viruses, and other malicious activities. at the same time, protecting individual and organizational privacy is no longer optional in today\'s business climate. we propose a set of data sanitization techniques and correlation, while maintaining privacy for alert contributors. our approach is practical, scalable, does not rely on trusted third parties or secure multiparty computation schemes, and does not require sophisticated schemes, and does not require sophisticated key management.privacy-preserving sharing and correction of security alerts','Social network security and privacy'
'death is an uncomfortable subject for many people, and digital systems are rarely designed to deal with this event. in particular, the wide array of existing digital authentication infrastructure rarely deals with gracefully retiring credentials in a uniform fashion. this research paper highlights an emerging paradigm: gracefully dealing with expired digital identities in a secure, privacy-preserving fashion. it examines the confluence of modern browser technology, cloud services, and human factors involved in managing a person\'s digital footprint while they live and retiring it when they die. we contemplate a potential approach to dealing with credentials after death by using cloud computing. we consider the reasons that such an approach may actually provide an opportunity for enhancing authentication security by frustrating identity stealing attacks. we note that this paper is not aimed at trivializing the real grief and loss that people feel, but rather an attempt to understand how security and privacy concerns are shaped by the end of life, with the ultimate goal of easing this transition for friends and family.security and privacy considerations in digital death','Social network security and privacy'
'security and privacy subscription information','Social network security and privacy'
'we show id cards at every juncture. is this necessary? is it helpful? or is it actually harmful, not just to our privacy but to security as well?security and privacy','Social network security and privacy'
'as the internet is used to a greater extent in business, issues of protection and privacy will have more importance. users and organizations must have the ability to control reads and writes to network accessible information, they must be assured of the integrity and confidentiality of the information accessed over the net, and they must have a means to determine the security, competence, and honesty of the commercial service providers with which they interact. they must also be able to pay for purchases made on the network, and they should be free from excessive monitoring of their activities. this paper discusses characteristics of the internet that make it difficult to provide such assurances and surveys some of the techniques that can used to protect users of the networksecurity, payment, and privacy for network commerce','Social network security and privacy'
'privacy is a concept which received relatively little attention during the rapid growth and spread of information technology through the 1980\'s and 1990\'s. design to make information easily accessible, without particular attention to issues such as whether an individual had a desire or right to control access to and use of particular information was seen as the more pressing goal. we believe that there will be an increasing awareness of a fundamental need to address privacy concerns in information technology, and that doing so will require an understanding of policies that govern information use as well as the development of technologies that can implement such policies. the research reported here describes our efforts to design a privacy management workbench which facilitates privacy policy authoring, implementation, and compliance monitoring. this case study highlights the work of identifying organizational privacy requirements, analyzing existing technology, on-going research to identify approaches that address these requirements, and iteratively designing and validating a prototype with target users for flexible privacy technologies.usable security and privacy','Social network security and privacy'
'reverse engineering tools aimed at facilitating software maintenance suffer from low adoption. many are developed, but few are used by software engineers in performing their maintenance work. we introduce an approach for tool design that is aimed at increasing the adoptability potential of tools.our approach is based on applying cognitive analysis to identify cognitively difficult aspects of maintenance work, then deriving cognitive requirements to address these difficulties. the approach is described in the context of the implementation of a reverse engineering tool we call dynasee, which we have used to for the visualization of traces generated by a large telecommunications system. we describe how dynasee addresses a specific set of cognitive difficulties.a cognitive and user centric based approach for reverse engineering tool design','Software reverse engineering'
'this paper describes a collaborative structured demonstration of reverse engineering tools that was presented at a working session at wcre 2001 in stuttgart, germany. a structured demonstration is a hybrid tool evaluation technique that combines elements from experiments, case studies, technology demonstrations, and benchmarking. the essence of the technique is to facilitate learning about software engineering tools using a common set of tasks. the collaborative experience discussed at wcre involved several peer and complementary technologies that were applied in concert to solve a real life reverse engineering problem. for the most part, the tool developers themselves applied their own tools to this problem. preliminary results have shown to the research community that we still have much to learn about our tools and how they can be applied as part of a reverse engineering and reengineering process. consequently, the participants agreed to continue participation in this demonstration beyond the wcre event.a collaborative demonstration of reverse engineering tools','Software reverse engineering'
'the usefulness of design patterns in forward engineering is already well-known and several tools provide support for their application in the development of software systems. while the role of design patterns in reverse engineering is still argued primarily due to their informal definition which leads to various possible implementations of each pattern. one of the most discussed aspects related to design patterns is about the need of their formalization according to the drawbacks this can represent. formalization leads to the identification of the so-called sub-patterns, which are the recurring fundamental elements design patterns are composed of. in this paper we analyze the role sub-patterns play in two reverse engineering tools: fujaba and spqr. attention is focused on how sub-patterns are exploited to define and to detect design patterns. to emphasize the similarities and differences between the two approaches, the composite design pattern is considered as example.a comparison of reverse engineering tools based on design pattern decomposition','Software reverse engineering'
'a form driven object-oriented reverse engineering methodology','Software reverse engineering'
'a dependence model for reverse engineering should treat procedures in a modular fashion and should be fine-grained, distinguishing dependences that are due to different variables. the program dependence graph (pdg) satisfies neither of these criteria. we present a new form of dependence graph that satisfies both, while retaining the advantages of the pdg: it is easy to construct and allows program slicing to be implemented as a simple graph traversal. we define \'chopping\', a generalization of slicing that can express most of its variants, and show that, using our dependence graph, it produces more accurate results than algorithms based directly on the pdg.a new model of program dependences for reverse engineering','Software reverse engineering'
'developers use class diagrams to describe the architecture of their programs intensively. class diagrams represent the structure and global behaviour of programs. they show the programs classes and interfaces and their relationships of inheritance, instantiation, use, association, aggregation and composition. class diagrams could provide useful data during programs maintenance. however, they often are obsolete and imprecise: they do not reflect the real implementation and behaviour of programs. we propose a reverse-engineering tool suite, ptidej, to build precise class diagrams from java programs, with respect to their implementation and behaviour. we describe static and dynamic models of java programs and algorithms to analyse these models and to build class diagrams. in particular, we detail algorithms to infer use, association, aggregation, and composition relationships, because these relationships do not have precise definitions. we show that class diagrams obtained semi-automatically are similar to those obtained manually and more precise than those provided usually. (a demonstration applet of the ptidej tool suite is provided also. the latest version of the ptidej tool suite is available at www.yann-gael.gueheneuc.net/work/.)a reverse engineering tool for precise class diagrams','Software reverse engineering'
'this paper reviews the progress-to-date of the application of program reverse engineering technologies to a large-scale legacy software product. basic reverse engineering concepts and a project overview are outlined, followed by a description of the legacy software product, the reverse engineering toolkit used, and analysis and discussion of the experiences so far. future research directions and summary comments are then detailed.a software reverse engineering experience','Software reverse engineering'
'we describe a tool chain that enables experimentation and study of real c++ applications. our tool chain enables reverse engineering and program analysis by exploiting gcc, and thus accepts any c++ application that can be analysed by the c++ parser and front end of gcc. our current test suite consists of large, open-source applications with diverse problem domains, including language processing and gaming. our tool chain is designed using a gxl-based pipe-filter architecture; therefore, the individual applications and libraries that constitute our tool chain each provide a point of access. the preferred point of access is the g4api application programming interface (api), which is located at the end of the chain. g4api provides access to information about the c++ program under study, including information about declarations, such as classes (including template instantiations); namespaces; functions; and variables, statements and some expressions. access to the information is via either a pointer to the global namespace, or a list interface.a tool chain for reverse engineering c++ applications','Software reverse engineering'
'a view on three r\'s (3rs): reuse, re-engineering, and reverse-engineering','Software reverse engineering'
'reverse engineering a program constructs a high-levelrepresentation suitable for various software developmentpurposes such as documentation or reengineering.unfortunately however, there are no establishedguidelines to assess the adequacy of such a representation.we propose two such criteria, completenessand accuracy, and show how they can be determinedduring the course of reversing the representation.a representation is successfully reversed when itis given as input to a suitable code generator, and aprogram equivalent to the original is produced. to explorethis idea, we reverse engineer a small but complexnumerical application, represent our understandingusing algebraic specifications, and then use a codegenerator to produce code from the specification. wediscuss the strengths and weaknesses of the approachas well as alternative approaches to reverse engineeringadequacy.adequate reverse engineering','Software reverse engineering'
'formal methods can be used at all stages of a software development project. in this paper we reflect on the roles of itu-t standardized system design languages and their interplay in design processes of cooperative systems, highlighting the usability of user requirements notation (urn) standard to capture requirements with workflow-based re-engineering process of complex systems. in this paper we give our re-engineering experiences with the urn-part use case maps (ucm) language capabilities and also the transformation processes of the ucm model elements to uml diagrams are presented. the new components can be very well documented and integrated into the existing system in a manner that even the stakeholders get involved in it.advanced steps with standardized languages in the re-engineering process','Software reverse engineering'
'the reverse engineering community has recognized the importance of interoperability, the cooperation of two or more systems to enable the exchange and utilization of data, and has noted that the current lack of interoperability is a contributing factor to the lack of adoption of available infrastructures. to address the problems of interoperability and reproducing previous results, we present an infrastructure that supports interoperability among reverse engineering tools and applications. we present the design of our infrastructure, including the hierarchy of schemas that captures the interactions among graph structures. we also develop and utilize our implementation, which is designed using a gxl-based pipe-filter architecture, to perform a case study that demonstrates the feasibility of our infrastructure.an infrastructure to support interoperability in reverse engineering','Software reverse engineering'
'extensible markup language (xml) has become a standard for data representation and exchange over the internet. xml schemas are often used to define vocabularies of xml document types and to validate whether the xml documents adhere to the rules defined in the xml schemas. since xml schemas are textual, programmatic, logical-level schemas, users of xml schemas often find it difficult to understand and communicate with each other the structure and content of the xml schemas and documents as the xml schemas grow in complexity. a solution to the problem would be to convert the logical-level xml schemas developed back to conceptual-level unified modeling language diagrams to facilitate easy understanding and communication. this research paper provides an overview of research on reverse engineering xml schemas into uml diagrams.an overview of research on reverse engineering xml schemas into uml diagrams','Software reverse engineering'
'in this work we present techniques and tools that enable effective reverse engineering procedures for web applications that were developed using the promising asp.net technology. we deal with model-driven development in its reverse aspect by implementing reverse engineering methods. our implemented methods model web applications using a well-known, web oriented and robust language, namely webml. this is, to the authors\' best knowledge, a novel re-engineering transformation. in this paper we propose a method to reverse engineer web applications in order to extract their conceptual model using webml notation. moreover, we present an efficient tool we have developed in order to implement the proposed method, along with a study of the application of our tool to an exemplar, content-management web application. the overall results are quite encouraging and indicate that our approach is efficient.application modeling using reverse engineering techniques','Software reverse engineering'
'architecting, analyzing and testing service-oriented systems','Software reverse engineering'
'we describe currawong, a tool to perform system software architecture optimisation. currawong is an extensible tool which applies optimisations at the point where an application invokes framework or library code. currawong does not require source code to perform optimisations, effectively decoupling the relationship between compilation and optimisation. we show, through examples written for the popular android smartphone platform, that currawong is capable of significant performance improvement to existing applications.architecture optimisation with currawong','Software reverse engineering'
'designers can easily become overwhelmed with details when dealing with large class diagrams. this article presents an approach for automated abstraction that allows designers to \"zoom out\" on class diagrams to investigate and reason about their bigger picture. the approach is based on a large number of abstraction rules that individually are not very powerful, but when used together, can abstract complex class structures quickly. this article presents those abstraction rules and an algorithm for applying them. the technique was validated on over a dozen models where it was shown to be well suited for model understanding, consistency checking, and reverse engineering.automated abstraction of class diagrams','Software reverse engineering'
'with the increasing popularity of agile software development and test-driven-development, also maintenance of acceptance test has become an important issue. in this paper, we describe a concept and a tool for automated acceptance test maintenance using a refactoring approach. acceptance tests are user tests which are used to determine if a system satisfies acceptance criteria and to enable a customer to determine whether or not to accept the system. in agile development acceptance test are also used as a mean for specification, i.e. acceptance tests are written in advance to the production code (called behavior-driven-development - bdd). in an agile project this poses three major challenges with respect to maintenance of acceptance tests: new requirements may cause changes in the acceptance criteria, which require the system under test to be adapted; when the system under test undergoes a major restructuring, even the acceptance test might have to be adapted; with the increasing acceptance test suite in an agile project the tests themselves may undergo a major reorganization. having a large acceptance test base, doing these refactorings manually is error prone and causes a lot of effort. in this paper we present a concept and tool for executing automated refactoring for fit acceptance tests, which significantly reduces the effort for test maintenance and makes them much less error prone.automated acceptance test refactoring','Software reverse engineering'
'regression testing involves testing the modified program in order to establish the confidence in the modifications. existing regression testing methods generate test cases to satisfy selected testing criteria in the hope that this process may reveal faults in the modified program. in this paper we present a novel approach of automated regression test generation in which all generated test cases uncover an error(s). this approach is used to test the common functionality of the original program and its modified version, i.e., it is used for programs whose functionality is unchanged after modifications. the goal in this approach is to identify test cases for which the original program and the modified program produce different outputs. if such a test is found, then this test uncovers an error. the problem of finding such a test case may be reduced to the problem of finding program input on which a selected statement is executed. as a result, existing methods of automated test data generation for white-box testing may be used to generate these tests. our experiments have shown that our approach may improve the chances of finding software errors as compared to the existing methods of regression testing. the advantage of our approach is that it is fully automated and that all generated test cases reveal an error(s).automated regression test generation','Software reverse engineering'
'in maintenance, the lack of documentation leads to high costs of reverse engineering. generally, design-pattern is a reusable solution to a commonly occurring problem in software design. if design-patterns could be captured and reused in reverse engineering, the reverse engineering would be very helpful those who develops and maintains software. so there have been many attempts to detect design-patterns during reverse engineering. however, the approaches suffer from serious drawbacks to its practical implementation; false positive, false negative rate, the number of detected patterns. in this paper, we propose a new taxonomy of gof design patterns that can guide the reverse-engineering process. this approach not only combines static analysis with dynamic analysis but also adds what we call the implementation-specific analysis. we apply a number of existing and new applications, including pure toolkit, jini based home application system, project management tool, mp3 player, and we demonstrate that the reverse engineering process is more accurate.automatic detection of design pattern for reverse engineering','Software reverse engineering'
'in order to generate high-quality code for modern processors, a compiler must aggressively schedule instructions, maximizing resource utilization for execution efficiency. for a compiler to produce such code, it must avoid structural hazards by being aware of the processor\'s available resources and of how these resources are utilized by each instruction. unfortunately, the most prevalent approach to constructing such a scheduler, manually discovering and specifying this information, is both tedious and error-prone. this paper presents a new approach which, when given a processor or processor model, automatically determines this information. after establishing that the problem of perfectly determining a processor\'s structural hazards through probing is not solvable, this paper proposes a heuristic algorithm that discovers most of this information in practice. this can be used either to alleviate the problems associated with manual creation or to verify an existing specification. scheduling with these automatically derived structural hazards yields almost all of the performance gain achieved using perfect hazard information.automatic instruction scheduler retargeting by reverse-engineering','Software reverse engineering'
'understanding the command-and-control (c&c) protocol used by a botnet is crucial for anticipating its repertoire of nefarious activity. however, the c&c protocols of botnets, similar to many other application layer protocols, are undocumented. automatic protocol reverse-engineering techniques enable understanding undocumented protocols and are important for many security applications, including the analysis and defense against botnets. for example, they enable active botnet infiltration, where a security analyst rewrites messages sent and received by a bot in order to contain malicious activity and to provide the botmaster with an illusion of successful and unhampered operation. in this work, we propose a novel approach to automatic protocol reverse engineering based on dynamic program binary analysis. compared to previous work that examines the network traffic, we leverage the availability of a program that implements the protocol. our approach extracts more accurate and complete protocol information and enables the analysis of encrypted protocols. our automatic protocol reverse-engineering techniques extract the message format and field semantics of protocol messages sent and received by an application that implements an unknown protocol specification. we implement our techniques into a tool called dispatcher and use it to analyze the previously undocumented c&c protocol of megad, a spam botnet that at its peak produced one third of the spam on the internet.automatic protocol reverse-engineering','Software reverse engineering'
'awe is a prototype system for performing analysis of x86 executables in the absence of source code or debugging information. it provides a modular infrastructure for integrating static and dynamic analyses into a single workflow. one of the major challenges with performing analysis of modern software is the amount of data that must be analyzed by a human to determine software behavior. this challenge is further compounded by the number of different tools and extensive expertise required to perform such analyses. the awe system addresses this challenge in two ways: first by focusing analyst\'s attention on a prioritized subset of software features of importance, and second by simplifying analysis through an integrated static and dynamic analysis workflo.awe','Software reverse engineering'
'although broadly available in major software development environments, refactoring tools are still underused. one of the reasons for this underuse is that existing refactoring tools assume that a developer recognizes that she is going to refactor before she even begins. in this paper, we present a flexible refactoring tool called benefactor that can be invoked after refactoring begins to safely complete a refactoring change.benefactor','Software reverse engineering'
'we present a language that integrates statically and dynamically typed components, similar to the gradual types of siek and taha (2006), and extend it to incorporate parametric polymorphism. our system permits a dynamically typed value to be cast to a polymorphic type, with the type enforced by dynamic sealing along the lines proposed by pierce and sumii (2000), matthews and ahmed (2008), and neis, dreyer, and rossberg (2009), in a way that ensures all terms satisfy relational parametricity. our system includes a notion of blame, which allows us to show that when more-typed and less-typed portions of a program interact, that any type failures are due to the less-typed portion.blame for all','Software reverse engineering'
'boundary extract and reduction of mass data in reverse engineering was studied in this paper by adopting variable precision rough set theory, introducing error factors, building up decision tables of mass data and making attributes reduction. this process would benefit to the valuation of the scan boundary data and result in the boundary extract and reduction as well as the 3d reconstruction of mass data. as an experimental sample, an art dog was used to discuss this improved process, which fundamentally proved that the partial intelligence and 3d reconstruction precision could be successfully realized.boundary extract and reduction of mass data in reverse engineering','Software reverse engineering'
'my dissertation explores a new approach to construct tools in the domain of reverse engineering. the approach uses already available software components as building blocks, combining and customizing them programmatically. this approach can be characterized as component-based tool-building. the goal of the dissertation is to advance the current state of component-based tool-building towards a discipline that is more predictable and formal. this is achieved with three research contributions: (1) an in-depth literature survey that identifies requirements for reverse en- gineering tools, (2) a number of tool case studies that utilize component-based tool-building, (3) and ten lessons learned for tool builders that have been distilled from these case studies.building reverse engineering tools with software components','Software reverse engineering'
'changes versus faults in formal methods?','Software reverse engineering'
'to understand a certain issue of the system we want to ask the knowledgeable developers. yet, in large systems, not every developer is knowledgeable in all the details of the system. thus, we would want to know which developer is knowledgeable in the issue at hand. in this paper we present the chronia tool that implements the ownership map visualization to understand when and how different developers interacted in which way and in which part of the system.chronia','Software reverse engineering'
'william s. evans will@cs.ubc.ca christopher w. fraser cwfraser@gmail.com fei ma fei.ma@microsoft.com abstract this paper describes the design, implementation, and application of a new algorithm to detect cloned code. it operates on the abstract syntax trees formed by many com- pilers as an intermediate representation. it extends prior work by identifying clones even when arbitrary subtrees have been changed. on a 440,000-line code corpus, 20- 50\% of the clones it detected were missed by previous meth- ods. the method also identifies cloning in declarations, so it is somewhat more general than conventional procedural abstraction.clone detection via structural abstraction','Software reverse engineering'
'the article addresses the problem of concept location in source code by proposing an approach that combines formal concept analysis and information retrieval. in the proposed approach, latent semantic indexing, an advanced information retrieval approach, is used to map textual descriptions of software features or bug reports to relevant parts of the source code, presented as a ranked list of source code elements. given the ranked list, the approach selects the most relevant attributes from the best ranked documents, clusters the results, and presents them as a concept lattice, generated using formal concept analysis. the approach is evaluated through a large case study on concept location in the source code on six open-source systems, using several hundred features and bugs. the empirical study focuses on the analysis of various configurations of the generated concept lattices and the results indicate that our approach is effective in organizing different concepts and their relationships present in the subset of the search results. in consequence, the proposed concept location method has been shown to outperform a standalone information retrieval based concept location technique by reducing the number of irrelevant search results across all the systems and lattice configurations evaluated, potentially reducing the programmers\' effort during software maintenance tasks involving concept location.concept location using formal concept analysis and information retrieval','Software reverse engineering'
'conference on software maintenance and reengineering - front cover','Software reverse engineering'
'data reverse engineering is a rapidly growing field, which is sometimes misunderstood. in our effort to promote the realization that data reverse engineering is a valuable and essential part of reverse engineering and is now finding its place in software engineering, we present a summary of the history of data reverse engineering. in this paper, a definition of data reverse engineering, a summary of the history of what has been done and published, a statement about the state-of-the-art today, and a comment on the future of dre is presented.data reverse engineering','Software reverse engineering'
'database reverse engineering','Software reverse engineering'
'the y2k problem is real, and it has attracted intense media attention and aggressive vendor response. for whatever reason-- whether they wanted to save precious memory in an era when memory was incredibly expensive, or because they didn\'t expect systems to last this long, or because they simply didn\'t recognize the problem-programmers long ago adopted a two-digit convention to represent the year. this convention will cause failures as we approach the turn of the century and beyond. on january 1, 2000, uncorrected software will assume that the maximum value of a year field is 99 and will roll systems over to the date 1900 instead of 2000, resulting in negative date calculations. incorrect leap year calculations will incorrectly assume that the year 2000 has only 365 days instead of 366. what\'s more, many date-dependent algorithms and forward-referencing systems are already beginning to fail. approaches for resolving the problem and managing the risks have tended to focus on how particular tools and vendors can help. this article sets forth the concepts, terminology, and individual aspects of a y2k effort and then defines a process that an organization can use to address its own y2k challenge in a forthright and level-headed manner.dealing with dates','Software reverse engineering'
'detecting code clones has many software engineering applications. existing approaches either do not scale to large code bases or are not robust against minor code modifications. in this paper, we present an efficient algorithm for identifying similar subtrees and apply it to tree representations of source code. our algorithm is based on a novel characterization of subtrees with numerical vectors in the euclidean space \\mathbb{r}^n and an efficient algorithm to cluster these vectors w.r.t. the euclidean distance metric. subtrees with vectors in one cluster are considered similar. we have implemented our tree similarity algorithm as a clone detection tool called deckard and evaluated it on large code bases written in c and java including the linux kernel and jdk. our experiments show that deckard is both scalable and accurate. it is also language independent, applicable to any language with a formally specified grammar.deckard','Software reverse engineering'
'a dependence cluster is a set of program statements, all of which are mutually inter-dependent. this article reports a large scale empirical study of dependence clusters in c program source code. the study reveals that large dependence clusters are surprisingly commonplace. most of the 45 programs studied have clusters of dependence that consume more than 10&percnt; of the whole program. some even have clusters consuming 80&percnt; or more. the widespread existence of clusters has implications for source code analyses such as program comprehension, software maintenance, software testing, reverse engineering, reuse, and parallelization.dependence clusters in source code','Software reverse engineering'
'the use of design patterns in a software system can provide strong indications about the rationale behind the system\'s design. as a result, automating the detection of design pattern instances could be of significant help to the process of reverse engineering large software systems. in this paper, we introduce dpvk (design pattern verification toolkit), the first reverse engineering tool to detect pattern instances in eiffel systems. dpvk is able to detect several different design patterns by examining both the static structure and the dynamic behaviour of a system written in eiffel. we present three case studies that were performed to assess dpvk\'s effectiveness.design pattern detection in eiffel systems','Software reverse engineering'
'large software projects contain significant code duplication, mainly due to copying and pasting code. many techniques have been developed to identify duplicated code to enable applications such as refactoring, detecting bugs, and protecting intellectual property. because source code is often unavailable, especially for third-party software, finding duplicated code in binaries becomes particularly important. however, existing techniques operate primarily on source code, and no effective tool exists for binaries. in this paper, we describe the first practical clone detection algorithm for binary executables. our algorithm extends an existing tree similarity framework based on clustering of characteristic vectors of labeled trees with novel techniques to normalize assembly instructions and to accurately and compactly model their structural information. we have implemented our technique and evaluated it on windows xp system binaries totaling over 50 million assembly instructions. results show that it is both scalable and precise: it analyzed windows xp system binaries in a few hours and produced few false positives. we believe our technique is a practical, enabling technology for many applications dealing with binary code.detecting code clones in binary executables','Software reverse engineering'
'repeated changes to a software system can introduce small weaknesses such as unplanned dependencies between different parts of the system. while such problems usually go undetected, their cumulative effect can result in a noticeable decrease in the quality of a system. we present an approach to warn developers about increased coupling between the (potentially scattered) implementation of different features. our automated approach can detect sections of the source code contributing to the increased coupling as soon as software changes are tested. developers can then inspect the results to assess whether the quality of their changes is adequate. we have implemented our approach for c++ and integrated it with the development process of a proprietary 3d graphics software. we report on our evaluation of the approach in the field, and on a study showing that, for files in the target system, causing increases in feature coupling is a significant predictor of future modifications due to bug fixes.detecting increases in feature coupling using regression tests','Software reverse engineering'
'software decay is a phenomenon that plagues aging software systems. while in recent years, there has been significant progress in the area of automatic detection of \"code smells\" on one hand, and code refactorings on the other hand, we claim that existing restructuring practices are seriously hampered by their symptomatic and informal (non-repeatable) nature. this paper makes a clear distinction between structural problems and structural symptoms (also known as code smells), and presents a novel, causal approach to restructuring object oriented systems. our approach is based on two innovations: the encapsulation of correlations of symptoms and additional contextual information into higher-level design problems, and the univocal, explicit mapping of problems to unique refactoring solutions. due to its explicit, repeatable nature, the approach shows high potential for increased levels of automation in the restructuring process, and consequently a decrease in maintenance costs.diagnosing design problems in object oriented systems','Software reverse engineering'
'direct integration of reverse engineering and rapid prototyping','Software reverse engineering'
'reflecting upon the recent experience of teaching our undergraduate software engineering course has caused me to revisit several questions at the core of the discipline. what is the essence of software design, how should it be taught and how does it relate to software engineering?disciplined design practices','Software reverse engineering'
'use case model is normally used to model software requirement. reverse engineering of the high-level requirement model from the source code of target system is an important way to promote the program comprehension. in this paper, an approach of discovering and mining the use case model from the source code of object-oriented software is presented. based on dynamic information, which could be obtained based on instrumentation techniques during the execution of target system, the approach discovers the basic use cases by specifying the beginning methods of method calling sequences of the dynamic information, and then utilizes some rules to mine the relations among basic use cases to build the whole use case model. this approach has been implemented in the xdre (xidian reverse engineering) tool. at the end a case study is provided to show the accuracy and usefulness of discovered use case model.discovering and mining use case model in reverse engineering','Software reverse engineering'
'document reverse engineering','Software reverse engineering'
'domain-retargetable reverse engineering','Software reverse engineering'
'languages and systems to support generative and transformational solutions have been around a long time. systems such as xvcl, dms, asf+sdf, stratego and txl have proven mature, efficient and effective in a wide range of applications. even so, adoption remains a serious issue - almost all successful production applications of these systems in practice either involve help from the original authors or years of experience to get rolling. while work on accessibility is active, with efforts such as etxl, stratego xt, rascal and colm, the fundamental big step remains - it\'s not obvious how to apply a general purpose transformational system to any given generation or transformation problem, and the real power is in the paradigms of use, not the languages themselves. in this talk i will propose an agenda for addressing this problem by taking our own advice - designing and implementing domain specific languages (dsls) for specific generative, transformational and analysis problem domains. we widely advise end users of the need for dsls for their kinds of problems - why not for our kinds? and we use our tools for implementing their dsls - why not our own? i will outline a general method for using transformational techniques to implement transformational and generative dsls, and review applications of the method to implementing example text-based dsls for model-based code generation and static code analysis. finally, i will outline some first steps in implementing model transformation dsls using the same idea - retaining the maturity and efficiency of our existing tools while bringing them to the masses by \"eating our own dogfood\".eating our own dog food','Software reverse engineering'
editorial,'Software reverse engineering'
'starting with the aim of modernizing legacy systems, often written in old programming languages, reverse engineering has extended its applicability to virtually every kind of software system. moreover, the methods originally designed to recover a diagrammatic, high-level view of the target system have been extended to address several other problems faced by programmers when they need to understand and modify existing software. the authors\' position is that the next stage of development for this discipline will necessarily be based on empirical evaluation of methods. in fact, this evaluation is required to gain knowledge about the actual effects of applying a given approach, as well as to convince the end users of the positive cost---benefit trade offs. the contribution of this paper to the state of the art is a roadmap for the future research in the field, which includes: clarifying the scope of investigation, defining a reference taxonomy, and adopting a common framework for the execution of the experiments.empirical studies in reverse engineering','Software reverse engineering'
'a significant aspect in applying the reflexion method is the mapping of components found in the source code onto the conceptual components defined in the hypothesized architecture. to date, this mapping is established manually, which requires a lot of work for large software systems. in this paper, we present a new approach, in which clustering techniques are applied to support the user in the mapping activity. the result is a semi-automated mapping technique that accommodates the automatic clustering of the source model with the user\'s hypothesized knowledge about the system\'s architecture. this paper describes also a case study in which our semi-automated mapping technique has been applied successfully to extend a partial map of a real-world software application.equipping the reflexion method with automated clustering','Software reverse engineering'
'eval endows javascript developers with great power. it allows developers and end-users, by turning text into executable code, to seamlessly extend and customize the behavior of deployed applications as they are running. with great power comes great responsibility, though not in our experience. in previous work we demonstrated through a large corpus study that programmers wield that power in rather irresponsible and arbitrary ways. we showed that most calls to eval fall into a small number of very predictable patterns. we argued that those patterns could easily be recognized by an automated algorithm and that they could almost always be replaced with safer javascript idioms. in this paper we set out to validate our claim by designing and implementing a tool, which we call evalorizer, that can assist programmers in getting rid of their unneeded evals. we use the tool to remove eval from a real-world website and validated our approach over logs taken from the top 100 websites with a success rate over 97\% under an open world assumption.eval begone!','Software reverse engineering'
'real world software systems undergo, during their lifetime, to repeated maintenance activities. due to the market pressure and to the need for having back the system operational in the shortest time possible, maintenance tends to introduce negative side effects. some examples are the growth of the cloning percentage, the increase of library size, the presence of unused objects, or the lost of source file organization. this thesis proposes a framework, named evolution doctor, to diagnose and cure such phenomena. the framework permits the analysis and prediction of several indicators of software system evolution (size, complexity, cloning). then, the framework defines a set of methods and tools to cure the problems: remove clones and unused objects, reorganize libraries, and restructure the source file directory organizations.evolution doctor','Software reverse engineering'
'better understanding manual reverse engineering can make it and any associated systems reengineering more effective. we reverse engineered a version of a system (referred to as \"bos/x\") in support of a broader reengineering effort. system reengineering goals and other circumstances dictated a focused, limited duration, manual reverse engineering exercise. this presented an opportunity to study the bos/x reverse engineering separately from other reengineering activities. we studied the bos/x reverse engineering, the results achieved, and some limited reverse engineering metrics. this paper describes the: systems reengineering context; circumstances preventing application of automated techniques and motivating manual reverse engineering; reverse engineering process developed; bos/x reverse engineering goals; evolution of the reverse engineering products; re-verse engineering results; resources required to produce the results; and an evaluation of the reverse engineering effectiveness. combined, these results may be used as measures - standards of comparison - that can be studied further - for example to determine potential areas for future automation application.experiences reverse engineering manually','Software reverse engineering'
'the development of a toot for reconstructing uml sequence diagrams from executing java programs is a challenging task. we implemented such a tool designed to analyze any kind of java program. its implementation relies heavily on several advanced features of the java platform. although there are a number of research projects in this area usually little information on implementation-related questions or the rationale behind implementation decisions is provided. in this paper we present a thorough study of technological options for the relevant concerns in such a system. the various options are explained and the trade-offs involved are analyzed. we focus on practical aspects of data collection, data representation and meta-model, visualization, editing, and export concerns. apart from analyzing the available options, we report our own experience in developing a prototype of such a tool in this study. it is of special interest to investigate systematically in what ways the java platform facilitates (or hinders) the construction of the described reverse engineering tool.experiences with the development of a reverse engineering tool for uml sequence diagrams','Software reverse engineering'
'several approaches have been proposed to support design by contract in java. in this paper, through the use of markers which are predefined dummy methods and attributes, a new approach to annotate contracts is presented. the annotated programs can be directly compiled by standard java compilers. a bytecode instrumentor is developed to manipulate the bytecode to inject contract evaluation instructions and make the contracts executable at runtime. the marker approach avoids two primary problems found in the existing practices: source compatibility that depends on language extension and symbolic barrier that leaves contracts and their targets unrelated. it also facilitates streamlined integration with ides and improves readability as well as writability of the contractannotated programs.ezcontract','Software reverse engineering'
'this paper uses a projection theory of slicing to formalize the definition of executable dynamic and forward program slicing. previous definitions, when given, have been operational, and previous descriptions have been algorithmic. the projection framework is used to provide a declarative formulation in terms of the different equivalences preserved by the different forms of slicing. the analysis of dynamic slicing reveals that the slicing criterion introduced by korel and laski contains three inter-woven criteria. it is shown how these three conceptually distinct criteria can be disentangled to reveal two new criteria. the analysis of dynamic slicing also reveals that the subsumes relationship between static and dynamic slicing is more intricate that previous authors have claimed. finally, the paper uses the projection theory to investigate theoretical properties of forward slicing. this is achieved by first re-formulating forward slicing to provide an executable forward slice. this definition allows for formal investigation of the relationship between forward and backward slicing.formalizing executable dynamic and forward slicing','Software reverse engineering'
'reverse engineering is immersed in a variety of tasks related to comprehending and modifying software. among these tasks are recovering of designs and architectures, migration and modernization of legacy systems. with the emergence of the model driven architecture (mda), new approaches should be developed in order to reverse engineering both platform-dependent and platform-independent models from object-oriented code. this paper contributes an mda-based framework. different techniques are integrated, with special emphasis on testing and verification. we propose to apply static and dynamic analysis to generate models from migrated object-oriented code and, semi-formal and formal metamodeling techniques for maintaining consistency in reverse engineering processes.formalizing mda-based reverse engineering processes','Software reverse engineering'
'as a software system evolves, it is common for the as-built architecture to diverge from the as-designed  architecture. this gap between the as-designed (conceptual) and the as-built (concrete) architecture leads to a false understanding of the system, resulting in error prone maintenance decisions. we present an approach to repair an architecture of a software system. our approach attempts to reconcile the conceptual architecture with the concrete architecture by performing a series of simple, semi-automatic repair actions. we applied our architecture repair actions to the linux kernel and were able to repair many of the anomalies its architecture.forward and reverse repair of software architecture','Software reverse engineering'
'geometric dimensioning and tolerancing constitutes the dominant approach for design and manufacture of mechanical parts that control inevitable dimensional and geometric deviations within appropriate limits. position tolerance is a critical geometric tolerance very frequently used in industry. its designation requires size data in conjunction with appropriate datums and location coordinates for the position. in reverse engineering, where typically relevant engineering information does not exist, conventional, human-based, trial and error approach for the allocation of positional tolerances requires much effort and time and offers no guarantee for the generation of the best results. this is mainly due to the large number of possible data combinations and the applicable relationships that have to be developed and processed. a methodology that aims towards the systematic solution of this problem in reasonable computing time and provides realistic and industry approved results is presented, demonstrated and discussed in the paper.geometrical position tolerance assignment in reverse engineering','Software reverse engineering'
gibbsgen,'Software reverse engineering'
'protecting application software from reverse engineering and piracy is key to ensuring the integrity of intellectual property and critical infrastructures. unorthodox protection strategies can help mitigate these types of attacks. such strategies must include random, dynamic protections to complicate the ease with which attackers can overcome standard approaches.hindering reverse engineering','Software reverse engineering'
'some software engineering research tools remain inshadow because of the lack of integration and interoperabilitywith well known commercial development environments.in this paper, we investigate and provide asolution to integrate rigi, a research tool used for softwarereverse engineering, with the microsoft visual studio.net (vs.net) integrated development environment(ide). the paper uses the add-in framework of vs.net toaccess the ide objects to allow vs.net and rigi to interoperate.a new component, called rigi add-in, is createdto provide .net developers the possibility of using the featuresof the rigi tool. this allows developers to combineboth forward and reverse engineering techniques in complementaryways. one contribution of this work includesthe improved access and potential adoption of an existingresearch solution by embedding it in a popular commercialenvironment.integrating a reverse engineering tool with microsoft visual studio .net','Software reverse engineering'
'no abstract availableinterpreting reverse-engineering law','Software reverse engineering'
'investigating reverse engineering technologies for the cas program understanding project','Software reverse engineering'
'impact analysis is the identification of the work products affected by a proposed change request, either a bug fix or a new feature demand. jimpa is a plug-in for eclipse that implements an impact analysis approach based on data stored in software repositories, such as cvs and bugzilla. plug-in architecture and functionalities are presented.jimpa','Software reverse engineering'
'there are many reasons why reverse engineering researchtools often fail to be evaluated or adopted in industry.their rough user interfaces and poor interoperabilityare just two frequently mentioned issues. the aim of theacse (adoption-centric software engineering) project,conducted at the university of victoria, is to investigatehow some of these impediments can be overcome by buildingsoftware engineering tools on top of commercial off-the-shelf (cots) products.this paper outlines how to leverage microsoft visio fora software visualization and metrics tool. software developersfamiliar with visio only have to learn the reverseengineering specific functions introduced by our tools andcan take advantage of their existing, domain-independentvisio knowledge. thus, compared to a stand-alone application,this visio-based tool leverages the cognitive supportpreviously acquired by developers using visio.leveraging visio for adoption-centric reverse engineering tools','Software reverse engineering'
'in our earlier work, we have proposed a generic framework for managing collections of related products realized via cloning -- both in the case when such products are refactored into a single-copy software product line representation and the case when they are maintained as distinct clones. in this paper, we ground the framework in empirical evidence and exemplify its usefulness. in particular, we systematically analyze three industrial case studies of organizations with cloned product lines and derive the set of basic operators comprising the framework. we discuss options for implementing the operators and benefits of the operator-based view.managing cloned variants','Software reverse engineering'
'managing multi-variant software configuration','Software reverse engineering'
'the paper introduces a number of mapping rules for reverse engineering uml class models from c++ source code. the mappings focus on accurately identifying such elements as relationship types, multiplicities, and aggregation semantics. these mappings are based on domain knowledge of the c++ language and common programming conventions and idioms. an application implementing these heuristics is used to reverse engineer a moderately sized open source, c++ application, and the resultant class model is compared against those produced by other uml reverse engineering applications. a comparison shows that these presented mapping rules effectively produce meaningful, semantically accurate uml models.mappings for accurately reverse engineering uml class models from c++','Software reverse engineering'
'it is important to understand semantic differences between a pair of java methods during maintenance. however, textual or syntactic difference is insufficient to give clear idea which code fragment realizes a single functionality in java methods. in this paper, we present an eclipse plugin for semantic differentiation of a given pair of java methods.method differentiator using slice-based cohesion metrics','Software reverse engineering'
'design patterns are micro architectures that have provedto be reliable, easy-to implement and robust. there is aneed in science and industry for recognizing these patterns.we present a new method for discovering design patterns inthe source code. this method provides a precise specificationof how the patterns work by describing basic structuralinformation like inheritance, composition, aggregation andassociation, and as an indispensable part, by defining calldelegation, object creation and operation overriding. weintroduce a new xml-based language, the design patternmarkup language (dpml), which provides an easy way forthe users to modify pattern descriptions to suit their needs,or even to define their own patterns or just classes in certainrelations they wish to find. we tested our method onfour open-source systems, and found it effective in discoveringdesign pattern instances.mining design patterns from c++ source code','Software reverse engineering'
'specification mining is a dynamic analysis process aimed at automatically inferring suggested specifications of a program from its execution traces. we describe a method, a framework, and a tool, for mining inter-object scenario-based specifications in the form of a uml2-compliant variant of damm and harel\'s live sequence charts (lsc), which extends the classical partial order semantics of sequence diagrams with temporal liveness and symbolic class level lifelines, in order to generate compact and expressive specifications. moreover, we use previous research work and tools developed for lsc to visualize, analyze, manipulate, test, and thus evaluate the scenario-based specifications we mine. our mining framework is supported by statistically sound metrics. its effectiveness and the usefulness of the mined scenarios are further improved by an array of extensions to the basic mining algorithm, which include various user-guided filters and abstraction mechanisms. we demonstrate and evaluate our work using a case study.mining modal scenarios from execution traces','Software reverse engineering'
'specification mining is a process of extracting specifications, often from program execution traces. these specifications can in turn be used to aid program understanding, monitoring and verification. there are a number of dynamic-analysis-based specification mining tools in the literature, however none so far extract past time temporal expressions in the form of rules stating: \"whenever a series of events occurs, previously another series of events has happened\". rules of this format are commonly found in practice and useful for various purposes. most rule-based specification mining tools only mine future-time temporal expression. many past-time temporal rules like \"whenever a resource is used, it was allocated before\" are asymmetric as the other direction does not holds. hence, there is a need to mine past-time temporal rules. in this paper, we describe an approach to mine significant rules of the above format occurring above a certain statistical thresholds from program execution traces. the approach start from a set of traces, each being a sequence of events (i.e., method invocations) and resulting in a set of significant rules obeying minimum thresholds of support and confidence. a rule compaction mechanism is employed to reduce the number of reported rules significantly. experiments on traces of jboss application server shows the utility of our approach in inferring interesting past-time temporal rules.mining past-time temporal rules from execution traces','Software reverse engineering'
'managing software maintenance projects is difficult, particularly when the project being managed includes programs written by a different group or even a different company. reverse engineering is an attractive technique to help understand a foreign program, but managing reverse-engineering efforts is difficult due to the lack of fixed standards for productivity and quality. model-driven reverse engineering can help solve this problem. in particular, mdre uses models to predict how much time a reverse-engineering effort will require and to provide a quality standard to evaluate that effort. this, in turn, enables better effort prediction and quality evaluation, reducing development risk. this article describes a particular approach to mdre and presents an example of its use.model-driven reverse engineering','Software reverse engineering'
'refactoring tools allow programmers to change source code much quicker than before. however, the complexity of these changes cause versioning tools that operate at a file level to lose the history of components. this problem can be solved by semantic, operation-based scm with persistent ids. we propose that versioning tools be aware of the program entities and the refactoring operations. molhadoref uses these techniques to ensure that it never loses history.molhadoref','Software reverse engineering'
'an emerging approach to multi-device application development requires developers to build an abstract semantic model that is translated into specific implementations for web browsers, pdas, voice systems and other user interfaces. specifying abstract semantics can be difficult for designers accustomed to working with concrete screen-oriented layout. we present an approach to model recovery: inferring semantic models from existing applications, enabling developers to use familiar tools but still reap the benefits of multi-device deployment. we describe more, a system that converts the visual layout of html forms into a semantic model with explicit captions and logical grouping. we evaluate mores performance on forms from existing web applications, and demonstrate that in most cases the difference between the recovered model and a hand-authored model is under 5\%more for less','Software reverse engineering'
more,'Software reverse engineering'
'panel on software re-engineering','Software reverse engineering'
'pattern-based reverse-engineering of design components','Software reverse engineering'
'different studies show that programmers are more interested in finding definitions of functions and their uses than variables, statements, or ordinary code fragments. therefore, developers require support in finding relevant functions and determining how these functions are used. unfortunately, existing code search engines do not provide enough of this support to developers, thus reducing the effectiveness of code reuse. we provide this support to programmers in a code search system called portfolio that retrieves and visualizes relevant functions and their usages. we have built portfolio using a combination of models that address surfing behavior of programmers and sharing related concepts among functions. we conducted two experiments: first, an experiment with 49 c/c&plus;&plus; programmers to compare portfolio to google code search and koders using a standard methodology for evaluating information-retrieval-based engines; and second, an experiment with 19 java programmers to compare portfolio to koders. the results show with strong statistical significance that users find more relevant functions with higher precision with portfolio than with google code search and koders. we also show that by using pagerank, portfolio is able to rank returned relevant functions more efficiently.portfolio','Software reverse engineering'
'practical legal aspects of software reverse engineering','Software reverse engineering'
'a common trend in service oriented architecture (soa) is to consider information systems exposing software as services. this current approach is not only applied to new software developments, but also it is related to the maintenance of legacy systems. nowadays, a cornerstone of information systems are relational databases, which constitute meaningful sources of services. these services can provide database\'s information in soa scenarios. this paper presents a reengineering process to recover and implement web services in automatic manner from relational databases. this process follows the adm approach (architecture-driven modernization). in this paper authors present a case study that has been carried out using a tool built to support the process. this tool is used to generate a set of web services which are integrated into a web development allowing to modernise the legacy database in a soa context. this case study has been carried out in the context of software company indra.preciso','Software reverse engineering'
'qualitative reverse engineering','Software reverse engineering'
'we propose a new paradigm to query information about programs, namely query by outlines. this paradigm relies on an outlining model that conceptually describe units of code according to the computations they perform. outlines are automatically constructed by our system prisme for c and lisp programs. currently, both our model and our system are restricted to loops.qbo is a prototype tool that implements the query by outline paradigm. it proposes to browse the loops of a program directly through their outline, and allows to restrict these loops to browse with queries expressed as constraints on the outlines. thus it enables to answer questions such as \"where is this variable modified?\", \"where is this kind of computation performed?\", or \"are there many places where this computation is performed?\".in this paper, we sketch our outlining model, introduce qbo and argue that query by outline is a helpful paradigm to manage programs.query by outlines','Software reverse engineering'
'both putnam and searle have argued that that every abstract automaton is realized by every physical system, a claim that leads to a reductio argument against cognitivism or strong ai: if it is possible for a computer to be conscious by virtue of realizing some abstract automaton, then by putnam\'s theorem every physical system also realizes that automaton, and so every physical system is conscious--a conclusion few supporters of strong ai would be willing to accept. dennett has suggested a criterion of reverse engineering for identifying \"real patterns,\" and i argue that this approach is also very effective at identifying \"real realizations.\" i focus on examples of real-world implementations of complex automata because previous attempts at answering putnam\'s challenge have been overly restrictive, ruling out some realizations that are in fact paradigmatic examples of practical automaton realization. i also argue that some previous approaches have at the same time been overly lenient in accepting counter-intuitive realizations of trivial automata. i argue that the reverse engineering approach avoids both of these flaws. moreover, dennett\'s approach allows us to recognize that some realizations are better than others, and the line between real realizations and non-realizations is not sharp.real realization','Software reverse engineering'
'reengineering towards product lines (r2pl 2005)','Software reverse engineering'
'aspect oriented programming aims at addressing the problem of the crosscutting concerns, i.e., those functionalities that are scattered among several modules in a given system. aspects can be defined to modularize such concerns. in this work, we focus on a specific kind of crosscutting concerns, the scattered implementation of methods declared by interfaces that do not belong to the principal decomposition. we call such interfaces aspectizable. all the aspectizable interfaces identified within a large number of classes from the java standard library and from three java applications have been automatically migrated to aspects. to assess the effects of the migration on the internal and external quality attributes of these systems, we collected a set of metrics and we conducted an empirical study, in which some maintenance tasks were executed on the two alternative versions (with and without aspects) of the same system. in this paper, we report the results of such a comparison.refactoring the aspectizable interfaces','Software reverse engineering'
'regression testing is applied to modified software to provide confidence that the changed parts behave as intended and that the unchanged parts have not been adversely affected by the modifications. to reduce the cost of regression testing, test cases are selected from the test suite that was used to test the original version of the software---this process is called regression test selection. a safe regression-test-selection algorithm selects every test case in the test suite that may reveal a fault in the modified software. safe regression-test-selection technique that, based on the use of a suitable representation, handles the features of the java language. unlike other safe regression test selection techniques, the presented technique also handles incomplete programs. the technique can thus be safely applied in the (very common) case of java software that uses external libraries of components; the analysis of the external code is note required for the technique to select test cases for such software. the paper also describes retest, a regression-test-selection algorithm can be effective in reducing the size of the test suite.regression test selection for java software','Software reverse engineering'
'approaches to relational database reverse engineering often expect that the input has desirable characteristics and that it is complete; they also often fail to provide formal guarantees that their results are faithful to the initial input. both of these problems can be addressed by using an incremental approach based on a formally defined target model. the incremental approach we propose here quickly produces an initial model instance that is provably equivalent to the original relational database, which is assumed to be correct but may lack desirable characteristics and may be incomplete. the approach then proceeds incrementally using provably correct transformations. these incremental transformations allow for user interaction to provide needed information that may be missing or hard to obtain because the input lacks some desirable characteristics.relational database reverse engineering','Software reverse engineering'
'as reverse engineering becomes a prevalent technique to analyze malware, malware writers leverage various anti-reverse engineering techniques to hide their code. one technique commonly used is code packing as packed executables hinder code analysis. while this problem has been previously researched, the existing solutions are either unable to handle novel samples, or vulnerable to various evasion techniques. in this paper, we propose a fully dynamic approach that captures an intrinsic nature of hidden code execution that the original code should be present in memory and executed at some point at run-time. thus, this approach monitors program execution and memory writes at run-time, determines if the code under execution is newly generated, and then extracts the hidden code of the executable. to demonstrate its effectiveness, we implement a system, renovo, and evaluate it with a large number of real-world malware samples. the experiments show that renovo is accurate compared to previous work, yet practical in terms of performancerenovo','Software reverse engineering'
'the research activities in software engineering at the center for lifelong learning &amp; design (l3d) in the past have been grounded in the basic assumption that important aspects of software engineering are best understood as human-centered                        design activities. some of the major objectives were to support designers with domain-oriented design environments, allowing them to interact at the problem domain level and to frame activities and artifacts based on an evolutionary approach.a fundamental shift occurring over the last few years is the formation of participation cultures enhanced and supported by a change from an industrialized information economy (specialized in producing finished goods to be consumed passively) to a cyber-enabled networked information economy (in which all people are provided with the means to participate actively in personally meaningful problems). some of the implications of this fundamental shift for software engineering, including meta-design, lessons learned from open source software, and distribution and diversity in communities, are explored, and their implications for the \"automate/informate\" perspectives are briefly discussed.rethinking software design in participation cultures','Software reverse engineering'
'reverse engineering aims at extracting many kinds of information from existing software and using this information for system renovation and program understanding. the goal of this full day wcre\'05 workshop is to identify methods and techniques for reverse engineering from software to requirements (retr).retr','Software reverse engineering'
'the mondex electronic purse is an outstanding example of industrial scale formal refinement, and was the first verification to achieve itsec level e6 certification. a formal abstract model and a formal concrete model were developed, and a formal refinement was hand-proved between them. nevertheless, certain requirements issues were set beyond the scope of the formal development, or handled in an unnatural manner. the retrenchment tower pattern is used to address one such issue in detail: the finiteness of the purse log (which records unsuccessful transactions). a retrenchment is constructed from the lowest level model of the purse system to a model in which logs are finite, and is then lifted to create two refinement developments of the purse, working at different levels of detail, and connected via retrenchments, forming the tower. the tower development is appropriately validated, vindicating the design used.retrenching the purse','Software reverse engineering'
'in this paper, we present a framework for reverse engineeringallowing the integration and interaction of differentanalysis and visualization tools. the framework architecturethat we propose uses a dynamic type system to guaranteethe proper exchange of data between the tools and aset of wrapper classes to handle their communication. thisallows for an easy and secure integration of tools that haveoriginally not been designed to work together. in this sense,existing tools can be (re-)used and integrated. as a proofof concept we also present our own instantiation of the proposedframework architecture.reuse in reverse engineering','Software reverse engineering'
'efforts to develop standards for learning technologies have developed along two distinct strands: standards for data and information models; and standards for components, interfaces and architectures. standards relating to architectural frameworks are less well developed, and responsibility for decisions concerning system architecture has been left largely in the hands of developers of proprietary software such as learning management systems. there is growing interest in the development of standards for open architectural frameworks, based on layering, a decomposition technique which is in widespread use in software development. as interoperability and reusability are key concerns for developers of e-learning systems, the choice of an appropriate layering strategy is crucial, and this paper illustrates how a reuse-based layering strategy (as opposed to a more typical responsibility-based strategy) might be applied to e-learning systems in order to enhance reuse and interoperability.reuse-based layering','Software reverse engineering'
'many systems are constructed without the use of modeling and visualization artifacts, due to constraints imposed by deadlines or a shortage of manpower. nevertheless, such systems might profit from the visualization provided by diagrams to facilitate maintenance of the constructed system. in this paper, we present a tool, reveal, to reverse engineer a class diagram from the c + + source code representation of the software. in reveal, we remain faithful to the uml standard definition of a class diagram wherever possible. however, to accommodate the vagaries of the c + + language, we offer some extensions to the standard notation to include representations for namespaces, stand-alone functions and friend functions. we compare our representation to three other tools that reverse-engineer class diagrams, for both compliance to the uml standard and for their ability to faithfully represent the software system under study.reveal','Software reverse engineering'
'reverse data engineering technology for visual database design','Software reverse engineering'
'reverse engineering and reengineering of a large serial system into a distributed-parallel version','Software reverse engineering'
'reverse engineering and reengineering','Software reverse engineering'
'captchas are automated turing tests used to determine if the end-user is human and not an automated program. users are asked to read and answer visual captchas, which often appear as bitmaps of text characters, in order to gain access to a low-cost resource such as webmail or a blog. captchas are generated by software and the structure of a captcha gives hints to its implementation. thus due to these properties of image processing and image composition, the process that creates captchas can often be reverse engineered. once the implementation strategy of a family of captchas has been reverse engineered the captcha instances may be solved automatically by leveraging weaknesses in the creation process or by comparing a captcha\'s output against itself. in this paper, we present a case study where we reverse engineer and solve real-world captchas using simple image processing techniques such as bitmap comparison, thresholding, fill-flood segmentation, dilation, and erosion. we present black-box and white-box methodologies for reverse engineering and solving captchas. as well we provide an open source toolkit for solving captchas that we have used with a success rates of 99, 95, 61, 30\%, and 27\% on hundreds of captchas from five real-world examples.reverse engineering captchas','Software reverse engineering'
'the definition and method of reverse engineering are introduced. the application of pro/e software in re and model building are also discussed. the application of re is described with example.reverse engineering design','Software reverse engineering'
'feature models describe the common and variable characteristics of a product line. their advantages are well recognized in product line methods. unfortunately, creating a feature model for an existing project is time-consuming and requires substantial effort from a modeler. we present procedures for reverse engineering feature models based on a crucial heuristic for identifying parents - the major challenge of this task. we also automatically recover constructs such as feature groups, mandatory features, and implies/excludes edges. we evaluate the technique on two large-scale software product lines with existing reference feature models--the linux and ecos kernels--and freebsd, a project without a feature model. our heuristic is effective across all three projects by ranking the correct parent among the top results for a vast majority of features. the procedures effectively reduce the information a modeler has to consider from thousands of choices to typically five or less.reverse engineering feature models','Software reverse engineering'
'software reverse engineering can benefit from software performance engineering (spe) techniques and vice versa. reverse engineering\'s system artifacts satisfy spe\'s need for a sequence of a software system in order to quantita- tively analyze the system\'s performance characteristics. the code profiling tools used in software performance engineering can assist reverse engineering in understand- ing the behavior of the source code of a system. the aim of this workshop is to gather together researchers and practi- tioners working in the area of software performance engi- neering with an emphasis on code optimization and root cause analysis. we are interested in investigating how per- formance analysis can benefit from reverse engineering techniques. the objective of the workshop is to find com- mon case studies and compare existing techniques. this workshop will be the first inclusion of this important indus- trial relevant topic within the software reverse engineering community. we aim to set up a forum for exchanging ex- periences, discussing solutions, and exploring new ideas. keywords software performance engineering, code profiling and re- verse engineeringreverse engineering for software performance engineering','Software reverse engineering'
'in this paper, we study reverse engineering functional classes in java. as a case study we use a middleware application. functional classes is a design style that merges both object-oriented and functional programming paradigms. a functional class is a class without variables having pure functions as methods. as a result, a functional class is naturally a mediator. in this paper, as the main contribution, we show that functional classes lead to java bytecode that is re-compilable. consequently, functional classes provide a promising basis for mixed design of applications, where the developers can work simultaneously on the project using a uml editor, source code editor, code generator, and optimizer. then, synchronization between the various tools is achieved on-the-fly using only the bytecode representation.reverse engineering functional classes','Software reverse engineering'
'segmenting garments from humanoid meshes or point clouds is a challenging problem with applications in the textile industry and in model based motion capturing. in this work we present a physically based template-matching technique for the automatic extraction of garment dimensions from 3d meshes or point clouds of dressed humans. the successfull identification of garment dimensions also allows the semantic segmentation of the mesh into naked and dressed parts.reverse engineering garments','Software reverse engineering'
'reverse engineering is focused on the challenging task of understanding legacy program code without having suitable documentation. using a transformational forward engineering perspective, we gain the insight that much of this difficulty is caused by design decisions made during system development. such decisions ``hide\'\' the program functionality and performance requirements in the final system by applying repeated refinements through layers of abstraction, and information--spreading optimizations, both of which change representations and force single program entities to serve multiple purposes. to be able to reverse engineer, we essentially have to reverse these design decisions. following the transformational approach we can use the transformations of a forward engineering methodology and apply them \"backwards\" to reverse engineer code to a more abstract specification. since most existing code was not generated by transformational synthesis, this produces a plausible formal transformational design rather than the original authors\' actual design. a by-product of the transformational reverse engineering process is a design database for the program that then can be maintained to minimize the need for further reverse engineering during the remaining lifetime of the system. a consequence of this perspective is the belief that plan recognition methods are not sufficient for reverse engineering. as an example, a small fragment of a real--time operating system is reverse-engineered using this approach.reverse engineering is reverse forward engineering','Software reverse engineering'
'legacy systems constitute valuable assets to the organizations that own them. however, due to the development of newer and faster hardware platforms and the invention of novel interface styles, there is a great demand for their migration to new platforms. in this paper, we present a method for reverse engineering the system interface that consists of two tasks. based on traces of the users interaction with the system, the ``interface mapping\'\' task constructs a ``map\'\' of the system interface, in terms of the individual system screens and the transitions between them. the subsequent ``task and domain modeling\'\' task uses the interface map and task-specific traces to construct an abstract model of a user\'s task as an information exchange plan. the task model specifies the screen transition diagram that the user has to traverse in order to accomplish the task in question, and the flow of information that the user exchanges with the system at each screen. this task model is later used as the basis for specifying a new graphical user interface tailored to the task in question.reverse engineering legacy interfaces','Software reverse engineering'
'analysis of molecular interaction networks is pervasive in systems biology. this research relies almost entirely on graphs for modeling interactions. however, edges in graphs cannot represent multiway interactions among molecules, which occur very often within cells. hypergraphs may be better representations for networks having such interactions, since hyperedges can naturally represent relationships among multiple molecules. here, we propose using hypergraphs to capture the uncertainty inherent in reverse engineering gene-gene networks. some subsets of nodes may induce highly varying subgraphs across an ensemble of networks inferred by a reverse engineering algorithm. we provide a novel formulation of hyperedges to capture this uncertainty in network topology. we propose a clustering-based approach to discover hyperedges. we show that our approach can recover hyperedges planted in synthetic data sets with high precision and recall, even for moderate amount of noise. we apply our techniques to a data set of pathways inferred from genetic interaction data in s. cerevisiae related to the unfolded protein response. our approach discovers several hyperedges that capture the uncertain connectivity of genes in relevant protein complexes, suggesting that further experiments may be required to precisely discern their interaction patterns. we also show that these complexes are not discovered by an algorithm that computes frequent and dense subgraphs.reverse engineering molecular hypergraphs','Software reverse engineering'
'this paper presents a technique that helps automate the reverse engineering of device drivers. it takes a closed-source binary driver, automatically reverse engineers the driver\'s logic, and synthesizes new device driver code that implements the exact same hardware protocol as the original driver. this code can be targeted at the same or a different os. no vendor documentation or source code is required. drivers are often proprietary and available for only one or two operating systems, thus restricting the range of device support on all other oses. restricted device support leads to low market viability of new oses and hampers os researchers in their efforts to make their ideas available to the \'real world.\' reverse engineering can help automate the porting of drivers, as well as produce replacement drivers with fewer bugs and fewer security vulnerabilities. our technique is embodied in revnic, a tool for reverse engineering network drivers. we use revnic to reverse engineer four proprietary windows drivers and port them to four different oses, both for pcs and embedded systems. the synthesized network drivers deliver performance nearly identical to that of the original drivers.reverse engineering of binary device drivers with revnic','Software reverse engineering'
'this paper describes static analyses for reverse engi- neering graphical user interfaces (guis). these analy- ses, implemented in the bauhaus tool suite, support typical maintenance tasks like migrating from a hand-written gui to so-called gui builders and redocumentation of the gui. our tool extracts the program\'s windows and their struc- ture, the attributes of the widgets and their values, the gui events that might occur at runtime, and the event handlers associated with those events. we explain our approach and report encouraging results for several programs.reverse engineering of graphical user interfaces using static analyses','Software reverse engineering'
'the discovery of gene regulatory networks is a major goal in the field of bioinformatics due to their relevance, for instance, in the development of new drugs and medical treatments. the idea underneath this task is to recover gene interactions in a global and simple way, identifying the most significant connections and thereby generating a model to depict the mechanisms and dynamics of gene expression and regulation. in the present paper we tackle this challenge by applying a genetic algorithm to boolean-based networks whose structures are inferred through the optimization of a tsallis entropy function, which has been already successfully used in the inference of gene networks with other search schemes. additionally, wisdom of crowds is applied to create a consensus network from the information contained within the last generation of the genetic algorithm. results show that the proposed method is a promising approach and that the combination of criterion function based on tsallis entropy with an heuristic search such as genetic algorithms yields networks up to 50\% more accurate when compared to other boolean-based approaches.reverse engineering of grns','Software reverse engineering'
'during software evolution, programmers devote most of their effort to the understanding of the structure and behavior of the system. for object-oriented code, this might be particularly hard, when multiple, scattered objects contribute to the same function. design views offer an invaluable help, but they are often not aligned with the code, when they are not missing at all.this tutorial describes some of the most advanced techniques that can be employed to reverse engineer several design views from the source code. the recovered diagrams, represented in uml (unified modeling language), include class, object, interaction (collaboration and sequence), state and package diagrams. a unifying static code analysis framework used by most of the involved algorithms is presented at the beginning of the tutorial. a single running example is referred all over the presentation. trade-offs (e.g., static vs. dynamic analysis), limitations and expected benefits are also discussed.reverse engineering of object oriented code','Software reverse engineering'
'reverse engineering of software threads','Software reverse engineering'
'reverse engineering of software','Software reverse engineering'
'tophat is a fast splice junction mapper for next generation sequencing analysis, a technology for functional genomic research. next generation sequencing technology allows more accurate analysis increasing data to elaborate, this opens to new challenges in terms of development of tools and computational infrastructures. we present a solution that cover aspects both software and hardware, the first one, after a reverse engineering phase, provides an improvement of algorithm of tophat making it parallelizable, the second aspect is an implementation of an hybrid infrastructure: grid and virtual grid computing. moreover the system allows to have a multi sample environment and is able to process automatically totally transparent to user.reverse engineering of tophat','Software reverse engineering'
'in the present work, we outline a reverse engineering approach for uml specifications in form of class diagrams from java bytecode. after a brief introduction to the subject we present some analyses which go beyond mere enumeration of methods and fields. a glance onto some related work shows that there seems to be no pat solution for the reverse engineering of the more difficult class diagram elements. we sketch our method of determining association multiplicities, being, in a sense, representative of our approach in general: \"intuitive\" analyses, producing results that can be understood by a programmer when inspecting the source code of a given class. finally, we introduce a tool that implements this work and we apply it onto a small real life example, discussing the results it gave.reverse engineering of uml specifications from java programs','Software reverse engineering'
'reverse engineering processes, design document production, and structure charts','Software reverse engineering'
'this is a questionnaire on program understanding and reverse engineering. it may be filled out manually or on-line. the results of the questionnaire will be used to guide the research of the two authors, both of whom are ph.d. students working in this area. copies of the resulting report will be mailed to all who participate, and a summary of the results will be published in an appropriate forum.reverse engineering questionnaire','Software reverse engineering'
'reverse engineering and software evolution research has been focused mostly on analyzing single software sys- tems. however, rarely a project exists in isolation; instead, projects exist in parallel within a larger context given by a company, a research group or the open-source community. technically, such a context manifests itself in the form of super-repositories, containers of several projects developed in parallel. well-known examples of such super-repositories include sourceforge and codehaus. we present an easily accessible platform which supports the analysis of such super-repositories. the platform can be valuable for reverse engineering both the projects and the structure of the organization as reflected in the inter- actions and collaborations between developers. through- out the paper we present various types of analysis applied to three open-source and one industrial smalltalk super- repositories, containing hundreds of projects developed by dozens of people.reverse engineering super-repositories','Software reverse engineering'
'web systems evolved in the last years starting from static websites to web applications, up to ajax-based rich internet applications (rias). reverse engineering techniques followed the same evolution, too. the authors and many other wse contributors proposed a lot of innovative and effective ideas providing important advances in the reverse engineering field. in this paper, we will show the historical evolution of reverse engineering approaches for web systems with particular attention to the ones presented in the wse events.reverse engineering techniques','Software reverse engineering'
'reverse engineering the bazaar','Software reverse engineering'
'what do fruit-fly brains have in common with microchips? that\'s not the setup for a bad joke; it\'s david adler\'s life. under adler\'s ultrasophisticated electron beam microscopes, advanced microprocessors with transistors far smaller than red blood cells have been reduced to their wiring diagrams. now the noggin of the humble drosophila melanogaster is next, as adler is being courted by researchers at a neurobiology wing of the howard hughes medical institute to help them reverse engineer the human brain. they\'re starting small, with the fruit fly.reverse engineering the brain','Software reverse engineering'
'to provide insight into internet operation and performance, recent efforts have measured various aspects of the internet, developing and improving measurement tools in the process. in this paper, we argue that these independent advances present the community with a startling opportunity: the collaborative reverse-engineering of the internet. by this, we mean annotating a map of the internet with properties such as: client populations, features and workloads; network ownership, capacity, connectivity, geography and routing policies; patterns of loss, congestion, failure and growth; and so forth. this combination of properties it greater than the sum of its parts, and exposes the attributes of network design easily overlooked by simpler, uncorrelated models. we argue that reverse engineering the internet is feasible based on continuing improvements in measurement techniques, the potential to infer new properties from external measurements, and an accounting of the resources required to complete the process.reverse engineering the internet','Software reverse engineering'
'reverse engineering is concerned with the reconstruction of surfaces from three-dimensional point clouds originating from laser-scanned objects. we present an adaptive surface reconstruction method providing a hierarchy of quadrilateral meshes adapting surface topology when a mesh is refined. this way, a user can choose a model with proper resolution and topology from the hierarchy without having to run the algorithm multiple times with different parameters. the multiresolution mesh representation can be used subsequently for view-dependent rendering and wavelet compression.reverse engineering with subdivision surfaces','Software reverse engineering'
'design pattern detection is a reverse engineering methodology that helps software engineers to analyze and understand legacy software by recovering its design and thereby aiding in the preparation of re-engineering activities. we present reclipse, a reverse engineering tool suite for static and dynamic design pattern detection in combination with a pattern candidate rating used to assess the detection results\' reliability.reverse engineering with the reclipse tool suite','Software reverse engineering'
'a great number of existing xml documents in various domain such as electrical business have to be maintained in order to constantly adapt to a dynamically changing environment to keep pace with business needs. a dtd or xml schema in its current textual form commonly lacks clarity and readability, which makes the maintenance process tedious and error-prone. this paper presents an approach to reverse engineering the xml documents to conceptual model, which makes the xml documents more close to real world and business needs, let the designers quickly gain a picture of the overall structure of xml documents in order to improve its quality, increase the maintainability and reusability. in this paper, the conceptual model is described by uml class diagram, a three-level model is defined, and a novel approach for extracting various structure and semantic information from existing dtd is given, especially the inheritance structure can be inferred from the dtd structure.reverse engineering xml','Software reverse engineering'
'reverse engineering, re-engineering, and conversion','Software reverse engineering'
'reverse engineering','Software reverse engineering'
'reverse engineering: progress along many dimensions','Software reverse engineering'
'reverse-engineering and intermodular data flow','Software reverse engineering'
'in recent years, several national and community-driven conference rankings have been compiled. these rankings are often taken as indicators of reputation and used for a variety of purposes, such as evaluating the performance of academic institutions and individual scientists, or selecting target conferences for paper submissions. current rankings are based on a combination of objective criteria and subjective opinions that are collated and reviewed through largely manual processes. in this setting, the aim of this paper is to shed light into the following question: to what extent existing conference rankings reflect objective criteria, specifically submission and acceptance statistics and bibliometric indicators? the paper specifically considers three conference rankings in the field of computer science: an australian national ranking, a brazilian national ranking and an informal community-built ranking. it is found that in all cases bibliometric indicators are the most important determinants of rank. it is also found that in all rankings, top-tier conferences can be identified with relatively high accuracy through acceptance rates and bibliometric indicators. on the other hand, acceptance rates and bibliometric indicators fail to discriminate between mid-tier and bottom-tier conferences.reverse-engineering conference rankings','Software reverse engineering'
'this paper reverse-engineers backoff-based random-access mac protocols in ad-hoc networks. we show that the contention resolution algorithm in such protocols is implicitly participating in a non-cooperative game. each link attempts to maximize a selfish local utility function, whose exact shape is reverse-engineered from the protocol description, through a stochastic subgradient method in which the link updates its persistence probability based on its transmission success or failure. we prove that existence of a nash equilibrium is guaranteed in general. then we establish the minimum amount of backoff aggressiveness needed, as a function of density of active users, for uniqueness of nash equilibrium and convergence of the best response strategy. convergence properties and connection with the best response strategy are also proved for variants of the stochastic-subgradient-based dynamics of the game. together with known results in reverse-engineering tcp and bgp, this paper further advances the recent efforts in reverse-engineering layers 2-4 protocols. in contrast to the tcp reverse-engineering results in earlier literature, mac reverse-engineering highlights the non-cooperative nature of random access.reverse-engineering mac','Software reverse engineering'
'scanning or soft keyboards are alternatives to physical computer keyboards that allow users with motor disabilities to compose text and control the computer using a small number of input actions. in this paper, we present the reverse huffman algorithm (rha), a novel information theoretic method that extracts a representative latent probability distribution from a given scanning keyboard design. by calculating the jensen-shannon divergence (jsd)between the extracted probability distribution and the probability distribution that represents the body of text that will be composed by the scanning keyboard, the efficiency of the design can be predicted and designs can be compared with each other. thus, using rhs provides a novel a priori context-aware method for reverse-engineering scanning keyboards.reverse-engineering scanning keyboards','Software reverse engineering'
rose-ada,'Software reverse engineering'
'in this paper we describe a two step process for reverse engineering the software architecture of a system directly from its source code. the first step involves clustering the modules from the source code into abstract structures called subsystems. the second step involves reverse engineering the subsystem-level relations using a formal (and visual) architectural constraint language. we use search techniques to accomplish both of these steps, and have implemented a suite of integrated tools to support the reverse engineering process. through a case study, we demonstrate how our tools can be used to extract the software architecture of an open-source software package from its source code without having any a priori knowledge about its design.search based reverse engineering','Software reverse engineering'
'shimba&#8212;an environment for reverse engineering java software systems','Software reverse engineering'
'softman: environment for forward and reverse case','Software reverse engineering'
'this paper covers current trends and issues in software architecture recovery. it consists of a summary of the presentations and discussions of the software architecture recovery and modelling discussion forum held during wcre 2001, the working conference on reverse engineering, stuttgart, germany, october 2, 2001.software architecture recovery and modelling','Software reverse engineering'
'decisions regarding software evolution strategies such as modernizations are economically important. we present results of our empirical study of the views of decision makers. we have asked their views of the relative importance of 49 software modernization decision criteria. we have gathered data from finnish software industry. there were 26 experts from 8 organizations involved. they were mainly upper or middle level managers. our study shows that there is a large set of criteria which should be taken into account, and that those studied by us provide a good coverage of the relevant ones. we list the top-twenty criteria. we also performed acluster analysis which produced two groups of subjects. views of the decision makers in software user andsoftware supplier organizations were different. we suggest that decision maker\'s work process could be enhanced by taking into account the received lessons.software modernization decision criteria','Software reverse engineering'
'the aim of reverse engineering is to draw out many kinds of information from existing software and using this information for system renovation and program understanding. based on traditional practice, reverse engineering and requirements engineering are two separate processes in software round trip engineering. in this paper, we argue that it is necessary to recover requirements from the reverse engineered outcome of legacy system and by integrating this outcome in the requirements phase of software life cycle, it is possible to have a better requirements elicitation, and clear understanding of what is redundant, what must be retained and what can be re-used. so we have presented a revised model of traditional re-engineering process and also described the rationality of the proposed model. in the paper we have also discussed briefly about software reverse engineering, requirement engineering and their basic practices and activities.software reverse engineering to requirements','Software reverse engineering'
'software reverse engineering','Software reverse engineering'
'this article presents the results of a 2002 survey on industry adoption of software review technologies. the study offers two important insights. first, many responding companies take advantage of reviews for various purposes, including early defect detection, monitoring and controlling quality, and better communication within the development team. second, many companies use reviews unsystematically, with a mismatch between the expected outcome and the review implementation. so, although review ideas have reached software practitioners, their full potential is seldom exploited.software reviews','Software reverse engineering'
'during recent years, the amount of variability that hasto be supported by a software artefact is growingconsiderably and its management is evolving into amajor challenge during development, usage, andevolution of software artefacts. successful managementof variability in software leads to better customizablesoftware products that are in turn likely to result inhigher market success.the aim of this tutorial is to present softwarevariability management both from a &#253;problems&#253; andfrom a &#253;solutions&#253; perspective by discussingexperiences from industrial practice and from appliedresearch in academia. issues that are addressedinclude, but are not limited to, technological, process,and organizational aspects as well as notation,assessment, design, and evolution aspects.software variability management','Software reverse engineering'
'software visualization is concerned with the static visualization as well as the animation of software artifacts, such as source code, executable programs, and the data they manipulate, and their attributes, such as size, complexity, or dependencies. software visualization techniques are widely used in the areas of software maintenance, reverse engineering, and re-engineering, where typically large amounts of complex data need to be understood and a high degree of interaction between software engineers and automatic analyses is required. this paper reports the results of a survey on the perspectives of 82 researchers in software maintenance, reverse engineering, and re-engineering on software visualization. it describes to which degree the researchers are involved in software visualization themselves, what is visualized and how, whether animation is frequently used, whether the researchers believe animation is useful at all, which automatic graph layouts are used if at all, whether the layout algorithms have deficiencies, and--last but not least--where the medium-term and long-term research in software visualization should be directed. the results of this survey help to ascertain the current role of software visualization in software engineering from the perspective of researchers in these domains and give hints on future research avenues.software visualization in software maintenance, reverse engineering, and re-engineering','Software reverse engineering'
'specification-based program slicing and its applications','Software reverse engineering'
'our research is driven by the motivation that change must be put in the center, if one wants to understand the complex processes of software evolution. we built a toolset named spyware which, using a monitoring plug-in for integrated development environments (ides), tracks the changes that a developer performs on a program as they happen. spyware stores these first-class changes in a change repository and offers a plethora of productivity-enhancing ide extensions to exploit the recorded information.spyware','Software reverse engineering'
'the number of possible refactorings is unlimited, so no tool vendor will ever be able to provide custom refactorings for all specific user needs. therefore, we propose a new kind of refactoring tools, which allow users to create, edit and compose required refactorings just like any other documents. the heart of such a refactoring editor is the ability to compose larger refactorings from existing ones. computing the precondition of the composite refactoring from the preconditions of the composed refactorings is non-trivial since earlier transformations influence the truth of preconditions of later ones. the ability to calculate these effects without referring to a particular program to which the refactorings should be applied is called program-independent composition. it is the prerequisite for creating composite refactorings that are reusable on arbitrary programs. the main contribution of this paper is a formal model for automatic, program-independent composition of conditional program transformations. we show that conditional transformations, including refactorings, can be composed flom a limited set of basic operations. program-independent derivation of a precondition for the composite is based on the notion of \"transformation description\", which can be seen as a simplified, yet equally powerful, variant of roberts\' \"postconditions\" (practical analysis for refactoring, ph.d. thesis (1999)). our approach simplifies the implementation of refactoring tools--only the basic operations and the ability for composition must be hard coded in a tool. as a proof of concept, we sketch a transformation framework that implements our approach (jconditioner) and, based on the framework, an experimental refactoring tool (contract) that includes the editing capabilities that motivated our work.static composition of refactorings','Software reverse engineering'
'a case study using a new complexity measurement framework called structure 101 tracked the structural complexity of three open source software products through their different releases. the analysis found that, as these software products evolved, a large proportion of structural complexity in early releases at the application-code level progressively migrated to higher-level design and architectural elements in subsequent releases, or vice-versa. this pattern repeated itself throughout the evolution of the software product. refactoring efforts successfully reduced complexity at lower levels, but shifted the complexity to higher levels in the design hierarchy. conversely, design restructuring at higher levels shifted complexity to lower levels. if this trend holds true for other software products, then mere code refactoring might not be enough to effectively managing structural complexity. periodic major restructuring of software applications at the design or architectural level could be necessary.structural epochs in the complexity of software over time','Software reverse engineering'
'authentic descriptions of a software architecture are requiredas a reliable foundation for any but trivial changesto a system. far too often, architecture descriptions of existingsystems are out of sync with the implementation. if theyare, they must be reconstructed.there are many existing techniques for reconstructing individualarchitecture views, but no information about how toselect views for reconstruction, or about process aspects ofarchitecture reconstruction in general. in this paper we describeview-driven process for reconstructing software architecturethat fills this gap. to describe symphony, we presentand compare different case studies, thus serving a secondarygoal of sharing real-life reconstruction experience.the symphony process incorporates the state of the practice,where reconstruction is problem-driven and uses a richset of architecture views. symphony provides a commonframework for reporting reconstruction experiences and forcomparing reconstruction approaches. finally, it is a vehiclefor exposing and demarcating research problems in softwarearchitecture reconstruction.symphony','Software reverse engineering'
'this paper reports on an experience in teaching database reverse engineering. we found a graduated sequence of case studies to be effective. our evidence is anecdotal, but we believe the observations will be helpful for improving teaching techniques.teaching database reverse engineering','Software reverse engineering'
'in the duplication of a physical part such as die and mold, aerospace part and so on, most of patches are trimmed surfaces resulting from boolean manipulations. direct generation of tool paths from the practical parts is a fundamental problem. this paper presents an efficient method for generating nc tool paths from some trimmed surfaces. three types of control points are determined to construct an underlying nurbs surfaces. nc tool paths are then generated based on these surfaces. the method can deal efficiently with parts composed of trimmed surfaces. it can be considered as a tool for reverse engineering software integration.the algorithms for trimmed surfaces construction and tool path generation in reverse engineering','Software reverse engineering'
'30340xvithe dark side of software reverse engineering','Software reverse engineering'
'the perils of reconstructing architectures','Software reverse engineering'
'thermoforming is commonly used to produce shaped plastic sheets for packaging consumer products. the conventional method of designing and making thermoforming moulds is laborious and time consuming. a method based on a reverse engineering approach and thermoforming feature concept is proposed. the method involves the use of a self-developed device to digitise the surface of a product. a cad model that corresponds to the thermoforming mould of the product is then constructed by using the digitised data. the construction of the mould surface is based on the concept of a defined set of thermoforming mould features. a modified laplacian smoothing technique is applied to process the digitised data for generating the thermoforming mould surfaces. several examples are used to explain the working principle and demonstrate the viability of the proposed method.thermoforming mould design using a reverse engineering approach','Software reverse engineering'
'thinking objectively','Software reverse engineering'
'in this paper we present an infrastructure that supports interoperability among various reverse engineering tools and applications. we include an application programmer\'s interface that permits extraction of information about declarations, including classes, functions and variables, as well as information about scopes, types and control statements in c++ applications. we also present a hierarchy of canonical schemas that capture minimal functionality for middle-level graph structures. this hierarchy facilitates an unbiased comparison of results for different tools that implement the same or a similar schema. we have a repository, hosted by sourceforge.net, where we have placed the artifacts of our infrastructure.toward an infrastructure to support interoperability in reverse engineering','Software reverse engineering'
'despite a great deal of research in the area, a number of challenges still need to be faced before making agentbased computing a widely accepted paradigm in software engineering practice. in order to realize an engineering change in agent oriented software engineering: it\'s necessary to turn agent oriented software abstractions into practical tools for facing the complexity of modern application areas. the paper presents a universal development architecture for multi-agent system to provide straight connection between agent oriented analysis and software implementation. a detailed agent structure plays a key role in the process. the development process presented fulfills our requirements in the construction of c4i system.towards an engineering change in agent oriented software engineering','Software reverse engineering'
'in [6], ian foster and karl kesselman explain that grids need \"a rethinking of existing programming models and, most likely, new thinking about novel models\". in this work, we investigate a \"novel programming model\" for grids based on the chemical metaphor.towards chemical coordination for grids','Software reverse engineering'
'architecture evolution, a key aspect of software evolution, is typically done in an ad hoc manner, guided only by the competence of the architect performing it. this process lacks the rigor of an engineering discipline. in this paper, we argue that architecture evolution must be engineered &#8212; based on rational decisions that are supported by formal models and objective analyses. we believe that evolutions of a restricted form &#8212; close-ended evolution, where the starting and ending design points are known a priori &#8212; are amenable to being engineered. we discuss some of the key challenges in engineering close-ended evolution. we present a conceptual framework in which an architecture evolutionary trajectory is modeled as a sequence of steps, each captured by an operator. the goal of our framework is to support exploration and objective evaluation of different evolutionary trajectories. we conclude with open research questions in developing this framework.towards engineered architecture evolution','Software reverse engineering'
'the aggregation of studies is of growing interest for the empirical software engineering community, since the numbers of studies steadily grow. in this paper we discuss challenges with the aggregation of studies into a common body of knowledge, based on a quantitative and qualitative evaluation of experience from the experimental software engineering network, esernet. challenges are that the number of studies available is usually low, and the studies that exist are often too scattered and diverse to allow systematic aggregation as a means for generating evidence. esernet therefore attempted to coordinate studies and thus create research synergies to achieve a sufficiently large number of comparable studies to allow for aggregation; however, the coordination approach of esernet proved to be insufficient. based on some lessons learned from esernet, a four-step procedure for evolving empirical software engineering towards the generation of evidence is proposed. this consists of (1) developing a methodology for aggregating different kinds of empirical results, (2) establishing guidelines for performing, analyzing, and reporting studies as well as for aggregating the results for every kind of empirical study, (3) extract evidence, that is, apply the methodology to different areas of software engineering, and (4) package the extracted evidence into guidelines for practice.towards evidence in software engineering','Software reverse engineering'
'powerful differencing algorithms\' availability is crucial to tracking source code\'s evolution, for example for monitoring clones or vulnerable statements. based on a novel differencing algorithm, our language-independent approach can track code elements\' evolution in real-world software systems with acceptable precision, overcoming the unix diff\'s versioning limitations.tracking your changes','Software reverse engineering'
'there\'s reverse engineering to understand, and then there\'s reverse engineering to copy. counterfeiting is a very old human temptation, but it is keeping up with the digital world very well indeed. putting aside ordinary movie piracy, we thought that for this issue we\'d just compare some counterfeiting metrics, old and new. putting the punchline right up front, counterfeiting matters in information technology (it)&#8212;and it might soon be where counterfeiting matters most.type ii reverse engineering','Software reverse engineering'
'class diagrams play an important role in software development. however, in some cases, these diagrams contain a lot of information. this makes it hard for software maintainers to use them to understand a system. in this paper, we aim to discover how to simplify class diagrams in a such way that they make systems easier to understand. to this end, we performed a survey to analyze what type of information software developers find important to include or exclude in order to simplify a class diagram. this survey involved 32 software developers with 75\% of the participants having more than 5 years of experience with class diagrams. as the result, we found that the important elements in a class diagram are class relationship, meaningful class names and class properties. we also found that information that should be excluded in a simplified class diagram is gui related information, private and protected operations, helper classes and library classes. in this survey we also tried to discover what types of features are needed for class diagram simplification tools.uml class diagram simplification','Software reverse engineering'
'legacy systems constitute valuable assets to the organizations that own them, and today, there is an increased demand to make them accessible through the world wide web to support e-commerce activities. as a result, the problem of legacy-interface migration is becoming very important. in the context of the cellest project, we have developed a new process for migrating legacy user interfaces to web-accessible platforms. instead of analyzing the application code to extract a model of its structure, the cellest process analyzes traces of the system-user interaction to model the behavior of the application\'s user interface. the produced state-transition model specifies the unique legacy-interface screens (as states) and the possible commands leading from one screen to another (as transitions between the states). the interface screens are identified as clusters of similar-in-appearance snapshots in the recorded trace. next, the syntax of each transition command is extracted as the pattern shared by all the transition instances found in the trace. this user-interface model is used as the basis for constructing models of the tasks performed by the legacy-application users; these task models are subsequently used to develop new web-accessible interface front ends for executing these tasks. in this paper, we discuss the cellest method for reverse engineering a state-transition model of the legacy interface, we illustrate it with examples, we discuss the results of our experimentation with it, and we discuss how this model can be used to support the development of new interface front ends.user interface reverse engineering in support of interface migration to the web','Software reverse engineering'
'using a document parser to automate software testing','Software reverse engineering'
'while a style guide typically covers good practices&#8212;what to do and what to avoid &#8212;an antipattern is somewhat more ambitious. it seeks to explain how good intentions can go awry and suggest meaningful ways to repair broken systems. the point isn\'t so much to say \"do this\" or \"avoid doing that\" as to suggest ways to prevent a problem or to skillfully apply a set of corrective actions.valuing design repair','Software reverse engineering'
'abstraction is one of the primary intellectual tools we have for managing complexity in software systems. when we think of abstractions we usually think about \"small\" abstractions, such as data abstraction (parameterization), type abstraction (polymorphism) and procedural or functional abstraction. these are the everyday kinds of things we work with - finding the right concepts to make the expression of our software solutions easier to understand and easier to reason about. here i propose we thing about \"large\" abstractions - abstractions that provide critical distinctions about our field of software engineering as a whole; abstractions that enable us to see what we do in different and important ways and provide significant improvements in how we do software engineering. i give a number of examples and delineate why i think they have been, and still are, important.\"large\" abstractions for software engineering','Software security engineering'
'since mid-1992, the national aeronautics and space administration (nasa) mission control center (mcc) in houston has pursued commonality and consolidation of facilities and equipment to reduce the implementation costs associated with the modernization of the control center, as well as the on-going operations and sustaining costs. in december of 1993, a management team, comprised almost entirely of individuals with manned spaceflight operations experience, was given the charter of completing the development. past practices and conventional thinking have been set aside in an effort to deliver the new control center &#8220;better, faster, and cheaper&#8221;. this new paradigm has carried-over to the engineering associated with securing the control center\'s information resources. the  group originally tasked with ensuring the security of the mcc\'s information resources was not directly associated with development or operations of the mcc, and thus had lost sight of the fact that the mcc\'s mission is the command and control of space vehicles, not the pursuit of information security. under the new paradigm for control center development, responsibility for ensuring system security and integrity has been brought into the development organization. old methods and approaches to security have been replaced by a new approach which emphasizes the implementation of an automated information systems security architecture that is both logical and cost effective.in both government, and industry, it is common for those tasked with information security responsibilities to  be separated from the mainstream of engineering. this separation often results in conflict during requirements definition, design, implementation and testing. the following is a case study of such an information security organization, and process, run amok. it also discusses the steps required to bring the process back into line with the goals of cost-effective development and operation.&#8220;mainstreaming&#8221; automated information systems security engineering (a case study in security run amok)','Software security engineering'
' computational science and engineering (cse) software supports a wide variety of domains including nuclear physics, crash simulation, satellite data processing, fluid dynamics, climate modeling, bioinformatics, and vehicle development. the increases importance of cse software motivates the need to identify and understand appropriate software engineering (se) practices for cse. because of the uniqueness of the cse domain, existing se tools and techniques developed for the business/it community are often not efficient or effective. appropriate se solutions must account for the salient characteristics of the cse development environment. se community members must interact with cse community members to understand this domain and to identify effective se practices tailored to cses needs. this workshop facilitates that collaboration by bringing together members of the cse and se communities to share perspectives and present findings from research and practice relevant to cse software and cse se education. a significant portion of the workshop is devoted to focused interaction among the participants with the goal of generating a research agenda to improve tools, techniques, and experimental methods for cse software engineering. 5th international workshop on software engineering for computational science and engineering (se-cse 2013)','Software security engineering'
'model-driven engineering (mde) is more and more used in collaborative settings. therefore, the usage of build servers to gain early integration of different system parts is desirable. current build server technologies only consider fully automated operations. further, the availability of all described artifacts is necessary for a correct run. however, in mde manual activities can occur in between automated operations, which also prevents that all necessary artifacts are already available when the build starts. therefore, state of the art build techniques are not sufficient to support mde development. in this paper, we present a build server prototype which is designed to fit the needs of development with multiple paradigms and languages.a build server for model-driven engineering','Software security engineering'
'a challenging task in security engineering concerns thespecification and integration of security with other requirementsat the top level of requirements engineering. empiricalstudies show that it is common that end users areable to express their security needs at the business processlevel. since many security requirements originate at thislevel, it is natural to try to capture and express them withinthe context of business models where end users feel mostcomfortable and where they conceptually belong. in thispaper we develop these views, present an ongoing work intendedto create a uml-based and business process-drivenframework for the development of security-critical systemsand propose an approach to a rigorous treatment of securityrequirements supported by formal methods.a business process-driven approach to security engineering','Software security engineering'
'common criteria(cc) provides only the standard forevaluating information security product or system, namelytarget of evaluation (toe). on the other hand, sse-cmmprovides the standard for security engineering processevaluation. based on the cc, toe\'s security quality maybe assured, but its disadvantage is that the developmentprocess is neglected. sse-cmm seems to assure thequality of toe developed in an organization equipped withsecurity engineering process, but the toe developed insuch environment cannot avoid cc-based securityassurance evaluation.we propose an effective method of integrating twoevaluation methods, cc and sse-cmm, and develop cc-based assurance evaluation model, cc_sse-cmm.cc_sse-cmm presents the specific and realisticallyoperable organizational security process maturityassessment and cc evaluation model.a cc-based security engineering process evaluation model','Software security engineering'
'this paper describes a collaborative structured demonstration of reverse engineering tools that was presented at a working session at wcre 2001 in stuttgart, germany. a structured demonstration is a hybrid tool evaluation technique that combines elements from experiments, case studies, technology demonstrations, and benchmarking. the essence of the technique is to facilitate learning about software engineering tools using a common set of tasks. the collaborative experience discussed at wcre involved several peer and complementary technologies that were applied in concert to solve a real life reverse engineering problem. for the most part, the tool developers themselves applied their own tools to this problem. preliminary results have shown to the research community that we still have much to learn about our tools and how they can be applied as part of a reverse engineering and reengineering process. consequently, the participants agreed to continue participation in this demonstration beyond the wcre event.a collaborative demonstration of reverse engineering tools','Software security engineering'
'in order to develop security critical information systems, specifying security quality requirements is vitally important, although it is a very difficult task. fortunately, there are several security standards, like the common criteria (iso/iec 15408), which help us handle security requirements. this article will present a common criteria centred and reuse-based process that deals with security requirements at the early stages of software development in a systematic and intuitive way, by providing a security resources repository as well as integrating the common criteria into the software lifecycle, so that it unifies the concepts of requirements engineering and security engineering.a common criteria based security requirements engineering process for the development of secure information systems','Software security engineering'
'this paper presents a conceptual framework for security engineering, with a strong focus on security requirements elicitation and analysis. this conceptual framework establishes a clear-cut vocabulary and makes explicit the interrelations between the different concepts and notions used in security engineering. further, we apply our conceptual framework to compare and evaluate current security requirements engineering approaches, such as the common criteria, secure tropos, srep, msra, as well as methods based on uml and problem frames. we review these methods and assess them according to different criteria, such as the general approach and scope of the method, its validation, and quality assurance capabilities. finally, we discuss how these methods are related to the conceptual framework and to one another.a comparison of security requirements engineering methods','Software security engineering'
'in model-driven development, system designs are specified using graphical modeling languages like uml and system artifacts such as code and configuration data are automatically generated from the models. model-driven security is a specialization of this paradigm, where system designs are modeled together with their security requirements and security infrastructures are directly generated from the models. over the past decade, we have explored different facets of model-driven security. this research includes different modeling languages, code generators, model analysis tools, and even model transformations. for example, in multi-tier systems, we used model transformations to transform a security policy, formulated for a system\'s data model, to a security policy governing the behavior of the system\'s graphical user interface. in this paper, we survey progress made, tool support, and case studies, which attest to the flexibility and power of such a multi-faceted approach to building secure systems.a decade of model-driven security','Software security engineering'
'the goal of this paper is to provide a framework for ensuring security in ubiquitous computing environment based on security engineering approach. security engineering is about building systems to remain dependable in the face of malice, error or mischance. in order to do this, we propose a framework which includes the concept of security state and security flow. the combinations of security state and security flow represent a unique viewpoint and a particular pattern for consideration of security services. we can analyze and identify security issues and threats from these combinations and patterns and provide suitable security services for concerns. for this purpose, we have applied our approach to rfid service network, which is one of the representative examples that are to practically realize ubiquitous computing environment.a framework for ensuring security in ubiquitous computing environment based on security engineering approach','Software security engineering'
'when modelling access control in distributed systems, the problem of security policies composition arises. much work has been done on different ways of combining policies, and using different logics to do this. in this paper, we propose a more general approach based on a 4-valued logic, that abstracts from the specific setting, and groups together many of the existing ways for combining policies. moreover, we propose going one step further, by twisting the 4-valued logic and obtaining a more traditional approach that might therefore be more appropriate for analysis.a generic approach for security policies composition','Software security engineering'
'context: software processes have become inherently complex to cope with the various situations we face in industrial project environments. in response to this problem, the research area of method engineering arose in the 1990s aiming at the systematization of process construction. objective: although the research area has gained much attention and offered a plethora of contributions so far, we still have little knowledge about the feasibility of method engineering. to overcome this shortcoming, necessary is a systematic investigation of the respective publication flora. method: we conduct a systematic mapping study and investigate, inter alia, which contributions were made over time and which research type facet they address to distill a common understanding of the state-of-the-art. results: based on the review of 64 publications, our results show that most of those contributions only repeat and discuss formerly introduced concepts, whereas empirically sound evidence on the feasibility of method engineering, is still missing. conclusion: although the research area constitutes many contributions, yet missing are empirically sound investigations that would allow for practical application and experience extraction.a mapping study on method engineering','Software security engineering'
'the field of software engineering is currently in its formative stages. consequently, it does not exhibit the structure and discipline present in other, more established engineering disciplines. the technique of morphological analysis is applied to the field of software engineering to identify the form and structure present. applications in the areas of software engineering research and software engineering curricula are discussed.a model of software engineering','Software security engineering'
'a paradigm change in software engineering','Software security engineering'
'we present a pattern system for security requirements engineering, consisting of security problem frames and concretized security problem frames. these are special kinds of problem frames that serve to structure, characterize, analyze, and finally solve software development problems in the area of software and system security. we equip each frame with formal preconditions and postconditions. the analysis of these conditions results in a pattern system that explicitly shows the dependencies between the different frames. moreover, we indicate related frames, which are commonly used together with the considered frame. hence, our approach helps security engineers to avoid omissions and to cover all security requirements that are relevant for a given problem.a pattern system for security requirements engineering','Software security engineering'
'the systems security engineering capability maturity model/sup sm/ (sse-cmm/sup sm/) describes the essential characteristics of an organization\'s security engineering process. the standard was developed by a unique government-industry consortium of leading security providers and acquirers. this paper summarizes the model and presents lessons learned in the model\'s development and from pilot appraisals.a process standard for system security engineering','Software security engineering'
'the reconfigurable attack-defend instructional computing laboratory (radicl) is a software engineering lab at the university of idaho. the lab is unique in that it can completely reconfigure its network topology and change the operating system of every machine in the lab in four minutes. the lab has been used for various research projects, classroom exercises, and academic experiments. the organization of the lab and some examples of its classroom use are discussed in this paper.a rapidly reconfigurable computer lab for software engineering security experiments and exercises','Software security engineering'
'software development is there such a thing anymore as a software system that doesn\'t need to be secure? we routinely hear vendors claim that their systems are \"secure.\" however, without knowing what assumptions are made by the vendor, it is hard to justify such a claim. almost every software controlled system faces threats from potential adversaries, from internet-aware client applications running on pcs, to complex telecommunications and power systems accessible over the internet, to commodity software with copy protection mechanisms. software engineers must be cognizant of these threats and engineer systems with credible defenses, while still delivering value to customers. in this paper, i present my perspectives on the issues that arise in the interactions between software engineering and security.a road map to the software engineering security','Software security engineering'
'we present a security engineering process based on security problem frames and concretized security problem frames. both kinds of frames constitute patterns for analyzing security problems and associated solution approaches. they are arranged in a pattern system that makes dependencies between them explicit. we describe step-by-step how the pattern system can be used to analyze a given security problem and how solution approaches can be found. further, we introduce a new frame that focuses on the privacy requirement anonymity.a security engineering process based on patterns','Software security engineering'
'even experienced developers struggle to implement security policies correctly. for example, despite 15 years of development, standard java libraries still suffer from missing and incorrectly applied permission checks, which enable untrusted applications to execute native calls or modify private class variables without authorization. previous techniques for static verification of authorization enforcement rely on manually specified policies or attempt to infer the policy by code-mining. neither approach guarantees that the policy used for verification is correct. in this paper, we exploit the fact that many modern apis have multiple, independent implementations. our flow- and context-sensitive analysis takes as input an api, multiple implementations thereof, and the definitions of security checks and security-sensitive events. for each api entry point, the analysis computes the security policies enforced by the checks before security-sensitive events such as native method calls and api returns, compares these policies across implementations, and reports the differences. unlike code-mining, this technique finds missing checks even if they are part of a rare pattern. security-policy differencing has no intrinsic false positives: implementations of the same api must enforce the same policy, or at least one of them is wrong! our analysis finds 20 new, confirmed security vulnerabilities and 11 interoperability bugs in the sun, harmony, and classpath implementations of the java class library, many of which were missed by prior analyses. these problems manifest in 499 entry points in these mature, well-studied libraries. multiple api implementations are proliferating due to cloud-based software services and standardization of library interfaces. comparing software implementations for consistency is a new approach to discovering \"deep\" bugs in them.a security policy oracle','Software security engineering'
'development of systems based on embedded components is a challenging task because of the distributed, reactive and real-time nature of such systems. from a security point of view, it is essential to take into account that frequently embedded devices are basically system components owned by a certain entity, used as part of systems owned by other entities and operated in a potentially hostile environment. currently, a security engineering process for systems with embedded components that takes these considerations into account does not exist. although many individual mechanisms to solve specific security problems are already available, the integration of these mechanisms in order to form a coherent system that can satisfy more complex security requirements is not trivial. this paper presents a process, which aims to support embedded systems developers in considering security aspects in the overall engineering process. particularly, the process provides means to identify and manage security properties and requirements. this security engineering process supports the representation of security aspects and mechanisms in a comprehensive and coherent modeling framework based on the uml metamodel. the process key characteristics are that (i) it\'s suited to the specific needs of systems with embedded components; (ii) it supports the developers in making sound security design decisions; (iii) it encourages the separation of responsibilities between security experts and system designers; and (iv) it integrates reusable security-focused models of embedded components. the main aspect to highlight in the process is that it\'s directed by security properties. we believe that the best approach is to base requirements on the positive expression of properties, as opposed to the negative expression by means of threats and attacks.a security-focused engineering process for systems of embedded components','Software security engineering'
'we enforce information flow policies in programs that run at multiple locations, with diverse levels of security. we build a compiler from a small imperative language with locality and security annotations down to distributed code linked to concrete cryptographic libraries. our compiler splits source programs into local threads; inserts checks on auxiliary variables to enforce the source control flow; implements shared distributed variables using instead a series of local replicas with explicit updates; and finally selects cryptographic mechanisms for securing the communication of updates between locations. we establish computational soundness for our compiler: under standard assumptions on cryptographic primitives, all confidentiality and integrity properties of the source program also hold with its distributed code, despite the presence of active adversaries that control all communications and some of the program locations. we also present performance results for the code obtained by compiling sample programs.a security-preserving compiler for distributed programs','Software security engineering'
'hsdms (highly secure data management system) is a secure, on-line and multi-user experimental database management system developed on the digital equipment corporation\'s pdp-10 computer system. it is a vehicle for testing new facilities and applications of data management and access control. furthermore the development of hsdms itself has been aimed at the outset as an exercise in software engineering management and control. in the first part of the paper, the software engineering techniques that were used in the management and control of the programming team and efforts are discussed. the discussion centers on the application of some of the known concepts such as the chief programmer team, structured programming and composite design to programming of the database management system. in the second part of the paper, system goals and capabilities are presented. the goals for hsdms were to achieve data independence, efficient storage and access, effective user interface, and secure access control. the ability of hsdms to meet the goals result from the utilization and integration of a number of design concepts and implementation approaches. for each system goal the concepts and approaches that have provided hsdms with the capabilities to achieve the goal are shown. in the final part of the paper, we attempt to relate this single experience with hsdms to its possible impact on software engineering of database management systems in general and to data secure systems in particular.a software engineering experience in the management, design and implementation of a data secure system','Software security engineering'
'one of the most important aspects in the achievement of secure software systems in the software development process is what is known as security requirements engineering. however, very few reviews focus on this theme in a systematic, thorough and unbiased manner, that is, none of them perform a systematic review of security requirements engineering, and there is not, therefore, a sufficiently good context in which to operate. in this paper we carry out a systematic review of the existing literature concerning security requirements engineering in order to summarize the evidence regarding this issue and to provide a framework/background in which to appropriately position new research activities.a systematic review of security requirements engineering','Software security engineering'
'the number of application areas where security of resources, whether this is people, information or physical property, is ever increasing as our world culture changes and the potential threats to individuals rises. the threats that security systems need to mitigate against are becoming both more complex and also asymmetric. in association with this the number of emerging technologies that can be applied to security systems is also increasing, and together with current & changing threats, is resulting in a more complex system with many interacting and possibly distributed elements. this paper describes how systems engineering provides a methodical design approach enabling a design engineer to understand all the contributing factors and analysing the cost/performance trade of alternative solutions. it shows that security elements should no longer be an afterthought once the main system has been designed &#8211; but security should be tightly integrated with other system functions as a set of requirements or design constraints. once these requirements and constraints have been determined the most appropriate security approach for a given situation can be determined. the paper then describes how the methodical approach is used to elaborate the solution based on the constraints, make appropriate design decisions to architect a solution and finally implement a system, using an integrated engineering discipline approach, that is fit for purpose.a systems engineering approach for security system design','Software security engineering'
'using software engineering (se) knowledge to support the software development process is challenging due to the complex structure of se knowledge and the uncertain nature of large-scale software projects. however, already before developing software, establishing a development process suitable for the project at hand is another challenge. this paper presents a requirements engineering (re) tool that contains a knowledge base to support re process development and selection of re techniques. the tool is built based on the framework for requirements engineering process development (frere). the major merits of the tool over others is that the tool uses knowledge representation to manage the knowledge of the re process and its technique, thus assisting development of the most suitable re process for a software project.a tool for requirements engineering process development','Software security engineering'
'a unix network protocol security study','Software security engineering'
'a view on three r\'s (3rs): reuse, re-engineering, and reverse-engineering','Software security engineering'
'we introduce the concept of \"abstract\" security patterns that deal with abstract security mechanisms, rather than concrete implementations. we also show an organization of abstract security patterns and concrete ones into hierarchies.abstract security patterns','Software security engineering'
'abstracts in software engineering -- reports','Software security engineering'
'abstracts in software engineering','Software security engineering'
'according to data received from an international survey, almost 6800 students are enrolled in software engineering degree programs in 11 countries, as of january, 2001. a total of 94 academic programs in software engineering are in place at 60 univcrsities with 350 full-time faculty and nearly 200 part-time faculty teaching hundreds of undergraduate and graduate courses in the discipline. over 5500 people have obtained degrees in software engineering since 1979. the authors are conducting the first of an ongoing annual survey of international academic software engineering programs, as a joint acm/ieee-cs project. this status report covers: history, audience, initial survey, initial partial results available on the www, request for evaluation of www-site, request for additional questions for next version of survey, time-line for next version of the survey, &#8220;lessons learned,&#8221; and some future directions. the annual report and survey results will be posted on a wide variety of web pages. a more current report, based on the sabbatical of the first author, will be presented at the conference. the sabbatical involves the initial development of an &#8220;international software engineering university consortium - iseuc.&#8221; a sample scenario for an employee in industry who becomes a student in iseuc is given.academic software engineering','Software security engineering'
'acm sac2002 software engineering','Software security engineering'
'motivated by applications from computer network security and software engineering, we study the problem of reducing reachability on a graph with unknown edge costs. when the costs are known, reachability reduction can be solved using a linear relaxation of sparsest cut. problems arise, however, when edge costs are unknown. in this case, blindly applying sparsest cut with incorrect edge costs can result in suboptimal or infeasible solutions. instead, we propose to solve the problem via edge classification using feedback on individual edges. we show that this approach outperforms competing approaches in accuracy and efficiency on our target applications.active graph reachability reduction for network security and software engineering','Software security engineering'
'agile software development has been used by industry to create a more flexible and lean software development process, i.e making it possible to develop software at a faster rate and with more agility during development. there are however concerns that the higher development pace and lack of documentation are creating less secure software. we have therefore looked at three known security engineering processes, microsoft sdl, cigatel touchpoints and common criteria and identified what specific security activities they performed. we then compared these activities with an agile development process that is used in industry. developers, from a large telecommunication manufacturer, were interviewed to learn their impressions on using these security activities in an agile development process. we produced a security enhanced agile development process that we present in this paper. this new agile process use activities from already established security engineering processes that provide the benefit the developers wanted but did not hinder or obstruct the agile process in a significant way.agile development with security engineering activities','Software security engineering'
'algorithm engineering','Software security engineering'
'in the last few years, web applications have evolved from static hypertext documents to complex information systems. this evolution leads to the necessity of methodologies specifically designed for development of web-based systems, focusing on agility in the process. this paper presents an agile approach for the development of web applications that applies the concept of agile modeling, adopts a standard software architecture and is heavily based on frameworks, speeding up system analysis, design and implementation.an agile approach for web systems engineering','Software security engineering'
'an apl system for increased engineering productivity','Software security engineering'
'the work demonstrates practical application of information security integral engineering technique to solve standards analysis and refinement problem. the application was exemplified by the development and analysis of the isms standards (iso/iec 27000 series) dictionary object model. standards refinement process consisting of model development, model and standards modification was described. as a result of the research the weaknesses related to \"asset\", \"risk management\", \"information security policy\" and \"certification document\" concepts were revealed and proposals on their elimination were formulated. the paper shows that semiformal modeling techniques can be successfully applied and efficiently used to analyze and amend international standards.an application of integral engineering technique to information security standards analysis and refinement','Software security engineering'
'this paper discusses important shortcomings of current approaches to systems security engineering. the value and limitations of perimeter security designs are examined. an architectural approach to systems security engineering is introduced as a complementary means for strengthening current approaches. accordingly, this paper outlines a methodology to identify classes of new reusable system security solutions and an architectural framework based on reuse of the patterns of solutions. it also introduces a new methodology for security metrics intended to stimulate critical solution design tradeoff analyses as part of security design reuse considerations. examples of problems, potential architectural solutions, and corresponding security metrics are provided. &#x00a9; 2011 wiley periodicals, inc. syst eng &#169; 2011 wiley periodicals, inc.an architectural systems engineering methodology for addressing cyber security','Software security engineering'
'this paper presents lessons learned and observations noted about the state of security-engineering practices by three information security practitioners with different perspectives - two in industry and one in academia. all authors have more than 20-years experience in this field and two were former members of the us national computer security center during the early days of the trusted computer system evaluation criteria and the strong promotion of trusted operating systems that accompanied the release of that document. in the last 20 years, it has been argued that security-engineering practices have not kept pace with the escalating threats to information systems. much has occurred since that time - new security paradigms, failure of evaluated products to emerge into common use, new systemic threats, and an increased awareness of the risk faced by information systems. this paper presents an empirical view of lessons learned in security-engineering, experiences in applying the trade, and observations made about the successes and failures of security practices and technology. this work was sponsored in part by nsf grant.an empirical study of industrial security-engineering practices','Software security engineering'
'for reliability and confidentiality of information security systems, the security engineering methodologies are accepted in many organizations. a security institution in korea faced the effectiveness of security engineering. to solve the problems of security engineering, the institution creates a security methodology called isem, and a tool called sent. this paper presents isem methodology considering both product assurance and production processes take advantages in terms of quality and cost. isem methodology can make up for the current security engineering methodology. for support isem methodology, sent tool, which is operated in internet, support the production processes and the product assurances which isem demands automatically.an empirical study of quality and cost based security engineering','Software security engineering'
' the engineering simulator  these notes are for the mod iv engineering simulator as used on the ibm 7040. the simulator is a digital differential analyzer designed primarily for ease of programming. most items included have been checked out and actually used. the original version of this paper served as a user\'s manual.an engineering simulator','Software security engineering'
'advances in internet technologies have enabled not only large corporations but also small/medium businesses to utilize the internet for their businesses. while enterprises can be more competitive with such high-tech tools, they are now inevitably exposed to various security risks and threats. in order to protect their information assets, large corporations purchase and operate enterprise security management systems which provide enterprise-level integrated security. however, they are so expensive that small/medium businesses can hardly afford to utilize one. we hold that one way of providing integrated security to small/medium businesses in a costeffective manner is by means of an asp solution. in this paper, we justify our proposition by analyzing the market trends for asp in conjunction with security management software. we also propose a web-based security management system that can be used as an asp solution. to the best of our knowledge, it is the first such system and is currently running as a commercial solution at a korean asp which provides services to 3,000 small/medium businesses.an enterprise security management system as an asp solution','Software security engineering'
'jfkr is a security protocol that establishes a shared encryption key between two participants. this paper briefly describes the different components of jfkr and the security property each component is intended to provide. it then describes an executable model, interleaving pieces of code to help the reader understand how the model represents the protocol specification. finally, it presents some theorems about the model. the contributions of this work include (1) an executable model for a key establishment protocol about which we can reason, (2) a model for an attacker that permits the injection, modification, and removal of messages between the participants, and (3) formalizations of a subset of desired security properties.an executable model for security protocol jfkr','Software security engineering'
'the information security crisis should be overcame by means of information security engineering paradigm. however, definition, approach and paradigm on security engineering are not clear yet. in this paper we survey on definitions on security engineering, and propose a new definition and paradigm. approaches and research topics on security engineering, to overcome the security crisis, modeled and described. results of paper are useful for establishing consensus on security engineering in community of information security and cryptography.an information security engineering paradigm for overcoming information security crisis','Software security engineering'
'this paper provides a survey of security features in modern programming languages for computer science instructors. we present the role that type safety and capabilities provide for the building of secure systems, and how language systems allow designers to model security issues that once were part-and-parcel of operating systems, or that can not be modeled by the latter.an overview of programming language based security','Software security engineering'
'in this work we present techniques and tools that enable effective reverse engineering procedures for web applications that were developed using the promising asp.net technology. we deal with model-driven development in its reverse aspect by implementing reverse engineering methods. our implemented methods model web applications using a well-known, web oriented and robust language, namely webml. this is, to the authors\' best knowledge, a novel re-engineering transformation. in this paper we propose a method to reverse engineer web applications in order to extract their conceptual model using webml notation. moreover, we present an efficient tool we have developed in order to implement the proposed method, along with a study of the application of our tool to an exemplar, content-management web application. the overall results are quite encouraging and indicate that our approach is efficient.application modeling using reverse engineering techniques','Software security engineering'
'nowadays, security solutions are mainly focused on providing security defences, instead of solving one of the main reasons for security problems that refers to an appropriate information systems (is) design. in fact, requirements engineering often neglects enough attention to security concerns. in this paper it will be presented a case study of our proposal, called srep (security requirements engineering process), which is a standard-centred process and a reuse-based approach which deals with the security requirements at the earlier stages of software development in a systematic and intuitive way by providing a security resources repository and by integrating the common criteria into the software development lifecycle. in brief, a case study is shown in this paper demonstrating how the security requirements for a security critical is can be obtained in a guided and systematic way by applying srep.applying a security requirements engineering process','Software security engineering'
'the general systems of today are composed of a number of components such as servers and clients, protocols, services, and so on. systems connected to network have become more complex and wide, but the researches for the systems are focused on the &#39;performance&#39; or &#39;efficiency&#39;. while most of the attention in system security has been focused on encryption technology and protocols for securing the data transaction, it is critical to note that a weakness (or security hole) in any one of the components may comprise whole system. security engineering is needed for reducing security holes may be included in the software. therefore, more security-related researches are needed to reduce security weakness may be included in the software. this paper introduces some methods for reducing the threat to the system by applying security engineering, and proposes a method for building security countermeasure.applying security engineering to build security countermeasures','Software security engineering'
'developing security-critical software correctly and securely is difficult. to address this problem, there has been a significant amount of work over the last 10 years on providing model-based development approaches based on the unified modeling language which aim to raise the trustworthiness of security-critical systems, some of them including tools allowing the user to check whether a uml model satisfies the relevant security requirements. however, when the requirements are not satisfied by a given model, it can be challenging for the user to determine which changes to do to the model so that it will indeed satisfy the security requirements. also, the fact that software continues to evolve on an ongoing basis, even after the implementation has been shipped to the customer, increases the challenge since in principle, the software has to be re-verified after each modification, requiring significant efforts. we present work on automated tool-support that exploits recent work on secure software evolution in the secure change project in order to support the security hardening of evolving uml models (within the context of the uml security extension umlsec).automated security hardening for evolving uml models','Software security engineering'
'most kinds of security vulnerabilities in web applications can be fixed by adding appropriate sanitization methods. finding the correct place for the sanitizers can be difficult due to complicated data and control flow. fixing sql injection vulnerabilities may require more complex transformations, such as replacing uses of statement by preparedstatement, which could include some code motion. we have developed algorithms to place sanitizers correctly, as well as to transform statement to preparedstatement. these have been implemented as \"quick fixes\" in an eclipse plugin that works together with a commercial tool that discovers security vulnerabilities in web applications.automatically fixing security vulnerabilities in java code','Software security engineering'
'in earlier work [1] we had looked at implementing the microsoft stride methodology in the context of evaluating security properties of fmc/tam architectural diagrams. however, a major drawback of this approach is that it requires significant manual work to assess all reported potential threats, as well as identify concrete follow-ups. equally, it is not possible to analyse an architecture from the perspective of the primary assets that require protection. this led us to two questions: a) whether using interaction information in architecture diagrams, supported by additional security semantics, can reduce the scope of analysis as well as partly automate it; b) whether using asset-centric and attacker-centric perspectives can complement the software-centric perspective of stride and thus add value to the current threat model.automating architectural security analysis','Software security engineering'
'beyond re-engineering','Software security engineering'
'a software component should be trustworthy and behave in a secure manner as it will be reused many times. despite extensive efforts, usually, it cannot be guaranteed that a developed software component is completely secure. hence, its execution in the real-world needs to be monitored against its security specifications. each time components are used to develop a component-based software (cbs), a new monitor has to be designed to observe the behavior of the cbs. this results in recurring costs as such monitors cannot be reused for other cbs. moreover, development life cycle artifacts are usually not available when a pre-fabricated component is used to build a cbs. given that, it is imperative that a specification-based security monitor is developed along with the monitored component (when all development artifacts are available) and is embedded in the component to increase the component\'s trustworthiness. in this paper, we identify the types of constraints that may be imposed by security specifications. these constraints should be taken into account while developing the software components and should also be monitored. furthermore, we propose a design approach to develop components with built in monitors that are able to observe these security constraints. components developed following this approach would be self-monitoring, promote greater reusability, and be more trustworthy. we evaluate our approach by analyzing the performance and design complexity of different versions of cbs. these versions are developed by following the traditional and proposed approaches for monitoring security aspects of cbs.building components with embedded security monitors','Software security engineering'
'traditionally, security requirements have been derived in an ad hoc manner. recently, commercial software development organizations have been looking for ways to produce effective security requirements.in this paper, we show how to build security requirements in a structured manner that is conducive to iterative refinement and, if followed properly, metrics for evaluation. while requirements specification cannot be a complete science, we provide a framework that is an obvious improvement over traditional methods that do not consider security at all.we provide an example using a simple three-tiered architecture. the methodology we document is a subset of clasp, a set of process pieces for application security that we have recently published, in conjunction with ibm/rational.building security requirements with clasp','Software security engineering'
'c2 security','Software security engineering'
'we propose a capabilities-based approach for building long-lived, complex systems that have lengthy development cycles. user needs and technology evolve during these extended development periods, and thereby, inhibit a fixed requirements-oriented solution specification. in effect, for complex emergent systems, the traditional approach of baselining requirements results in an unsatisfactory system. therefore, we present an alternative approach, capabilities engineering, which mathematically exploits the structural semantics of the function decomposition graph -- a representation of user needs -- to formulate capabilities. for any given software system, the set of derived capabilities embodies change-tolerant characteristics. more specifically, each individual capability is a functional abstraction constructed to be highly cohesive and to be minimally coupled with its neighbors. moreover, the capability set is chosen to accommodate an incremental development approach, and to reflect the constraints of technology feasibility and implementation schedules. we discuss our validation activities to empirically prove that the capabilities-based approach results in change-tolerant systems.capabilities engineering','Software security engineering'
' in a dynamic environment where context changes frequently, users&#8217; privacy requirements can also change. to satisfy such changing requirements, there is a need for continuous analysis to discover new threats and possible mitigation actions. a frequently changing context can also blur the boundary between public and personal space, making it difficult for users to discover and mitigate emerging privacy threats. this challenge necessitates some degree of self-adaptive privacy management in software applications.   this paper presents caprice - a tool for enabling software engineers to design systems that discover and mitigate context-sensitive privacy threats. the tool uses privacy policies, and associated domain and software behavioural models, to reason over the contexts that threaten privacy. based on the severity of a discovered threat, adaptation actions are then suggested to the designer. we present the caprice architecture and demonstrate, through an example, that the tool can enable designers to focus on specific privacy threats that arise from changing context and the plausible category of adaptation action, such as ignoring, preventing, reacting, and terminating interactions that threaten privacy. caprice: a tool for engineering adaptive privacy','Software security engineering'
'developing and integrating automotive embedded software is a complex undertaking. the software is large. it is developed by many contributors. it is distributed over many control units connected by a variety of in-vehicle buses. often much of the equipment or functions in a car are optional and regulatory requirements also vary between markets, leading to large combinatorial variations of software features. targets running the software have to be cheap. errors can be extremely expensive. new software and system features are demanded by the market and also by governmental regulations. model-based design (mbd) of functional behaviour has been a big help in the recent past on the one hand, and on the other hand has by itself created new complexity by allowing relatively quick development of ever more features, especially when combined with autocoding. all this creates new challenges that did not exist a few years ago when feature development was slow. major new challenges now are to tame all the complexity, get a system view on top of the individual functions, and to leverage executable system models to put more comprehensive testing into early phases of a development. tools are required which really help those engineers and software developers. their needs may not ask for a lot of computer science glamour. they can be quite basic and sophisticated concepts from computer science may find it difficult to find acceptance outside some niches. this presentation will outline the achievements, the current challenges and will point to upcoming tools and approaches that help meeting those challenges.challenges in automotive software engineering','Software security engineering'
'requirements engineering is likely to be a major issue in this decade. the author examines two widely held beliefs: requirements describe a system\'s \"what\", not its \"how\". requirements must be represented as abstractions.challenging universal truths of requirements engineering','Software security engineering'
'during development of complex products, such as automotive software, models -- formal and informal -- are used throughout the development process by different roles and for different purposes -- as requirement, as implementation or as documentation. this paper reports results from a case-study of the development of embedded software at a swedish vehicle manufacturer. we investigated use of models from high-level product planning to low-level requirements specifications for software components. furthermore, we investigated the distribution of effort among the models, requirements and other artefacts. the goal was to explore the spectrum of modelling techniques, methods and languages used and to establish a baseline for comparison with the state-of-the-art and other companies. the results show that there exist at least 8 different modelling notations. moreover, we found that the majority of effort was spent on behaviour models, while static models -- such as high-level design and requirements -- were considered most important.characterizing model usage in embedded software engineering','Software security engineering'
'checking user security','Software security engineering'
'classification of research efforts in requirements engineering','Software security engineering'
'client sponsored projects in software engineering courses','Software security engineering'
'can the cloud truly be secured? can enterprises, universities, small businesses and governments securely utilize the cloud for their critical infrastructure? it will take rethinking our current security policies and what we consider secure. this session will cover what is necessary to utilize the cloud securely today and how the cloud should adapt for the future.cloud security','Software security engineering'
'our technological society has become more and more dependent on software that is used to automate everyday processes. this dependence increasingly exposes us to security threats that originate from malicious software (malware) such as computer viruses and worms and software vulnerability exploits such as remote execution of code or denial of service attacks. moreover, this exposure is not limited to computer systems but is spreading to common appliances such as mobile phones, pdas and consumer electronics such as media centers, personal video recorders, etc. since a growing number of these products are made extensible and adaptable by means of embedded software.code based software security assessments','Software security engineering'
'communicating in software engineering environments','Software security engineering'
'an agile software development team relies on communication and collaboration to perform requirements engineering activities, rather than on dedicated analysis tools or documentation. evidence from practice indicates that two simple physical artefacts (story cards and the wall), used in a particular and disciplined manner, and supported by appropriate social activity, are key to the success of co-located agile teams. however, little is known about this social activity or how communication and collaboration supports requirements activities in this setting. this paper reports an empirical study of a commercial agile team to investigate this issue. using a combination of qualitative data collection and cognitive analysis techniques, we found evidence of gathering, evolving and clarifying requirements that are managed through patterns of communication. these patterns suggest that a form of situated conceptualization, which we have termed \'shared conceptualization\', underpins the team\'s requirements engineering activities.communication patterns of agile requirements engineering','Software security engineering'
'although attractive, cbd has not been widely adopted in domains of embedded systems. the main reason is inability of these technologies to cope with the important concerns of embedded systems, such as resource constraints, real-time or dependability requirements. however an increasing understanding of principles of cbd makes it possible to utilize these principles in implementation of different component-based models more appropriate for embedded systems. the aim of this tutorial is to point to the opportunity of applying this approach for development and maintenance of embedded systems. the tutorial gives insights into basic principles of cbd, the main concerns and characteristics of embedded systems and possible directions of adaptation of component-based approach for these systems. different types of embedded systems and approaches for applying cbd are presented and illustrated by examples from research and practices. also, challenges and research directions of cbd for embedded systems are discussed.component-based software engineering for embedded systems','Software security engineering'
'what\'s the difference between computer science and software engineering? the serendipity principle gives us the answer.computer science vs. software engineering','Software security engineering'
'computer security revisited','Software security engineering'
'sometime in the future, requirements engineers (also known as systems analysts) will replace their present manual methods by a computer aided method (a requirements engineering system) just as programmers have replaced manual by on-line programming. the requirements engineering system will be part of the (logically) integrated decision support system of the systems department. this paper briefly describes the need for formal recorded requirements and analyzes the reasons why organizations do not record and maintain requirements. the future requirement engineering system is described and two issues concerned with its usability are discussed. the first is concerned with the language or representation method and the second with the facilities that will aid the analyst. the final section summarizes the current situation and outlines some reasons for believing that such a requirements engineering system is technically feasible and can be cost effective.computer-aided requirements engineering','Software security engineering'
'organizations are subject to constant evolution and must systematically analyze and design the impact of change to implement it consistently across all organizational domains. a thorough understanding of all relevant business-related artifacts as well as their relationships is a prerequisite to achieve this. for many organizations, business architecture management is a means to ensure the correct and up-to-date documentation of these artifacts. one challenge of business architecture management is the development a company-specific business architecture meta model. two directions of existing work provide partial solutions: (1) generic (meta) modeling methods and (2) business architecture meta models and languages. we argue that these two approaches complement each other and should be applied in an integrated way. the goal of this contribution is to propose such an integrated approach to business architecture engineering. the development of this approach follows the design research process and is based on experiences gained in three industrial business architecture engineering projects.concern-oriented business architecture engineering','Software security engineering'
'the evolution of software engineering methodology, from waterfall to spiral, from spiral to agile, indicates that high concurrency, iterative development and short cycles are key factors for effective software engineering. it is widely accepted that supporting (i.e., formalizing controlling, automating and optimizing) concurrent engineering processes is needed to increase predictability of cost, quality and development time. unfortunately, current systems (e.g., workflows, software configuration management) are too simple and deterministic; they do not include real support for concurrent engineering. we claim this shortcoming is one of the major reasons why current workflow and process support do not significantly help in the support of software engineering. in this paper we present the celine system, which extends workflows with the definition of high-level executable description of concurrent engineering and therefore contributes to provide effective control over cost, quality and development time.concurrent engineering support in software engineering','Software security engineering'
'software systems bridge the gap between information processing needs and available computer hardware. as system requirements grow in complexity and hardware evolves, the gap does not necessarily widen, but it certainly changes. although today\'s applications require concurrency and today\'s hardware provides concurrency, programming languages remain predominantly sequential. concurrent programming is considered too difficult and too risky to be practiced by \"ordinary programmers\". however, software engineering is moving towards a paradigm shift, following which concurrency will play a more fundamental role in programming languages. we discuss some of the implications of the shift towards process-oriented programming. we outline some of the features of our own process-oriented language. finally, we review the potential impact on software engineering and on software development processes.concurrent software engineering','Software security engineering'
'aspects of software engineering environments are discussed, namely motivations, life cycle models, concepts, methods, description means and tools. some general conclusions about these aspects as well as about the area of software engineering environments are drawn. the paper is based on a study of selected software engineering environments.conspectus of software engineering environments','Software security engineering'
'context-aware web services are identified as an important technology to support new applications on the future internet. context information has several qualities that make the development of these services challenging, compared to conventional, web services. therefore, sound software engineering practices are needed during their development and execution. this article discusses a novel software engineering-based approach, which leverages the benefits of model-driven architecture, aspect-oriented modeling, and formal model checking, for modeling and verifying context-aware services. the approach is explored using a real-world case study in intelligent transport. an evaluation framework is established to validate the main methods and tools employed.context-aware services engineering','Software security engineering'
'in this talk i will discuss our experience with one particular development methodology for security related software. i will describe the general principles it follows, the tools used, and the resources needed. then i will offer some opinions on why this approach is effective and practical for achieving even moderate levels of security. when the goal is a very high level security, i will explain why i believe that at least the general principles, if not the specific details, are probably essential.cost effective software engineering for security','Software security engineering'
'secure, distributed collaboration between different organizations is a key challenge in grid computing today. the gdcd project has produced a grid-based demonstrator virtual collaborative facility (vcf) for the european space agency. the purpose of this work is to show the potential of grid technology to support fully distributed concurrent design, while addressing practical considerations including network security, interoperability, and integration of legacy applications. the vcf allows domain engineers to use the concurrent design methodology in a distributed fashion to perform studies for future space missions. to demonstrate the interoperability and integration capabilities of grid computing in concurrent design, we developed prototype vcf components based on esa\'s current excel-based concurrent design facility (a non-distributed environment), using a step-compliant database that stores design parameters. the database was exposed as a secure gria 5.1 grid service, whilst a .net/wse3.0-based library was developed to enable secure communication between the excel client and step database.cross-middleware interoperability in distributed concurrent engineering','Software security engineering'
'as requirements change, database administrators come under pressure to change the schema which is a description of the database structure. although writing a new schema is a relatively easy job and transforming the database to match the schema can be accomplished with a modest effort, transforming the numerous programs which operate on the database often requires enormous effort. this interim report describes previous research, defines the problem and proposes a framework for research on the automatic conversion of database programs to match the schema transformations. the approach is based on a precise description of the data structures, integrity constraints, and permissible operations. this work will help designers of manual and computer aided conversion facilities, database administrators who are considering conversions and developers of future database management systems, which will have ease of conversion as a design goal.database program conversion','Software security engineering'
'database reverse engineering','Software security engineering'
'determining whether a software architecture meets its security requirements is an early step in assuring the security of the products developed from the architecture. in this paper, we propose a tool-based technique using an authorization scheme to analyze the security of software architectures. such technique will serve as debugging support for software architectures to identify the portion in the software architecture that fails to meet the required level. security is analyzed in terms of its aggregate attributes: availability, confidentiality, and integrity. in this paper, we address confidentiality and show that integrity is measured in a complementary manner to confidentiality. a scenario based approach is taken to analyze security in a software architecture. our work is implemented in the osate environment and analyzes software architectures modeled using aadl (architecture analysis and design language).debugging support for security properties of software architectures','Software security engineering'
'the structures designed by gehry partners seem impossible to build-and they might be, were it not for technologists like shelden.dennis shelden','Software security engineering'
'derivational software engineering','Software security engineering'
'design and analysis in software engineering','Software security engineering'
'value creation increasingly relies on the bundling of physical products and services from different providers in networks such as supply chains. the complexity that arises from this cooperation is of utmost importance for management since the provision of an undisturbed flow of information affects the performance of the whole network. in this paper we develop a method for the analysis and design of information and communication structures. building on the design science research framework and using an exemplary case, we show how coordination complexity can be analyzed and measured. the results are combined with theoretical concepts from management cybernetics and create the foundation for the constructed method.designing ic structures by variety engineering','Software security engineering'
'&lt;u&gt;background.&lt;/u&gt; even though the amount of researches related to the behavior of software development teams has significantly increased in recent years, researches focusing on motivation as an alternative to lead software projects to success are still rare. &lt;u&gt;objective/method:&lt;/u&gt; this article describes a survey conducted to identify the relative importance of some factors that affect the motivation of software engineers at work. the conceptual underpinnings of human motivation used in the research are the expectancy theory and the motivation-hygiene theory. &lt;u&gt;results/conclusion:&lt;/u&gt; in the study, a cross-sectional survey was conducted involving 176 software engineers employed at 20 software firms from the state of pernambuco, brazil. data collected with the survey revealed not only the order in which the set of motivators influences the software engineers\' motivation and other peripheral findings, but also served as a basis to design three motivation strategies for software engineering teams.designing motivation strategies for software engineering teams','Software security engineering'
'in the past 40 years, software engineering has emerged as an important sub-field of computer science. the quality and productivity of software have been improved and the cost and risk of software development been decreased due to the contributions made in this sub-field. the software engineering community needs to invest much more efforts to cope with the drastically increasing demands on the information technology as well as the extremely open and dynamic nature of the internet. the history of software engineering is reviewed with emphasis on the driving forces of software and the milestones of software engineering development. the history of software engineering in china is reviewed with emphasis on the relationship between software engineering and the software industry. based on the above reviews, we argue that software engineering should become an independent discipline along with computer science and co-operative efforts from academia, governments and industries should be needed for the harmonious development of software engineering. some results are presented based on china\'s experience of developing software engineering under this model.development of software engineering','Software security engineering'
'application-level protocol specifications are useful for many security applications, including intrusion prevention and detection that performs deep packet inspection and traffic normalization, and penetration testing that generates network inputs to an application to uncover potential vulnerabilities. however, current practice in deriving protocol specifications is mostly manual. in this paper, we present discoverer, a tool for automatically reverse engineering the protocol message formats of an application from its network trace. a key property of discoverer is that it operates in a protocol-independent fashion by inferring protocol idioms commonly seen in message formats of many application-level protocols. we evaluated the efficacy of discoverer over one text protocol (http) and two binary protocols (rpc and cifs/smb) by comparing our inferred formats with true formats obtained from ethereal [5]. for all three protocols, more than 90\% of our inferred formats correspond to exactly one true format; one true format is reflected in five inferred formats on average; our inferred formats cover over 95\% of messages, which belong to 30-40\% of true formats observed in the trace.discoverer','Software security engineering'
'distributed open interworking in software engineering environments','Software security engineering'
'distributed software engineering','Software security engineering'
' one of the goals of software engineering research is to achieve generality: are the phenomena found in a few projects reflective of others? will a technique perform as well on projects other than the projects it is evaluated on? while it is common sense to select a sample that is representative of a population, the importance of diversity is often overlooked, yet as important. in this paper, we combine ideas from representativeness and diversity and introduce a measure called sample coverage, defined as the percentage of projects in a population that are similar to the given sample. we introduce algorithms to compute the sample coverage for a given set of projects and to select the projects that increase the coverage the most. we demonstrate our technique on research presented over the span of two years at icse and fse with respect to a population of 20,000 active open source projects monitored by ohloh.net. knowing the coverage of a sample enhances our ability to reason about the findings of a study. furthermore, we propose reporting guidelines for research: in addition to coverage scores, papers should discuss the target population of the research (universe) and dimensions that potentially can influence the outcomes of a research (space). diversity in software engineering research','Software security engineering'
'documentation and computer security','Software security engineering'
'variable data printing solutions provide means to generate documents whose content varies according to some criteria. since the early mail merge-like applications that generated letters with destination data taken from databases, different languages and frameworks have been developed with increasing levels of sophistication. current tools allow the generation of highly customized documents that are variable not only in content, but also in layout. however, most frameworks are technology-oriented, and their use requires high skills in implementation-related tools (xml, xpath, and others), which do not include support for domain-related tasks like identification of document content variability. in this paper, we introduce dplfw, a framework for variable content document generation based on software product line engineering principles. it is an implementation of the document product lines (dpl) approach, which was defined with the aim of supporting variable content document generation from a domain-oriented point of view. dpl models document content variability in terms of features, and product line-like processes support the generation of documents. we define the dplfw architecture, and illustrate its use in the definition of variable-content emergency plans.dplfw','Software security engineering'
'e-commerce and security','Software security engineering'
'novel types of malware on mobile devices have raised researchers interest in implementing static and dynamic techniques for detecting and mitigating malicious behavior of mobile applications. in this hands-on tutorial we will demonstrate and explain different techniques for instrumenting android applications using the aspect bench compiler (abc) and the program analysis and transformation tool soot. through high-level abstractions such as aspectj aspects and tracematches, abc supports a declarative style of instrumentation that lends itself to the rapid prototyping of at least simple instrumentation schemes. soot supports instrumentation in an imperative style, which requires more work but allows more fine-grained control. both abc and soot are inter operable, as they instrument the same intermediate program representation. furthermore, as we show, both can be easily integrated with static program analyses that can be used to specialize instrumentation schemes based on additional information extracted from the static structure of the instrumented app.easily instrumenting android applications for security purposes','Software security engineering'
'failures in systems closely correlate to shortcomings in the system\'s requirements. some historic data suggests that requirements are responsible for nearly half of all system development failures. this is especially true for critical systems that are real-time and embedded. expectations for fault tolerance, graceful degradation, degraded performance modes, and temporal challenges (latency and synchronization) fail to be fully satisfied by common practice. this tutorial discusses shortcomings in current practices, and provides guidance for enhanced practices that address historic shortcomings, and provide an approach to weighing tradeoffs associated with ambitious goals and realistic limits. it clarifies terminology to facilitate a clearer focus on underlying concepts. in addition, it specifically addresses the issue of stakeholder acceptability, allowing trade-offs of various system qualities to determine overall system acceptance. the tutorial does not describe in detail any specific techniques. rather, it describes the ways that requirements need to be handled to maximize the likelihood of success.effective requirements engineering','Software security engineering'
'e-health is a health care system which is supported by electronic process and communication. the information that is kept in the system must be accurate. in case of false information, it may cause harm to human life. so this system needs more security to protect the credential information. e-health system is the most security sensitive process handled electronically. the highest achievable security is never too much for an e-health system. so when system is being built, tasks such as security requirements elicitation, specification and validation are essential to assure the quality of the resulting secure e-health system. by considering the security requirements as functional requirements in the requirement phase, the completeness of security requirements for e-health system can be developed. in this paper we propose model oriented security requirements engineering (mosre) framework in the early phases of e-health system development, to identify assets, threats and vulnerabilities. this helps in standardizing the security requirements for secure e-health system without any security issues.elicitation of security requirements for e-health system by applying model oriented security requirements engineering (mosre) framework','Software security engineering'
'declarative policies play a central role in many modern software systems. engineering policies and their interactions with programs raises many interesting open questions.embracing policy engineering','Software security engineering'
'encapsulating rules of prudent security engineering (transcript of discussion)','Software security engineering'
'in practice, security of computer systems is compromised most often not by breaking dedicated mechanisms (such as security protocols), but by exploiting vulnerabilities in the way they are employed. towards a solution of this problem we aim to encapsulate rules of prudent security engineering in sucha way that a system specification formulated in (a formal core of) the unified modeling language (uml, the industrystandard in object-oriented modelling) can be evaluated wrt. these rules, violations be indicated and suggestions for modifications be derived.encapsulating rules of prudent security engineering','Software security engineering'
'knowledge from software engineering research often does not make it to practice. a reason for this is the not applicable here (nah) syndrome -- practitioners often feel that the methods being proposed, and maybe even evaluated in controlled studies, are not applicable to their situation. another reason is that often there is lack of wide spread experimentation and empirical validation, casting doubts on the applicability of the new knowledge. this talk will argue that we need to engage with practitioners in a more meaningful manner by having them do research and answer some questions that are relevant to them, rather than just provide data to researchers with minimal use of that data for themselves. this understanding achieved through research by practitioners may sometimes be applicable in a limited context, but can help software engineering researchers build more general models and approaches. if we can get the practitioners to participate in research while continuing their practice, we have \"crowd researching\" which can potentially provide benefits similar to crowd sourcing for information. we will discuss some examples of the types of questions that practitioners can do research on, while continuing their practice.engaging practitioners in software engineering research','Software security engineering'
'engineering a hard real-time system','Software security engineering'
'this paper describes a research project to engineer a security kernel for multics, a general-purpose, remotely accessed, multiuser computer system. the goals are to identify the minimum mechanism that must be correct to guarantee computer enforcement of desired constraints on information access, to simplify the structure of that minimum mechanism to make verification of correctness by auditing possible, and to demonstrate by test implementation that the security kernel so developed is capable of supporting the functionality of multics completely and efficiently. the paper presents the overall viewpoint and plan for the project and discusses initial strategies being employed to define and structure the security kernel.engineering a security kernel for multics','Software security engineering'
'graphical user interfaces used to be static, graphically representing one software state after the other. however, animated transitions between these static states are an integral part in modern user interfaces and processes for both their design and implementation remain a challenge for designers and developers. this paper proposes a petri net model-based approach to support the design, implementation and validation of animated user interfaces by providing a complete and unambiguous description of the entire user interface including animations. a process for designing interactive systems focusing on animations is presented, along with a framework for the definition and implementation of animation in user interfaces. the framework proposes a two levels approach for defining a high-level view of an animation (focusing on animated objects, their properties to be animated and on the composition of animations) and a low-level one dealing with detailed aspects of animations such as timing and optimization. a case study (in the domain of interactive television) elaborating the application of the presented process and framework exemplifies the contribution.engineering animations in user interfaces','Software security engineering'
'burstsort is a trie-based string sorting algorithm that distributes strings into small buckets whose contents are then sorted in cache. this approach has earlier been demonstrated to be efficient on modern cache-based processors [sinha & zobel, jea 2004]. in this article, we introduce improvements that reduce by a significant margin the memory requirement of burstsort: it is now less than 1&percnt; greater than an in-place algorithm. these techniques can be applied to existing variants of burstsort, as well as other string algorithms such as for string management. we redesigned the buckets, introducing sub-buckets and an index structure for them, which resulted in an order-of-magnitude space reduction. we also show the practicality of moving some fields from the trie nodes to the insertion point (for the next string pointer) in the bucket; this technique reduces memory usage of the trie nodes by one-third. importantly, the trade-off for the reduction in memory use is only a very slight increase in the running time of burstsort on real-world string collections. in addition, during the bucket-sorting phase, the string suffixes are copied to a small buffer to improve their spatial locality, lowering the running time of burstsort by up to 30&percnt;. these memory usage enhancements have enabled the copy-based approach [sinha et al., jea 2006] to also reduce the memory usage with negligible impact on speed.engineering burstsort','Software security engineering'
'distributed systems now encounter extreme heterogeneity in the form of diverse devices, network types etc., and also need to dynamically adapt to changing environmental conditions. self-adaptive middleware is ideally situated to address these challenges. however, developing such software is a complex task. in this paper, we present the gridkit self approach to the engineering of reflective middleware; this embraces state of the art software engineering practices, and flexible dynamic adaptation mechanisms to better support system developers. domain specific frameworks are modeled and developed to enhance configurability and reconfigurability. we evaluate this approach using case studies in the domains of service discovery and network overlays. these demonstrate the benefits of the approach in terms of aiding and simplifying the process of creating self-configuring and self-adaptive software.engineering complex adaptations in highly heterogeneous distributed systems','Software security engineering'
'a definitional interpreter should be clear and easy to write, but it may run 4--10 times slower than a well-crafted bytecode interpreter. in a case study focused on implementation choices, we explore ways of making definitional interpreters faster without expending much programming effort. we implement, in ocaml, interpreters based on three semantics for a simple subset of lua. we compile the ocaml to x86 native code, and we systematically investigate hundreds of combinations of algorithms and data structures. in this experimental context, our fastest interpreters are based on natural semantics; good algorithms and data structures make them 2--3 times faster than na&#239;ve interpreters. our best interpreter, created using only modest effort, runs only 1.5 times slower than a mature bytecode interpreter implemented in c.engineering definitional interpreters','Software security engineering'
'the role of structure in specifying, designing, analysing, constructing and evolving software has been the central theme of our research in distributed software engineering. this structural discipline dictates formalisms and techniques that are compositional, components that are context independent and systems that can be constructed and evolved incrementally. this extended abstract overviews our development of a structural approach to engineering distributed software and gives indications of our future work which moves from explicit to implicit structural specification. with the benefit of hindsight we attempt to give a \"rational history\" to our research.engineering distributed software','Software security engineering'
'domain-specific languages (dsls) are useful for capturing and reusing engineering expertise. they can formalize industrial patterns and practices while increasing the scalability of verification, because input programs are written at a higher level of abstraction. however, engineering new dsls with custom verification is a non-trivial task in its own right, and usually requires programming language, formal methods, and automated theorem proving expertise. in this tutorial we present formula 2.0, which is formal framework for developing dsls. formula specifications are succinct descriptions of dsls, and specifications can be immediately connected to state-of-the-art analysis engines without additional expertise. formula provides: (1) succinct specifications of dsls and compilers, (2) efficient compilation and execution of input programs, (3) program synthesis and compiler verification. we take a unique approach to provide these features: specifications are written as strongly-typed open-world logic programs. these specifications are highly declarative and easily express rich synthesis / verification problems. automated reasoning is enabled by efficient symbolic execution of logic programs into quantifier-free sub-problems, which are dispatched to the state-of-the-art smt solver z3. formula has been applied within microsoft to develop dsls for verifiable device drivers and protocols. it has been used by the automotive / embedded systems industries for software / hardware co-design and design-space exploration under hard resource allocation constraints. it is being used to develop semantic specifications for complex cyber-physical systems.engineering domain-specific languages with formula 2.0','Software security engineering'
'we explore various definitions and characteristics of emergence, how we might recognise and measure emergence, and how we might engineer emergent systems. we discuss the tuna (\"theory underpinning nanotech assemblers\") project, which is investigating emergent engineering in the context of molecular nanotechnology, and use the tuna case study to explore an architecture suitable for emergent complex systems.engineering emergence','Software security engineering'
'first page of the articleengineering follies','Software security engineering'
'this paper presents an overview of some of the technical and managerial aspects of protecting the personnel, data processing systems and computer facility from deliberate or accidental damage by internal or external forces.with regard to site planning, the environmental compatibility between the computer system, its data communication and electric energy systems must be established. electro-magnetic spectrum area profiles are utilized to detect high energy transmitters which could necessitate shielding of the facility and filtering of all incoming wire lines. the effects of magnetic fields on magnetic storage media is also discussed.the unique requirements of real-time, on-line systems, i.e. airline reservation, satellite tracking, require special solutions related to uniterruptable power systems (ups). in this context this paper will also examine the management decisions which must be made regarding the cost effectiveness of a \"fail safe\" vs \"fail soft\" system.data security is examined in light of the time-value relationship of information applied to programming changes, data transmission to remote terminals, available software and hardward cryptography systems.the application of various electro mechanical devices and computing systems for use in area control situations is examined. additionally, the relatively new use of voice, hand and fingerprints for access control is surveyed.the use of closed circuit television and its effect on personnel management will also be considered. low light level techniques and time-lapse video tape recording sub systems applied to unmanned surveillance of critical areas or processes is discussed.engineering management considerations in data center security','Software security engineering'
'engineering mathematics','Software security engineering'
'we envision next-generation field workers to use interactive web applications deployed on sophisticated mobile devices in order to access remote services and data in support of their business processes. engineering such applications requires combining services computing with mobile computing and end-user oriented web application development. in this position paper, we identify the main challenges in this context, and propose a preliminary platform architecture for application deployment on field worker devices that addresses some of these challenges.engineering mobile field worker applications','Software security engineering'
'many software-intensive systems have significant safety ramifications and need to have their associated safety-related requirements properly engineered. it has been observed by several consultants, researchers, and authors that inadequate requirements are a major cause of accidents involving software-intensive systems. yet in practice, there is very little interaction between the requirements and safety disciplines and little collaboration between their respective communities. most requirements engineers know little about safety engineering, and most safety engineers know little about requirements engineering. also, safety engineering typically concentrates on architectures and designs rather than requirements because hazard analysis typically depends on the identification of hardware and software components, the failure of which can cause accidents. this leads to safety-related requirements that are often ambiguous, incomplete, and even missing. the tutorial begins with a single common realistic example of a safety critical system that will be used throughout to provide good examples of safety-related requirements. the tutorial then provides an introduction to requirements engineering for safety engineers and an introduction to safety engineering for requirements engineers. the tutorial then provides clear definitions and descriptions of the different kinds of safety-related requirements and finishes with a practical process for producing them.engineering safety - and security-related requirements for software-intensive systems','Software security engineering'
'many software-intensive systems have significant safety and security ramifications and need to have their associated safety- and security-related requirements properly engineered. it has been observed by several consultants, researchers, and authors that inadequate requirements are a major cause of accidents involving software-intensives systems, and poor security requirements prevent the early incorporation of security concerns into the architecture. yet in practice, there is very little interaction between the requirements, safety, and security disciplines and little collaboration between their respective communities. most requirements engineers, safety engineers, and security engineers know little about their respective disciplines. also, safety and security engineering typically concentrates on architectures and designs rather than requirements because hazard and threat analysis typically depends on the identification of hardware and software components, the failure of which can cause accidents and vulnerabilities which can enable successful attacks. this leads to safety- and security-related requirements that are often ambiguous, incomplete, unverifiable, and even missing. this tutorial begins with a single common realistic example of a safety- and security-critical system that will be used throughout to provide good examples of safety- and security-related requirements. the tutorial provides a consistent ontology of safety, security, and requirements concepts and terminology, provides clear definitions and descriptions of the different kinds of safety- and security-related requirements, and finishes with a practical consistent combined process for engineering them.engineering safety and security related requirements for software intensive systems','Software security engineering'
'this full-day tutorial introduces the attendee to the engineering of safety- and security-related requirements for software-intensive systems. it provides a consistent, effective, and efficient method for identifying, analyzing, specifying, verifying, and validating the four different types of safety- and security-related requirements.engineering safety- and security-related requirements for software-intensive systems','Software security engineering'
'engineering self-coordinating software intensive systems','Software security engineering'
'design and quality are fundamental themes in engineering education. functional programming builds software from small components, a central element of good design, and facilitates reasoning about correctness, an important aspect of quality. software engineering courses that employ functional programming provide a platform for educating students in the design of quality software. this pearl describes experiments in the use of acl2, a purely functional subset of common lisp with an embedded mechanical logic, to focus on design and correctness in software engineering courses. students find the courses challenging and interesting. a few acquire enough skill to use an automated theorem prover on the job without additional training. many students, but not quite a majority, find enough success to suggest that additional experience would make them effective users of mechanized logic in commercial software development. nearly all gain a new perspective on what it means for software to be correct and acquire a good understanding of functional programming.engineering software correctness','Software security engineering'
'the international workshop on the engineering of software services for pervasive environments (esspe) brings together researchers interested in the software engineering challenges found at the convergence of software services and pervasive environments. this summary presents the motivation for the workshop and a brief review of the papers appearing in the proceedings.engineering software services for pervasive environments','Software security engineering'
'we propose an architecture of four complimentary technologies increasingly relevant to a growing number of home users and organizations: cryptography, separation kernels, formal verification, and rapidly improving techniques relevant to software defect density estimation. cryptographic separation protects information in transmission and storage. formally proven properties of separation kernel based secure virtualization can bound risk for information in processing. then, within each strongly separated domain, risk can be measured as a function of people and technology within that domain. where hardware, software, and their interactions are proven to behave as and only as desired under all circumstances, such hardware and software can be considered to not substantially increase risk. where the size or complexity of software is beyond such formal proofs, we discuss estimating risk related to software defect densities, and emerging work related to binary analysis with potential for improving software defect density estimation.engineering sufficiently secure computing','Software security engineering'
'engineering work station is design tool for computer-aided engineering of material flow systems','Software security engineering'
'multidimensional security protocol engineering is effective for creating cryptographic protocols since it encompasses a variety of design, analysis, and deployment techniques, thereby providing a higher level of confidence than individual approaches offer. spear ii, the security protocol engineering and analysis resource ii, is a protocol engineering tool built on the foundation of previous experience garnered during the spear i project in 1997. the goal of the spear ii tool is to facilitate cryptographic protocol engineering and to aid users in distilling the critical issues during an engineering session by presenting them with an appropriate level of detail and guiding them as much as possible during design, analysis and implementation. the spear ii tool currently consists of four components that have been integrated into one consistent and unified graphical interface: a protocol specification environment (gypsie), a gny statement construction interface (visual gny), a prolog-based gny analysis engine (gynger), and a message rounds calculator. the multidimensional approach realized by spear ii is combined with a graphical interface that focuses on making specification of a protocol and its associated conditions for formal analysis as straight forward and painless as possible. experiments that we have conducted confirm that the analysis engine is able to generate accurate proofs for achievable gny-based goals, while preliminary usability experiments have indicated that the interface utilized by spear ii is both expressive and simple to use for specifying cryptographic protocols and constructing logic statements pertaining to these protocols.enhanced security protocol engineering through a unified multidimensional framework','Software security engineering'
'more and more software projects today are security-related in one way or the other. requirements engineers without expertise in security are at risk of overlooking security requirements, which often leads to security vulnerabilities that can later be exploited in practice. identifying security-relevant requirements is labor-intensive and error-prone. in order to facilitate the security requirements elicitation process, we present an approach supporting organizational learning on security requirements by establishing company-wide experience resources and a socio-technical network to benefit from them. the approach is based on modeling the flow of requirements and related experiences. based on those models, we enable people to exchange experiences about security-relevant requirements while they write and discuss project requirements. at the same time, the approach enables participating stakeholders to learn while they write requirements. this can increase security awareness and facilitate learning on both individual and organizational levels. as a basis for our approach, we introduce heuristic assistant tools. they support reuse of existing experiences that are relevant for security. in particular, they include bayesian classifiers that issue a warning automatically when new requirements seem to be security-relevant. our results indicate that this is feasible, in particular if the classifier is trained with domain-specific data and documents from previous projects. we show how the ability to identify security-relevant requirements can be improved using this approach. we illustrate our approach by providing a step-by-step example of how we improved the security requirements engineering process at the european telecommunications standards institute (etsi) and report on experiences made in this application.enhancing security requirements engineering by organizational learning','Software security engineering'
'buffer overflows have been the most common form of security vulnerability in the past decade. a number of techniques have been proposed to address such attacks. some are limited to protecting the return address on the stack; others are more general, but have undesirable properties such as large overhead and false warnings. the approach described in this paper uses legality assertions, source code assertions inserted before each subscript and pointer dereference that explicitly check that the referencing expression actually specifies a location within the array or object pointed at run time. a transformation system is developed to analyze a program and annotate it with appropriate assertions automatically. this approach detects buffer vulnerabilities in both stack and heap memory as well as potential buffer overflows in library functions. runtime checking through using automatically inferred assertions considerably enhances the accuracy and efficiency of buffer overflow detection. a number of example buffer over-flow-exploitingc programs are used to demonstrate the effectiveness of this approach.enhancing security using legality assertions','Software security engineering'
'enterprise engineering and security','Software security engineering'
'in cooperative design and engineering, work context has dramatic implications when it comes to building understanding and teams, and getting the work done. from an information security perspective, the context of operation for software or hardware can drive the expectations for security, and may indeed determine the levels of security policy that would be required for collaborative operation. there is a place in this domain for proactive, context-based security implementation. this paper describes a context-centric security enforcement system intended for cooperative design and engineering environments that we call: environment-aware security enforcement (ease). we describe the application of this approach to an existing client-server, e-manufacturing application.environmentally-aware security enforcement (ease) for cooperative design and engineering','Software security engineering'
'eternal software engineering questions','Software security engineering'
'security of the data manipulated by a program is often a key attribute of the architecture of that program. scenarios drive software architecture design. quality attribute scenarios, such as security scenarios, describe a use of the system in terms of specific quality attribute values. an analytic theory describes an algorithm that will use the attribute architecture representation to predict the quality attribute values represented by the scenarios. it is difficult to determine how much confidence can be placed in the calculation of quality attribute values. confidence intervals on those predictions can be used to help the software architect make better decisions about the architecture. in this paper, we explore a specific aspect of a reasoning framework for security, how to compute confidence levels for the quality attribute values resulting from security scenarios.evaluating confidence levels for security scenarios in attribute architectures','Software security engineering'
'evolutionary software engineering','Software security engineering'
'experimental software engineering (stese)','Software security engineering'
'experimental software engineering','Software security engineering'
'there is a strong demand for techniques to aid development and modelling of security critical systems. based on general security evaluation criteria, we show how to extend the system structure diagrams of the case tool autofocus (which are related to uml-rt collaboration diagrams) to allow modelling of security critical systems, in particular concerning components and channels. both high-level and low-level models of systems are supported, and the notion of security patterns is introduced to provide generic solutions for security requirements. we explain our approach on the example of an electronic purse card system.extended description techniques for security engineering','Software security engineering'
'high performance computing (hpc) within the department of defense (dod) is vital to the execution of the research and development mission. however, users must access these resources using interfaces that are complicated, specialized, primitive, and opaque. ezhpc was developed to address this complexity. the ezhpc project is interesting in that it abstracts for the user many of the technical details and tools related to hpc within the dod high performance computing modernization program (hpcmp) while providing a web-based application that allows user access to hpc resources. leveraging open-source, widely available technology, the ezhpc security model addresses the dod security requirements for authentication, confidentiality, availability, and integrity. this paper discusses an architecture that employs end-to-end encryption, open-source technologies, message authentication codes, and authentication of users via the hpcmp-modified implementation of the massachusetts institute of technology kerberos. this architecture provides ezhpc the flexibility to reduce the technology barrier to those unfamiliar with interactive use of hpc systems while providing significant capability to more advanced users. the subsequent abstraction of hpc resources constitutes a security model for any web-enabled application programming interface.ezhpc security architecture','Software security engineering'
'in this paper, i describe a new outlook on the history of software engineering. i portray large-scale structures within software engineering to give a better understanding of the flow of history. i use these large-scale structures to reveal the steady, ongoing evolution of concepts, and show how they relate to the myriad whorls and eddies of change. i also have four smaller, more specific purposes in writing this paper.first, i want to point out that old ideas do not die. in the mythical man-month after 20 years, brooks claims \"the waterfall model is wrong.\" but if the waterfall model were wrong, we would stop arguing over it. though the waterfall model may not describe the whole truth, it describes an interesting structure that occurs in many well-defined projects and it will continue to describe this truth for a long time to come. i expect the waterfall model will live on for the next one hundred years and more.second, i want to show that the chaos model, chaos life cycle, complexity gap, and chaos strategy are part of the natural evolution of software engineering. the chaos model and strategy supersede, but do not contradict, the waterfall and spiral models, and the stepwise refinement strategy. they are more up to date because they express contemporary issues more effectively, and fit our contemporary situations better. the chaos model, life cycle, and strategy are equally as important, but not better than, other concepts.third, i compare the chaos model, life cycle, and strategy to other models, life cycles, and strategies. this paper can be considered a comparison of the ideas presented in my papers about chaos with other ideas in the field. i avoided comparisons in my other papers because i wanted to define those ideas in their own terms and the comparisons did not further the new ideas.fourth, i make a few predictions about the next ten years of software engineering. the large-scale structures described in this history provide a stronger base for understanding how software engineering will evolve in the future.this paper is laid out as follows. in the first section, i use the flow of water as a metaphor to describe the flow of progress in software engineering. i use the water metaphor to show some of the structures within software engineering. the current work builds on top of the historical work, and future work will build on top of current work. in the remaining sections, i describe the waves, streams, and tides that portray the evolution of concepts and technologies in software engineering.fifty years of progress in software engineering','Software security engineering'
'as wireless networks proliferate, web browsers operate in an increasingly hostile network environment. the https protocol has the potential to protect web users from network attackers, but real-world deployments must cope with misconfigured servers, causing imperfect web sites and users to compromise browsing sessions inadvertently. forcehttps is a simple browser security mechanism that web sites or users can use to opt in to stricter error processing, improving the security of https by preventing network attacks that leverage the browser\'s lax error processing. by augmenting the browser with a database of custom url rewrite rules, forcehttps allows sophisticated users to transparently retrofit security onto some insecure sites that support https. we provide a prototype implementation of forcehttps as a firefox browser extension.forcehttps','Software security engineering'
'formal methods and requirements engineering','Software security engineering'
'starting with the trusted computer system evaluation criteria (aka the \"orange book\"), the information security community within the us department of defense has been advocating formal methods for decades. others have followed suit, culminating in the appearance of the common criteria. the advantages of formal analysis seem self-evident. first, of the three things that are subject to certification -- people, process, and product -- product seems to be the most immediately relevant. second, if we focus on product, testing seems insufficient; as dijkstra famously noted, testing can reveal the presence flaws, but not their absence. this is especially true of security, where flaws may be intentionally constructed not to reveal themselves during normal testing. despite this, the acceptance of formal methods has been less than universal. in this talk i will discuss the history of formal methods, but with a focus on how that history has shaped our current situation. i\'ll also discuss what we need to do to make formal methods more appealing. this will involve the development of formal methods or new ways of using formal methods that will: have a more predictable, quantifiable impact on validation costs; support software engineering for nonstandard properties and multiple properties; incorporate untrusted software; and support flexible release strategies. i will also discuss a recent application of formal methods nrl undertook as part of the development of a new security device [1] and new directions formal methods must take if they are to be applicable to future systems. the focus here will be on their use in autonomous systems that incorporate nondeterministic learning algorithms.formal methods in security engineering','Software security engineering'
'formal software engineering','Software security engineering'
'formal user-centered requirements engineering','Software security engineering'
'research in requirements engineering has produced an extensive body of knowledge, but there are four areas in which the foundation of the discipline seems weak or obscure. this article shines some light in the &#8220;four dark corners,&#8221; exposing problems and proposing solutions. we show that all descriptions involved in requirements engineering should be descriptions of the environment. we show that certain control information is necessary for sound requirements engineering, and we explain the close association between domain knowledge and refinement of requirements. together these conclusions explain the precise nature of requirements, specifications, and domain knowledge, as well as the precise nature of the relationships among them. they establish minimum standards for what  information should be represented in a requirements language. they also make it possible to determine exactly what it means for requirements and engineering to be successfully completed.four dark corners of requirements engineering','Software security engineering'
'computational science and engineering (cse) software supports a wide variety of domains including nuclear physics, crash simulation, satellite data processing, fluid dynamics, climate modeling, bioinformatics, and vehicle development. the increase in the importance of cse software motivates the need to identify and understand appropriate software engineering (se) practices for cse. because of the uniqueness of cse software development, existing se tools and techniques developed for the business/it community are often not efficient or effective. appropriate se solutions must account for the salient characteristics of the cse development environment. this situation creates an opportunity for members of the se community to interact with members of the cse community to address this need. this workshop facilitates that collaboration by bringing together members of the se community and the cse community to share perspectives and present findings from research and practice relevant to cse software. a significant portion of the workshop is devoted to focused interaction among the participants with the goal of generating a research agenda to improve tools, techniques, and experimental methods for studying cse software engineering.fourth international workshop on software engineering for computational science and engineering (se-cse2011)','Software security engineering'
'freeware security web tools','Software security engineering'
'security of intelligent software systems is an important area of research. although security is traditionally considered a technical issue; security is, in fact, a two-dimensional problem, which involves technical as well as social challenges. goal-driven requirements engineering (gdre) has been proposed in the literature as a suitable paradigm for the analysis of security issues and elicitation of security requirements at both the social and technical level. nevertheless, there is lack of approaches, which would support the successful transformation of the elicited, using gdre approaches, security requirements to design. this paper presents work that fills this gap. the presented approach, which is based on the integration of a goal-driven security requirements engineering (gdsre) methodology and a model-based security engineering (mbse) method, has some important features: (1) it provides a structured process to translate the results of the gdsre method to a design, which satisfies these requirements; (2) it allows the simultaneous elicitation and analysis of the security requirements and the functional requirements of the system; (3) it allows consideration of both the social and the technical dimensions of the system\'s security; (4) it guides software engineers toward a design that is amenable to formal verification with the aid of automated tools. we demonstrate the applicability of the proposed approach at the hand of an application to the electronic purse standard common electronic purse specifications (released by visa international and others). &#169; 2010 wiley periodicals, inc. supported by the eu project secure change.from goal-driven security requirements engineering to secure design','Software security engineering'
'for building and leading successful software engineering teams it is vital to understand their team structure as well as many other \"soft\" factors, e.g. the personality and skills of individual team members. a key element of the team structure, besides power distribution, knowledge distribution etc., is the role distribution within the team. the role distribution has a twofold aspect: first, the formal role distribution, which is defined by the standard process and role model or the project management, and second, the informal role distribution which grows within a team by the natural interactions between the team members and is based to some extent upon their individual characteristics.this paper presents an empirical examination of the informal role distribution in student software engineering teams and compares the results to the concept of functional group roles.functional group roles in software engineering teams','Software security engineering'
'this paper is about generating security tests, in addition to functional tests previously generated by a model-based testing approach. the method that we present re-uses the functional model and the adaptation layer developed for the functional testing, and relies on an additional security model. we propose to compute the tests by using some test purposes as guides for the tests to be extracted from the models. we see a test purpose as the combination of a security property and a test need issued from the know-how of a security engineer. we propose a language based on regular expressions for the expression of such test purposes. we illustrate our approach with experiments on ias.generating security tests in addition to functional tests','Software security engineering'
'we advocate goal-oriented software securityengineering to produce highly secure software in a constructive,provable and cost-effective manner. our approach is to couplegoal-oriented semi-formal requirements specifications withformal design and implementation. to this effect, we proposedfades (formal analysis and design for engineering security)in [14] as the first goal-oriented software security engineeringapproach that provides a systematic and automated bridgebetween the goal-directed semi-formal kaos (knowledgeacquisition for automated specifications) framework and the bformal method to derive formal design and implementation fromsecurity requirements. in this paper, we demonstrate theapplicability of fades and study its effectiveness through ageneric electronic smart card case study and a comparativeanalysis between fades and strictly applying formal methods.we use the case study to demonstrate how the goal-orientedformulation of security requirements in fades paves the wayfor formal design that provably preserves the security properties.further, the results of the comparison between fades and zshow that fades achieves better requirements completeness,consistency and security quality.goal-oriented software security engineering','Software security engineering'
'guest editorial: security requirements engineering: past, present and future','Software security engineering'
'hci and requirements engineering','Software security engineering'
'this poster paper outlines a method for a search based approach to the development of provably correct protocols.human competitive security protocols synthesis','Software security engineering'
'research into how humans interact with computers has a long and rich history. only a small fraction of this research has considered how humans interact with computers when engineering software. a similarly small amount of research has considered how humans interact with humans when engineering software. for the last forty years, we have largely taken an artifact-centric approach to software engineering research. to meet the challenges of building future software systems, i argue that we need to balance the artifact-centric approach with a human-centric approach, in which the focus is on amplifying the human intelligence required to build great software systems. a human-centric approach involves performing empirical studies to understand how software engineers work with software and with each other, developing new methods for both decomposing and composing models of software to to ease the cognitive load placed on engineers and on creating computationally intelligent tools aimed at focusing the humans on the tasks only the humans can solve.human-centric software engineering','Software security engineering'
'with the trend of service and experience economy, emerging service science is perfectly poised for service innovation. the classifying framework (idesign) proposed in this research is an approach roadmap to fulfill valued service innovation with an ecological perspective. the framework identifies two dimensions (continuity of co-production and mutual adaptability), characterizing the service exchange involved. the novelty of idesign is that it is based on the notion of species (partners) interacting with each other ecologically (in which value co-production is unfolded with different forms of mutualism, driven by mutual adaptability associated with the behaviors of customers and producers). this paper can be viewed as a visionary research for new thinking in relation to e-service processes in services science. providers and customers accomplish service design, fulfilling value co-production and mutual adaptability, by means of a set of particular criteria of performance (i.e., ed, ur, pr).idesign','Software security engineering'
'advances in software engineering have led to the creation of many new software engineering techniques. however, industrial adoption of these techniques is often quite low, as development organizations are skeptical of their value and applicability. empirical studies are commonly used to show this value to potential adopters, with open source software used as an approximation of industrial applications. however, little data exists on the similarity of open source and industrial software. we present a large metrics-based study comparing the most commonly evaluated open source programs to a large set of industrial programs. source metrics are calculated and compared between 24 open source and 21 industrial programs. the results identify open source programs that are most similar to industrial programs. using these identified open source programs in empirical studies can provide the best generalization to industrial software.improving industrial adoption of software engineering research','Software security engineering'
'buffer overflows cause serious problems in different categories of software systems. for example, if present in network or security applications, they can be exploited to gain unauthorized grant or access to the system. in embedded systems, such as avionics or automotive systems, they can be the cause of serious accidents.this paper proposes to combine static analysis and program slicing with evolutionary testing, to detect buffer overflow threats. static analysis identifies vulnerable statements, while slicing and data dependency analysis identify the relationship between these statements and program or function inputs, thus reducing the search space.to guide the search towards discovering buffer overflow in this work we define three multi-objective fitness functions and compare them on two open-source systems. these functions account for terms such as the statement coverage, the coverage of vulnerable statements, the distance form buffer boundaries and the coverage of unconstrained nodes of the control flow graph.improving network applications security','Software security engineering'
'improving user manuals in software engineering education','Software security engineering'
'this paper provides a roadmap for addressing security requirements on projects using an agile approach. the dynamic systems development method (dsdm) is used as an example framework for development. security quality requirements engineering (square) is used as an example methodology to address security issues early in the development life cycle. square can be more effective when it fits into an organization&#8217;s existing development process. hence this paper describes a way to fit the square methodology into the dsdm framework.incorporating security requirements engineering into the dynamic systems development method','Software security engineering'
'this paper provides a roadmap for developing security-critical projects using rational unified process as a framework for development. the security quality requirements engineering (square) methodology provides a way to address security issues early in the development lifecycle. square can be more effective when it fits into an organization&#8217;s existing development process. hence this paper describes a way to fit the square methodology into the rational unified process.incorporating security requirements engineering into the rational unified process','Software security engineering'
'secure software development has become a topic of increasing importance, as a general fear rises due to security holes, vulnerabilities, and attacks.to ensure the security of information in a society of file sharing, on-line business transactions, and e-communication, undergraduate students will soon be required to implement software security concepts into their software development processes as soon as they complete their degrees. consequently, it is imperative for graduates of computer science departments to be trained in the fundamentals of information security and to gain hands-on experience with secure software development. to address this issue computer science educators at the undergraduate level are turning their attentions to incorporating security issues within traditional computer science courses. the paper describes an existing undergraduate software engineering course that has been modified to include software security concepts.challenges and future work are also presented.incorporating software security into an undergraduate software engineering course','Software security engineering'
'information requirements analysis for data warehouse systems differs significantly from requirements analysis for conventional information systems. based on interviews with project managers and information systems managers, requirements for a methodological support of information requirements analysis for data warehouse systems are derived. existing approaches are reviewed with regard to these requirements. using the method engineering approach, a comprehensive methodology that supports the entire process of determining information requirements of data warehouse users, matching information requirements with actual information supply, evaluating and homogenizing resulting information requirements, establishing priorities for unsatisfied information requirements, and formally specifying the results as a basis for subsequent phases of the data warehouse development (sub)project has been proposed. the most important sources for methodology components were four in-depth case studies of information requirements analysis practices observed in data warehousing development projects of large organizations. in this paper, these case studies are presented and the resulting consolidated methodology is summarized. while an application of the proposed methodology in its entirety is still outstanding, its components have been successfully applied in actual data warehouse development projects.information requirements engineering for data warehouse systems','Software security engineering'
'information system security engineering','Software security engineering'
'the purpose of this research paper is to illustrate the industrial and federal need for information systems security engineering (isse) in order to build information assurance (ia) into a system rather than the current costly practice of fixing systems after production. extensive research was performed by collecting information from throughout the world wide web to include sites such as the national security agency\'s homepage, the information assurance technical framework homepage, the workshop for application of engineering principles to system security design, as well as many others. this research realized the following findings: (1) ia is dangerously left out of systems engineering processes; (2) a consortium from academia, industry and the federal government have formalized isse and its processes; (3) federally sponsored and industrially sponsored professional certifications exist for security engineers practicing isse; (4) isse, however, is not greatly used today due to a lack of understanding and a perceived high cost; (5) end-users are beginning to understand ia and are calling for more secure systems. this paper was written to illustrate a way forward, a method to bring isse to the frontlines of systems engineering and bring to life a notional concept of designing for security. this paper does not provide quantitative analyses as to the benefits of isse vs. the initial up front costs; however, further research should be accomplished in the future to address this. in conclusion, i recommend that isse must be identified as a critical component of the systems engineering lifecycle and be properly utilized to ensure that future products meet the ia demands of the end user. to achieve this, academia must build degree programs to educate isse and incorporate isse into existing degree programs; industry and the federal government must both embrace these principles and apply these techniques to their post-production, active engineering as well as new program developments.information systems security engineering','Software security engineering'
'inside windows nt security','Software security engineering'
'a standards-based information security methodologies integral engineering (isie) technique is proposed that makes it possible to develop reliable and multipurpose procedural foundation for solving a wide range of theoretical and practical information security problems. the main technique matter concerns designing a generalized primary (root) semiformal (dfd and uml) domain models system that serves as the source for a wide range of secondary (derived) models for particular information security problems. within the isie framework, the derived models are obtained from the root model by extracting the necessary (for a given particular problem) partial model and its subsequent regulated modification: supplement, generalization, evolving etc. with these operations, the derived models, in general, remain coordinated with the root model and the corresponding is standard that ensures the possibility of reuse of every particular derived model in other tasks. the application of the proposed technique in several specific information security organizational problems is discussed, alongside with some logical schemes of common engineering operations.integral engineering technique for information security methodologies','Software security engineering'
'integrating information security engineering with system engineering with system engineering tools','Software security engineering'
'security is a crucial issue for information systems. traditionally, security is considered after the definition of the system. however, this approach often leads to problems, which translate into security vulnerabilities. from the viewpoint of the traditional security paradigm, it should be possible to eliminate such problems through better integration of security and systems engineering. this paper argues for the need to develop a methodology that considers security as an integral part of the whole system development process. the paper contributes to the current state of the art by proposing an approach that considers security concerns as an integral part of the entire system development process and by relating this approach with existing work. the different stages of the approach are described with the aid of a case study; a health and social care information system.integrating security and systems engineering','Software security engineering'
'integration of computer security into the software engineering and computer science programs','Software security engineering'
'internationalizing software engineering standards','Software security engineering'
'internet infrastructure security','Software security engineering'
'large, distributed application plays an increasing central role in today\'s information technology environment. the diversity and openness of these systems have given raise to questions of trust, certification and security. this paper deals with the up gradation of security certificate of a component. as security certification is necessary for third-party components, taking certification from the beginning whenever a component is updated turns out to be a redundant process. it leads to wastage of time especially in large component based software systems. to remove that redundant certification we propose a certdriver. the proposed certdriver takes the component, its required security properties as security goals and version number specified by user as input and gives output a certified component providing its security in terms of percentages. the whole process centers on version number. in the whole process, it will also take the help of test cases which can be useful while finding out errors in the component. the aim of this paper to examine certification, trust and security related questions and try to find out possible solutions.intra-component security certification','Software security engineering'
'introduction to software engineering for secure systems','Software security engineering'
'these days, if you say that you are doing research in the area of computer security you instantly receive attention. sadly, the same cannot be said of software engineering. but are the two areas really so different? both seem to be concerned with issues that range from the finely technical to the broadly social and that force us to make difficult tradeoffs among cost, performance, quality, and usability. both seem to require that we conduct our research in an interdisciplinary context. in the end we realize that fully solving the security problem for ever larger and more complex systems is as intractable as fully solving the traditional software engineering problem. in this talk i will attempt to relate the challenges of security engineering and software engineering, and will argue that security engineering is more of a software engineering problem than many people would like to admit.is security engineering really just good software engineering?','Software security engineering'
'is seeks security in unix','Software security engineering'
'knowledge engineering versus software engineering','Software security engineering'
'the knowledge-based intelligent engineering systems centre (kes) is focused on modelling, analysis and design in the areas of intelligent information systems, physiological sciences systems, electronic commerce and service engineering. the centre aims to provide applied research support to the information, defence and health industries. the overall goal will be to synergise contributions from researchers in the diverse disciplines such as engineering, information technology, science, health, commerce and security engineering. the research projects undertaken in the centre include adaptive mobile robots, aircraft landing support, learning paradigms, teaming in multi-agent systems, target detection, image registration and detection in intelligent environment, unmanned air vehicles (uavs) and simulation, intelligent decision support systems, neuro-fuzzy systems, medical diagnostic systems in intelligent environment and so on. this talk will focus on knowledge-based intelligent engineering systems in defence and security applications.knowledge-based intelligent engineering systems in defence and security','Software security engineering'
'programs encounter increasingly complex and fragile mappings to computing platforms, resulting in performance characteristics that are often mysterious to students, practitioners, and even researchers. we discuss some steps toward an experimental methodology that demands and provides a deep understanding of complete systems, the necessary instrumentation and tools to support such a methodology, and a curriculum that teaches the methodology and tools as a fundamental part of the discipline.languages and performance engineering','Software security engineering'
'legislation is constantly affecting the way in which software developers can create software systems, and deliver them to their users. this raises the need for methods and tools that support developers in the creation and re-distribution of software systems with the ability of properly coping with legal constraints. we conjecture that legal constraints are another dimension software analysts, architects and developers have to consider, making them an important area of future research in software engineering.lawful software engineering','Software security engineering'
'lessons learned with the systems security engineering capability maturity model','Software security engineering'
'limitations of mathematics in software engineering','Software security engineering'
'linux  system administration: maximizing system security: part 2','Software security engineering'
'linux systems administration: maximizing linux security, part i','Software security engineering'
'software engineering has become a de rigueur part of all self-respecting conferences on computer subjects, and has complete masters degree curricula devoted to itself in some universities. special templates for drawing &#8220;software engineering symbols&#8221; are being issued to employees of programming divisions of large companies, and uncounted audiences attend software engineering courses, schools, symposia and workshops. all this indicates that as a fad software engineering is enormously successful. this paper attempts to bring into focus some issues that in the nearest future are very likely to enter the field that we today consider as &#8220;software engineering.&#8221;look ahead at software engineering','Software security engineering'
'too often, software engineering (se) tool research is focused on creating small, stand-alone tools that address rarely understood developer needs. we believe that research should instead provide developers with flexible environments and interoperable tools, and then study how developers appropriate and tailor these tools in practice. although there has been some prior work on this, we feel that flexible tool environments for se have not yet been fully explored. in particular, we propose adopting the web 2.0 idea of mashups and mashup environments to support se practitioners in analytic activities involving multiple information sources.mashup environments in software engineering','Software security engineering'
'although there has been much research work in security requirements engineering, we do not have adequate ways of measuring this and other security engineering processes. in this paper, we study a measurement approach to security requirements engineering, align it with the security quality requirements engineering (square) method, and use both the original and revised security requirements measurement approach to analyze projects that were developed with and without square.measuring the software security requirements engineering process','Software security engineering'
'this paper presents a brief on evolution and subsequent developments in the field of situation method engineering (sme) through exhaustive literature review. the efforts of the various method engineers are gathered, summarized and presented to show the overall growth of this vital discipline. this research paper starts with the assembly-based approaches and moves towards method generation. the paper further analyzes the proposals presented on the architecture of sme processes followed by the open process framework (opf). opf depend on the four major components-object-oriented process, environment and notation (open). we evaluate these proposals for various issues leading to method configuration. the survey concludes with the proposals on configurability of methods and current unresolved issues that need to be addressed in one single approach to configure a situation-specific coherent methodmethod configuration from situational method engineering','Software security engineering'
'this position paper argues that a successful cots evaluation process should be based on the principles of method engineering (me). following a brief description of an me approach underpinned by a metamodel, some method fragments related to component-based software engineering are offered as the starting point for the creation of a complete suite of method fragments for future cots evaluation processes.method engineering and cots evaluation','Software security engineering'
'method engineering','Software security engineering'
'method rationale in method engineering and use','Software security engineering'
'microcontroller software engineering','Software security engineering'
'software systems are becoming more complex, interconnected and liable to adopt continuous change and evolution. it\'s necessary to develop appropriate methods and techniques to ensure security and privacy of such systems. research efforts that aim to ensure security and privacy of software systems are distinguished through two main categories: 1 the development of requirements engineering methods, and 2 implementation techniques. approaches that fall in the first category usually aim to address either security or privacy in an implicit way, with emphasis on the security aspects by developing methods to elicit and analyse security and privacy requirements. works that fall in the latter categories focus specifically on the later stages of the development process irrespective of the organisational context in which the system will be incorporated. this work introduces a model-based process for security and privacy requirements engineering. in particular, the authors\' work includes activities which support to identify and analyse security and privacy requirements for the software system. their purpose process combines concepts from two well-known requirements engineering methods, secure tropos and pris. a real case study from the eu project e-vote, i.e., an internet based voting system, is employed to demonstrate the applicability of the approach.model based process to support security and privacy requirements engineering','Software security engineering'
'service oriented architectures with underlying technologies like web services and web services orchestration have opened the door to a wide range of novel application scenarios, especially in the context of inter-organizational cooperation. one of the remaining obstacles for a wide-spread use of these techniques is security. companies and organizations open their systems and core business processes to partners only if a high level of trust can be guaranteed. the emergence of web services security standards provides a valuable and effective paradigm for addressing the security issues arising in the context of inter-organizational cooperation. the low level of abstraction of these standards is, however, still an unresolved issue which makes them inaccessible to the domain expert and remains a major obstacle when aligning security objectives with the customer needs. their complexity makes implementation easily prone of error. this paper provides a bird eye view of a doctoral work, where an effort is made to develop a conceptual framework - called sectet in order to apply model driven security engineering techniques for the realization of high-level security requirements.model driven security engineering for the realization of dynamic security requirements in collaborative systems','Software security engineering'
'model driven architecture is an approach to increasing the quality of complex software systems based on creating high-level system models and automatically generating system architectures from the models. we show how this paradigm can be specialized to what we call model driven security. in our specialization, a designer builds a system model along with security requirements, and automatically generates from this a complete, configured security infrastructure.we propose a modular approach to constructing modeling languages supporting this process, which combines languages for modeling system design with languages for modeling security. we present an application to constructing systems from process models, where we combine a uml-based process design language with a security modeling language for formalizing access control requirements. from models in the combined language, we automatically generate security architectures for distributed applications.model driven security for process-oriented systems','Software security engineering'
'the research vision of the unified component metamodel framework (uniframe) is to develop aninfrastructure for components that enables a plug andplay component environment where the securitycontracts are a part of the component description andthe security aware middleware is generated by thecomponent integration toolkits. that is, the componentsproviders will define security contracts in addition tothe functional contracts. these security contracts willbe used to analyze the ability of a service to meet thesecurity constraints when used in a composition ofcomponents. a difficulty in progressing the securityrelated aspects of this infrastructure is the lack of aunified access control model that can be leveraged toidentify protected resources and access control points atthe model level. existing component technologies utilizevarious mechanisms for specifying security constraints.this paper will explore issues related to expressingaccess control requirements of components and theresources they manage. it proposes a platformindependent model (pim) for the access control that canbe leveraged to parameterize domain models. it alsooutlines the analysis necessary to progress a standardtransformation from this pim to three existing platformspecific models (psms).model driven security','Software security engineering'
'the effectiveness of model-driven engineering relies on our ability to build high-quality models. this task is intrinsically difficult. we need to produce sufficiently complete, adequate, consistent, and well-structured models from incomplete, imprecise, and sparse material originating from multiple, often conflicting sources. the system we need to consider in the early stages comprises software and environment components including people and devices. such models should integrate the intentional, structural, functional, and behavioral facets of the system being developed. rigorous techniques are needed for model construction, analysis, and evolution. they should support early and incremental reasoning about partial models for a variety of purposes, including satisfaction arguments, property checks, animations, the evaluation of alternative options, the analysis of risks, threats and conflicts, and traceability management. the tension between technical precision and practical applicability calls for a suitable mix of heuristic, deductive, and inductive forms of reasoning on a suitable mix of declarative and operational models. formal techniques should be deployed only when and where needed, and kept hidden wherever possible. the talk will provide a retrospective account of our research efforts and practical experience along this route, including recent progress in model engineering for safety-critical medical workflows. problem-oriented abstractions, analyzable models, and constructive techniques are pervasive concerns.model engineering for model-driven engineering','Software security engineering'
'model-based engineering (mbe) and product line engineering (ple) have been combined, to handle new system development constraints like: increasing complexity, higher product quality, faster time-to-market and cost reduction. as observed by some authors, the derivation of a product from product line shared core assets has been insufficiently addressed and can remain tedious in practice. we cope with this issue focusing on having a flexible and reactive model-based derivation, and propose an incremental evolution by extension of the product line coupled with this derivation activity. process and tools bridge the gap between application and domain engineering introducing a semi-automatic feedback to benefits from the developments made in the application engineering. the approach is applied to a model-based product line dedicated to class diagrams, and is tooled within the eclipse environment.model-based product line evolution','Software security engineering'
'mobile communication systems are increasingly used in companies. in order to make these applications secure, the security analysis has to be an integral part of the system design and it management process for such mobile communication systems. this work presents the experiences and results from the security analysis of a mobile system architecture at a large german telecommunications company, by making use of an approach to model-based security engineering that is based on the uml extension umlsec. the focus lies on the security mechanisms and security policies of the mobile applications which were analyzed using the umlsec method and tools. main results of the paper include a field report on the employment of the umlsec method in an industrial telecommunications context as well as indications of its benefits and limitations.model-based security analysis for mobile communications','Software security engineering'
'we give an overview over a soundly based secure software engineering methodology and associated tool-support developed over the last few years under the name of model-based security engineering (mbse). we focus in particular on applications in industry.model-based security engineering for real','Software security engineering'
'developing security-critical systems is difficult and there are many well-known examples of security weaknesses exploited in practice. thus a sound methodology supporting secure systems development is urgently needed. our aim is to aid the difficult task of developing security-critical systems in a formally based approach using the notation of the unified modeling language. we present the extension umlsec of uml that allows one to express security-relevant information within the diagrams in a system specification. umlsec is defined in form of a uml profile using the standard uml extension mechanisms. in particular, the associated constraints give criteria to evaluate the security aspects of a system design, by referring to a formal semantics of a simplified fragment of uml. in this tutorial exposition, we concentrate on an approach to develop and analyze security-critical specifications and implementations using aspect-oriented modeling.model-based security engineering with uml','Software security engineering'
'we have seen many efforts invested in research on engineering security aspects of software and systems over the last years, but we have also seen spectacular security breaches and privacy leaks in web applications, mobile apps, and enterprise systems. in fact, in both the industrial and the academic context, we are still far from satisfactory, integrative development approaches covering the many different facets of security, such as access control, secure user interaction, privacy, secure protocols, trustworthiness, etc. model-driven and model-based approaches to security integrate security aspects in the early phases of software development at an abstract level. they thus pave the way to reduce the gap between security requirements and their enforcement mechanisms and to verify security properties on an appropriate level of detail, following the principle of separation of concerns. these approaches allow designers to decouple functional architecture from mechanisms that ensure the security properties of the system. the main objective of this workshop was to bring together researchers and practitioners to discuss the approaches, key issues, innovative applications, and trends in model-driven engineering of secure and trustworthy service composition, software, and systems.model-driven security','Software security engineering'
'the modeling in software engineering (mise) workshops are a collaboration between the icse and models research communities, with a focus on using models to facilitate software development.modeling in software engineering','Software security engineering'
'software security is becoming a key quality concern as software applications are increasingly being used in untrustworthy computing environments such as the internet. software is designed with the mindset of its functionalities and cost, where the focus is on the operational behavior while security concerns are neglected or marginally considered. as a result, software engineers build the software while lacking the knowledge about security and its effect on the system. this paper presents an approach for modeling the behavior of security threats using statecharts. the proposed approach introduces modular design for representing threats through the use of components and reusability. through the focus on the behavior of an attack, software engineers can clearly define and understand security concerns as the application is being designed and developed. in addition, modeling security threats with statecharts makes it convenient to build a consistent semantic link between functional behaviors and security concerns.modeling security attacks with statecharts','Software security engineering'
'security is an important quality attribute for service oriented architecture (soa) based system. however, there is no sufficient support for modelling security-centric concerns for soa based application. this paper presents a metamodel called soaml4security, which introduces qos concepts into service oriented modelling language (soaml) in order to support the modelling of security aspect. we motivate the need of extending soaml for modelling security concerns from different viewpoints. we describe the process of developing the metamodel, which can support model driven engineering (mde) approach for service-oriented applications. the use of the extended metamodel has been demonstrated by modelling a real world service-oriented application for security requirements.modeling security for service oriented applications','Software security engineering'
'objective: in this paper, we present a systematic literature review of motivation in software engineering. the objective of this review is to plot the landscape of current reported knowledge in terms of what motivates developers, what de-motivates them and how existing models address motivation. methods: we perform a systematic literature review of peer reviewed published studies that focus on motivation in software engineering. systematic reviews are well established in medical research and are used to systematically analyse the literature addressing specific research questions. results: we found 92 papers related to motivation in software engineering. fifty-six percent of the studies reported that software engineers are distinguishable from other occupational groups. our findings suggest that software engineers are likely to be motivated according to three related factors: their \'characteristics\' (for example, their need for variety); internal \'controls\' (for example, their personality) and external \'moderators\' (for example, their career stage). the literature indicates that de-motivated engineers may leave the organisation or take more sick-leave, while motivated engineers will increase their productivity and remain longer in the organisation. aspects of the job that motivate software engineers include problem solving, working to benefit others and technical challenge. our key finding is that the published models of motivation in software engineering are disparate and do not reflect the complex needs of software engineers in their career stages, cultural and environmental settings. conclusions: the literature on motivation in software engineering presents a conflicting and partial picture of the area. it is clear that motivation is context dependent and varies from one engineer to another. the most commonly cited motivator is the job itself, yet we found very little work on what it is about that job that software engineers find motivating. furthermore, surveys are often aimed at how software engineers feel about \'the organisation\', rather than \'the profession\'. although models of motivation in software engineering are reported in the literature, they do not account for the changing roles and environment in which software engineers operate. overall, our findings indicate that there is no clear understanding of the software engineers\' job, what motivates software engineers, how they are motivated, or the outcome and benefits of motivating software engineers.motivation in software engineering','Software security engineering'
'the emergence of wireless and mobile networks has made possible the introduction of a new research area m-commerce or mobile commerce. mobile payment is a natural successor to web centric payments which has emerged as one of the sub domains of mobile commerce applications. a study reveals that there are wide ranges of mobile payment solutions and models which are available with the aid of various services such as short message service (sms), but there is no specific mobile payment system for educational institutions to collect the fees as well as for student community to pay the fees without huge investment. this paper proposes a secured framework for mobile payment consortia system (mpcs) to carry out the transactions from the bank to the academic institutions for the payment of fees by students through mobile phone. mobile payment consortia system provides an end-to-end security using public key infrastructure (pki) through a mobile information device profile (midp)-enabled mobile device. this framework provides an efficient, reliable and secured system to perform mobile payment transactions and reduces transactional cost for both students and educational institutions. mobile payment consortia system is designed with strong authentication and non-repudiation by employing digital signatures. confidentiality and message integrity are also provided by encrypting the messages at application level and by using public key certificates and digital signature envelops.mpcs','Software security engineering'
'due to stagnating clock rates, future increases in processor performance will have to come from parallelism. inexpensive multicore processors with several cores on a chip have become standard in pcs, laptops, servers, and embedded devices will follow; manycore chips with hundreds of processors on a single chip are predicted. software engineers are now asked to write parallel applications of all sorts, and need to quickly grasp the relevant aspects of general-purpose parallel programming. this tutorial at icse 2010 prepares them for this challenge.multicore software engineering','Software security engineering'
'context: for many years, we have observed industry struggling in defining a high quality requirements engineering (re) and researchers trying to understand industrial expectations and problems. although we are investigating the discipline with a plethora of empirical studies, those studies either concentrate on validating specific methods or on single companies or countries. therefore, they allow only for limited empirical generalisations. objective: to lay an empirical and generalisable foundation about the state of the practice in re, we aim at a series of open and reproducible surveys that allow us to steer future research in a problem-driven manner. method: we designed a globally distributed family of surveys in joint collaborations with different researchers from different countries. the instrument is based on an initial theory inferred from available studies. as a long-term goal, the survey will be regularly replicated to manifest a clear understanding on the status quo and practical needs in re. in this paper, we present the design of the family of surveys and first results of its start in germany. results: our first results contain responses from 30 german companies. the results are not yet generalisable, but already indicate several trends and problems. for instance, a commonly stated problem respondents see in their company standards are artefacts being underrepresented, and important problems they experience in their projects are incomplete and inconsistent requirements. conclusion: the results suggest that the survey design and instrument are well-suited to be replicated and, thereby, to create a generalisable empirical basis of re in practice.naming the pain in requirements engineering','Software security engineering'
'the transmission control protocol internet protocol (tcp/ip) [1] suite is widely used to interconnect computing facilities in modern network environments. however, there exist several security vulnerabilities in the tcp specification and additional weaknesses in a number of its implementations. these vulnerabilities may enable an intruder to \"attack\" tcp-based systems, allowing him/her to \"hijack\" a tcp connection or cause denial of service to legitimate users. we analyze tcp code via a \"reverse engineering\" technique called \"slicing\" to identify several of these vulnerabilities, especially those that are related to the tcp state-transition diagram. we discuss many of the paws present in the tcp implementation of many widely used operating systems, such as sunos 4.1.3, svr4, and ultrix 4.3. we describe the corresponding tcp attack \"signatures\"(including the well-known 1994 christmas day mitnick attack) and provide recommendations to improve the security state of a tcp-based system, e.g., incorporation of a \"timer escape route\" from every tcp state.network security via reverse engineering of tcp code','Software security engineering'
'societal dependence on largescale, networkcentric systems continues to grow. but cost and complexity are increasing, and errors and vulnerabilities persist despite best efforts. evidence suggests that software engineering is reaching the limits of development technologies evolved in the first fifty years of computing. a need exists to create a next-generation software engineering (ngse) for the next fifty years that will reduce cost and complexity for fast and correct development of the ultralargescale systems of the future. ngse is envisioned as a computational engineering discipline, largely enabled by theorybased, semanticsdirected automation that permits intellectual control at a scope and scale unattainable with present methods.next-generation software engineering introduction to minitrack','Software security engineering'
'the ultra-large-scale systems of the future require the transformation of software engineering into a computational discipline capable of fast and dependable software development. this paper discusses an emerging next-generation software engineering research area: function extraction (fx) technology for automated computation to the maximum extent possible of the behavior, correctness, and quality attributes of software components and their compositions into systems. an introduction to the mathematical foundations for computation of software behavior is provided, followed by an overview description of a rigorously designed experiment to quantify the potential for fx technology, and a discussion of a cert star*lab first application of fx technology to compute the behavior of code expressed in the intel assembly language instruction set.next-generation software engineering','Software security engineering'
'software product line engineering enables customization and reuse during the development of software intensive systems. a typical product line process consists of a domain engineering process and several application engineering processes. we use two different heuristics to characterize artifact redundancies in a product line system: 1) artifacts should not be developed redundantly across domain engineering and application engineering and 2) no two application engineering teams should develop same artifacts redundantly. to provide a formal basis for the heuristics, we derive consistency equations by using mathematical notations of sets. we also use these consistency equations to describe artifact redundancies that occur due to conflicts-of-interests between domain engineering and application engineering. in particular, conflicts-of interests during product line scoping, product instantiation and product line evolution are covered. furthermore, based on a literature review, we elicit several requirements to address the conflicts-of-interests and artifact redundancies.on the communication problem between domain engineering and application engineering','Software security engineering'
'how to design a security engineering process that can cope with the dynamic evolution of future internet scenarios and the rigidity of existing system engineering processes? the securechange approach is to orchestrate (as opposed to integrate) security and system engineering concerns by two types of relations between engineering processes: (i) vertical relations between successive security-related processes; and (ii) horizontal relations between mainstream system engineering processes and concurrent security-related processes. this approach can be extended to cover the complete system/ software lifecycle, from early security requirement elicitation to runtime configuration and monitoring, via high-level architecting, detailed design, development, integration and design-time testing. in this paper we illustrate the high-level scientific principles of the approach.orchestrating security and system engineering for evolving systems','Software security engineering'
'the link between security engineering and systems engineering exists at the earliest stage of systems development, and, as a whole, there is sufficient evidence to suggest the discipline of security engineering is sufficiently mature to support its implementation. however, there is little in the literature on the practical application of security engineering and even less empirical work grounded in adoption theory. in contrast, the body of knowledge on quality programs is quite extensive and includes general literature on quality models as well as adoption studies of their implementation. specific factors related to quality implementations are also well documented and generally well understood. this survey study clearly substantiates a connection between these quality factors and security engineering, provides the opportunity for further research on causal models, and supports the application of lessons learned from quality program efforts to the implementation of a security engineering methodology in system acquisition and developmentorganizational barriers to the implementation of security engineering','Software security engineering'
'software engineering evolved from a rigid process to a dynamic interplay of people (e.g., stakeholders or developers). organizational and social literature call this interplay an organizational social structure (oss). software practitioners still lack a systematic way to select, analyze, and support osss best fitting their problems (e.g., software development). we provide the state-of-the-art in osss, and discuss mechanisms to support oss-related decisions in software engineering (e.g., choosing the oss best fitting development scenarios). our data supports two conclusions. first, software engineering focused on building software using project teams alone, yet these are one of thirteen oss flavors from literature. second, an emerging oss should be further explored for software development: social networks. this article represents a first glimpse at oss-aware software engineering, that is, to engineer software using osss best fit for the problem.organizational social structures for software engineering','Software security engineering'
'every empire, after a period of rapid expansion, needs some time for consolidation or it risks disintegration. the expansion of software patterns has produced a large body of work that now needs organization. this article documents early efforts to consolidate and organize a subset of software patterns in the security domain. lessons learned through this process can help people trying to organize patterns for other domains. this article is part of the special issue on software patterns.organizing security patterns','Software security engineering'
'paranoid penguin: using iptables for local security','Software security engineering'
'patterns, teams and domain engineering','Software security engineering'
'context: making best use of the growing number of empirical studies in software engineering, for making decisions and formulating research questions, requires the ability to construct an objective summary of available research evidence. adopting a systematic approach to assessing and aggregating the outcomes from a set of empirical studies is also particularly important in software engineering, given that such studies may employ very different experimental forms and be undertaken in very different experimental contexts.objectives: to provide an introduction to the role, form and processes involved in performing systematic literature reviews. after the tutorial, participants should be able to read and use such reviews, and have gained the knowledge needed to conduct systematic reviews of their own.method: we will use a blend of information presentation (including some experiences of the problems that can arise in the software engineering domain), and also of interactive working, using review material prepared in advance.performing systematic literature reviews in software engineering','Software security engineering'
'pithy software engineering quotes','Software security engineering'
'reputation plays a critical role in managing trust in decentralized systems. quite a few reputation-based trust functions have been proposed in the literature for many different application domains. however, one cannot always obtain all information required by the trust evaluation process. for example, access control restrictions or high collect costs might limit the ability gather all required records. thus, one key question is how to analytically quantify the quality of scores computed using incomplete information. in this paper, we start a first effort to answer the above question by studying the following problem: given the existence of certain missing information, what are the worst and best trust scores (i.e., the bounds of trust) a target entity can be assigned? we formulate this problem based on a general model of reputation systems, and examine the monotonicity property of representative trust functions in the literature. we show that most existing trust functions are monotonic in terms of direct missing information about the target of a trust evaluation.poster','Software security engineering'
'preparing students for industry\'s software engineering needs','Software security engineering'
'as computing becomes pervasive, concerns about data protection have taken on new urgency. what makes securing digital data so difficult is that it is rarely static rather, data is manipulated by software, often in a net-worked environment.software is increasingly being distributed as mobile code in architecture-independent formats. using reverse engineering, malicious parties can steal the intellectual property associated with such code with relative ease. three promising techniques under development--tamper proofing, obfuscation, and watermarking--offer hope for providing more efficient and effective mechanisms for protecting software.preventing piracy, reverse engineering, and tampering','Software security engineering'
'problem orientation is gaining interest as a way of approaching the development of software intensive systems and yet a significant example that explores its use is missing from the literature. in this paper, we present the basic elements of problem oriented software engineering (pose) which aims to bring both non-formal and formal aspects of software development together in a single framework. we provide an example of a detailed and systematic pose development of a software problem, that of designing the controller for a package router. the problem is drawn from the literature, but the analysis presented here is new. the aim of the example is twofold: to illustrate the main aspects of pose and how it supports software engineering design, and to demonstrate how a non-trivial problem can be dealt with by the approach.problem oriented software engineering','Software security engineering'
'the purpose of this panel is to inform attendees about the current state of the accreditation of software engineering programs and licensing for software engineers in canada and the us. we also intend to stimulate discussion of the more controversial aspects of this topic.professional engineering and software engineering','Software security engineering'
'in order to play a bigger role in software engineering tools, static analysis techniques must take into account the specific needs of this application area, in particular in terms of interaction with the user and scalability. this new perspective requires a reexamination of the field of static program analysis both internally and in connection with related areas like theorem proving and debugging.program analysis for software engineering','Software security engineering'
'every communication system requiring security properties is certainly critical. in order to study the security of communication systems, we have developed a methodology for the application of the formal analysis techniques of communication protocols to the analysis of cryptographic ones. we have extended the design and analysis phases with security properties. our methodology uses a specification technique based on the hmsc/msc requirement languages, and translates it into a generic schema for the sdl specification language, which is used for the analysis. thus, the technique allows the specification of security protocols using a standard formal language and uses object-orientation for reusability purposes. the final goal is not only the formal specification of a security system, but to examine the possible attacks, and later use the specification in more complex systems.protocol engineering applied to formal analysis of security systems','Software security engineering'
'is software development really a form of engineering?or is it just some kind of elaborate craftsmanship? arewe just fooling ourselves thinking that we are doingengineering? if so, it is certainly not from lack of tryinghard over the last 20 years. but maybe we tackled theproblem from the wrong end: we tried to imposetechniques from other engineering disciplines ontosoftware development models without understanding thereal nature of software.this paper analyzes the similarities and thedifferences, and pinpoints the key discriminants (lack offundamental laws, technology churn, no manufacturingstage) that make software endeavours somewhat differentthan, for example, those in civil or mechanicalengineering. we look at a few recent developments thathelp fill the gaps: iterative development and model-drivendesign. and we use frank gero\'s function-behaviour-structureframe-work to help us contrast softwareengineering with other engineering disciplines.putting the \"engineering\" into \"software engineering\"','Software security engineering'
'models are frequently used for illustrations in software design documents. commonly they are used to show static structure and less often, external dynamic behavior. however, in software engineering, the lack of conceptual models often inhibits creativity and understanding, which may in turn lead to incomplete or poor design. this paper describes our experience using models for the architectural, conceptual and detailed design for software systems, identifies perceived weaknesses in traditional approaches and makes recommendations for future modeling tools and techniques.putting the \"engineering\" into software engineering with models','Software security engineering'
'we propose a set of quantitative metrics to empirically evaluate security quality levels on an open (software) engineering service bus (engsb) platform.quantitative software security measurement in an engineering service bus platform','Software security engineering'
'the xenon project is investigating the construction of a higher-assurance open source separation kernel based on the xen open source hypervisor. just as the xen open source hypervisor was initially developed from the open source linux operating system, by simplifying linux and modifying its design, the xenon separation kernel is being developed from xen. the primary goal of the xenon project is to investigate issues in creating an open source software product with higher security assurance than conventional open source software. the xenon project is also focused on (1) problems relating to separation kernels that support unmodified uninterpreted commercial off the shelf (cots) guests and (2) distinctions between these kinds of separation kernels and hypervisors. this paper explains the xenon project\'s approach to re-engineering xen\'s internal structure into a higher-assurance form. if conventional open source software cannot be brought into this form with moderate amounts of re-engineering then higher-assurance open source software is probably not practical. our results indicate that moderate amounts of re-engineering will be sufficient for all but a small part of the code. the remaining code is small enough to be addressed in a reasonable time, even though more effort is required.re-engineering xen internals for higher-assurance security','Software security engineering'
'this paper provides an introduction to the papers for the workshop on realising evidence-based software engineering.realising evidence-based software engineering','Software security engineering'
'adoption strategies for software product lines (spl) frequently involve bootstrapping existing products into a spl and extending an existing spl to encompass another product. one way to do that is to use program refactorings. however, the traditional notion of refactoring does not handle appropriately feature models (fm), nor transformations involving multiple instances of the same spl. for instance, it is not desirable to apply a refactoring into a spl and reduce its configurability. in this paper, we extend the traditional notion of refactoring to an spl context. besides refactoring programs, fms must also be refactored. we present a set of sound refactorings for fms. we evaluate this extended refactoring definition for spl in a real case study in the mobile games domain.refactoring product lines','Software security engineering'
'report on future software engineering standards direction','Software security engineering'
'large software systems are often deployed putting together many commercial-off-the-shelf software components (cots). the selection of the cots to be integrated is driven by the software system requirements. in this paper, we propose the recss method aimed at supporting requirements elicitation and analysis in the context of cots-based software systems. recss builds a goal model of the system environment which identifies the external elements that interact with it. next, the system is decomposed into actors for which iso/iec-based quality models are built. as part of the process, environmental and platform characteristics that influence the behaviour of the system are identified. last, the resulting artefacts are used to analyse and refine the system requirements.requirements engineering for cots-based software systems','Software security engineering'
'as for other information systems, service-oriented systems (soss) should be engineering to satisfy the requirements of its stakeholders. but, while requirements are understood in terms of, e.g., goals and assumptions, services are viewed in terms of, e.g., preconditions and effects. in order to reduce the gap between those two conceptualizations, we propose and discuss the relations between two ontologies: core and wsmo. our work lies in conceptual foundations, i.e., the two ontologies along with their bridge rules, on which to build methodologies for the requirements engineering of soss.requirements engineering for services','Software security engineering'
'requirements engineering','Software security engineering'
'in this paper, we review current requirements engineering (re) research and identify future research directions suggested by emerging software needs. first, we overview the state of the art in re research. the research is considered with respect to technologies developed to address specific requirements tasks, such as elicitation, modeling, and analysis. such a review enables us to identify mature areas of research, as well as areas that warrant further investigation. next, we review several strategies for performing and extending re research results, to help delineate the scope of future research directions. finally, we highlight what we consider to be the \"hot\" current and future research topics, which aim to address re needs for emerging systems of the future.research directions in requirements engineering','Software security engineering'
'software engineering should aspire to be a true engineering discipline. we have made good progress in some areas, but a number of aspects of practical engineering are under-represented in our research portfolio. we have been slow to move beyond well-delimited systems developed by professional programmers to systems integrated from multiple public sources that evolve in the hands of their users. we have focused on formal reasoning and systematic testing to the detriment of qualitative and incremental reasoning supporting cost-effective, rather than perfect solutions. we have been slow to codify our results into unified theories and practical reference material. to establish a true engineering discipline for software, we need to broaden our view of what constitutes a \"software system\" and we need to develop techniques that help to provide cost-effective quality despite associated uncertainties.research toward an engineering discipline for software','Software security engineering'
'in this paper, we present a framework for reverse engineeringallowing the integration and interaction of differentanalysis and visualization tools. the framework architecturethat we propose uses a dynamic type system to guaranteethe proper exchange of data between the tools and aset of wrapper classes to handle their communication. thisallows for an easy and secure integration of tools that haveoriginally not been designed to work together. in this sense,existing tools can be (re-)used and integrated. as a proofof concept we also present our own instantiation of the proposedframework architecture.reuse in reverse engineering','Software security engineering'
'reverse engineering and reengineering','Software security engineering'
'feature models describe the common and variable characteristics of a product line. their advantages are well recognized in product line methods. unfortunately, creating a feature model for an existing project is time-consuming and requires substantial effort from a modeler. we present procedures for reverse engineering feature models based on a crucial heuristic for identifying parents - the major challenge of this task. we also automatically recover constructs such as feature groups, mandatory features, and implies/excludes edges. we evaluate the technique on two large-scale software product lines with existing reference feature models--the linux and ecos kernels--and freebsd, a project without a feature model. our heuristic is effective across all three projects by ranking the correct parent among the top results for a vast majority of features. the procedures effectively reduce the information a modeler has to consider from thousands of choices to typically five or less.reverse engineering feature models','Software security engineering'
'reverse engineering is reverse forward engineering','Software security engineering'
'during software evolution, programmers devote most of their effort to the understanding of the structure and behavior of the system. for object-oriented code, this might be particularly hard, when multiple, scattered objects contribute to the same function. design views offer an invaluable help, but they are often not aligned with the code, when they are not missing at all.this tutorial describes some of the most advanced techniques that can be employed to reverse engineer several design views from the source code. the recovered diagrams, represented in uml (unified modeling language), include class, object, interaction (collaboration and sequence), state and package diagrams. a unifying static code analysis framework used by most of the involved algorithms is presented at the beginning of the tutorial. a single running example is referred all over the presentation. trade-offs (e.g., static vs. dynamic analysis), limitations and expected benefits are also discussed.reverse engineering of object oriented code','Software security engineering'
'reverse engineering of software','Software security engineering'
'this is a questionnaire on program understanding and reverse engineering. it may be filled out manually or on-line. the results of the questionnaire will be used to guide the research of the two authors, both of whom are ph.d. students working in this area. copies of the resulting report will be mailed to all who participate, and a summary of the results will be published in an appropriate forum.reverse engineering questionnaire','Software security engineering'
'a great number of existing xml documents in various domain such as electrical business have to be maintained in order to constantly adapt to a dynamically changing environment to keep pace with business needs. a dtd or xml schema in its current textual form commonly lacks clarity and readability, which makes the maintenance process tedious and error-prone. this paper presents an approach to reverse engineering the xml documents to conceptual model, which makes the xml documents more close to real world and business needs, let the designers quickly gain a picture of the overall structure of xml documents in order to improve its quality, increase the maintainability and reusability. in this paper, the conceptual model is described by uml class diagram, a three-level model is defined, and a novel approach for extracting various structure and semantic information from existing dtd is given, especially the inheritance structure can be inferred from the dtd structure.reverse engineering xml','Software security engineering'
'reverse engineering, re-engineering, and conversion','Software security engineering'
'reverse engineering: progress along many dimensions','Software security engineering'
'review of security engineering','Software security engineering'
'currently, 3d rendering is accessible within web browsers through open standards such as webgl, x3d, and x3dom. at the same time, there is wealth of mature desktop software which comprises algorithms, data structures, user interfaces, databases, etc. it is a challenge to reuse such desktop software using the web visualization resources. in response to this challenge, this article presents a novel framework, called reweb3d, which minimizes the redevelopment for migration of existing 3d applications to the web. the redeployed application runs on a web server. reweb3d captures low-level graphic calls including geometry, texture, and shader programs. the captured content is then served as a webgl-enabled web page that conveys full interactivity to the client. by splitting the graphics pipeline between client and server, the workload can be balanced, and high-level implementation details and 3d content are hidden. the feasibility of reweb3d has been tested with applications which use openscenegraph as rendering platform. the approach shows good results for applications with large data sets (e.g. geodata), but is less suited for applications intensive in animations (e.g. games).reweb3d','Software security engineering'
'government and industry increasingly rely on modern information systems (is) for mission successes. but their critical is must survive in hostile environments; thus, mission owners need systems security engineers to build systems that are secure against real-world attacks but not over-engineered against a particular one. by understanding which attacks are most likely and which risks are most serious, mission owners can make cost-effective countermeasures decisions. we describe a systems security-engineering methodology for enumerating system attacks, assessing risks, and choosing countermeasures that best mitigate the risks.risk-based systems security engineering','Software security engineering'
'due to the spreading of sms services and appearing of new business models, value-added sms services have been introduced. according to the research results about wide distribution of security incidents on ict systems worldwide, in spite of known security solutions, there is a necessity for organizational approach to implement security. this paper presents research and development efforts in building process model securup for security requirements engineering conformed to rup framework. the model consists of processes, artifacts, activities and according roles for successful elicitation, analysis and specification of recognized security requirements and is validated on presented case study. the model validation results have shown significant process improvement, especially on roles and activities identification in securup elaboration process, but only further case studies in industry can be best indicators for usefulness of such models.rup-based process model for security requirements engineering in value-added service development','Software security engineering'
'this mini tutorial explains the concepts and process of scenario based requirements engineering. definitions of scenarios are reviewed, with their informal and more formal representations, and roles in the requirements process. the relationships between scenarios,specifications and prototypes is explored, and set in the perspective of human reasoning about requirements. methods for scenario based re are described and one method, scram, is covered in more depth. the tutorial concludes with a look forward to the future of scenariobased re and research directions.scenario-based requirements engineering','Software security engineering'
'science &amp; engineering','Software security engineering'
'security of network applications has become increasingly important in the past several years. syntax-based testing is a black box, data driven testing technique, for applications for which input can be described formally. scl is a component of protocol tester, a project at rmc and queen\'s, that uses syntax-based testing to evaluate the security of network applications. as a language, scl can describe the syntax and the semantic constraints of a given protocol, constraints that pertain to the testing of network application security. this paper describes how scl captures the input syntax of a network application including both syntax and semantic constraints. standard reverse engineering and program comprehension techniques are used to extract a detailed model from the description. this model can be used to automate the selection and generation of test cases in protocol tester.scl','Software security engineering'
'in the past five years there has been a dramatic increase in work on search-based software engineering (sbse), an approach to software engineering (se) in which search-based optimization (sbo) algorithms are used to address problems in se. sbse has been applied to problems throughout the se lifecycle, from requirements and project planning to maintenance and reengineering. the approach is attractive because it offers a suite of adaptive automated and semiautomated solutions in situations typified by large complex problem spaces with multiple competing and conflicting objectives. this article1 provides a review and classification of literature on sbse. the work identifies research trends and relationships between the techniques applied and the applications to which they have been applied and highlights gaps in the literature and avenues for further research.search-based software engineering','Software security engineering'
'we describe our success in defeating the security of an rfid device known as a digital signature transponder (dst). manufactured by texas instruments, dst (and variant) devices help secure millions of speedpasstm payment transponders and automobile ignition keys. our analysis of the dst involved three phases: 1. reverse engineering: starting from a rough published schematic, we determined the complete functional details of the cipher underpinning the challenge-response protocol in the dst. we accomplished this with only \"oracle\" or \"black-box\" access to an ordinary dst, that is, by experimental observation of responses output by the device. 2. key cracking: the key length for the dst is only 40 bits. with an array of of sixteen fpgas operating in parallel, we can recover a dst key in under an hour using two responses to arbitrary challenges. 3. simulation: given the key (and serial number) of a dst, we are able to simulate its rf output so as to spoof a reader. as validation of our results, we purchased gasoline at a service station and started an automobile using simulated dst devices. we accomplished all of these steps using inexpensive off-the-shelf equipment, and with minimal rf expertise. this suggests that an attacker with modest resources can emulate a target dst after brief short-range scanning or long-range eavesdropping across several authentication sessions. we conclude that the cryptographic protection afforded by the dst device is relatively weak.security analysis of a cryptographically-enabled rfid device','Software security engineering'
'online banking is one of the most sensitive tasks performed by general internet users. most traditional banks now offer online banking services, and strongly encourage customers to do online banking with \'peace of mind.\' although banks heavily advertise an apparent \'100\% online security guarantee,\' typically the fine print makes this conditional on users fulfilling certain security requirements. we examine some of these requirements as set by major canadian banks, in terms of security and usability. we opened personal checking accounts at the five largest canadian banks, and one online-only bank. we found that many security requirements are too difficult for regular users to follow, and believe that some marketing-related messages about safety and security actually mislead users. we are also interested in what kind of computer systems people really use for online banking, and whether users satisfy common online banking requirements. our survey of 123 technically advanced users from a university environment strongly supports our view of an emerging gap between banks\' expectations (or at least what their written customer policy agreements imply) and users\' actions related to security requirements of online banking. our participants, being more security-aware than the general population, arguably makes our results best-case regarding what can be expected from regular users. yet most participants failed to satisfy common security requirements, implying most online banking customers do not (or cannot) follow banks\' stated end-user security requirements and guidelines. the survey also sheds light on the security settings of systems used for sensitive online transactions. this work is intended to spur a discussion on real-world system security and user responsibilities, in a scenario where everyday users are heavily encouraged to perform critical tasks over the internet, despite the continuing absence of appropriate tools to do so.security and usability','Software security engineering'
'game theory has played an important role in security decisions. recent work using stackelberg games [fudenberg and tirole 1991] to model security domains has been particularly influential [basilico et al. 2009; kiekintveld et al. 2009; paruchuri et al. 2008; pita et al. 2008; pita et al. 2009]. in a stackelberg game, a leader (in this case the defender) acts first and commits to a randomized security policy. the follower (attacker) optimizes its reward considering the strategy chosen by the leader. these games are well-suited to representing the problem security forces face in allocating limited resources, such as officers, canine units, and checkpoints. in particular, the fact that the attacker is able to observe the policy reflects the way real terrorist organizations plan attacks using extensive surveillance and long planning cycles.security applications','Software security engineering'
'building secure systems is a difficult job for most engineers since it requires in-depth understanding of security aspects. this task, however, can be assisted by capturing security knowledge in a particular domain and reusing the knowledge when designing applications. we use this strategy and employ an information security ontology to represent the security knowledge. the ontology is associated with system designs which are modelled in collaborative building blocks specifying the behaviour of several entities. in this paper, we identify rules to be applied to the elements of collaborations in order to identify security assets present in the design. further, required protection mechanisms are determined by applying a reasoner to the ontology and the obtained assets. we exemplify our approach with a case study from the smart metering domain.security asset elicitation for collaborative models','Software security engineering'
'with the release of java 2 se (standard edition, also commonly known as jdk 1.2), java technology has matured such that it is increasingly deployed as part of the information infrastructure in today\'s economy and for mission-critical applications. these applications require a high degree of assurance of the underlying technologies, including jdk 1.2. this paper outlines the jdk 1.2 software development process and the special efforts to increase the quality assurance of the security features.security assurance efforts in engineering java 2 se (jdk 1.2)','Software security engineering'
'ensuring that software protects its users\' privacy has become an increasingly pressing challenge. requiring software to be certified with a secure type system is one enforcement mechanism. protecting privacy with type systems, however, has only been studied for programs written entirely in a single language, whereas software is frequently implemented using multiple languages specialized for different tasks. this paper presents an approach that facilitates reasoning over composed languages. it outlines sufficient requirements for the component languages to lift privacy guarantees of the component languages to well-typed composed programs, significantly lowering the burden necessary to certify that such composite programs safe. the approach relies on computability and security-level separability. this paper defines completeness with respect to secure computations and formally establishes conditions sufficient for a security-typed language to be complete. we demonstrate the applicability of the results with a case study of three seminal security-typed languages.security completeness','Software security engineering'
'this article considers the synthesis of two long-standing lines of research in computer security: security correctness for multilevel databases, and language-based security. the motivation is an approach to supporting end-to-end security for a wide class of enterprise applications, those of concurrent transactional applications. the approach extends nested transactions with retroactive abort, a new form of semantics for transactional execution, motivated by security concerns. a semantics is given in terms of a local constrained labelled transition system, the tauone calculus. this allows a noninterference result to be verified based on adapting results on observational equivalence from concurrency theory.security correctness for secure nested transactions','Software security engineering'
'this paper presents a security engineering process for the development of secure systems focusing on the specification and development of the set-top boxes. the paper describes the set-top box characteristics and functionalities and, using the process and its secure artefacts, models what we call a domain security metamodel that defines all the security properties of that domain and implements them using security building blocks. this security artefact can be used by system engineers when modelling their system model in order to fulfil its security requirements and, as a result, create a secure system that has security naturally integrated in its architecture and functionality.security engineering and modelling of set-top boxes','Software security engineering'
'as information security and privacy become increasingly important to organizations, the demand grows for software development processes that assure information integrity, availability, and confidentiality. unfortunately, despite the investments made in process improvement, there is still no guarantee that the developed software products are protected from attacks or do not present security vulnerabilities. as soon as software products continue to present security flaws and be compromised by attacks, the systems security engineering &#8211; capability maturity model (sse-cmm) becomes the de facto model to structure a software security approach. moreover, security best practices, practical experience or international standards, like iso/iec 15408, should also be considered to support security engineering as they propose activities that can be adapted to enhance security in a software development process and contribute towards the overall software security. this paper proposes a security engineering approach to support software security through a specialized process that helps develop more secure software, entitled process to support software security (psss). in addition, one of psss&#8217;s subprocess, model security threat, is explained in detail. this paper also presents the results of the case study when the psss was first applied in a software development project as well as the preliminary results of a large project implementation.security engineering approach to support software security','Software security engineering'
'security by design and secure engineering are among the most pressing challenges in it security research and practice. increased attacker potential and dependence on it-systems in economy and in critical infrastructures cause a higher demand in securely engineered systems and thus in new approaches and methodologies. this paper introduces a consistent methodology for designing secure systems during the specification phase. the security modeling framework semf serves as basis for its security vocabulary. we extend semf by the concept of semf building blocks sebbs as reasoning tool and provide a security design process utilizing them as refinement artifacts. this process guides the decision making during the system specification phase focused on the security aspects and integrates with refinement driven functional engineering processes. our approach further results in a security design documentation and residual assumptions that can serve as a basis for risk assessment, code review, and organizational security means during deployment.security engineering based on structured formal reasoning','Software security engineering'
'security is usually not in the main focus in the development of embedded systems. however, strongly interconnected embedded systems play vital roles in many everyday processes and also in industry and critical infrastructures. therefore, security engineering for embedded systems is a discipline that currently attracts more interest. this paper presents the vision of security engineering for embedded systems formulated by the fp7 project secfutur [1].security engineering for embedded systems','Software security engineering'
'security engineering in an evolutionary acquisition environment','Software security engineering'
'this paper answers the difficult problems that organizations face in business environments when they try to solve information security issues by suggesting the integrated methodology for security engineering. contributions of this paper are summarized as following. the first is the provision of requirements of security engineering methodology based on the model of ill-structured problem solving. the second is the framework which integrates various methods and tools of security engineering. the third is a suggestion of the process model and components which support an entire lifecycle of security management.security engineering methodology based on problem solving theory','Software security engineering'
'this paper describes an algebraic approach to the security engineering of lattice policies. the approach extends earlier lattice and algebraic work, and has two main goals. first, it seeks to model access control policies with anti-symmetry, reflexivity and transitivity exceptions using a lattice, and to propose an information flow security definition for the resulting set (pol) of policies. second, it supports a constructive approach to policy specification through an algebraic structure (pol, and, or, not, =, security engineering of lattice-based policies','Software security engineering'
'we present a method for security engineering, which is based on two special kinds of problem frames that serve to structure, characterize, analyze, and finally solve software development problems in the area of software and system security. both kinds of problem frames constitute patterns for representing security problems, variants of which occur frequently in practice. we present security problem frames, which are instantiated in the initial step of our method. they explicitly distinguish security problems from their solutions. to prepare the solution of the security problems in the next step, we employ concretized security problem frames capturing known approaches to achieve security. finally, the last step of our method results in a specification of the system to be implemented given by concrete security mechanisms and instantiated generic sequence diagrams. we illustrate our approach by the example of a secure remote display system.security engineering using problem frames','Software security engineering'
'security evaluation criteria','Software security engineering'
'as messaging middleware technology matures, users demand increasingly many features, leading to modular middleware architectures. however, extra complexity increases the risk of a security breach, arising from a vulnerability in one module or misconfiguration of the module linkages. this position paper presents a framework for enforcing security policies between middleware modules, which simultaneously facilitates co-design of application and middleware security. for example, a healthcare application might require (1) all clinical data to be encrypted in transit, (2) a log of all messages sent and delivered (revealing no disclosive patient information), and (3) parameterised role based access control on message delivery. in our framework, we can satisfy all of these requirements, even when each feature is implemented as a separate extension module: extensions tag events with meta-data, and this meta-data guides the enforcement of the security policy. exposing this meta-data to applications can help to unite application and middleware security policy.security for middleware extensions','Software security engineering'
'software engineering students need the ability to identify security concerns and then be able to design and build solutions that implement those security requirements. similarly, security students would benefit from knowledge of standard software engineering design practices. this paper discusses methods developed to address both concerns.security for software engineering education','Software security engineering'
'over the last fifteen years the world has experienced a wide variety of computer virus and general computer security problems, against which a wide variety of countermeasures have been deployed. the record tells us that what most strongly determines the size and nature of the worldwide virus and security problem is not any particular countermeasure or security technology, but rather the characteristics of the underlying platform (the operating system, macro execution environment, and so on). autonomic computing is a new effort, intended to make it systems more self-managing. since it undertakes to change the nature of the underlying computing platform, it is likely to change the size and nature of the world\'s virus and security problem as well. this paper briefly describes both the relevant historical record and some current architectural approaches to autonomic computing, and offers a few thoughts on how autonomic computing relates to computer security and virus prevention.security in autonomic computing','Software security engineering'
'we present a general methodology for integrating arbitrary security requirements in the development of business processes in a both elegant and rigorous way. we show how trust relationships between different parties and their respective security goals can be reflected in a specification, which results in a realistic modeling of business processes in the presence of malicious adversaries. special attention is given to the incorporation of cryptography in the development process with the main goal of achieving specifications that are sufficiently simple to be suited for formal verification, yet allow for a provably secure cryptographic implementation.security in business process engineering','Software security engineering'
'security modules: potent information security system components','Software security engineering'
'security has become a priority for software development and many security testing techniques have been developed over the years. benchmarks based on real-world systems, however, are in great demand for evaluating the vulnerability detection capability of these techniques. to develop such a benchmark, this paper presents an approach to security mutation analysis of filezilla server, a popular ftp server implementation as a case study. in the existing mutation testing research, mutants are created through syntactic changes. such syntactic changes may not result in meaningful security vulnerabilities in security-intensive software. our approach creates security mutants by considering the causes and consequences of vulnerabilities. the causes of vulnerabilities include design-level (e.g., incorrect policy enforcement) and implementation-level defects (such programming errors as buffer overflow and unsafe function calls). the consequences of vulnerabilities refer to various potential attacks, such as spoofing, tampering, repudiation, information disclosure, denial of service, and elevation of privilege (stride). using this approach, we have created 30 distinct mutants for filezilla server. they have been applied to the evaluation of two security testing methods that use attack trees and attack nets as threat models for test generation. the results show that, while these testing methods can kill most of the mutants, they have an important limitation -- they cannot detect the vulnerabilities that are not captured by the threat models.security mutation testing of the filezilla ftp server','Software security engineering'
'obviously, there is a need for automated information security analysis, validation, evaluation and testing approaches. unfortunately, there is no state-of-art approach to carrying out information security evaluation in a systematic way. information security evaluation of software-intensive and telecommunications systems typically relies heavily on the experience of the security professionals. requirements are within the focus of the information security evaluation process. information security requirements can be based on iterative risk, threat and vulnerability analyses, and technical and architectural information. there is a need for more practical ways to carry out this iterative process. in this paper we discuss security evaluation process, security objectives and security requirements from the basis of the experiences of a security testing project.security objectives within a security testing case study','Software security engineering'
'agent oriented software engineering and security patterns have been proposed as suitable paradigms for the development of secure information systems. however, so far, the proposed solutions are focused on one of these paradigms. in this paper we propose an agent oriented security pattern language and we discuss how it can be used together with the tropos methodology to develop secure information systems. we also present a formalisation of our pattern language using formal tropos. this allows us to gain a deeper understanding of the patterns and their relationships, and thus to assess the completeness of the language.security patterns meet agent oriented software engineering','Software security engineering'
'the successful design and implementation of secure systems must include security concerns from the beginning. a component that processes data at multiple security levels is critical and must go through additional evaluation to ensure the processing is secure. it is common practice to isolate and separate the processing of data at different levels into different components. in this paper we present policy-based architectural refinement techniques for the design of multi-level secure (mls) systems. in addition, a policy refinement language is proposed to specify the rules of refinement patterns, and the hierarchy of the refinement patterns is presented. we discuss which security policies must be satisfied through the refinement process, including when separation works and when it does not. the process oriented approach will lead to verified engineering techniques for the design of mls systems, which should greatly reduce the cost of certification of those systems.security policy refinement and enforcement for the design of multi-level secure systems','Software security engineering'
'requirements engineering, a vital component in successful project development, often neglects sufficient attention to security concerns. further, industry lacks a useful model for incorporating security requirements into project development. studies show that upfront attention to security saves the economy billions of dollars. industry is thus in need of a model to examine security and quality requirements in the development stages of the production lifecycle.in this paper, we examine a methodology for both eliciting and prioritizing security requirements on a development project within an organization. we present a model developed by the software engineering institute\'s networked systems survivability (nss) program, and then examine two case studies where the model was applied to a client system. the nss program continues to develop this useful model, which has proven effective in helping an organization understand its security posture.security quality requirements engineering (square) methodology','Software security engineering'
'to offer competitive products and services in the telecom business information security serves as an enabler and competitive factor. unfortunately, traditional risk analysis and security engineering methods have shown to suffer from several shortcomings when applied to the telecom business. to overcome these shortcomings we propose a security engineering method called skydd covering information, infrastructure, and business requirements based on information classification. the method uses a combination of reference tables and checklists and addresses many of the shortcomings of traditional methods. well-integrated in to the development process skydd has proven to simplify security requirement gathering, reduce lead times and provide consistent requirements across different projects and project organizations, much of this due to the fact that the method is designed to be used by non-security experts.security requirement engineering at a telecom provider','Software security engineering'
'software engineering curricula too often neglect the development of security requirements for software systems. as a consequence, programmers often produce buggy code with weak security measures. this report focuses on three case studies in which graduate students applied a novel security requirements engineering methodology to real-world software development projects. the experiences showed promise for curriculum integration in educating students about the importance of security requirements in software engineering, as well as how to develop such requirements.security requirements engineering for software systems','Software security engineering'
'context: the correct analysis and understanding of security requirements are important because they assist in the discovery of any security or requirement defects or mistakes during the early stages of development. security requirements engineering is therefore both a central task and a critical success factor in product line development owing to the complexity and extensive nature of software product lines (spl). however, most of the current spl practices in requirements engineering do not adequately address security requirements engineering. objective: the aim of this approach is to describe a holistic security requirements engineering framework with which to facilitate the development of secure spls and their derived products. it will conform with the most relevant security standards with regard to the management of security requirements, such as iso/iec 27001 and iso/iec 15408. results: this framework is composed of: a security requirements engineering process for spl (sreppline) driven by security standards; a security reference meta model to manage the variability of those spl artefacts related to security requirements; and a tool (srepplinetool) which implements the meta-model and supports the process. method: a complete explanation of the framework will be provided. the process will be formally specified with spem 2.0 and the repository will be formally specified with an xml grammar. the application of sreppline and srepplinetool will be illustrated through a description of a simple example as a preliminary validation. conclusion: although there have been several attempts to fill the gap between requirements engineering and spl requirements engineering, no systematic approach with which to define security quality requirements and to manage their variability and their related security artefacts in spl models is, as yet, available. the contribution of this work is that of providing a systematic approach for the management of the security requirements and their variability from the early stages of product line development in order to facilitate the conformance of spl products with the most relevant security standards.security requirements engineering framework for software product lines','Software security engineering'
'various governmental or academic institutes survey current security trends, and report vulnerabilities, security breaches, and their costs. however, it is unclear whether (and how) practitioners analyze these vulnerabilities and attacks to arrive at security requirements and decide on security solutions. what modeling methods are used for eliciting, analyzing, and documenting security requirements in real-world practice? this paper intends to answer such questions through a survey of security requirements engineering practices. 374 software professionals from 237 international and chinese firms participated in the survey. the results show businesses often try to consider security from early stages of the development life cycle, however, ultimately, security is left to be built into the system at the implementation phase. we observed that practitioners favour qualitative risk assessment rather than quantitative approaches, and this helps them consider more varieties of factors when comparing alternative security design solutions.security requirements engineering in the wild','Software security engineering'
'the majority of the current product line practices in requirements engineering do not adequately address security requirements engineering despite the fact that security requirements engineering is both a central task and a critical success factor in product line development due to the complexity and extensive nature of product lines. therefore, our contribution is to present and to demonstrate the applicability of our proposed security quality requirements engineering process (sreppline), which is based on a security requirements decision model driven by security standards along with a security variability model. we shall demonstrate our proposal by describing part of a real case study as a preliminary validation of these models. the final aim of this approach is to deal with security requirements variability from the early stages of the product line development in a systematic way, in order to facilitate conformance of the products with the most relevant security standards with regard to the management of security requirements, such as iso/iec 27001 and iso/iec 15408security requirements engineering process for software product lines','Software security engineering'
'this paper presents a framework for security requirements elicitation and analysis, based upon the construction of a context for the system, representation of security requirements as constraints, and satisfaction arguments for the requirements in the system context. the system context is described using a problem-centered notation, then is validated against the security requirements through construction of a satisfaction argument. the satisfaction argument is in two parts: a formal argument that the system can meet its security requirements, and a structured informal argument supporting the assumptions expressed in the formal argument. the construction of the satisfaction argument may fail, revealing either that the security requirement cannot be satisfied in the context, or that the context does not contain sufficient information to develop the argument. in this case, designers and architects are asked to provide additional design information to resolve the problems. we evaluate the framework by applying it to a security requirements analysis within an air traffic control technology evaluation project.security requirements engineering','Software security engineering'
'secure software development is one of the most information system issues that raised through the use of the internet and networked systems. the importance of developing secure software increases. in this work we present a process for the development of security critical software projects and an overview of some of the existing processes, standards, life cycle models that support the secure software development. it is a guide to the common body of knowledge for producing, acquiring, and sustaining secure software.security software engineering','Software security engineering'
'security solutions','Software security engineering'
'biologists have long recognized the dangers of the lack of diversity or monocultures in biological systems. recently, it has been noted that much of the fragility of our networked computing systems can be attributed to the lack of diversity or monoculture of our software systems. the problem is severe. because it is virtually inevitable that software will ship with flaws, our software monoculture leaves systems open to large-scale attacks by knowledgeable adversaries. inspired by the resilience of diverse biological systems, the authors developed the genesis software development toolchain. an innovative aspect of genesis is the use of an application-level virtual machine technology that enables the application of diversity transforms at any point in the software toolchain. using genesis, they authors demonstrated that diversity, when judiciously applied, is a practical and effective defense against two widely used types of attacks&#8212;return-to-libc and code injection.security through diversity','Software security engineering'
'researchers interested in security often wish to introduce new primitives into a language. extensible languages hold promise in such scenarios, but only if the extension mechanism is sufficiently safe and expressive. this paper describes several modifications to an extensible language motivated by end-to-end security concerns.security through extensible type systems','Software security engineering'
'security toolbox','Software security engineering'
'a good understanding of the impact of different types of bugs on various project aspects is essential to improve software quality research and practice. for instance, we would expect that security bugs are fixed faster than other types of bugs due to their critical nature. however, prior research has often treated all bugs as similar when studying various aspects of software quality (e.g., predicting the time to fix a bug), or has focused on one particular type of bug (e.g., security bugs) with little comparison to other types. in this paper, we study how different types of bugs (performance and security bugs) differ from each other and from the rest of the bugs in a software project. through a case study on the firefox project, we find that security bugs are fixed and triaged much faster, but are reopened and tossed more frequently. furthermore, we also find that security bugs involve more developers and impact more files in a project. our work is the first work to ever empirically study performance bugs and compare it to frequently-studied security bugs. our findings highlight the importance of considering the different types of bugs in software quality research and practice.security versus performance bugs','Software security engineering'
'this paper presents a study that uses extensive analysis of real security vulnerabilities to drive the development of: 1) runtime techniques for detection/masking of security attacks and 2) formal source code analysis methods to enable identification and removal of potential security vulnerabilities. the presentation will describe the hardware architecture of a reliability and security engine (rse) that embodies the proposed techniques, to provide run-time checking at the processor level.security vulnerabilities','Software security engineering'
'humans are \"smart components\" in a system, but cannot be directly programmed to perform; rather, their autonomy must be respected as a design constraint and incentives provided to induce desired behavior. sometimes these incentives are properly aligned, and the humans don\'t represent a vulnerability. but often, a misalignment of incentives causes a weakness in the system that can be exploited by clever attackers. incentive-centered design tools help us understand these problems, and provide design principles to alleviate them. we describe incentive-centered design and some tools it provides. we provide a number of examples of security problems for which incentive centered design might be helpful. we elaborate with a general screening model that offers strong design principles for a class of security problems.security when people matter','Software security engineering'
'security-oriented program transformations to cure integer overflow vulnerabilities','Software security engineering'
'software security attacks represent an ever growing problem. one way to make software more secure is to use inlined reference monitors (irms), which allow security specifications to be inlined inside a target program to ensure its compliance with the desired security specifications. the irm approach has been developed primarily by the security community. runtime verification (rv), on the other hand, is a software engineering approach, which is intended to formally encode system specifications within a target program such that those specifications can be later enforced during the execution of the program. until now, the irm and rv approaches have lived separate lives; in particular rv techniques have not been applied to the security domain, being used instead to aid program correctness and testing. this paper discusses the usage of a formalism-generic rv system, javamop, as a means to specify irms, leveraging the careful engineering of the javamop system for ensuring secure operation of software in an efficient manner.security-policy monitoring and enforcement with javamop','Software security engineering'
security,'Software security engineering'
' as security requirements of software often change, developers may modify security policies such as access control policies (policies in short) according to evolving requirements. to increase confidence that the modification of policies is correct, developers conduct regression testing. however, rerunning all of existing system test cases could be costly and time-consuming. to address this issue, we develop a regression-test-selection approach, which selects every system test case that may reveal regression faults caused by policy changes. our evaluation results show that our test-selection approach reduces a substantial number of system test cases efficiently. selection of regression system tests for security policy evolution','Software security engineering'
'semi-automated detection of architectural threats for security testing','Software security engineering'
'sensible network security','Software security engineering'
'no secure network file system has ever grown to span the internet. existing systems all lack adequate key management for security at a global scale. given the diversity of the internet, any particular mechanism a file system employs to manage keys will fail to support many types of use.we propose separating key management from file system security, letting the world share a single global file system no matter how individuals manage keys. we present sfs, a secure file system that avoids internal key management. while other file systems need key management to map file names to encryption keys, sfs file names effectively contain public keys, making them self-certifying pathnames. key management in sfs occurs outside of the file system, in whatever procedure users choose to generate file names.self-certifying pathnames free sfs clients from any notion of administrative realm, making inter-realm file sharing trivial. they let users authenticate servers through a number of different techniques. the file namespace doubles as a key certification namespace, so that people can realize many key management schemes using only standard file utilities. finally, with self-certifying pathnames, people can bootstrap one key management mechanism using another. these properties make sfs more versatile than any file system with built-in key management.separating key management from file system security','Software security engineering'
'software engineering research has long borrowed and adapted ideas from other disciplines to adapt to the peculiar context of building software. that context is less and less peculiar, as automation and communication transform other fields, and it is time for us to consider how approaches developed in software engineering can be transferred and generalized to other fields. considering generalization of software engineering to domains outside computer science has implications for both software engineering research and education.sharing what we know about software engineering','Software security engineering'
'silver bullet security podcast series','Software security engineering'
'web services security is a major concern in the web engineering domain. several approaches emphasizing secure web services design (swsd) process have been proposed. however, they provide a little guidance as to what developers should do exactly to conduct this highly intellectual process. swsd practices cannot be defined independently of the situation in which they are applied. to address this challenge, we propose an approach for guiding the construction and execution of situational swsd methods by reengineering and integrating different existing swsd method components suited to the specific situation on hand.situational secure web services design methods','Software security engineering'
'this paper introduces a captcha based on upright orientation of line drawings rendered from 3d models. the models are selected from a large database, and images are rendered from random viewpoints, affording many different drawings from a single 3d model. the captcha presents the user with a set of images, and the user must choose an upright orientation for each image. this task generally requires understanding of the semantic content of the image, which is believed to be difficult for automatic algorithms. we describe a process called covert filtering whereby the image database can be continually refreshed with drawings that are known to have a high success rate for humans, by inserting randomly into the captcha new images to be evaluated. our analysis shows that covert filtering can ensure that captchas are likely to be solvable by humans while deterring attackers who wish to learn a portion of the database. we performed several user studies that evaluate how effectively people can solve the captcha. comparing these results to an attack based on machine learning, we find that humans possess a substantial performance advantage over computers.sketcha','Software security engineering'
'the key to maintaining the confidentiality, integrity, and availability of an organizations information and information systems is controlling who accesses what information. this is accomplished by being able to identify the requestor, verifying the requestor is not an impostor, and ensuring that the requestor has the proper level of clearance to access a given resource. there have always been those that attempt to by-pass this security mechanism through brute force or guile. in the past, those who use guile have been called confidence men and con artists. today, these people are called social engineers, but the tactics remain the same even if the objectives have changed.social engineering','Software security engineering'
'security requirements strongly influence the architectural design of complex it systems in a similar way as other non-functional requirements. both security engineering as well as software engineering provide methods to deal with such requirements. however, there is still a critical gap concerning the integration of the methods of these separate fields. in this paper we close this gap with respect to security requirements by proposing a method that combines software engineering approaches with state-of-the-art security engineering principles. this method establishes an explicit alignment between the non-functional goal, the principles in the field of security engineering, and the implementation of a security architecture. the method aims at designing a system\'s security architecture based on a small, precisely defined, and application-specific trusted computing base. we illustrate this method by means of a case study which describes distributed enterprise resource planning systems using web services to implement business processes across company boundaries.software architectural design meets security engineering','Software security engineering'
'software as engineering','Software security engineering'
'software documentation as an engineering process','Software security engineering'
'software engineering (part ii)','Software security engineering'
'this paper is an overview of software engineering 2004, the software engineering volume of the computing curricula 2001 project. we briefly describe the contents of the volume, the process used in developing the volume\'s guidelines, and how we expect the volume to be used in practice.software engineering 2004','Software security engineering'
'software engineering and knowledge engineering','Software security engineering'
'this panel discussion will address the need for modifying current software engineering courses to include additional security instruction and modifying software engineering curricula to include computer security courses. other options will be expressed by panel members which range from separate degree programs in information security to no change needed in current computer science curricula.software engineering and security engineering - an argument for merger','Software security engineering'
'we discuss two software engineering aspects in the development of complex swarm-based systems.software engineering and swarm-based systems','Software security engineering'
'software engineering environments','Software security engineering'
' since turing, we have wanted to use computers to store, process, and check mathematics. however even with the assistance of modern software tools, the formalization of research-level mathematics remains a daunting task, not least because of the talent with which working mathematicians combine diverse theories to achieve their ends. by drawing on tools and techniques from type theory, language design, and software engineering we captured enough of these practices to formalize the proof of the odd order theorem, a landmark result in group theory, which ultimately lead to the monumental classification of finite simple groups. this involved recasting the software component concept in the setting of higher-order, higher-kinded type theory to create a library of mathematical components covering most of the undergraduate algebra and graduate group theory syllabus. this library then allowed us to write a formal proof comparable in size and abstraction level to the 250-page textbook proof of the odd order theorem. software engineering for mathematics (keynote)','Software security engineering'
'the emergence of inexpensive parallel computers powered by multicore chips combined with stagnating clock rates raises new challenges for software engineering. as future performance improvements will not come \"for free\" from increased clock rates, performance critical applications will need to be parallelized. however, little is known about the engineering principles for parallel general-purpose applications. this paper presents an experience report with four diverse case studies on multicore software development for general-purpose applications. they were programmed in different languages and benchmarked on several multicore computers. empirical findings include: 1) multicore computers deliver: real speedups are achievable, albeit with significant programming effort and speedups that are typically lower than the number of cores employed; 2) massive refactoring of sequential programs is required, sometimes at several levels. special tools for parallelization refactorings appear to be an important area of research; 3) autotuning is indispensable, as manually tuning thread assignment, number of pipeline stages, size of data partitions and other parameters is difficult and error prone; 4) architectures that encompass several parallel components are poorly understood. tuneable architectural patterns with parallelism at several levels need to be discovered.software engineering for multicore systems','Software security engineering'
'software engineering for secure systems','Software security engineering'
'software engineering for security','Software security engineering'
'software engineering glossary','Software security engineering'
'software engineering has been a fundamental part of many computing undergraduate courses for a number of years. although many of the tools and techniques used to undertake software engineering have changed, the assessment has typically stayed the same. students are commonly tasked with producing a number of software artefacts, for example designs using the unified modelling language (uml). we recently attempted to extend the software engineering experience for a group of second year students with them participating in groups that attempt to replicate industrial practice. this paper reports our investigation into the correlation between the personality of group members, their approach with respect to using design patterns and their learning achievements.software engineering group work','Software security engineering'
'the paper attempts to portray the 1968 software scene, by recalling the principle technical issues and concerns of the time. these are discussed under the headings software as a commodity, programming languages, multiprogramming and time-sharing, modularity and structuring, and the problems of large systems. the paper ends with an account of the major debates at the first conference ever held on the subject of &#8220;software engineering&#8221;, the nato conference that took place in garmisch in october 1968.software engineering in 1968','Software security engineering'
'in this paper, we argue that the reality of today\'s software systems requires us to consider uncertainty as a first-class concern in the design, implementation, and deployment of those systems. we further argue that this induces a paradigm shift, and a number of research challenges that must be addressed.software engineering in an uncertain world','Software security engineering'
'this paper reflects on our work in deriving targeted methodologies to develop it applications and content in a developing world environment. this paper argues that a common thread over more than a decade of experience in building information and communication technology systems has been a community centred approach. we relate this to the african philosophy of ubuntu. these approaches are wrapped into an iterative action research paradigm to include the communities of users directly.software engineering in developing communities','Software security engineering'
'this paper provides an overview of important software engineering research issues related to the development of applications that run on mobile devices. among the topics are development processes, tools, user interface design, application portability, quality, and security.software engineering issues for mobile application development','Software security engineering'
'a framework for professionalism produced by the international federation for information processing is used to identify progress and challenges with regard to practice and education in software engineering. finally recommendations are made with respect to opportunities that could be relevant in east and southern europesoftware engineering practice and educationan international view','Software security engineering'
'this study presents a survey and an analysis of the literature on software engineering principles. the literature survey, covering a period of thirty years, has come up with 14 different papers and books, which have proposed a total of 313 distinct principles for software engineering. our analysis of these works is carried out based on a rigorous definition of the term \'principle\' and on the identification a set of explicit criteria to assess whether or not any of the proposed principles qualify as fundamental principles of software engineering. the analytical approach makes it possible to arrive at a set of 24 candidate software engineering principles which all meet the required criteria.software engineering principles','Software security engineering'
'the main theme of the 8th international conference on software engineering is the &#8220;establishment of a better understanding of the software process and its improvement through the provision of better models, methods and tools.&#8221; this paper addresses the second half of this theme by defining the issues involved with making strategic improvements to environments and suggesting a process for &#8220;provisioning&#8221; (or providing) better computer-aided software engineering methods and tools.the paper focuses primarily on the process for transforming existing, large to very large scale software development and support environments into conceptual &#8220;software factories.&#8221; to achieve this goal, arguments are made for strategically planning and developing well architected, modifiable software engineering environments. this is considered a prerequisite to effective &#8220;provisioning&#8221; of new and improved methods and tools.taken as a whole, the paper should be useful to software developers, tool providers and major software buyers alike. through typical environment evolution scenarios and concrete examples, many useful suggestions are provided to each of these players on how they can assist in the software engineering improvement process.software engineering provisioning process','Software security engineering'
'software engineering research agendas (panel session)','Software security engineering'
'although this is a talk about the design of predictive models to determine where faults are likely to be in the next release of a large software system, the primary focus of the talk is the process that was followed when doing this type of software engineering research. we follow the project from problem inception (cradle) to productization (grave), describing each of the intermediate stages to try to give a picture of why such research takes so long, and also why it is necessary to perform each of the steps.software engineering research','Software security engineering'
'for the fourth consecutive year, the national acm conference is giving attention to the field of software engineering. this year builds on what has gone before&#8212;&#8220;structured program planning and design: standardization needs&#8221; (acm \'79 - detroit), &#8220;more on structured design&#8221; (acm \'80 - nashville), and the &#8220;software engineering tutorial&#8221; (acm \'81 - los angeles)&#8212;providing for some measure of continuity. this session contains original papers stressing techniques for implementing reliable, maintainable, well-engineered software. presented are state-of-the-art applications of software engineering techniques addressing architectural, structural, behavioral, and informational considerations. the ariel pashtan paper &#8220;object oriented operating systems: an emerging design methodology&#8221; emphasizes architectural considerations. this survey analyzes eight major operating systems. these operating systems undergo a functional decomposition with a view toward the behavioral and informational requirements by way of the object model concept. pashtan shows us how object implementation techniques may be applied and leaves us to consider &#8220;whether \'thinking in terms of objects\' will become a standard design methology for operating systems&#8221;.software engineering session overview and introductory comments','Software security engineering'
'this paper outlines the content of the workshop keynote presentation on the use of theory derived from other disciplines in empirical software engineering research. this presentation uses three previous studies, two of which involved the author, to illustrate this usage and to explore the lessons learned from this work.software engineering theory and inter-disciplinary research','Software security engineering'
'the communications web site, http://cacm.acm.org, features more than a dozen bloggers in the blog@cacm community. in each issue of communications, we\'ll publish excerpts from selected posts.twitterfollow us on twitter at http://twitter.com/blogcacmgreg linden writes about frequent software deployments, ruben ortega reports on smartphones and health systems research, and jason hong discusses designing effective security warnings.software engineering, smartphones and health systems, and security warnings','Software security engineering'
'this note reviews aspects of professionalism as related to computers, questions the structuring of responsibilities in software development, and commends subsuming overspecialized branches of engineering, such as computer systems engineering and software engineering, within a more general discipline of data engineering.software engineering','Software security engineering'
'software process engineering','Software security engineering'
'maintenance is one of the important phases of the software development life cycle which contributes to the effective and long term use of any software system. it can become cumbersome and costly for software maintainers when subsystem boundaries are not clearly defined. further, the problem gets worse due to the system evolution, lack of current documentation and lack of original design documentation. the application of clustering techniques and tools helps software maintenance programmers to recover high-level views of system designs and hence leads to better understanding and maintenance of software systems. in this paper, we have used a sociopolitical imperialist competitive algorithm for software module clustering and compared it with existing evolutionary approaches. we conclude that the novel socio-political approach produces better quality clusters as compared to the earlier evolutionary genetic approaches.software re-engineering using imperialist competitive algorithm','Software security engineering'
'software reliability engineering is focused on engineering techniques for developing and maintaining software systems whose reliability can be quantitatively evaluated. in order to estimate as well as to predict the reliability of software systems, failure data need to be properly measured by various means during software development and operational phases. moreover, credible software reliability models are required to track underlying software failure processes for accurate reliability analysis and forecasting. although software reliability has remained an active research subject over the past 35 years, challenges and open questions still exist. in particular, vital future goals include the development of new software reliability engineering paradigms that take software architectures, testing techniques, and software failure manifestation mechanisms into consideration. in this paper, we review the history of software reliability engineering, the current trends and existing problems, and specific difficulties. possible future directions and promising research subjects in software reliability engineering are also addressed.software reliability engineering','Software security engineering'
'security vulnerabilities are increasingly due to software. while we focus much of our attention today on code-level vulnerabilities, such as buffer overflows, we should be paying more attention to design-level vulnerabilities. independently designed and implemented components may individually behave properly, but when put together, unanticipated interactions may occur. an unanticipated interaction between two software components is an opportunity for an attacker to exploit.software security','Software security engineering'
'software visualization is concerned with the static visualization as well as the animation of software artifacts, such as source code, executable programs, and the data they manipulate, and their attributes, such as size, complexity, or dependencies. software visualization techniques are widely used in the areas of software maintenance, reverse engineering, and re-engineering, where typically large amounts of complex data need to be understood and a high degree of interaction between software engineers and automatic analyses is required. this paper reports the results of a survey on the perspectives of 82 researchers in software maintenance, reverse engineering, and re-engineering on software visualization. it describes to which degree the researchers are involved in software visualization themselves, what is visualized and how, whether animation is frequently used, whether the researchers believe animation is useful at all, which automatic graph layouts are used if at all, whether the layout algorithms have deficiencies, and--last but not least--where the medium-term and long-term research in software visualization should be directed. the results of this survey help to ascertain the current role of software visualization in software engineering from the perspective of researchers in these domains and give hints on future research avenues.software visualization in software maintenance, reverse engineering, and re-engineering','Software security engineering'
'developing security-critical systems is difficult and there are many well-known examples of security weaknesses exploited in practice. thus a sound methodology supporting secure systems development is urgently needed.we present an extensible verification framework for verifying uml models for security requirements. in particular, it includes various plugins performing different security analyses on models of the security extension umlsec of uml. here, we concentrate on an automated theorem prover binding to verify security properties of umlsec models which make use of cryptography (such as cryptographic protocols). the work aims to contribute towards usage of uml for secure systems development in practice by offering automated analysis routines connected to popular case tools. we present an example of such an application where our approach found and corrected several serious design flaws in an industrial biometric authentication system.sound methods and effective tools for model-based security engineering with uml','Software security engineering'
'engineering involves choosing the right tool, which implies an understanding of both the tools and the problem. such behavior is agile.standards, agility, and engineering','Software security engineering'
'security engineering is considered to be a challenging task in order to build systems that remain dependable in the face of malice, error, or mischance. recent approaches propose the application of domain specific modeling languages (dsmls) in order to facilitate security engineering activities. to support the development and application of adequate dsmls, agile approaches and frameworks to provide appropriate tooling are needed. in this paper, we document our experiences developing modeling tools for two different dsmls in the domain of security engineering. we sketch the language and implementation requirements for our modeling tools, design and implementation considerations, and report on pitfalls and remaining issues with regard to the development of modeling tools based on our experiences.supporting security engineering at design time with adequate tooling','Software security engineering'
'assembling an information security management system according to the iso 27001 standard is difficult, because the standard provides only sparse support for system development and documentation. we analyse the iso 27001 standard to determine what techniques and documentation are necessary and instrumental to develop and document systems according to this standard. based on these insights, we inspect a number of current security requirements engineering approaches to evaluate whether and to what extent these approaches support iso 27001 system development and documentation. we re-use a conceptual framework originally developed for comparing security requirements engineering methods to relate important terms, techniques, and documentation artifacts of the security requirements engineering methods to the iso 27001.supporting the development and documentation of iso 27001 information security management systems through security requirements engineering approaches','Software security engineering'
'surfing the net for software engineering notes','Software security engineering'
'security requirements engineering is a new research area in software engineering, with the realization that security must be analyzed early during the requirements phase. many researchers are working in this area; however, there is a lack in security requirements treatment. the security requirements are one of the non-functional requirements, which act as constraints on functions of the system. organizations are depending on information systems for communicating and sharing information. thus, it security is becoming central in fulfilling business goals, to guard assets and to create trustworthy systems. to develop systems with adequate security features, it is essential to capture the security requirements. in this paper, we present a view on security requirements, issues, types, security requirements engineering (sre) and methods. we analyzed and compared different methods and found that square and security requirements engineering process methods cover most of the important activities of sre. the developers can adopt these sre methods and easily identify the security requirements for software systems.survey and analysis on security requirements engineering','Software security engineering'
'synthesizing the evidence from a set of studies that spans many countries and years, and that incorporates a wide variety of research methods and theoretical perspectives, is probably the single most challenging task of performing a systematic review. in this paper, we perform a tertiary review to assess the types and methods of research synthesis in systematic reviews in software engineering. almost half of the 31 studies included in our review did not contain any synthesis; of the ones that did, two thirds performed a narrative or a thematic synthesis. the results show that, despite the focus on systematic reviews, there is, currently, limited attention to research synthesis in software engineering. this needs to change and a repertoire of synthesis methods needs to be an integral part of systematic reviews to increase their significance and utility for research and practice.synthesizing evidence in software engineering research','Software security engineering'
'organizations\' integrate different systems and software applications in order to provide a complete set of services to their customers. however, different types of organisations are facing a common problem today, namely problems with security in their systems. the reason is that focus is on functionality rather than security. besides that, security, if considered, comes too late in the system and software engineering processes; often during design or implementation phase. moreover, majority of system engineers do not have knowledge in security. however, security experts are rarely involved in development process. thus, systems are not developed with security in mind, which usually lead to problems and security breaches. we propose an approach of integration security throughout engineering process. to assure that necessary actions concerning security have been taken during development process, we propose semi-automated preventive controls. system engineering security','Software security engineering'
'in 2004 kitchenham et al. first proposed the idea of evidence-based software engineering (ebse). ebse requires a systematic and unbiased method of aggregating empirical studies and has encouraged software engineering researches to undertake systematic literature reviews (slrs) of software engineering topics and research questions. as software engineers began to use the slr technology, they also began to comment on the slr process itself. brereton et al (2007) was one of the first papers that commented on issues connected with performing slrs and many such papers have followed since covering topics such as: the use of slrs in education; experiences of novices using slrs; the adoption of mapping and scoping studies; the repeatability of slrs; improving the search and selection processes; quality assessment of primary studies; improving aggregation processes. it therefore seems appropriate to identify the current status of such studies in software engineering, and identify whether there is evidence for revising and/or extending the guidelines for performing systematic literature reviews (kitchenham and charters, 2007). this keynote will report the current results of an ongoing systematic literature review that aims: a1: to identify and categorise papers investigating the slr process and the claims relating to that process; a2: to identify the extent to which the claims of repeatability, lack of bias, and openness are supported; a3: to identify any areas where current guidelines need to be amended or extended to reflect current knowledge of applying slrs in the context of software engineering.systematic review in software engineering','Software security engineering'
'systems engineering environments of atmosphere','Software security engineering'
'the engineering of systems in the 21st century demands robust use of the systems approach given the nature of our times, as well as the systems being created. the global marketplace, changing competition dynamics, shorter life cycles, and increasing complexity characterize our environment. we are building systems that are much larger than ever before. and, we are building systems that are infinitely smaller than ever before. maturity of technical, management, and infrastructure processes are competitive discriminators. systems engineering, both as a profession and as practiced by multi-discipline practitioners, is key to addressing these challenges.over the past decade, there have been frequent debates on whether systems engineering is an approach or a formal field of engineering. given the technical, management, and environmental challenges of this century, i believe that systems engineering must be an essential engineering discipline for the 21st century. this talk will discuss the state of the art and practice of systems engineering, and several initiatives focused on its evolution as a formal engineering discipline.while systems engineering approaches date back to ancient times, the recent few decades have largely featured practices and methods that extend from efforts of the 1950\'s, thus drawing heavily from hardware engineering. as software engineering has grown as a discipline, it has had significant influence on the field of systems engineering. further, as systems engineering becomes a more integral part of commercial product development, the character of the systems engineering discipline expands and the associated research agenda takes new shape. systems engineering and software engineering must each evolve as unique engineering disciplines to address the engineering problems of the 21st century. we must ensure their evolution results in shared knowledge, and highly collaborative approaches and methods drawing on the unique strengths of each discipline.systems engineering','Software security engineering'
'today\'s security standards aren\'t based in empirical success at securing systems but in the combined experience of successful security engineers. traditional systems-engineering approaches haven\'t until recently been systematically applied to security problems. these methods now show promise in shedding light on increasingly hard security problems. a systems-engineering security roadmap recommends that systems engineers and security engineers converge on empirical methods.systems security engineering','Software security engineering'
'while the trust paradigm is essential to broadly extend the communication between the environment\'s actors, the evaluation of trust becomes a challenge when confronted with initializing the trust relationship and validating the transitive propriety of trust. whether between users or between organizations, existing solutions work to create for peer to peer networks, flexible and decentralized security mechanisms with trust approach. however, we have noticed that the trust management systems do not make the most of the subjectivity, more specifically, the notion of disposition to trust although this aspect of subjectivity has a strong influence on how to assess direct and a transitive trust. for this reason in our study, we tackle this problem by introducing a new distributed trust model called t2d (trust to distrust) which is designed to incorporate the following contributions: (i) a behavior model which represents the disposition to trust; (ii) initialization of trust relationship (direct and transitive) according to the defined behavior model.t2d','Software security engineering'
'this paper details the validation of a comprehensive teaching model for security requirements engineering which ensures that security is built into the software from its inception. it centers on the employment of the square method for secure software requirements engineering, which was developed at carnegie mellon university. the effectiveness of the square method, its learning system and the initial results of using it in student case studies and in a practical, higher education classroom application are reported.teaching security requirements engineering using square','Software security engineering'
'while important efforts are dedicated to system functional testing, very few works study how to test specifically security mechanisms, implementing a security policy. this paper introduces security policy testing as a specific target for testing. we propose two strategies for producing security policy test cases, depending if they are built in complement of existing functional test cases or independently from them. indeed, any security policy is strongly connected to system functionality: testing functions includes exercising many security mechanisms. however, testing functionality does not intend at putting to the test security aspects. we thus propose test selection criteria to produce tests from a security policy. to quantify the effectiveness of a set of test cases to detect security policy flaws, we adapt mutation analysis and define security policy mutation operators. a library case study, a 3-tiers architecture, is used to obtain experimental trends. results confirm that security must become a specific target of testing to reach a satisfying level of confidence in security mechanisms.testing security policies','Software security engineering'
'the evolution of information engineering','Software security engineering'
'this paper argues that the essential point of software engineering is not to (semi-automatically) improve the quality of software, but rather to help human problem solvers to improve the quality of software. this paper argues that this essential point of software engineering research has been forgotten, and that without regaining that perspective, significant progress is not likely.the future of software engineering','Software security engineering'
'the importance of ignorance in requirements engineering','Software security engineering'
'columnist bruce schneier makes the case for the community needing to learn to talk about security from a nontechnical angle, especially for policy makers, who need to learn how to follow a logical approach instead of an emotional one&#x2014;an approach that includes threat modeling, failure analysis, searching for unintended consequences, and everything else in an engineer\'s approach to design.the importance of security engineering','Software security engineering'
'the logic of engineering design','Software security engineering'
'this paper is focussed on the notion of a formal model of security policy (fmsp). this kind of model is essential when reasoning about the security of information technology devices like a specific it-product or it-system. without an unambiguous definition of what security means, it is impossible to say whether a product is really secure.the new topicality of using formal models of security policy within the security engineering process','Software security engineering'
'the next generation of computer assistance for software engineering','Software security engineering'
'companies and organizations are faced with quite a tough competition and increasing regulatory and legal constraints. therefore, the use of security risk management is evolving and becoming more and more important in companies and organizations. we define engineering of security of information systems as a process whose aim is to guarantee the global security of information systems, in their eco-system in order to meet the stakes of companies. after our article focused on the encapsulation of security know-how into uml profiles, we focus this work on the presentation of the process of engineering of security into the formalism of business processes. the main idea is to succeed the adherence, of all stakeholders of the enterprise, into the security problem. to meet these pragmatic and actual needs of companies and organizations, we would suggest an approach to engineering of security, firstly, based on the standards and good practices of security and, secondly, inspired from the best practices and feedback of advances in the engineering of information systems. this paper shows the feasibility of mapping the process of engineering of security of information systems into the formalism of business process, and presents the concepts of engineering of security of information systems using the foundations and models of information systems engineering.the process of engineering of security of information systems (esis)','Software security engineering'
'with recent advances in electronic design files, computer aided design (cad) has become a common place to store the design knowledge. therefore, it is critical to secure the cad files so that such valuable intellectual property is not available to competitors by internal users. because most cad drawings are composed of a collection of files with various extensions, there exist problems associated with the processing speed and the accuracy of cad files encryption (decryption) using file based secure methods. in this study, an innovative idea of securing cad files based on the workplace against illegal piracy of design knowledge is presented. the proposed technology is to store all design files in the secure workplace which can be accessed by the authorized users and design applications only using application programming hooking at user level and system service table at kernel level. the technology is demonstrated in this paper using its implementation example in a civil engineering firm to verify it and cad files can be shared among users without a concern of its leakage to the competitors by internal user.the research of security system for sharing engineering drawings','Software security engineering'
'this workshop focuses on the concept of abstraction in software engineering. the aim is to explore the role of abstraction in dealing with complexity in the software engineering process, to discuss how the use of different levels of abstraction may facilitate performance of different activities, and to examine whether abstraction skills can be taught.the role of abstraction in software engineering','Software security engineering'
'i will use the rare opportunity of this keynote talk to give my perspective on the general state and future prospects for cyber security, and the consequences of this perspective with respect to cyber security research and education. the ambiguous status of computer science in modern academia has persisted through the thirty plus years of my career. does it belong in the college of science or the college of engineering? how about the college of business? is it worthy of a separate college of its own? i believe this ambiguity is a manifestation of the fundamental difference between computer science relative to traditional sciences and engineering disciplines. the forces of science, engineering and business come together and reconcile in a particularly unique way in computer science, and within computer science cyber security brings additional peculiarities to this reconciliation. my outlook on cyber security is generally optimistic. i believe at the consumer level market and social forces will drive developed societies to a relatively low assurance of security and privacy analogous to the current state of internet security. the large-scale adoption of internet services across diverse global populations is one indicator that the average consumer is reasonably comfortable with the collateral risks. but nothing is automatic, so social organization will be required to compensate for the intrusions of big government and big business which may turn out to be the much bigger problem than big crime. at the same time i share the concern of many senior national security officials and thought leaders on the increasingly grave threat of cyberwar and cyberterrorism. the us department of defense has publicly recognized cyberspace as a man-made domain on par with land, sea, air and space within which wars will be conducted and facilitated. many other nations and militaries are preparing offensive and defensive cyber capabilities. my talk will elaborate on these notions and seek to glean some lessons for cyber security researchers.the science, engineering and business of cyber security','Software security engineering'
'the security and software engineering research center (s2erc)','Software security engineering'
'the software engineering learning facility (self) is a web-based environment designed to enhance learning the art of software development. the system consists of three components. the practice component enables students to solve problems related to language constructs and algorithms. the process component guides students through a waterfall model of software development, emphasizing product development and verification. the performance component monitors progress and provides feedback for improving each student\'s personal software development process. this paper reports on the motivation and design of the system, the state of its implementation, and lessons learned.the software engineering learning facility','Software security engineering'
'in his 1960 essay, eugenewigner raised the question of &#8221;the unreasonable effectiveness of mathematics in natural sciences&#8221; [32]. after several decades of security research, we are tempted to ask the opposite question: are we not unreasonably ineffective? why are we not more secure from all the security technologies? i sketch a conceptual landscape of security that may provide some answers, on the background of ever increasing dynamics and pervasiveness of software and computation.the unreasonable ineffectiveness of security engineering','Software security engineering'
'in this paper, the author aim to present a threat and risk-driven methodology to security requirements engineering. the chosen approach has a strong focus on gathering, modeling, and analyzing the environment in which a secure ict-system to be built is located. the knowledge about the environment comprises threat and risk models. as presented in the paper, this security-relevant knowledge is used to assess the adequacy of security mechanisms, which are then selected to establish security requirements.threat and risk-driven security requirements engineering','Software security engineering'
'we present tool-support for checking uml models and c code against security requirements. a framework supports implementing verification routines, based on xmi output of the diagrams from uml case tools, and on control flow generated from the c code. the tool also supports weaving security aspects into the code generated from the models. advanced users can use this open-source framework to implement verification routines for the constraints of self-defined security requirements. we focus on a verification routine that automatically verifies crypto-based software for security requirements by using automated theorem provers.tools for model-based security engineering','Software security engineering'
'current software engineering practices have significant effects on the environment. examples include e-waste from computers made obsolete due to software upgrades, and changes in the power demands of new versions of software. sustainable software engineering aims to create reliable, long-lasting software that meets the needs of users while reducing environmental impacts. we conducted three related research efforts to explore this area. first, we investigated the extent to which users thought about the environmental impact of their software usage. second, we created a tool called greentracker, which measures the energy consumption of software in order to raise awareness about the environmental impact of software usage. finally, we explored the indirect environmental effects of software in order to understand how software affects sustainability beyond its own power consumption. the relationship between environmental sustainability and software engineering is complex; understanding both direct and indirect effects is critical to helping humans live more sustainably.toward sustainable software engineering (nier track)','Software security engineering'
'managing the security of information systems (sis) is at once a tedious task and the cornerstone of business in a highly competitive environment. successful control of the sis in a business requires many years of experience, expertise and continuous improvement. this implies real know-how for the employees who are responsible for sis control. this article focuses on the encapsulation of this know-how into uml models through profiles according to the meta-object facility (mof) standards from the object management group (omg). the main idea is to understand, manipulate and exploit this delicate and valuable know-how without being necessarily an expert on sis. this challenge is threefold: firstly, to find a common language, as well as approaches and tools for engineering of both is and sis; secondly, to generate earnings and make use of the enormous progress in engineering of is, to establish and improve engineering of sis; and thirdly, to achieve homogeneous management and follow-up of it projects and their security. this paper presents the context of the engineering of security of information systems, its importance and the feasibility of this challenge.toward the engineering of security of information systems (esis)','Software security engineering'
'security is a very important concern for software architecture and software components. previous modeling approaches provide insufficient support for an in-depth treatment of security. this paper argues for a more comprehensive treatment based on software connectors. connectors provide a suitable vehicle to model, capture, and enforce security. our approach models security principal, privilege, trust, and context of architectural constituents. extending our existing architecture description language and support tools, our approach can facilitate describing the security characteristics of an architecture generating enabling infrastructure, and monitoring run-time conformance. initial results of applying this approach are illustrated through a case study. the contribution of this research is a deeper and more comprehensive treatment of architectural security through software connectors.towards an architectural treatment of software security','Software security engineering'
'software engineering instructors face many challenges. among these challenges is the course project. instructors are required to train their students on the professional skills to be ready for the real world business, which requires the students to work on real projects. however, because of the low quality of the students\' work, not all of the professional organizations are cooperating to offer a chance for the software engineering students to work on a real project. therefore, most of the software engineering courses\' projects are in-class project, in which the instructors represent the clients. in this paper, i proposed a solution to this problem based on my experience in involving my software engineering students in real world projects.towards an effective software engineering course project','Software security engineering'
'despite a great deal of research in the area, a number of challenges still need to be faced before making agentbased computing a widely accepted paradigm in software engineering practice. in order to realize an engineering change in agent oriented software engineering: it\'s necessary to turn agent oriented software abstractions into practical tools for facing the complexity of modern application areas. the paper presents a universal development architecture for multi-agent system to provide straight connection between agent oriented analysis and software implementation. a detailed agent structure plays a key role in the process. the development process presented fulfills our requirements in the construction of c4i system.towards an engineering change in agent oriented software engineering','Software security engineering'
'the specifications of an application\'s security configuration are crucial for understanding its security policies, which can be very helpful in security-related contexts such as misconfiguration detection. such specifications, however, are often ill-documented, or even close because of the increasing use of graphic user interfaces to set program options. in this paper, we propose configre, a new technique for automatic reverse engineering of an application\'s access-control configurations. our approach first partitions a configuration input into fields, and then identifies the semantic relations among these fields and the roles they play in enforcing an access control policy. based upon such knowledge, configre automatically generates a specification language to describe the syntactic relations of these fields. the language can be converted into a scanner using standard parser generators for scanning configuration files and discovering the security policies specified in an application. we implemented configre in our research and evaluated it against real applications. the experiment results demonstrate the efficacy of our approach.towards automatic reverse engineering of software security configurations','Software security engineering'
'the aggregation of studies is of growing interest for the empirical software engineering community, since the numbers of studies steadily grow. in this paper we discuss challenges with the aggregation of studies into a common body of knowledge, based on a quantitative and qualitative evaluation of experience from the experimental software engineering network, esernet. challenges are that the number of studies available is usually low, and the studies that exist are often too scattered and diverse to allow systematic aggregation as a means for generating evidence. esernet therefore attempted to coordinate studies and thus create research synergies to achieve a sufficiently large number of comparable studies to allow for aggregation; however, the coordination approach of esernet proved to be insufficient. based on some lessons learned from esernet, a four-step procedure for evolving empirical software engineering towards the generation of evidence is proposed. this consists of (1) developing a methodology for aggregating different kinds of empirical results, (2) establishing guidelines for performing, analyzing, and reporting studies as well as for aggregating the results for every kind of empirical study, (3) extract evidence, that is, apply the methodology to different areas of software engineering, and (4) package the extracted evidence into guidelines for practice.towards evidence in software engineering','Software security engineering'
'nowadays, computer systems consist of many components such as servers and clients, protocols, services, and so on. systems connected to network have become more complex, with research focused on performance and efficiency. while most of the attention in system security has been paid to encryption technology and protocols for securing data transactions, a weakness (security hole) in any component may comprise the whole system. security engineering is needed for eliminating such holes. this paper outlines some novel challenges of security engineering, as well as their relations to other areas of scientific research.towards new areas of security engineering','Software security engineering'
'building realistic end user scenarios for ubiquitous computing applications entails large up-front investments. many context adaptive applications so far fail to live up to their expectations. firstly, this is due to poorly conceived development tools and methods compared to other, more mature domains. and secondly, they seem to be particularly prone to problems related to a discrepancy between user expectation and systems behavior. this unwanted behavior prevents the vision of an emerging trend of context aware and adaptive applications in ubiquitous computing to become reality. a good understanding of business and customer\'s requirements may be of immense importance. this paper presents a model-based requirements engineering approach to systematically analyze and specify the basic system behavior as well as the adaptation behavior starting from customer and business needs.towards requirements engineering for context adaptive systems','Software security engineering'
'security assurance is a property that ensures that the application code behaves consistently with the access control policy specified at the design level. security assurance proofs are valid as long as software engineers do not modify the generated code. this assumption does not hold in round-trip engineering, since programmers may modify the generated code and the models are automatically re-generated. this paper proposes a round-trip engineering approach for access control that preserves security assurance both when generating code from models and vice versa. the approach is to extend programming languages@? typing mechanisms with additional rules that ensure consistency between models and code, even when code is arbitrarily modified by programmers. this paper presents a formal description of the solution and an initial sketch of the required proofs of correctness. ongoing work is the development of a prototype to automate most of the process and its validation in a case study.towards security assurance in round-trip engineering','Software security engineering'
'runtime monitoring is performed during system execution to detect whether the system\'s behaviour deviates from that described by requirements. to support this activity we have developed a monitoring framework that expresses the requirements to be monitored in event calculus - a formal temporal first order language. following an investigation of how this framework could be used to monitor security requirements, in this paper we propose patterns for expressing three basic types of such requirements, namely confidentiality, integrity and availability. these patterns aim to ease the task of specifying confidentiality, integrity and availability requirements in monitorable forms by non-expert users. the paper illustrates the use of these patterns using examples of an industrial case study.towards security monitoring patterns','Software security engineering'
'cross site scripting is considered the major threat to the security of web applications. removing vulnerabilities from existing web applications is a manual expensive task that would benefit from some level of automatic assistance. static analysis represents a valuable support for security review, by suggesting candidate vulnerable points to be checked manually. however, potential benefits are quite limited when too many false positives, safe portions of code classified as vulnerable, are reported. in this paper, we present a preliminary investigation on the integration of static analysis with genetic algorithms. we show that this approach can suggest candidate false positives reported by static analysis and provide input vectors that expose actual vulnerabilities, to be used as test cases in security testing.towards security testing with taint analysis and genetic algorithms','Software security engineering'
'software tools processing partially common set of data should share an understanding of what these data mean. since ontologies have been used to express formally a shared understanding of information, we argue that they are a way towards semantic sees. in this paper we discuss an ontology-based approach to tool integration and present ode, an ontology-based see.towards semantic software engineering environments','Software security engineering'
'there has been some research conducted around the motivation for the use of twitter and the value brought by micro-blogging tools to individuals and business environments. this paper builds on our understanding of how the phenomenon affects the population which birthed the technology: software engineers. we find that the software engineering community extensively leverages twitter\'s capabilities for conversation and information sharing and that use of the tool is notably different between distinct software engineering groups. our work exposes topics for future research and outlines some of the challenges in exploring this type of data.towards understanding twitter use in software engineering','Software security engineering'
'security has become a primary and prevalent concern for software systems. the past decade has witnessed a tremendous increase in not only the sheer number of attacks but also the ease with which attacks can be performed on systems. in this paper we exemplify the usage of a novel technique for developing security requirements, by demonstrating each step in the technique when applied to an example usage scenario. furthermore, this new technique also provides support for deriving testing artifacts from the specified security requirements. we believe that in order to protect a system against harm (intended or not), attention must be given to its requirements. similar to other system properties and quality attributes, security must be considered at the requirements.towards usable cyber security requirements','Software security engineering'
'in 1973, john reynold\'s and james morris\' gedanken language retrofit object-capability security into an algol-like base. today, there are active projects retrofitting java, javascript, python, mozart/oz, ocaml, perl, and pict. these represent a variety of approaches, with different tradeoffs regarding legacy compatibility, safety, and expressivity. in this talk i propose a taxonomy of these approaches, and discuss some of the lessons learned to date.tradeoffs in retrofitting security','Software security engineering'
'information systems designers have been increasingly convinced about the importance of dealing with quality issues at early stages of development. over the landscape of quality issues, several proposals have been published as to help with respect to security. on the other hand, designers do also need to care about other quality issues; for instance, transparency. transparency is the quality of having open information to the public. at first, the general intuition is that security and transparency conflict, but how should designers deal with these antagonistic issues? departing from the use of the non-functional requirements framework we propose a process, based on personal construct theory, to perform early analysis of antagonistic design issues. having early analysis of antagonistic quality issues makes it possible for informed decision to be taken early on during is design. we use the election domain to illustrate the application of our proposal.transparency versus security','Software security engineering'
'security protocols form a central part of the trust infrastructure of the online world. they allow principals to make decisions that authorize or prohibit actions of other principals, and to make those decisions based on information gathered from other principals. in this invited talk at socio-technical aspects in security and trust 2012 (stast), i described a view of protocol design that can serve as a trust infrastructure.trust engineering via security protocols','Software security engineering'
'security requirements often have implicit assumptions about trust relationships among actors. the more actors trust each other, the less stringent the security requirements are likely to be. trust always involves the risk of mistrust; hence, trust implies a trade-off: gaining some benefits from depending on a second party in trade for getting exposed to security and privacy risks. when trust assumptions are implicit, these trust trade-offs are made implicitly and in an ad-hoc way. by taking advantage of agent- and goal-oriented analysis, we propose a method for discovering trade-offs that trust relationships bring. this method aims to help the analyst select among alternative dependency relationships by making explicit trust trade-offs. we propose a simple algorithm for making the trade-offs in a way that reaches a balance between costs and benefits.trust trade-off analysis for security requirements engineering','Software security engineering'
'pervasive computing systems require a security architecture based on trust rather than just user authentication and access control.trust-based security in pervasive computing environments','Software security engineering'
'software clone detection techniques identify fragments of code that share some level of syntactic similarity. in this study, we investigate security-sensitive clone clusters: clusters of syntactically similar fragments of code that are protected by some privileges. from a security perspective, security-sensitive clone clusters can help reason about the implemented security model: given syntactically similar fragments of code, it is expected that they are protected by similar privileges. we hypothesize that clones that violate this assumption, defined as security-discordant clones, are likely to reveal weaknesses and flaws in access control models. in order to characterize security-discordant clones, we investigated two of the largest and most popular open-source php applications: joomla! and moodle, with sizes ranging from hundred thousands to more than a million lines of code. investigation of security-discordant clone clusters in these systems revealed several previously undocumented, recurring, and application-independent security weaknesses. moreover, security-discordant clones also revealed four, previously unreported, security flaws. results also show how these flaws were revealed through the investigation of as little as 2\% of the code base. distribution of weaknesses and flaws between the two systems is investigated and discussed. potential extensions to this exploratory work are also presented.uncovering access control weaknesses and flaws with security-discordant software clones','Software security engineering'
'recently, both miami university of ohio and the rochester institute of technology received nsf grants to develop undergraduate software engineering laboratories. while the general goals of the labs are similar, specific hardware and software selections were driven by differing curricular emphases and educational environments at the two institutions. this paper presents the distinctive characteristics of each school\'s program, discusses the influence of these features on the selection process, and describes our experiences to date with the resulting labs. the goal is to provide useful information guidance to others considering such facilities.undergraduate software engineering laboratories','Software security engineering'
'engineering and reusing feedback control systems face challenging issues, such as structuring control loops to allow for fine-grained reasoning about their architecture. we propose a model-driven approach in which all major parts of the feedback control are uniformly designed as first-class adaptive elements. expected properties of the approach are discussed and illustrated on a real scenario of overload control in a grid middleware.uniform and model-driven engineering of feedback control systems','Software security engineering'
'uniquest: determining the semantics of complex uniqueness constraints','Software security engineering'
'unix system security issues','Software security engineering'
'extending requirements engineering modelling and formal analysis methodologies to cope with security requirements has been a major effort in the past decade. yet, only few works describe complex case studies that show the ability of the informal and formal approaches to cope with the level complexity required by compliance with iso-17799 security management requirements. in this paper we present a comprehensive case study of the application of the secure tropos re methodology for compliance to the italian legislation on privacy and data protection by the university of trento, leading to the definition and analysis of a iso-17799-like security management scheme.using a security requirements engineering methodology in practice','Software security engineering'
'web service composition languages promise a cheap and effectivemeans for application integration over the internet as in typical b2binteraction scenarios. bpel is the upcoming standard for web servicecomposition and several implementations of it are already available. however, for web service composition languages to keep theirpromises it is essential to provide more support for security. companieswill embrace web service composition languages only if theirrequirements of confidentiality, integrity, authentication, etc. are fulfilled. in this paper, we look at security in web services compositions andpresent a framework for securing bpel compositions using ws-securityand ws-policy. the main components of our framework are theprocess container implemented by a set of aspects in ao4bpel, anaspectoriented extension to bpel, the security service and the deployment descriptor. we also introduce the notion of policy-basedprocess deployment to check the compatibility of the security policiesof the composition and its partners at deployment time.using aspects for security engineering of web service compositions','Software security engineering'
'existing security testing techniques often fail to reveal critical security threats, partly because testers focus on testing known and expected behaviours, and consequently, ignore testing for unspecified behaviours that are frequently targeted by attackers. the novel contribution of this paper is an exploratory example of the use of implied scenarios detection to the problem of security testing. implied scenarios arise when the desired global behaviour is implemented component-wise. these scenarios can have security consequences on the system, and thus provide useful feedback for the security posture of the system. we introduce the application of implied scenario detection for security testing to reveal unexpected interactions between system components. we motivate its need by drawing on the limitations of existing work on testing for security. we adapt a model-driven approach to guide the testing process. we use an example to illustrate the feasibility and the applicability of the suggestion, and for evaluating its potential benefits.using implied scenarios in security testing','Software security engineering'
'the software engineering institute (sei) has participated in several projects1 in which the focus was on helping contractors make use of good software engineering methods and ada. during this participation, we have learned several important lessons about the development of software for both large-scale and embedded systems. we have noticed that after a long period of time where the focus on productivity generated searches for new methodologies, tools, and ways to write reusable software, the emphasis has shifted to quality, in recognition of the fact that the new methods and tools were not adequate to address the problems occurring at the design level. we propose that the industry instead concentrate the search for the old methods still in use in the other branches of engineering, and apply those methods to the software problem.using models in software engineering','Software security engineering'
'traditionally, a security policy is defined from an informal set of requirements, generally written using natural language. it is then difficult to appreciate the compatibility degree of the manually generated security policy with the informal requirements definition. the idea of this paper is to automate the process of deriving the formal security policy, using a more structured specification of the security objectives issued by the administrator of the information system to be secured. we chose the goal-oriented methodology kaos to express the functional objectives, then based on the results of a risk analysis, we integrate the security objectives to the obtained kaos framework. finally, through a process of transformation applied to this structured security objectives specification, we automatically generate the corresponding security policy. this policy is consistent with the access control model orbac (organization access control).using requirements engineering in an automatic security policy derivation process','Software security engineering'
'an iso 27001 compliant information securitymanagement system is difficult to create, due to the the limitedsupport for system development and documentation providedin the standard. we present a structured analysis of the documentationand development requirements in the iso 27001 standard. moreover, we investigate to what extent existing securityrequirements engineering approaches fulfill these requirements. we developed relations between these approaches and theiso 27001 standard using a conceptual framework originallydeveloped for comparing security requirements engineeringmethods. the relations include comparisons of importantterms, techniques, and documentation artifacts. in addition, we show practical applications of our results.using security requirements engineering approaches to support iso 27001 information security management systems development and documentation','Software security engineering'
'empirical studies that use software repository artifacts have become popular in the last decade due to the ready availability of open source project archives. in this paper, we survey empirical studies in the last three years of icse and fse proceedings, and categorize these studies in terms of open source projects vs. proprietary source projects and the diversity of subject programs used in these studies. our survey has shown that almost half (49\%) of recent empirical studies used solely open source projects. existing studies either draw general conclusions from these results or explicitly disclaim any conclusions that can extend beyond specific subject software. we conclude that researchers in empirical software engineering must consider the external validity concerns that arise from using only several well-known open source software projects, and that discussion of data source selection is an important discussion topic in software engineering research. furthermore, we propose a community research infrastructure for software repository benchmarks and sharing the empirical analysis results, in order to address external validity concerns and to raise the bar for empirical software engineering research that analyzes software artifacts.validity concerns in software engineering research','Software security engineering'
'value-based software engineering (vbse)','Software security engineering'
'security for applications running on mobile devices is important. in this paper we present expressos, a new os for enabling high-assurance applications to run on commodity mobile devices securely. our main contributions are a new os architecture and our use of formal methods for proving key security invariants about our implementation. in our use of formal methods, we focus solely on proving that our os implements our security invariants correctly, rather than striving for full functional correctness, requiring significantly less verification effort while still proving the security relevant aspects of our system. we built expressos, analyzed its security, and tested its performance. our evaluation shows that the performance of expressos is comparable to an android-based system. in one test, we ran the same web browser on expressos and on an android-based system, and found that expressos adds 16\% overhead on average to the page load latency time for nine popular web sites.verifying security invariants in expressos','Software security engineering'
'virtualization is becoming increasingly popular because it helps organizations optimize their hardware utilization. however, the technology has raised numerous security concerns.virtualization sparks security concerns','Software security engineering'
'visual software engineering with rules','Software security engineering'
'defining constraints at the business process level is an often demanded feature. our approach guides a business user in the analysis of threats to resources used in a business process, and provides the means to specify appropriate controls on the identified threats. these controls are of a highly visual nature and address both safety as well as security concerns.visualizing security in business processes','Software security engineering'
'w2k security','Software security engineering'
'integrating security throughout the life cycle can improve overall web application security. with adetailed review of the steps involved in applying security-specific activities throughout the softwaredevelopment life cycle, the author walks practitioners through effective, efficient application design,development, and testing.web application security engineering','Software security engineering'
'security is an elusive target in today\'s high-speed and extremely complex, web enabled, information rich business environment. this paper presents the idea that there are essential, basic organizational elements that need to be identified, defined and addressed before examining security aspects of a web engineering development process. these elements are derived from empirical evidence based on a web survey and supporting literature. this paper makes two contributions. the first contribution is the identification of the web engineering specific elements that need to be acknowledged and resolved prior to the assessment of a web engineering process from a security perspective. the second contribution is that these elements can be used to help guide security improvement initiatives in web engineering.web engineering security','Software security engineering'
'although security requirements analysis plays a very significant role in secure software development, it is difficult since it requires much security expertise and man-power. plain and practical security requirements patterns are needed. we have presented a visualized analysis approach for eliciting security requirements by extending misuse cases, and found that some of its results can be pattern candidates. this paper proposes 8 new web security requirements patterns with our analysis approach. the proposed patterns give analysts a way to find a proper pattern for a specific security goal. they are related to security solutions, and also produce some security design possibilities. we have applied these patterns to some case studies and evaluated that they are effective for web security analysis.web security patterns for analysis and design','Software security engineering'
'web services (ws hereafter) security is a crucial aspect for technologies based on this paradigm to be completely adopted by the industry. as a consequence, a lot of initiativesof initiatives have arisen during the last years setting as their main purpose the standardization of the security factors related to this paradigm. in fact, over the past years, the most important consortiums ofof internet internet, like ietf, w3c or oasis, are producing a huge number of ws-based security standards. despite of this growing, there\'s not exist yet a process that guides developers in the critical task of integrating security within all the stages of the development\'s life cycle of ws-based software. such a process should facilitate developers in the activities of web service-specific security requirents specification, web services-based security architecture design and web services security standards selection, integration and deployment. in this article we briefly present the pwssec (process for web services security) process that is composed of three stages, wssecreq (web services security requirents), wssecarch (web services security architecture) and wssectech (web services security technologies) that accomplishes the mentioned activities, respectively. in this article wwe also provide an thorough explanation of the wssecarch (web services security stage) stage intended to design the web services-based security architecture. in addition, a real case study where this stage in being applied is also included.web services enterprise security architecture','Software security engineering'
'although security is a crucial issue for information systems, traditionally, it is considered after the definition of the system. this approach often leads to problems, which most of the times translate into security vulnerabilities. from the viewpoint of the traditional security paradigm, it should be possible to eliminate such problems through better integration of security and software engineering. this paper firstly argues for the need to develop a methodology that considers security as an integral part of the whole system development process, and secondly it contributes to the current state of the art by proposing an approach that considers security concerns as an integral part of the entire system development process and by relating this approach with existing work. the different stages of the approach are described with the aid of a real-life case study; a health and social care information system.when security meets software engineering','Software security engineering'
'wireless security requires slightly different thinking from wired security because it gives potential attackers easy transport medium access. this access significantly increases the threat that any security architecture must address. wireless networking broadcast nature makes traditional link-layer attacks readily available to anyone. wireless network security based on the ieee 802.11 standard has received a lot of negative attention, since it is coupled with several design errors and security problems. ieee 802.11 uses spread-spectrum signaling technology, which the military depends on for secure communications. newer architectures are becoming available to dramatically increase the security of 802.11-based networks.wireless security is different','Software security engineering'
'abstract: electronic payment protocols are designed to work correctly in the presence of an adversary that can prompt honest principals to engage in an unbounded number of concurrent instances of the protocol. this paper establishes an upper bound on the number of protocol instances needed to attack a large class of protocols, which contains versions of some well-known electronic payment protocols, including set and 1kp. such bounds clarify the nature of attacks on and provide a rigorous basis for automated verification of payment protocols.a bound on attacks on payment protocols','Spoofing attacks'
'ip spoofing is one of the most common forms of on-line disguise. hackers have long employed the tactic of disguising their true identity. it exploits the security weaknesses in tcp/ip protocol suite. this paper evaluates basic techniques to exploit vulnerabilities of tcp/ip protocol suite such as initial sequence number prediction and forging the source address. this paper covers certain threats and attack methods that employ ip spoofing and analyze preventive measures such as ingress and egress filtering at routers, authentication and encryption.a novel solution for ip spoofing attacks','Spoofing attacks'
'i am going to be speaking about protocol verification again; i&#39;m going to take a rather different perspective from the one we normally take, and i&#39;ll be talking about what happens after an attack takes place. is there a life for a protocol beyond the attacks? we all know about verification. on the one hand we have the model checking community trying to find a witness of an attack, trying to find if something went wrong and why the specific property of interest failed. on the other hand we have the opposite approach, assuring that there&#39;s no such witness therefore the specific property holds. but the question here is, is this the whole story? it appears that everything is about finding the attack: is there an attack, is there no attack against confidentiality or authentication? it appears kind of weird. is it only the attack we are really interested in? is this really all we should look at? i&#39;ll try and convince you that there&#39;s something more. so, let&#39;s suppose for a minute we own a jewellers, and one day we find that the main window has been completely smashed by someone. in the worst case there is no-one around and basically all we can do is suspect anyone, any passer-by, because there&#39;s really no evidence against anyone. if we&#39;re luckier, we could find the people there while they&#39;re still at work carrying away the stuff. basically we detect who actually mounted the attack and we&#39;re kind of happy with that, as we&#39;re sure who the attacker is because we saw them. but we can even do more than that, maybe we have time to call the police, and the attackers will be caught, punished, and sent to jail, so we basically retaliate against them. if you move this to a different context perhaps retaliating means that i go up to the attacker&#39;s window and smash the window. anyway, this is just a general notion of punishment and retaliation. this is certainly about the best we can do. this line attempts to convince us that there are some measures we normally take after an attack takes place in the real world. so the idea here is to apply these very same concepts to the world of security protocols and see what we can get out of it.a protocol\'s life after attacks...','Spoofing attacks'
'we use a special operational semantics which helps us in predicting quantitative measures on systems describing cryptographic protocols: we also consider a possible attacker. the transitions of the system carry enhanced labels. we assign rates to transitions by only looking at these labels. we then map transition systems to markov chains and evaluate performance of systems, using standard tools.a quantitative study of two attacks','Spoofing attacks'
'for peer-to-peer services to be effective, participating nodes must cooperate, but in most scenarios a node represents a self-interested party and cooperation can neither be expected nor enforced. a reasonable assumption is that a large fraction of p2p nodes are rational and will attempt to maximize their consumption of system resources while minimizing the use of their own. if such behavior violates system policy then it constitutes an attack. in this paper we identify and create a taxonomy for rational attacks and then identify corresponding solutions if they exist. the most effective solutions directly incentivize cooperative behavior, but when this is not feasible the common alternative is to incentivize evidence of cooperation instead.a taxonomy of rational attacks','Spoofing attacks'
'a new taxonomy of web attacks is proposed in this paper, with the objective of obtaining a useful security reference framework. possible applications are described, which might benefit from this taxonomy, such as intrusion detection systems and application firewalls.a taxonomy of web attacks','Spoofing attacks'
'a3-codes under collusion attacks','Spoofing attacks'
'as research in automatic signature generators (asgs) receives more attention, various attacks against these systems are being identified. one of these attacks is the \"allergy attack\" which induces the target asg into generating harmful signatures to filter out normal traffic at the perimeter defense, resulting in a dos against the protected network. it is tempting to attribute the success of allergy attacks to a failure in not checking the generated signatures against a corpus of known \"normal\" traffic, as suggested by some researchers. in this paper, we argue that the problem is more fundamental in nature; the alleged \"solution\" is not effective against allergy attacks as long as the normal traffic exhibits certain characteristics that are commonly found in reality. we have come up with two advanced allergy attacks that cannot be stopped by a corpus-based defense. we also propose a page-rank-based metric for quantifying the damage caused by an allergy attack. both the analysis based on the proposed metric and our experiments with polygraph and hamsa show that the advanced attacks presented will block out 10\% to 100\% of http requests to the three websites studied: cnn.com, amazon. com and google.com.advanced allergy attacks','Spoofing attacks'
'slide attacks are powerful tools that enable the cryptanalyst to break ciphers with up to 4-round self-similarity. this paper introduces an advanced sliding technique that breaks ciphers with self-similarity more than 4 rounds, and even allows for sliding encryptions with dissimilar rounds in the middle of the slide. in particular, we present the realigning slide attack on variants of 14-, 15- and full 16-round des. we hope our results will spur more effort into ways to extend the slide attacks to apply to larger classes of block ciphers with complex key schedules.advanced slide attacks revisited','Spoofing attacks'
'recently a powerful cryptanalytic tool--the slide attack-- was introduced [3]. slide attacks are very successful in breaking iterative ciphers with a high degree of self-similarity and even more surprisingly are independent of the number of rounds of a cipher. in this paper we extend the applicability of slide attacks to a larger class of ciphers. we find very efficient known- and chosen-text attacks on generic feistel ciphers with a periodic key-schedule with four independent subkeys, and consequently we are able to break a des variant proposed in [2] using just 128 chosen texts and negligible time for the analysis (for one out of every 216 keys). we also describe known-plaintext attacks on desx and even-mansour schemes with the same complexity as the best previously known chosen-plaintext attacks on these ciphers. finally, we provide new insight into the design of gost by successfully analyzing a 20-round variant (gost&#8853;) and demonstrating weak key classes for all 32 rounds.advanced slide attacks','Spoofing attacks'
'intrusion detection systems (idss) are one of the key components for securing computing infrastructures. their objective is to protect against attempts to violate defense mechanisms. indeed, idss themselves are part of the computing infrastructure, and thus they may be attacked by the same adversaries they are designed to detect. this is a relevant aspect, especially in safety-critical environments, such as hospitals, aircrafts, nuclear power plants, etc. to the best of our knowledge, this survey is the first work to present an overview on adversarial attacks against idss. in particular, this paper will provide the following original contributions: (a) a general taxonomy of attack tactics against idss; (b) an extensive description of how such attacks can be implemented by exploiting ids weaknesses at different abstraction levels; (c) for each attack implementation, a critical investigation of proposed solutions and open points. finally, this paper will highlight the most promising research directions for the design of adversary-aware, harder-to-defeat ids solutions. to this end, we leverage on our research experience in the field of intrusion detection, as well as on a thorough investigation of the relevant related works published so far.adversarial attacks against intrusion detection systems','Spoofing attacks'
'data-mining techniques can be used not only to study collective behavior about customers, but also to discover private information about individuals. in this study, we demonstrate that decision trees, a popular classification technique for data mining, can be used to effectively reveal individuals\' confidential data, even when the identities of the individuals are not present in the data. we propose a novel approach for organizations to protect confidential data from such a classification attack. the key components of this approach include a set of entropy-based measures to evaluate disclosure risks of individual records, an optimal pruning algorithm to identify high-risk records, and a pair of data-swapping procedures to reduce the disclosure risks. the proposed method provides the best trade-off between data utility and privacy protection against classification attacks. it can be applied to data with both numeric and categorical attributes. an experimental study on six real-world data sets shows that the proposed method is very effective in protecting privacy while enabling legitimate data mining and analysis.against classification attacks','Spoofing attacks'
'this work aims to identify the algebraic problems which enable many attacks on rfid protocols. toward this goal, three emerging types of attacks on rfid protocols, concerning authentication, untraceability, and secrecy are discussed. we demonstrate the types of attacks by exhibiting previously unpublished vulnerabilities in several protocols and referring to various other flawed protocols. the common theme in these attacks is the fact that the algebraic properties of operators employed by the protocols are abused. while the methodology is applicable to any operator with algebraic properties, the protocols considered in this paper make use of &lt;em&gt;xor&lt;/em&gt; , modular addition, and elliptic curve point addition. algebraic attacks on rfid protocols','Spoofing attacks'
'in 2002, algebraic attacks using overdefined systems of equations have been proposed as a potentially very powerful cryptanalysis technique against block ciphers. however, although a number of convincing experiments have been performed against certain reduced algorithms, it is not clear whether these attacks can be successfully applied in general and to a large class of ciphers. in this paper, we show that algebraic techniques can be combined with side-channel attacks in a very effective and natural fashion. as an illustration, we apply them to the block cipher present that is a stimulating first target, due to its simple algebraic structure. the proposed attacks have a number of interesting features: (1) they exploit the information leakages of all the cipher rounds, (2) in common implementation contexts (e.g. assuming a hamming weight leakage model), they recover the block cipher keys after the observation of a single encryption, (3) these attacks can succeed in an unknown-plaintext/ciphertext adversarial scenario and (4) they directly defeat countermeasures such as boolean masking. eventually, we argue that algebraic side-channel attacks can take advantage of any kind of physical leakage, leading to a new tradeoff between the robustness and informativeness of the side-channel information extraction.algebraic side-channel attacks','Spoofing attacks'
'the domain name system (dns) is a critical fundamental service of the internet that provides mapping between domain names and ip addresses. in the past few years, distributed denial of service (ddos) attacks aimed at core dns servers have caused huge losses. in this paper, we present a simple, practical scheme that can significantly reduce the extent of the dns ddos attacks. firstly, we support that dns servers should not clean-up ttl-expired domain-name records in the cache when they detected that relevant dns servers are unavailable. secondly, according to the data of 7-day dns trace collected from three different dns servers on the internet, it shows that the dns can still work well during ddos attacks with a simple modification of the caching behavior.alleviating the impact of dns ddos attacks','Spoofing attacks'
'in this paper we consider guessing attacks upon security protocols, where an intruder guesses one of the values used (typically a poorly-chosen password) and then seeks to verify that guess. we formalise such attacks, and in particular the way in which the guess is verified. we then describe how to model such attacks within the process algebra csp, so that they can be detected using the model checker fdr, and illustrate our technique on some examples.analysing protocols subject to guessing attacks','Spoofing attacks'
'with the development of e-commerce, ssl protocol is more and more widely applied to various network services. for the defect of ssl authentication, this paper analyses two kinds of drawbacks in ssl handshake, and respectively conducts fake certificate and conversion from https to http data to attack. both of them are dangerous to https communication. for that reason, we have proposed three different measures to strengthen data security, which are static arp table, enhanced certificate system, and two-way authentication. experimental results show that three methods are effectively defensive against the https hijacking attacks.analysis and research on https hijacking attacks','Spoofing attacks'
'information theory turned out to be very useful in analyzing anonymity attacks in general. the concept of channel information leak is a good indicator of how successful an attack can be. while different information leak measures exist in the literature, the problem of representing anonymity systems using noisy channels has not been well studied. the main goal of this paper is to show how anonymity attacks on mix systems can be formally represented as noisy channels in the information-theoretic sense. this formal representation provides a deeper understanding of mix systems and prepares the field for a more rigorous and accurate analysis of possible attacks. we performed empirical analysis using three information leak measures (mutual information, klsd, and min-entropy) which revealed interesting findings about some mix variants. this paper tries to bridge the gap between theory and practice in the field of anonymous communication systems.anonymity attacks on mix systems','Spoofing attacks'
'this paper proposes a kind of architecture for preventing xss attacks. the server can identify the content written by dom and trusted html tags and attributes of the original page with the randomization markup. the server proxy verifies all the data of response page according to randomization markup and policy. experiments show that the performance of the architecture is good and it is able to prevent xss attacks effectively.architecture for preventing xss attacks','Spoofing attacks'
'the paper presents an approach and formal framework for modeling attacks against computer network and its software implementation on the basis of a multi-agent architecture. the model of an attack is considered as a complex process of contest of adversary entities those are malefactor or team of malefactors, on the one hand, and network security system implementing a security policy, on the other hand. the paper focuses on the conceptual justification of the chosen approach, specification of the basic components composing attack model, formal frameworks for specification of the above components and their interaction in simulation procedure. the peculiarities of the developed approach are the followings: (1) malefactor\'s intention-centric attack modeling; (2) multi-level attack specification; (3) ontology-based distributed attack model structuring; (4) attributed stochastic ll(2) context-free grammar for formal specification of attack scenarios and its components (\"simple attacks\"); (5) using operation of formal grammar substitution for specification of multi-level structure of attacks; (6) state machine-based formal grammar framework implementation; (7) on-line generation of the malefactor\'s activity resulting from the reaction of the attacked network security system.attacks against computer network','Spoofing attacks'
'in recent years, network coordinate systems which map nodes into a geometrical space can effectively support overlay applications relying on topology-awareness. however, these systems base on an ideal assumption that the nodes in them are honest to cooperate with each other. although there have been some studies about attacks on network coordinate systems, the effect of attacks on pic---one of the representative systems---has not been studied. moreover, since pic itself has proposed a security policy, how well it can protect pic from attacks is another significant problem to be researched. we apply four typical attacks on pic with security and without security. our extensive experiments show that pic is vulnerable by attacks and when the percentage of malicious nodes is more than 40\%, pic with security performs barely better than without security.attacks against network coordinate system','Spoofing attacks'
'smart card are often the target of software or hardware attacks. recently several logical attacks have been developed that allows to dump the eeprom memory. this kind of attack are particularly affordable for students who can learn reverse engineering techniques on devices known to be tamper resistant. this tutorial will demonstrate how with a few material a graduate student within a couple of hours is able to reverse an application.attacks against smart cards','Spoofing attacks'
'signcryption is an important cryptography primitive that provides confidentiality and authenticity for the messages simultaneously, at lower costs than traditional sign-then-encrypt method. this paper presents the attacks against two identity-based signcryption schemes: the zhang et al.&#8217;s assertion signcryption scheme in decentralized autonomous environments, which is not consistent in multi-pkg scenarios, and the yu et al.&#8217;s identity-based signcryption scheme in the standard model, which cannot achieve indistinguishability against chosen plaintext attacks.attacks against two identity-based signcryption schemes','Spoofing attacks'
'editor\'s note:jtag is a well-known standard mechanism for in-field test. although it provides high controllability and observability, it also poses great security challenges. this article analyzes various attacks and proposes protection schemes.&#x2014;mohammad tehranipoor, university of connecticutattacks and defenses for jtag','Spoofing attacks'
'singelee and preneel have recently proposed a enhancement of hancke and kuhn\'s distance bounding protocol for rfid. the authors claim that their protocol offers substantial reductions in the number of rounds, though preserving its advantages: suitable to be employed in noisy wireless environments, and requiring so few resources to run that it can be implemented on a low-cost device. subsequently, the same authors have also proposed it as an efficient key establishment protocol in wireless personal area networks. nevertheless, in this paper we show effective relay attacks on this protocol, which dramatically increase the success probability of an adversary. as a result, the effectiveness of singelee and preneel\'s protocol is seriously questioned.attacks on a distance bounding protocol','Spoofing attacks'
'a vehicular ad-hoc network (vanet), is an instance of intelligent transportation system that provides vehicle-to-vehicle communication aided by road side infrastructure for in-vehicle entertainment and safer road environment. vanet is characterized by highly mobile vehicles, predetermined topology and the requirement of reliable time bound message delivery over error prone shared wireless medium. the security solutions are constrained by these characteristics. in this paper we are discussing the various type of attack on vanet along with intersection attack on anonymity and the security issue that need to kept in mind while developing any protocol to make vanet secure. simulation and results indicate that intersection attack is breach the privacy even though cryptographic and noncryptographic security mechanism is enabled over the vanet.attacks on anonymity in vanet','Spoofing attacks'
'in the last few years, a large number of schemes have been proposed for hiding copyright marks and other information in digital pictures, video, audio and other multimedia objects. we describe some contenders that have appeared in the research literature and in the field; we then present a number of attacks that enable the information hidden by them to be removed or otherwise rendered unusable.attacks on copyright marking systems','Spoofing attacks'
'attacks are presented on the ibm 4758 cca and the visa security module. two new attack principles are demonstrated. related key attacks use known or chosen differences between two cryptographic keys. data protected with one key can then be abused by manipulation using the other key. meet in the middle attacks work by generating a large number of unknown keys of the same type, thus reducing the key space that must be searched to discover the value of one of the keys in the type. design heuristics are presented to avoid these attacks and other common errors.attacks on cryptoprocessor transaction sets','Spoofing attacks'
'watermarking is a potential method for protection of ownership rights on digital audio, image, and video data. benchmarks are used to evaluate the performance of different watermarking algorithms. for image watermarking, the stirmark package is the most popular benchmark, and the best current algorithms perform well against it. however, results obtained by the stirmark benchmark have to be handled carefully since stirmark does not properly model the watermarking process and consequently is limited in its potential for impairing sophisticated image watermarking schemes. in this context, the goal of this article is threefold. first, we give an overview of the current attacking methods. second, we describe attacks exploiting knowledge about the statistics of the original data and the embedded watermark. we propose a stochastic formulation of estimation-based attacks. such attacks consist of two main stages: watermark estimation, exploitation of the estimated watermark to trick watermark detection or create ownership ambiguity. the full strength of estimation-based attacks can be achieved by introducing additional noise, where the attacker tries to combine the estimated watermark and the additive noise to impair watermark communication as much as possible while fulfilling a quality constraint on the attacked data. with a sophisticated quality constraint it is also possible to exploit human perception: the human auditory system in case of audio watermarks and the human visual system in case of image and video watermarks. third, we discuss the current status of image watermarking benchmarks. we present petitcolas\'(see electronic imaging \'99: security and watermarking of multimedia content, spie proc., vol.3657, san jose, ca, 1999) stirmark benchmarking tool. next, we consider the benchmark proposed by the university of geneva vision group that contains more deliberate attacks. finally, we summarize the current work of the european certimark project, whose goal is to accelerate efforts from a number of research groups and companies in order to produce an improved ensemble of benchmarking toolsattacks on digital watermarks','Spoofing attacks'
'java cards have been threatened so far by attacks using ill-formed applications which assume that the application bytecode is not verified. this assumption remained realistic as long as the bytecode verifier was commonly executed off-card and could thus be bypassed. nevertheless it can no longer be applied to the java card 3 connected edition context where the bytecode verification is necessarily performed on-card. therefore java card 3 connected edition seems to be immune against this kind of attacks. in this paper, we demonstrate that running ill-formed application does not necessarily mean loading and installing ill-formed application. for that purpose, we introduce a brand new kind of attack which combines fault injection and logical tampering. by these means, we describe two case studies taking place in the new java card 3 context. the first one shows how ill-formed applications can still be introduced and executed despite the on-card bytecode verifier. the second example leads to the modification of any method already installed on the card into any malicious bytecode. finally we successfully mount these attacks on a recent device, emphasizing the necessity of taking into account these new threats when implementing java card 3 features.attacks on java card 3.0 combining fault and logical attacks','Spoofing attacks'
'two simple redundancy schemes are shown to be inadequate in securing rsa signatures against attacks based on multiplicative properties. the schemes generalize the requirement that each valid message starts or ends with a fixed number of zero bits. even though only messages with proper redundancy are signed, forgers are able to construct signatures on messages of their choice.attacks on some rsa signatures','Spoofing attacks'
'attacks on steganographic systems','Spoofing attacks'
'this paper describes a set of innovative attribute based checks for defending against phishing attacks. we explain a number of anti-phishing algorithms implemented as plugins and highlight which attributes of phishing sites they consider.to assess the effectiveness and applicability of this prototype,we performed extensive experimental testing. we present a fully automated crawling framework that we developed for testing,along with the main experimental results.attribute-based prevention of phishing attacks','Spoofing attacks'
'model-based anomaly detection systems restrict program execution by a predefined model of allowed system call sequences. these systems are useful only if they detect actual attacks. previous research developed manually-constructed mimicry and evasion attacks that avoided detection by hiding a malicious series of system calls within a valid sequence allowed by the model. our work helps to automate the discovery of such attacks. we start with two models: a program model of the application&#39;s system call behavior and a model of security-critical operating system state. given unsafe os state configurations that describe the goals of an attack, we then find system call sequences allowed as valid execution by the program model that produce the unsafe configurations. our experiments show that we can automatically find attack sequences in models of programs such as wu-ftpd and passwd that previously have only been discovered manually. when undetected attacks are present, we frequently find the sequences with less than 2 seconds of computation.automated discovery of mimicry attacks','Spoofing attacks'
'a critical problem facing today&#8217;s internet community is the increasing number of attacks exploiting flaws found in web applications. this paper specifically targets input validation vulnerabilities found in sql queries that may lead to sql injection attacks (sqlias). we introduce a tool that automatically detects and suggests fixes to sql queries that are found to contain sql injection vulnerabilities (sqlivs). testing was performed against phpbb v2.0, an open source forum package, to determine the accuracy and efficacy of our software.automated fix generator for sql injection attacks','Spoofing attacks'
'cryptographic and physical leakage attacks on devices and systems which implement cryptosystems, is an area of much recent activities. one type of attacks are what is called kleptographic attacks which are mounted against black-box cryptosystems. they are issued by and serve solely the designer/manufacturer giving it unique advantage. klep-tographic attacks are capable of leaking the private keys of users securely and subliminally to the manufacturer of the black-box system (based on the availability of public values, such as keys (produced when the system is initiated) or signature/ciphertext values (produced by systems in operation). these attacks provide a very high level of security against reverse-engineering since even if the black-box is successfully reverse-engineered, no information can be obtained that compromises the secrets of the users (thus, the unique advantage of the attacker is retained). numerous open questions remain in the area. one issue is that the only key generation procedure with known attack is the rsa/ factoring based pkc, while for discrete logarithm based keys attacks are not known. similarly open, is the existence of bandwidth-optimal leakage attacks, namely attacks on a \"single signature\" in discrete logarithm based signatures (both in the full group and prime order sub-group cases). in this paper, we solve the above open questions. we develop new attack techniques, which unlike earlier attacks, require only one value in order to leak the secret. this gives an attack on modular exponentiation keys. we then show how to implement an attack on elgamal signature which leaks the private key in each signature, and which requires only 160 bits of smoothness in p-1, where p is the common elgamal prime. the attack utilizes the newton channel. this channel, however, does not extend to dsa, since dsa operates in a prime order subgroup of zp. in the second part of this work, we nevertheless show a subliminal channel attack on dsa that assumes the existence of a small amount of non-volatile memory in the device. this gives a kleptographic attack against dsa that leaks the private key in each signature as well. nonvolatility is only needed to assure the polynomial indistinguishability of the outputs of the devices under attack from that of a normal devices\' outputs. we investigate our non-volatility assumption against hardware feasibility (in quite a popular eeprom devices, used in manufacturing of smart-cards).bandwidth-optimal kleptographic attacks','Spoofing attacks'
'published attacks against smartphones have concentrated on software running on the application processor. with numerous countermeasures like aslr, dep and code signing being deployed by operating system vendors, practical exploitation of memory corruptions on this processor has become a time-consuming endeavor. at the same time, the cellular baseband stack of most smart-phones runs on a separate processor and is significantly less hardened, if at all. in this paper we demonstrate the risk of remotely exploitable memory corruptions in cellular baseband stacks. we analyze two widely deployed baseband stacks and give exemplary cases of memory corruptions that can be leveraged to inject and execute arbitrary code on the baseband processor. the vulnerabilities can be triggered over the air interface using a rogue gsm base station, for instance using openbts together with a usrp software defined radio.baseband attacks','Spoofing attacks'
'this note attempts to raise awareness within the network research community about the security of the interdomain routing infrastructure. we identify several attack objectives and mechanisms, assuming that one or more bgp routers have been compromised. then, we review the existing and proposed countermeasures, showing that they are either generally ineffective (route filtering), or probably too heavyweight to deploy (s-bgp). we also review several recent proposals, and conclude by arguing that a significant research effort is urgently needed in the area of routing security.beware of bgp attacks','Spoofing attacks'
'we propose a countermeasure for a class of known attacks on the pin processing api used in the atm (cash machine) network. this api controls access to the tamper-resistant hardware security modules where pin encryption, decryption and verification takes place. the attacks are differential attacks, whereby an attacker gains information about the plaintext values of encrypted customer pins by making changes to the non-confidential inputs to a command. our proposed fix adds an integrity check to the parameters passed to the command. it is novel in that it involves very little change to the existing atm network infrastructure. blunting differential attacks on pin processing apis','Spoofing attacks'
'we present high probability differential trails on 2 and 3 rounds of blake-32. using the trails we are able to launch boomerang attacks on up to 8 round-reduced keyed permutation of blake-32. also, we show that boomerangs can be used as distinguishers for hash/ compression functions and present such distinguishers for the compression function of blake-32 reduced to 7 rounds. since our distinguishers on up to 6 round-reduced keyed permutation of blake-32 are practical (complexity of only 212 encryptions), we are able to find boomerang quartets on a pc.boomerang attacks on blake-32','Spoofing attacks'
'this paper describes a new attack on the anonymity of web browsing with tor. the attack tricks a user\'s web browser into sending a distinctive signal over the tor network that can be detected using traffic analysis. it is delivered by a malicious exit node using a man-in-the-middle attack on http. both the attack and the traffic analysis can be performed by an adversary with limited resources. while the attack can only succeed if the attacker controls one of the victim\'s entry guards, the method reduces the time required for a traffic analysis attack on tor from o(nk) to o(n + k), where n is the number of exit nodes and k is the number of entry guards. this paper presents techniques that exploit the tor exit policy system to greatly simplify the traffic analysis. the fundamental vulnerability exposed by this paper is not specific to tor but rather to the problem of anonymous web browsing itself. this paper also describes a related attack on users who toggle the use of tor with the popular firefox extension torbutton.browser-based attacks on tor','Spoofing attacks'
'in this paper we present a new kind of cryptanalytic attack which utilizes bugs in the hardware implementation of computer instructions. the best known example of such a bug is the intel division bug, which resulted in slightly inaccurate results for extremely rare inputs. whereas in most applications such bugs can be viewed as a minor nuisance, we show that in the case of rsa (even when protected by oaep), pohlig-hellman, elliptic curve cryptography, and several other schemes, such bugs can be a security disaster: decrypting ciphertexts on &lt;em&gt;any&lt;/em&gt;computer which multiplies &lt;em&gt;even one pair of numbers&lt;/em&gt;incorrectly can lead to full leakage of the secret key, sometimes with a single well-chosen ciphertext.bug attacks','Spoofing attacks'
'we describe several software side-channel attacks based on inter-process leakage through the state of the cpu&#8217;s memory cache. this leakage reveals memory access patterns, which can be used for cryptanalysis of cryptographic primitives that employ data-dependent table lookups. the attacks allow an unprivileged process to attack other processes running in parallel on the same processor, despite partitioning methods such as memory protection, sandboxing and virtualization. some of our methods require only the ability to trigger services that perform encryption or mac using the unknown key, such as encrypted disk partitions or secure network links. moreover, we demonstrate an extremely strong type of attack, which requires knowledge of neither the specific plaintexts nor ciphertexts, and works by merely monitoring the effect of the cryptographic process on the cache. we discuss in detail several such attacks on aes, and experimentally demonstrate their applicability to real systems, such as openssl and linux&#8217;s dm-crypt encrypted partitions (in the latter case, the full key can be recovered after just 800 writes to the partition, taking 65 milliseconds). finally, we describe several countermeasures for mitigating such attacks.cache attacks and countermeasures','Spoofing attacks'
'the paper discusses the performance of cache timing attacks on clefia, which is based on the generalized feistel structure and implemented using small tables. we mention the difficulties on mounting a timing based cache attack on the cipher, and then explain why a cache attack is still possible. to the best of our knowledge, no reported work on cache attacks target ciphers which are implemented with small tables. our attack uses the fact that parallelization and pipelining of memory accesses can only be done within a single round of a cipher, but not across rounds. our findings show that 121 bits of the 128 bit key can be revealed in 226.64 clefia encryptions on an intel core 2 duo machine.cache timing attacks on clefia','Spoofing attacks'
'cache-timing attacks are a serious threat to security-critical software. we show that the combination of vector quantization and hidden markov model cryptanalysis is a powerful tool for automated analysis of cache-timing data; it can be used to recover critical algorithm state such as key material. we demonstrate its effectiveness by running an attack on the elliptic curve portion of openssl (0.9.8k and under). this involves automated lattice attacks leading to key recovery within hours. we carry out the attack on live cache-timing data without simulating the side channel, showing these attacks are practical and realistic. cache-timing template attacks','Spoofing attacks'
'as the internet has entered everyday life and become tightly bound to telephony, both in the form of voice over ip technology as well as internet-enabled cellular devices, several attacks have emerged that target both landline and mobile devices. we present a variation of an existing attack, that exploits smart phone devices to launch a dos attack against a telephone device by issuing a large amount of missed calls. in that light, we conduct an excessive study of phone captcha usage for preventing attacks that render telephone devices unusable, and provide information on the design and implementation of our system that protects landline devices. subsequently, we propose the integration of phone captchas in smart phone software as a countermeasure against a series of attacks that target such devices. we also present various enhancements to strengthen captchas against automated attacks. finally, we conduct a user study to measure the applicability of our enhanced phone captchas.captchuring automated (smart) phone attacks','Spoofing attacks'
'the advancement of computers has also led to the evolution of telephone technology. from traditional pstn networks and mobile devices we have moved on to the era of voice over ip (voip) and smartphones. integration of the internet into everyday life has led to the demand for web access on-the-go and the widespread adoption of new generation mobile devices. such internet-enabled devices are becoming increasingly popular, with smartphone users expected to exceed 1 billion worldwide by 2014. additionally, voip subscribers will reach almost half a billion worldwide by 2012. thus, one can expect that in the near future, legacy telephony technologies will slowly become obsolete. nonetheless, in this transitional period where such technologies co-exist, we can expect the emergence of new threats that exploit their interconnection. as demonstrated in our previous work, as well as in the wild, an attacker can leverage voip technology to flood traditional telephone devices with a large number of missed calls1 and render them unusable. we demonstrated the feasibility of such an attack, which we refer to as dial attacks, by leveraging voip technology.captchuring automated (smart)phone attacks','Spoofing attacks'
'this paper describes an empirical research study to characterize attackers and attacks against targets of opportunity. a honey net infrastructure was built and deployed over 167 days that leveraged three different honey pot configurations and a ssh-based authentication proxy to attract and follow attackers over several weeks. a total of 211 attack sessions were recorded and evidence was collected at each stage of the attack sequence: from discovery to intrusion and exploitation of rogue software. this study makes two important contributions: 1) we introduce a new approach to measure attacker skills, and 2) we leverage keystroke profile analysis to differentiate attackers beyond their ip address of origin.characterizing attackers and attacks','Spoofing attacks'
'self-synchronizing stream ciphers (sssc) are a particular class of symmetric encryption algorithms, such that the resynchronization is automatic, in case of error during the transmission of the ciphertext. in this paper, we extend the scope of chosen-ciphertext attacks against sssc. previous work in this area include the cryptanalysis of dedicated constructions, like knot, hbb or sss. we go further to break the last standing dedicated design of sssc, i.e. the ecrypt proposal mosquito. our attack costs about 270 computation steps, while a 96-bit security level was expected. it also applies to &#915;&#933; (an ancestor of mosquito) therefore the only secure remaining sssc are block-cipher-based constructions.chosen-ciphertext attacks against mosquito','Spoofing attacks'
'rfid (radio frequency identification) systems are one of the most pervasive computing technologies with technical potential and profitable opportunities in a diverse area of applications. among their advantages is included their low cost and their broad applicability. however, they also present a number of inherent vulnerabilities. this paper develops a structural methodology for risks that rfid networks face by developing a classification of rfid attacks, presenting their important features, and discussing possible countermeasures. the goal of the paper is to categorize the existing weaknesses of rfid communication so that a better understanding of rfid attacks can be achieved and subsequently more efficient and effective algorithms, techniques and procedures to combat these attacks may be developed.classifying rfid attacks and defenses','Spoofing attacks'
'this article is the first in a 2-part series on the vulnerabilities of networks to client-side attacks, and the risk to networks from \'inside-out\' compromises. internet and computer usage is exponentially increasing; more people are beginning to use the internet as part of their everyday lives, these activities ranging from instant messaging to ordering shopping online. even most large corporate businesses provide their employees with some kind of access to the internet.client attacks','Spoofing attacks'
'logical attacks on smart cards have been used for many years, but their attack potential is hindered by the processes used by issuers to verify the validity of code, in particular bytecode verification. more recently, the idea has emerged to combine logical attacks with a physical attack, in order to evade bytecode verification. we present practical work done recently on this topic, as well as some countermeasures that can be put in place against such attacks, and how they can be evaluated by security laboratories.combined attacks and countermeasures','Spoofing attacks'
'the literature about side-channel attacks is very rich. many side-channel distinguishers have been devised and studied; in the meantime, many different side-channels have been identified. also, it has been underlined that the various samples garnered during the same acquisition can carry complementary information. in this context, there is an opportunity to study how to best combine many attacks with many leakages from different sources or using different samples from a single source. this problematic has been evoked as an open issue in recent articles. in this paper, we bring two concrete answers to the attacks combination problem. first of all, we experimentally show that two partitionings can be constructively combined. then, we explore the richness of electromagnetic curves to combine several timing samples in such a way a sample-adaptative model attack yields better key recovery success rates than a mono-model attack using only a combination of samples (via a principal component analysis).combined side-channel attacks','Spoofing attacks'
'just as errors in sequential programs can lead to security exploits, errors in concurrent programs can lead to concurrency attacks. questions such as whether these attacks are feasible and what characteristics they have remain largely unknown. in this paper, we present a preliminary study of concurrency attacks and the security implications of real world concurrency errors. our study yields several interesting findings. for instance, we observe that the exploitability of a concurrency error depends on the duration of the timing window within which the error may occur. we further observe that attackers can increase this window through carefully crafted inputs. we also find that four out of five commonly used sequential defenses become unsafe when applied to concurrent programs. based on our findings, we propose new defense directions and fixes to existing defenses.concurrency attacks','Spoofing attacks'
'in order to protect network from the denial of service attacks which sends excessive traffic to a host, it is required for network components to throttle unauthorized traffics. the attacker must be identified through the cooperation of routers and must be isolated by the nearest router. it is the most important to identify and isolate the attacker since the nearest router can make ideal blocking of the dos attacks. in this research, we will present a protocol which can identify the attacker of dos by the request of victim in cooperation of routers on the attacking path between a victim and the attacker. the performance of our protocol will be verified by simulations and the experiments show that it takes considerably small time to identify the location of attacker.cooperative routers against dos attacks','Spoofing attacks'
'the combination generator is a popular stream cipher construction. it consists of several independent devices working in parallel whose outputs are combined by a boolean function. the output of this function is the keystream. the security of this generator has been extensively studied in the case where the devices are lfsrs. some particular cases where the devices are nonlinear have also been studied, most notably the different versions of the estream proposal named achterbahn. several cryptanalysis techniques against these ciphers have been published, extending the classical correlation attack. but each of these attacks has been presented mainly in a very particular scenario. therefore, this paper aims at generalising these methods to any combination generator in order to be able to compare their respective advantages and to determine the optimal attack for each particular generator. generic formulas for the data-time-space complexities are then provided, which only depend on the number of devices, their periods and the number of their internal states and of the boolean combining function. some of the considered improvements can also be used in a much more general context, which includes linear attacks against some block ciphers.correlation attacks on combination generators','Spoofing attacks'
'correlation attacks on stream ciphers','Spoofing attacks'
'the security protocols for wlan such as wep have fundamental weakness which can be exploited by the attacker to obtain unauthorized access to the wireless networks and generate attacks. in this paper, we propose a security architecture for counteracting denial of service attacks in wireless based network architecture with mobile nodes. we describe the system model and discuss the different cases of attack scenarios involving the mobility of the attacking and victim nodes. we describe how mobile ip protocol in conjunction with our model can be used to deal efficiently with the attacks on mobile nodes.counteracting ddos attacks in wlan','Spoofing attacks'
'hardware implementations of cryptographic systems are becoming common, due to new market needs and to reduced costs. however, the security of a system may be seriously compromised by implementation attacks, such as side channel analysis or fault analysis. thus, a large number of solutions have been proposed to counteract these threats. in this paper, we present most common architectural countermeasures against natural or malicious fault injections, highlighting their strengths, weaknesses, and giving a few nontrivial considerations.countermeasures against fault attacks','Spoofing attacks'
'we say that a cryptographic scheme is continuous leakage-resilient (clr), if it allows users to refresh their secret keys, using only fresh local randomness, such that: 1. the scheme remains functional after any number of key refreshes, although the public key never changes. thus, the &#8220;outside world\'\' is neither affected by these key refreshes, nor needs to know about their frequency. 2. the scheme remains secure even if the adversary can continuously leak arbitrary information about the current secret-key, as long as the amount of leaked information is bounded in between any two successive key refreshes. there is no bound on the total amount of information that can be leaked during the lifetime of the system. in this work, we construct a variety of practical clr schemes, including clr one-way relations, clr signatures, clr identification schemes, and clr authenticated key agreement protocols. for each of the above, we give general constructions, and then show how to instantiate them efficiently using a well established assumption on bilinear groups, called the k-linear assumption (for any constant k greater than or equal to 1). our constructions are highly modular, and we develop many interesting techniques and building-blocks along the way, including: leakage-indistinguishable re-randomizable relations, homomorphic nizks, and leakage-of-cipher text non-malleable encryption schemes.cryptography against continuous memory attacks','Spoofing attacks'
'this paper describes malicious applets that use java\'s sophisticated graphic features to rectify the browser\'s padlock area and cover the address bar with a false https domain name. the attack was successfully tested on netscape\'s navigator and microsoft\'s internet explorer; we consequently recommend to neutralize java whenever funds or private data transit via these browsers and patch the flaw in the coming releases. the degree of novelty of our attack is unclear since similar (yet nonidentical) results can be achieved by spoofing as described in [6]; however our scenario is much simpler to mount as it only demands the inclusion of an applet in the attacker\'s web page. in any case, we believe that the technical dissection of our malicious java code has an illustrative value in itself.cut-&-paste attacks with java','Spoofing attacks'
'this article aims at showing that side-channel analyses constitute powerful tools for reverse-engineering applications. we present two new attacks that only require known plaintext or ciphertext. the first one targets a stream cipher and points out how an attacker can recover unknown linear parts of an algorithm which is in our case the parameters of a linear feedback shift register. the second technique allows to retrieve an unknown non-linear function such as a substitution box. it can be applied on every kind of symmetric algorithm (typically feistel or substitution permutation network) and also on stream ciphers. twelve years after the first publication about side-channel attacks, we show that the potential of these analyses has been initially seriously under-estimated. every cryptography, either public or secret, is indeed at risk when implemented in a device accessible by an attacker. this illustrates how vulnerable cryptography is without a trusted tamperproof hardware support.defeating any secret cryptography with scare attacks','Spoofing attacks'
'security experts generally acknowledge that the long-term solution to distributed denial of service attacks is to increase the security level of internet computers. attackers would then be unable to find zombie computers to control. internet users would also have to set up globally coordinated filters to stop attacks early. however, the critical challenge in these solutions lies in identifying the incentives for the internet\'s tens of millions of independent companies and individuals to cooperate on security and traffic control issues that do not appear to directly affect them. we give a brief introduction to: network weaknesses that ddos attacks exploit; the technological futility of addressing the problem solely at the local level; potential global solutions; and why global solutions require an economic incentive frameworkdefeating distributed denial of service attacks','Spoofing attacks'
'virtualization technologies have been explored to mitigate the vulnerabilities in the current tcg architecture. but only a small number of efforts address the handling of the detected tcg toctou attacks. this paper aims to defeat tcg toctou attacks occurring in xen hardware virtual machines. we propose an efficient response approach (era) in the environment of the xen virtual machine monitor and the virtual tpm facility shipped with the xen. era has the following features: (1) effective in defeating the tcg toctou attacks; (2) impose less overhead on the system during normal execution; and (3) transparent to guest virtual machines. we describe the era implementation in detail and evaluate its ability via experiments.defeating tcg toctou attacks in trusted hvm','Spoofing attacks'
'in this paper, we present a security weakness of a forward secure authentication protocol proposed by tri van le et al. called o-frap which stands for optimistic forward secure rfid authentication protocol. in particular, we point out that in the o-frap protocol, the server can be subject to a denial-of-service attack due to a flaw in the database querying procedure. our attack also applies to a simplified version of o-frap called o-rap (optimistic rfid authentication protocol) which is essentially o-frap but without a secret key updating procedure (and thus forward security). we then propose two improved protocols called o-frap^+ and o-rap^+ which prevent the said denial-of-service attack. in addition, the o-frap^+ protocol also addresses two security weaknesses of o-frap pointed out earlier by khaled and raphael. in terms of performance, comparing to o-frap, o-frap^+ requires a few more computational steps but much less storage at the back-end server.defending rfid authentication protocols against dos attacks','Spoofing attacks'
'attacks targeting something or someone indirectly&#8212;nth order attacks against end systems--have occurred on a large scale in real life. it goes to the heart of how conflicts between open societies and their enemies are waged: trust subsystems. trust helps lower tangible and intangible transaction costs between individuals, corporations, and the state. members of \"high-trust\" societies like the united states leverage trust beyond family ties to form efficient civic and economic organizations. because trust permeates every facet of open societies, it\'s a very easy assumption for malicious actors to violate.degradation and subversion through subsystem attacks','Spoofing attacks'
'application dos attack, which aims at disrupting application service rather than depleting the network resource, has emerged as a larger threat to network services, compared to the classic dos attack. owing to its high similarity to legitimate traffic and much lower launching overhead than classic ddos attack, this new assault type cannot be efficiently detected or prevented by existing detection solutions. to identify application dos attack, we propose a novel group testing (gt)-based approach deployed on back-end servers, which not only offers a theoretical method to obtain short detection delay and low false positive/negative rate, but also provides an underlying framework against general network attacks. more specifically, we first extend classic gt model with size constraints for practice purposes, then redistribute the client service requests to multiple virtual servers embedded within each back-end server machine, according to specific testing matrices. based on this framework, we propose a two-mode detection mechanism using some dynamic thresholds to efficiently identify the attackers. the focus of this work lies in the detection algorithms proposed and the corresponding theoretical complexity analysis. we also provide preliminary simulation results regarding the efficiency and practicability of this new scheme. further discussions over implementation issues and performance enhancements are also appended to show its great potentials.detecting application denial-of-service attacks','Spoofing attacks'
'as internet based and intranet based network systems have evolved, they have become invaluable tools that businesses can use to share information and conduct business with online partners. however, hackers have also learned to use these systems to access private networks and their resources. studies have shown that many organizations have suffered external and internal network intrusions. internet systems are subject to various types of attacks. traditional network security products, such as firewalls, can be penetrated from outside and can also leave organizations vulnerable to internal attacks. generally, victims do not find out that their networks have been attacked until they examine system logs the next day, after the damage has been done. network intrusion detection systems solve this problem by detecting external and internal security breaches as they happen and immediately notifying security personnel and network administrators by e mail or pager. intrusion detection systems use several types of algorithms to detect possible security breaches, including algorithms for statistical anomaly detection, rule based anomaly detection, and a hybrid of the twodetecting attacks on networks','Spoofing attacks'
'surveys of businesses and other organizations that rely on the internet for their communications show that around 83\% of inbound email traffic is either spam, or other types of illegitimate messages (1). together these are known as \'\'dark traffic\'\'.detecting attacks','Spoofing attacks'
'recent distributed denial-of-service (ddos) attacks have demonstrated horrible destructive power by paralyzing web servers within short time. as the volume of internet traffic rapidly grows up, the current ddos detection technologies have met a new challenge that should efficiently deal with a huge amount of traffic within the affordable response time. in this work, we propose a novel ddos detection method based on hadoop that implements a http get flooding detection algorithm in mapreduce on the distributed computing platform.detecting ddos attacks with hadoop','Spoofing attacks'
'dns amplification attacks massively exploit open recursive dns servers mainly for performing bandwidth consumption ddos attacks. the amplification effect lies in the fact that dns response messages may be substantially larger than dns query messages. in this paper, we present and evaluate a novel and practical method that is able to distinguish between authentic and bogus dns replies. the proposed scheme can effectively protect local dns servers acting both proactively and reactively. our analysis and the corresponding real-usage experimental results demonstrate that the proposed scheme offers a flexible, robust and effective solution.detecting dns amplification attacks','Spoofing attacks'
'sybil attacks have been regarded as a serious security threat to ad hoc networks and sensor networks. they may also impair the potential applications in vehicular ad hoc networks (vanets) by creating an illusion of traffic congestion. in this paper, we make various attempts to explore the feasibility of detecting sybil attacks by analyzing signal strength distribution. first, we propose a cooperative method to verify the positions of potential sybil nodes. we use a random sample consensus (ransac)-based algorithm to make this cooperative method more robust against outlier data fabricated by sybil nodes. however, several inherent drawbacks of this cooperative method prompt us to explore additional approaches. we introduce a statistical method and design a system which is able to verify where a vehicle comes from. the system is termed the presence evidence system (pes). with pes, we are able to enhance the detection accuracy using statistical analysis over an observation period. finally, based on realistic us maps and traffic models, we conducted simulations to evaluate the feasibility and efficiency of our methods. our scheme proves to be an economical approach to suppressing sybil attacks without extra support from specific positioning hardware.detecting sybil attacks in vanets','Spoofing attacks'
'delay-tolerant networks are especially useful in providing mission-critical services including emergency scenarios and battlefield applications. however, dtns are vulnerable to wormhole attacks, in which a malicious node records the packets at one location and tunnels them to another colluding node, which relays them locally into the network. wormhole attacks are a severe threat to normal network operation in dtns. in this article we describe various methods that have been developed to detect wormhole attacks. however, most of them cannot work efficiently in dtns. to detect the presence of a wormhole attack, we propose a detection mechanism that exploits the existence of a forbidden topology in the network. we evaluated our approach through extensive simulations using both random way point and zebranet mobility models. our results show that the proposed method can detect wormhole attacks efficiently and effectively in dtns.detecting wormhole attacks in delay-tolerant networks','Spoofing attacks'
'in this paper, a general model of multibit differential power analysis (dpa) attacks to precharged buses is discussed, with emphasis on symmetric-key cryptographic algorithms. analysis provides a deeper insight into the dependence of the dpa effectiveness (i.e., the vulnerability of cryptographic chips) on the parameters that define the attack, the algorithm, and the processor architecture in which the latter is implemented. to this aim, the main parameters that are of interest in practical dpa attacks are analytically derived under appropriate approximations, and a novel figure of merit to measure the dpa effectiveness of multibit attacks is proposed. this figure of merit allows for identifying conditions that maximize the effectiveness of dpa attacks, i.e., conditions under which a cryptographic chip should be tested to assess its robustness. several interesting properties of dpa attacks are derived, and suggestions to design algorithms and circuits with higher robustness against dpa are given. the proposed model is validated in the case of des and aes algorithms with both simulations on an mips32 architecture and measurements on an fpga-based implementation of aes. the model accuracy is shown to be adequate, as the resulting error is always lower than 10 percent and typically of a few percentage points.differential power analysis attacks to precharged buses','Spoofing attacks'
'a new method lets you trace the origins of spam, virus attacks, or illegally copyrighted materials by locating internet messages&#253; regions of origin.digital postmark helps fight spam, virus attacks','Spoofing attacks'
'klimov and shamir proposed a new class of simple cryptographic primitives named t-functions. for two concrete proposals based on the squaring operation, a single word t-function and a previously unbroken multi-word t-function with a 256-bit state, we describe an efficient distinguishing attack having a 232 data complexity. furthermore, hong et al. recently proposed two fully specified stream ciphers, consisting of multi-word t-functions with 128-bit states and filtering functions. we describe distinguishing attacks having a 222 and a 234 data complexity, respectively. the attacks have been implemented.distinguishing attacks on t-functions','Spoofing attacks'
'for the power consumption model called hamming weight model, we rewrite dpa attacks in terms of correlation coefficients between two boolean functions. we exhibit properties of s-boxes (also called (n,m)-functions) relied on dpa attacks. we show that these properties are opposite to the non-linearity criterion and to the propagation criterion. to quantify the resistance of an s-box to dpa attacks, we introduce the notion of transparency order of an s-box and we study this new criterion with respect to the non-linearity and to the propagation criterion.dpa attacks and s-boxes','Spoofing attacks'
'this article reports on the results of our measurement study of the kad network. although several fully decentralized peer-to-peer systems have been proposed in the literature, most existing systems still employ a centralized architecture. the kad network is a notable exception. since the demise of the overnet network, the kad network has become the most popular peer-to-peer system based on a distributed hash table. it is likely that its user base will continue to grow in numbers over the next few years due to the system\'s scalability and reliability. the contribution of the article is twofold. first, we compare the two networks accessed by emule: the centralized paradigmof the edonkey network and the structured, distributed approach pursued by the kad network. we re-engineer the edonkey server software and integrate two modified servers into the edonkey network in order to monitor traffic. additionally, we implement a kad client exploiting a design weakness to spy on the traffic at arbitrary locations in the id space. the collected data provides insights into the spacial and temporal distributions of the peers\' activity. moreover, it allows us to study the searched content. the article also discusses problems related to the collection of such data sets and investigates techniques to verify the representativeness of the measured data. second, this article shows that today\'s kad network can be attacked in several ways. our simple attacks could be used either to hamper the correct functioning of the network itself, to censor content, or to harm other entities in the internet not participating in the kad network, such as ordinary web servers. while there are heuristics to improve the robustness of kad, we believe that the attacks cannot be thwarted easily in a fully decentralized peer-to-peer system, i.e., without some kind of a centralized certification and verification authority. this result may be relevant in the context of the current debate on the design of a clean-slate network architecture for the internet which is based on concepts known from the peer-to-peer paradigm.edonkey & emule\'s kad: measurements & attacks','Spoofing attacks'
'as an anonymous internet communication system tor is popular and famous, being used by lots of users. the security of tor is based on the authentication protocol. although the tor authentication protocol has been proved secure, this paper discovers its security vulnerability through its concurrency analysis, and shows it cannot be securely executed by multiple concurrent sessions. a new session-key exchange protocol for tor is proposed to dispose of the security vulnerability, where a modular method is adopted to design a secure key exchange protocol in realistic world. finally, the proposed protocol is proved secure in the uc (universally composable) model which defines conditions for a protocol to securely compose with other protocols in a concurrent environment.effective attacks in the tor authentication protocol','Spoofing attacks'
'we describe several software side-channel attacks based on inter-process leakage through the state of the cpu&#x2019;s memory cache. this leakage reveals memory access patterns, which can be used for cryptanalysis of cryptographic primitives that employ data-dependent table lookups. the attacks allow an unprivileged process to attack other processes running in parallel on the same processor, despite partitioning methods such as memory protection, sandboxing, and virtualization. some of our methods require only the ability to trigger services that perform encryption or mac using the unknown key, such as encrypted disk partitions or secure network links. moreover, we demonstrate an extremely strong type of attack, which requires knowledge of neither the specific plaintexts nor ciphertexts and works by merely monitoring the effect of the cryptographic process on the cache. we discuss in detail several attacks on aes and experimentally demonstrate their applicability to real systems, such as openssl and linux&#x2019;s dm-crypt encrypted partitions (in the latter case, the full key was recovered after just 800 writes to the partition, taking 65 milliseconds). finally, we discuss a variety of countermeasures which can be used to mitigate such attacks.efficient cache attacks on aes, and countermeasures','Spoofing attacks'
'in this paper the e-mail directory harvest attacks (dha) are investigated. we elaborated a method for optimizing the wordlist size used by the attacker in a resource limited environment. we analyzed the results and proved that our method is optimal. we also present an efficient countermeasure against dha.efficient directory harvest attacks','Spoofing attacks'
'in the traditional definition of dung\'s abstract argumentation framework ($\\ensuremath{af}$), the notion of attack is understood as a relation between arguments, thus bounding attacks to start from and be directed to arguments. this paper introduces a generalized definition of abstract argumentation framework called $\\ensuremath{afra}$ (argumentation framework with recursive attacks), where an attack is allowed to be directed towards another attack. from a conceptual point of view, we claim that this generalization supports a straightforward representation of reasoning situations which are not easily accommodated within the traditional framework. from the technical side, we first investigate the extension to the generalized framework of the basic notions of conflict-free set, acceptable argument, admissible set and of dung\'s fundamental lemma. then we propose a correspondence from the $\\ensuremath{afra}$ to the $\\ensuremath{af}$ formalism, showing that it satisfies some basic desirable properties. finally we analyze the relationships between $\\ensuremath{afra}$ and a similar extension of dung\'s abstract argumentation framework, called $\\ensuremath{eaf+}$ and derived from the recently proposed formalism $\\ensuremath{eaf}$. encompassing attacks to attacks in abstract argumentation frameworks','Spoofing attacks'
'accurate recovery from a cyber attack depends on fast and perfect damage assessment. for damage assessment, traditional recovery methods require that the log of an affected database must be scanned starting from the attacking transaction until the end. this is a time-consuming task. our objective in this research is to provide techniques that can be used to accelerate the damage appraisal process and produce a correct result. we have presented a damage assessment model and four data structures associated with the model. each of these structures uses dependency relationships among transactions, which update the database. these relationships are later used to determine exactly which transactions and exactly which data items are affected by the attacker. a performance comparison analysis obtained using simulation is provided to demonstrate the benefit of our modelevaluating damage from cyber attacks','Spoofing attacks'
'randomness extractors are important tools in cryptography. their goal is to compress a high-entropy source into a more uniform output. beyond their theoretical interest, they have recently gained attention because of their use in the design and proof of leakage-resilient primitives, such as stream ciphers and pseudorandom functions. however, for these proofs of leakage resilience to be meaningful in practice, it is important to instantiate and implement the components they are based on. in this context, while numerous works have investigated the implementation properties of block ciphers such as the aes rijndael, very little is known about the application of side-channel attacks against extractor implementations. in order to close this gap, this paper instantiates a low-cost hardware extractor and analyzes it both from a performance and from a side-channel security point of view. our investigations lead to contrasted conclusions. on the one hand, extractors can be efficiently implemented and protected with masking. on the other hand, they provide adversaries with many more exploitable leakage samples than, e.g. block ciphers. as a result, they can ensure high security margins against standard (non-profiled) side-channel attacks and turn out to be much weaker against profiled attacks. from a methodological point of view, our analysis consequently raises the question of which attack strategies should be considered in security evaluations.extractors against side-channel attacks','Spoofing attacks'
'passive network monitors, known as telescopes or darknets, have been invaluable in detecting and characterizing malware outbreaks. however, as the use of such monitors becomes commonplace, it is likely that malware will evolve to actively detect and evade them. this paper highlights the threat of simple, yet effective, evasive attacks that undermine the usefulness of passive monitors. our results raise an alarm to the research and operational communities to take proactive countermeasures before we are forced to defend against similar attacks appearing in the wild. specifically, we show how lightweight, coordinated sampling of the ip address space can be used to successfully detect and evade passive network monitors. equally troubling is the fact that in doing so attackers can locate the &#8220;live&#8221; ip space clusters and divert malware scanning solely toward active networks. we show that evasive attacks exploiting this knowledge are also extremely fast, overtaking the entire vulnerable population within seconds.fast and evasive attacks','Spoofing attacks'
'in this paper, we present some major algorithmic improvements to fast correlation attacks. in previous articles about fast correlations, algorithmics never was the main topic. instead, the authors of these articles were usually addressing theoretical issues in order to get better attacks. this viewpoint has produced a long sequence of increasingly successful attacks against stream ciphers, which share a main common point: the need to find and evaluate parity-checks for the underlying linear feedback shift register. in the present work, we deliberately take a different point of view and we focus on the search for efficient algorithms for finding and evaluating parity-checks. we show that the simple algorithmic techniques that are usually used to perform these steps can be replaced by algorithms with better asymptotic complexity using more advanced algorithmic techniques. in practice, these new algorithms yield large improvements on the efficiency of fast correlation attacks.fast correlation attacks','Spoofing attacks'
'at ches 2009, coron, joux, kizhvatov, naccache and paillier (cjknp) exhibited a fault attack against rsa signatures with partially known messages. this fault attack allows factoring the public modulus n. while the size of the unknown message part (ump) increases with the number of faulty signatures available, the complexity of cjknp&#39;s attack increases exponentially with the number of faulty signatures. this paper describes a simpler attack, whose complexity remains polynomial in the number of faults; consequently, the new attack can handle much larger umps. the new technique can factor n in a fraction of a second using ten faulty emv signatures &#8211; a target beyond cjknp&#39;s reach. we also show how to apply the attack even when n is unknown, a frequent situation in real-life attacks.fault attacks against emv signatures','Spoofing attacks'
'one of the most crucial development phases of a network intrusion detection system is the feature selection one. a poorly chosen set of features may lead to a significant drop in the detection rate, regardless of the employed detection method. despite its importance, we believe, that this research area lacks of comprehensive studies. our research proposes a model for mining the best features that can be extracted directly from the network packets, by ranking them against their statistical properties during the normal and intrusive stages. as proof of concept, we study the performance of 673 network features while considering a set of 180 different tuning parameters. the main contribution of this work is that it proposes a ranking mechanism to evaluate the effectiveness of features against different types of attacks, and that it suggests a pool of features that could be used to improve the detection process.features vs. attacks','Spoofing attacks'
'this paper introduces a new class of optical fault injection attacks called bumping attacks. these attacks are aimed at data extraction from secure embedded memory, which usually stores critical parts of algorithms, sensitive data and cryptographic keys. as a security measure, read-back access to the memory is not implemented leaving only authentication and verification options for integrity check. verification is usually performed on relatively large blocks of data, making brute force searching infeasible. this paper evaluates memory verification and aes authentication schemes used in secure microcontrollers and a highly secure fpga. by attacking the security in three steps, the search space can be reduced from infeasible &gt; 2100 to affordable &#8776; 215 guesses per block of data. this progress was achieved by finding a way to preset certain bits in the data path to a known state using optical bumping. research into positioning and timing dependency showed that flash memory bumping attacks are relatively easy to carry out.flash memory \'bumping\' attacks','Spoofing attacks'
'friend-in-the-middle attacks on social networking sites can be used to harvest social data in an automated fashion. attackers can then exploit this data for large-scale attacks using context-aware spam and social phishing. the authors prove the feasibility of such an attack and simulate the impact on facebook. alarmingly, all major social networking sites are vulnerable to this attack because they fail to appropriately secure the network layer.friend-in-the-middle attacks','Spoofing attacks'
'let a be a feistel scheme with 5 rounds from 2n bits to 2n bits. in the present paper we show that for most such schemes a: 1. it is possible to distinguish a from a random permutation from 2n bits to 2n bits after doing at most o(2 7n/4) computations with o(2 7n/4) random plaintext/ciphertext pairs. 2. it is possible to distinguish a from a random permutation from 2n bits to 2n bits after doing at most o(2 3n/2) computations with o(2 3n/2) chosen plaintexts. since the complexities are smaller than the number 22n of possible inputs, they show that some generic attacks always exist on feistel schemes with 5 rounds. therefore we recommend in cryptography to use feistel schemes with at least 6 rounds in the design of pseudo-random permutations. we will also show in this paper that it is possible to distinguish most of 6 round feistel permutations generator from a truly random permutation generator by using a few (i.e. o(1)) permutations of the generator and by using a total number of o(22n) queries and a total of o(22n) computations. this result is not really useful to attack a single 6 round feistel permutation, but it shows that when we have to generate several pseudorandom permutations on a small number of bits we recommend to use more than 6 rounds. we also show that it is also possible to extend these results to any number of rounds, however with an even larger complexity.generic attacks on feistel schemes','Spoofing attacks'
'misty schemes are classic cryptographic schemes used to construct pseudo-random permutations from 2n bits to 2n bits by using d pseudo-random permutations from n bits to n bits. these d permutations will be called the \"internal\" permutations, and d is the number of rounds of the misty scheme. misty schemes are important from a practical point of view since for example, the kasumi algorithm based on misty schemes has been adopted as the standard block cipher in the third generation mobile systems. in this paper we describe the best known \"generic\" attacks on misty schemes, i.e. attacks when the internal permutations do not have special properties, or are randomly chosen. we describe known plaintext attacks (kpa), non-adaptive chosen plaintext attacks (cpa-1) and adaptive chosen plaintext and ciphertext attacks (cpca-2) against these schemes. some of these attacks were previously known, some are new. when d = 5 rounds, it is shown in [6] that a cpa-1 exists with complexity 2n. we will present completely different attacks with d = 5 and the same complexity. we will also present new attacks for d &#8804; 4 and d &#8805; 6. for d &#8805; 6 the complexity will be greater than 22n, so these attacks will be useful only when the number of rounds d is small.generic attacks on misty schemes','Spoofing attacks'
'in this talk, we consider the important problem of generic attacks on symmetric cipher algorithms. we review the work on exhaustive search attacks, especially with respect to des. an interesting variant of exhaustive search is to use a so-called time-memory trade-off (tmto) attack. this attack and its many variants will be described and recent research on tmto attacks will be surveyed. the effectiveness of generic attacks is determined by improvements in computer technology and the related costs. we will attempt to derive suitable relationships between these parameters. such relationships will be used to address questions about how secure are 80-bit and 128-bit ciphers. the talk will also cover research on generic attacks using non-conventional technology. finally, we will relate some modern ideas to earlier ideas used in the cryptanalysis of enigma.generic attacks on symmetric ciphers','Spoofing attacks'
'since its creation, the global positioning system (gps) has grown from a limited purpose positioning system to a ubiquitous trusted source for positioning, navigation, and timing data. to date, researchers have essentially taken a signal processing approach to gps security and shown that gps is vulnerable to jamming and spoofing. in this work, we systematically map out a larger attack surface by viewing gps as a computer system. our surface includes higher level gps protocol messages than previous work, as well as the gps os and downstream dependent systems. we develop a new hardware platform for gps attacks, and develop novel attacks against gps infrastructure. our experiments on consumer and professional-grade receivers show that gps and gps-dependent systems are significantly more vulnerable than previously thought. for example, we show that remote attacks via malicious gps broadcasts are capable of bringing down up to 30\% and 20\% of the global cors navigation and ntrip networks, respectively, using hardware that costs about the same as a laptop. in order to improve security, we propose systems-level defenses and principles that can be deployed to secure critical gps and dependent systems.gps software attacks','Spoofing attacks'
'this paper presents a non-invasive, half-blind firmware extraction technique that subverts a mask rom boot-loader in order to recover the firmware of a microcontroller. a stack based buffer overflow is used to forcibly enter the bootloader. further, a practical method of blind return-oriented programming is presented in which a gadget\'s entry point is brute forced, being unknown a priori. in this paper we show that when a software vulnerability has been found (e.g. by fuzzing), the attacker can locate by brute force the sequences of instructions, gadgets, required for bypassing the protections present in a bootloader. in a half-blind attack, the presence of a bootloader in mask rom helps the attacker in that, while he must still discover blindly a vulnerability in unknown firmware and the appropriate gadgets, he knows the exact contents of the bootloader.half-blind attacks','Spoofing attacks'
'the timing attack (ta) is a side-channel analysis (sca) variant that exploits information leakage through the computation duration. previously, leakages in timing have been exploited by comparison analysis, most often thanks to \"correlation - collision\" or pre-characterization on a clone device. time bias can also be used to break a secret crypto-system by linear correlations in a non-profiled setting. there is direct parallel between the correlation power attack (cpa) and ta, the distinguisher being the same, but the exploited data being either vertical or horizontal. the countermeasures against such attacks consist in making the algorithm run in either random or constant time. in this paper, we show that the former is prone to high-order attacks that analyse the higher moments of the time computation during code execution. we present successful second-order timing attacks (2o-ta) based on a correlation and compare it to the second-order power attack. all experiments have been conducted on an 8-bit processor running an aes-128.high-order timing attacks','Spoofing attacks'
'highly parallel cryptographic attacks','Spoofing attacks'
'rebound attacks are a state-of-the-art analysis method for hash functions. these cryptanalysis methods are based on a well chosen differential path and have been applied to several hash functions from the sha-3 competition, providing the best known analysis in these cases. in this paper we study rebound attacks in detail and find for a large number of cases that the complexities of existing attacks can be improved. cryptanalytic situation, and by using better algorithms to find solutions for the differential path. our improvements affect one particular operation that appears in most rebound attacks and which is often the bottleneck of the attacks. this operation, which varies depending on the attack, can be roughly described as merging large lists. as a result, we introduce new general purpose algorithms for enabling further rebound analysis to be as performant as possible. we illustrate our new algorithms on real hash functions.how to improve rebound attacks','Spoofing attacks'
'in recent years, researchers have proposed systems for running trusted code on an untrusted operating system. protection mechanisms deployed by such systems keep a malicious kernel from directly manipulating a trusted application\'s state. under such systems, the application and kernel are, conceptually, peers, and the system call api defines an rpc interface between them. we introduce iago attacks, attacks that a malicious kernel can mount in this model. we show how a carefully chosen sequence of integer return values to linux system calls can lead a supposedly protected process to act against its interests, and even to undertake arbitrary computation at the malicious kernel\'s behest. iago attacks are evidence that protecting applications from malicious kernels is more difficult than previously realized.iago attacks','Spoofing attacks'
'lblock is a lightweight block cipher with 32 rounds, which can be implemented efficiently not only in hardware environment but also in software platforms. in this paper, by exploiting the structure of lblock and the redundancy in its key schedule, we propose an impossible differential attack on 21-round lblock based on a 14-round impossible differential. the data and time complexities are about 262.5 chosen plaintexts and 273.7 21-round encryptions, respectively. as far as we know, these results are the currently best results on lblock in the single key scenario.impossible differential attacks on reduced-round lblock','Spoofing attacks'
'gost is a well known block cipher which was developed in the soviet union during the 1970\'s as an alternative to the us-developed des. in spite of considerable cryptanalytic effort, until very recently there were no published single key attacks against its full 32-round version which were faster than the 2256 time complexity of exhaustive search. in february 2011, isobe used the previously discovered reflection property in order to develop the first such attack, which requires 232 data, 264 memory and 2224 time. in this paper we introduce a new fixed point property and a better way to attack 8-round gost in order to find improved attacks on full gost: given 232 data we can reduce the memory complexity from an impractical 264 to a practical 236 without changing the 2224 time complexity, and given 264 data we can simultaneously reduce the time complexity to 2192 and the memory complexity to 236.improved attacks on full gost','Spoofing attacks'
'in this paper we investigate the strength of the secret-key algorithm rc5 newly proposed by ron rivest. the target version of rc5 works on words of 32 bits, has 12 rounds and a user-selected key of 128 bits. at crypto\'95 kaliski and yin estimated the strength of rc5 by differential and linear cryptanalysis. they conjectured that their linear analysis is optimal and that the use of 12 rounds for rc5 is sufficient to make both differential and linear cryptanalysis impractical. in this paper we show that the differential analysis made by kaliski and yin is not optimal. we give differential attacks better by up to a factor of 512. also we show that rc5 has many weak keys with respect to differential attacks. this weakness relies on the structure of the cipher and not on the key schedule.improved differential attacks on rc5','Spoofing attacks'
'the slide attack is applicable to ciphers that can be represented as an iterative application of the same keyed permutation. the slide attack leverages simple attacks on the keyed permutation to more complicated (and time consuming) attacks on the entire cipher. in this paper we extend the slide attack by examining the cycle structures of the entire cipher and of the underlying keyed permutation. our method allows to find slid pairs much faster than was previously known, and hence reduces the time complexity of the entire slide attack significantly. in addition, since our attack finds as many slid pairs as the attacker requires, it allows to leverage all types of attacks on the underlying permutation (and not only simple attacks) to an attack on the entire cipher. we demonstrate the strength of our technique by presenting an attack on 24-round reduced gost whose s-boxes are unknown. our attack retrieves the unknown s-boxes as well as the secret key with a time complexity of about 263 encryptions. thus, this attack allows an easier attack on other instances of gost that use the same s-boxes. when the s-boxes are known to the attacker, our attack can retrieve the secret key of 30-round gost (out of the 32 rounds).improved slide attacks','Spoofing attacks'
'recent high-profile attacks against governments and large industry demonstrate that malware can be used for effective industrial espionage. most previous incident reports have focused on describing the anatomy of specific incidents and data breaches. in this paper, we provide an in-depth analysis of a large corpus of targeted attacks identified by symantec during the year 2011. using advanced triage data analytics, we are able to attribute series of targeted attacks to attack campaigns quite likely performed by the same individuals. by analyzing the characteristics and dynamics of those campaigns, we provide new insights into the modus operandi of attackers involved in those campaigns. finally, we evaluate the prevalence and sophistication level of those targeted attacks by analyzing the malicious attachments used as droppers. while a majority of the observed attacks rely mostly on social engineering, have a low level of malware sophistication and use little obfuscation, our malware analysis also shows that at least eight attack campaigns started about two weeks before the disclosure date of the exploited vulnerabilities, and therefore were probably using zero-day attacks at that time.industrial espionage and targeted attacks','Spoofing attacks'
'although the privacy threats and countermeasures associated with location data are well known, there has not been a thorough experiment to assess the effectiveness of either. we examine location data gathered from volunteer subjects to quantify how well four different algorithms can identify the subjects\' home locations and then their identities using a freely available, programmable web search engine. our procedure can identify at least a small fraction of the subjects and a larger fraction of their home addresses. we then apply three different obscuration countermeasures designed to foil the privacy attacks: spatial cloaking, noise, and rounding. we show how much obscuration is necessary to maintain the privacy of all the subjects.inference attacks on location tracks','Spoofing attacks'
'in a masquerade attack, an adversary who has stolen a legitimate user\'s credentials attempts to impersonate him to carry out malicious actions. automatic detection of such attacks is often undertaken constructing models of normal behaviour of each user and then measuring significant departures from them. one potential vulnerability of this approach is that anomaly detection algorithms are generally susceptible of being deceived. in this paper, we first investigate how a resourceful masquerader can successfully evade detection while still accomplishing his goals. we then propose an algorithm based on the kullback-leibler divergence which attempts to identify if a sufficiently anomalous attack is present within an apparently normal request. our experimental results indicate that the proposed scheme achieves considerably better detection quality than adversarial-unaware approaches.information-theoretic detection of masquerade mimicry attacks','Spoofing attacks'
'the satellite tv industry relies heavily on the use of smart card technology at the very heart of broadcasted services that are protected by legacy conditional access systems. the process of satellite tv signal protection is distributed amongst a number of system components, e.g. smart cards, receivers, conditional access modules (cam) and the content provider. however, the introduction of &#8220;open&#8221; satellite receivers, providing a highly configurable environment with software emulation of conditional access systems, enabled the implementation of whole range of new attacks. a widely deployed attack is often referred to as the &#8220;card sharing&#8221; attack, by which one legitimate user colludes to provide protected content to a larger group of unauthorised users. this paper proposes a countermeasure that increases the bandwidth requirements of this attack to the point where it is no longer practical with a standard internet connection, with a minimal impact on existing protocols and architectures.inhibiting card sharing attacks','Spoofing attacks'
'the computer-security industry is familiar with the concept of a malicious insider. however, a malicious insider in the cloud might have access to an unprecedented amount of information and on a much greater scale. given the level of threat posed by insiders, and the rapid growth of the cloud computing ecosystem, we examine here the concept of insider attacks in cloud computing. specifically, if more of our assets are going to reside in the cloud, and as increasingly our lives, enterprises and prosperity may depend upon cloud, it is imperative that we understand the scope for insider attacks so that we might best prepare defenses. we need to understand whether cloud might expose our assets to increased threat in terms of both actors and attack surface. we present here an assessment of current insider threat definitions and classifications, and their applicability to the cloud. we elucidate the nature of insiders with reference to the cloud ecosystem and close with examples of insider attacks which are specific to cloud environments (and hence hard to detect using current techniques).insider attacks in cloud computing','Spoofing attacks'
'a common misconception concerning network security is that the infrastructure is at considerable risk from external attackers. although there is always the opportunity that an enterprise environment may be targeted by a skilled, and above all, patient external attacker, enterprises face a significant challenge in identifying and reacting to, insider attacks. this article focuses on the range of insider based attacks that the enterprise environment may be vulnerable to, as well as methods of responding to an attack and mitigating the risks in the first instance.insider attacks','Spoofing attacks'
'cloud computing has emerged as a model to process large volumetric data. though cloud computing is very popular, cloud security could delay its adoption. security of the cloud must provide data confidentiality and protection of resources. such architecture seems to be vulnerable when confronted to distributed attacks also known as large-scale coordinated attacks. in this paper, we study the impact of large-scale coordinated attacks on cloud computing and its current security solutions. we experiment the open-source ids snort and a commercialized firewall using distributed port scan. our results show that these security solutions are not designed to detect distributed attacks. indeed, an attacker who controls about 32 hosts can easily achieve a distributed port scan without being detected.large-scale coordinated attacks','Spoofing attacks'
'ntru is a new public key cryptosystem proposed at crypto 96 by hoffstein, pipher and silverman from the mathematics department of brown university. it attracted considerable attention, and is being advertised over the internet by ntru cryptosystems. its security is based on the difficulty of analyzing the result of polynomial arithmetic modulo two unrelated moduli, and its correctness is based on clustering properties of the sums of random variables. in this paper, we apply new lattice basis reduction techniques to cryptanalyze the scheme, to discover either the original secret key, or an alternative secret key which is equally useful in decoding the ciphertexts.lattice attacks on ntru','Spoofing attacks'
'in this paper, a novel class of power analysis attacks to cryptographic circuits is presented. these attacks aim at recovering the secret key of a cryptographic core from measurements of its static (leakage) power. these attacks exploit the dependence of the leakage current of cmos integrated circuits on their inputs (including the secret key of the cryptographic algorithm that they implement), as opposite to traditional power analysis attacks that are focused on the dynamic power. for this reason, this novel class of attacks is named \"leakage power analysis\" (lpa). since the leakage power increases much faster than the dynamic power at each new technology generation, lpa attacks are a serious threat to the information security of cryptographic circuits in sub-100-nm technologies. for the first time in the literature, a well-defined procedure to perform lpa attacks that is based on a solid theoretical background is presented. advantages and measurement issues are also analyzed in comparison with traditional power analysis attacks based on dynamic power measurements. examples are provided for various circuits, and an experimental attack to a register is performed for the first time. an analytical model of the lpa attack result is also provided to better understand the effectiveness of this technique. the impact of technology scaling is explicitly addressed by means of a simple analytical model and monte carlo simulations. simulations on a 65- and 90-nm technology and experimental results are presented to justify the assumptions and validate the leakage power models that are adopted.leakage power analysis attacks','Spoofing attacks'
'since it is essentially impossible to write large-scale software without errors, any intrusion tolerant system must be able to tolerate rapid, repeated unknown attacks without exhausting its redundancy. our system provides continued application services to critical users while under attack with a goal of less than 25\% degradation of productivity. initial experimental results are promising. it is not yet a general open solution. specification-based behavior sensors (allowable actions, objects, and qos) detect attacks. the system learns unknown attacks by relying on two characteristics of network-accessible software faults: attacks that exploit them must be repeatable (at least in a probabilistic sense) and, if known, attacks can be stopped at component boundaries. random rejuvenation limits the scope of undetected errors. the current system learns and blocks single-stage unknown attacks against a protected web server by searching and testing service history logs in a sandbox after a successful attack. we also have an initial class-based attack generalization technique that stops web-server buffer overflow attacks. we are working to extend both techniques.learning unknown attacks - a start','Spoofing attacks'
'in this paper, an improved differential cryptanalysis framework for finding collisions in hash functions is provided. its principle is based on linearization of compression functions in order to find low weight differential characteristics as initiated by chabaud and joux. this is formalized and refined however in several ways: for the problem of finding a conforming message pair whose differential trail follows a linear trail, a condition function is introduced so that finding a collision is equivalent to finding a preimage of the zero vector under the condition function. then, the dependency table concept shows how much influence every input bit of the condition function has on each output bit. careful analysis of the dependency table reveals degrees of freedom that can be exploited in accelerated preimage reconstruction under the condition function. these concepts are applied to an in-depth collision analysis of reduced-round versions of the two sha-3 candidates cubehash and md6, and are demonstrated to give by far the best currently known collision attacks on these sha-3 candidates. linearization framework for collision attacks','Spoofing attacks'
'internet threat monitoring (itm) systems are a widely deployed facility to detect, analyze, and characterize dangerous internet threats such as worms and distributed denial-of-service (ddos) attacks. nonetheless, an itm system can also become the target of attacks. in this paper, we address localization attacks against itm systems in which an attacker impairs the effectiveness of an itm system by identifying the locations of itm monitors. we propose an information-theoretic framework that models localization attacks as communication channels. based on this model, we generalize all existing attacks as \"temporal attacks,&#8221; derive closed formulas of their performance, and propose an effective attack detection approach. the information-theoretic model also inspires a new attack called a spatial attack and motivates the corresponding detection approach. we show simulation results that support our theoretic findings.localization attacks to internet threat monitors','Spoofing attacks'
'low cost attacks on tamper resistant devices','Spoofing attacks'
'in smart grid, sensor measurements are often sent to a control node over a hop-by-hop network of sensors themselves. to prevent en route accidental and malicious data corruption, each message is authenticated with a mac, keyed with a symmetric key known to the generating sensor and the control node. macs represent a significant overhead: a typical 128-bit mac may often authenticate a 10-bit temperature reading. to mitigate these overheads, mac aggregation methods were proposed. however, previously proposed mac aggregation schemes are not resilient to denial-of-service (dos) attacks, where a rogue node or a man-in-the-middle attacker can easily disrupt the entire set of macs, and hence prevent using any of the transmitted data. in this work we propose a new way of mac aggregation, which will allow the relay sensors to greatly reduce transmission overhead due to macs, while achieving full unforgeability, and, simultaneously, much stronger resilience to dos attacks.mac aggregation protocols resilient to dos attacks','Spoofing attacks'
'details about arp poisoning attacks as well as countermeasures have been known for years. yet, most networks are still vulnerable to these attacks because they haven\'t implemented defenses. this article explains how arp poisoning attacks work and analyzes how such attacks are exploited in the wild against web hosting companies, where the arp poisoning attacks are used for real-time network traffic modification.malicious javascript insertion through arp poisoning attacks','Spoofing attacks'
'jaques erasmus from anti-malware company prevx looks at the way malware authors and distributors get their end product to the victim\'s machine. he explores the routes these coders take and how they achieve the most effective distribution of their newly built malware. in doing so, he touches on a myriad of elements, from malware sdks, to exploitation packs and loaders. pulling examples of real world attacks into the article he looks at some trends that are becoming prevalent, and outlines what we can expect for 2009. since the first pc virus called brain appeared in january 1986, technologies available to aid the creation and monetisation of malware have undergone tremendous development. we have seen the rise and fall of a variety of different underground groups and are currently fighting a new breed of malware authors and distributors that have honed their tools to monetise as many victims as efficiently as possible. in this article we will focus on the tools that are keeping this criminal ecosystem alive.malware attacks','Spoofing attacks'
'man-in-the-browser attacks are a sophisticated new hacking technique associated with internet crime, especially that which targets customers of internet banking. the security community has been aware of them as such for time but they have grown in ability and success during that time. these attacks are a specialised version of man-in-the-middle attack, and operate by stealing authentication data and altering legitimate user transactions to benefit the attackers. this paper examines what man-in-the-browser attacks are capable of and how specific versions of the attack are executed, with reference to their control structure, data interaction techniques, and methods for circumventing security. finally the authors discuss the effectiveness of counter-man-in-the-middle strategies, and speculate upon what these attacks tell us about the internet environment.man in the browser attacks','Spoofing attacks'
'the block cipher mars has been designed by a team from ibm and became one of the five finalists for the aes. a unique feature is the usage of two entirely different round function types. the \"wrapper rounds\" are unkeyed, while the key schedule for the \"core rounds\" is a slow and complex one, much more demanding then, e.g., the key schedule for the aes. each core round employs a 62-bit round key. the best attack published so far [kks00] was applicable to 11 core rounds, and succeeded in recovering some 163 round key bits. but neither did it deal with inverting the key schedule, nor did it provide any other means to recover the remaining 519 round key bits in usage. our attack applies to 12 core rounds, needs 2252 operations, 265 chosen plaintexts and 269 memory cells. after recovering a limited number of cipher key bits, we deal with the inverse key-schedule to recover the original encryption key. this allows the attacker to easily generate all the round keys in the full.mars attacks! revisited','Spoofing attacks'
'mars attacks!','Spoofing attacks'
'in the recent past, anti-p2p companies have successfully curtailed the distribution of targeted content over a number of p2p file-sharing systems, including kazaa and edonkey. more recently, anti-p2p companies have begun to attack bittorrent. in this paper, we analyze the resilience of bittorrent leechers to two different kinds of attacks: the connection attack and the piece attack. we present the results of both passive and active measurements. using passive measurements, we performed a detailed analysis of a recent album that is under attack, and identified the behavior of attackers. for our active measurements, we developed a crawler that contacts all the peers in any given swarm, determines whether the swarm is under attack, and identifies the attack peers in the swarm. we used the crawler to analyze 8 top box-office movies. based on the measurement results, we discovered that bittorrent architecture is fundamentally resilient to internet leecher attacks. finally, we also propose defense mechanisms to further mitigate the attacks and evaluate their effectiveness using trace-driven simulation.measurement and mitigation of bittorrent leecher attacks','Spoofing attacks'
'mimicry attacks have been the focus of detector research where the objective of the attacker is to generate an attack that evades detection while achieving the attacker&#8217;s goals. if such an attack can be found, it implies that the target detector is vulnerable against mimicry attacks. in this work, we emphasize that there are two components of a buffer overflow attack: the preamble and the exploit. although the attacker can modify the exploit component easily, the attacker may not be able to prevent preamble from generating anomalous behavior since during preamble stage, the attacker does not have full control. previous work on mimicry attacks considered an attack to completely evade detection, if the exploit raises no alarms. on the other hand, in this work, we investigate the source of anomalies in both the preamble and the exploit components against two anomaly detectors that monitor four vulnerable unix applications. our experiment results show that preamble can be a source of anomalies, particularly if it is lengthy and anomalous.mimicry attacks demystified','Spoofing attacks'
'kademlia-based dht has been deployed in many p2p applications and it is reported that there are millions of simultaneous users in kad network. for such a protocol that significantly involves so many peers, its robustness and security must be evaluated carefully. in this paper, we analyze the kademlia protocol and identify several potential vulnerabilities. we classify potential attacks as three types: asymmetric attack, routing table reflection attack and index reflection attack. a limited real-world experiment was run on emule and the results show that these attacks tie up bandwidth and tcp connection resources of victim. we analyze the results of our experiment in three aspects: the effect of ddos attacks by misusing kad in emule, the comparison between asymmetric attack and routing table reflection attack, and the distribution of attacks. more large-scale ddos attack can be performed by means of a little more effort. we introduce some methods to amplify the performance of attack and some strategies to evade detection. finally, we further discuss several solutions for these ddos attacks.misusing kademlia protocol to perform ddos attacks','Spoofing attacks'
'this paper considers dos attacks on dns wherein attackers flood the nameservers of a zone to disrupt resolution of resource records belonging to the zone and consequently, any of its sub-zones. we propose a minor change in the caching behavior of dns resolvers that can significantly alleviate the impact of such attacks. in our proposal, dns resolvers do not completely evict cached resource records whose ttl has expired; rather, such resource records are stored in a separate \"stale cache\". if, during the resolution of a query, a resolver does not receive any response from the nameservers that are responsible for authoritatively answering the query, it can use the information stored in the stale cache to answer the query. in effect, the stale cache is the part of the global dns database that has been accessed by the resolver and represents an insurance policy that the resolver uses only when the relevant dns servers are unavailable. we analyze a 65-day dns trace to quantify the benefits of a stale cache under different attack scenarios. further, while the proposed change to dns resolvers also changes dns semantics, we argue that it does not adversely impact any of the fundamental dns characteristics such as the autonomy of zone operators and hence, is a very simple and practical candidate for mitigating the impact of dos attacks on dns.mitigating dns dos attacks','Spoofing attacks'
'smartphones\' features are great, but with the power they provide, there\'s also a threat. smartphones are becoming a target of attackers in the same way pcs have been for many years. this article examines the security models of two popular smart phone operating systems: apple\'s ios and google\'s android.mobile attacks and defense','Spoofing attacks'
'a first contribution of this paper is a theoretical yet practically applicable model covering a large set of phishing attacks, aimed towards developing an understanding of threats relating to phishing. we model an attack by a phishing graph in which nodes correspond to knowledge or access rights, and (directed) edges correspond to means of obtaining information or access rights from already possessed information or access rights &#8211; whether this involves interaction with the victim or not. edges may also be associated with probabilities, costs, or other measures of the hardness of traversing the graph. this allows us to quantify the effort of traversing a graph from some starting node (corresponding to publicly available information) to a target node that corresponds to access to a resource of the attacker&#8217;s choice. we discuss how to perform economic analysis on the viability of attacks. a quantification of the economical viability of various attacks allows a pinpointing of weak links for which improved security mechanisms would improve overall system security.modeling and preventing phishing attacks','Spoofing attacks'
'modeling partial attacks with alloy','Spoofing attacks'
'nemode is a declarative system for computer network intrusion detection, providing a declarative domain specific language for describing network intrusion signatures which can span several network packets, by stating constraints over network packets, describing relations between several packets in a declarative and expressive way. it provides several back-end detection mechanisms, all based on a constraint programming framework, to perform the detection of the desired signatures. in this work, we demonstrate how to model and perform the detection of distributed network attacks using each of the detection mechanisms provided by nemode, based in gecode, adaptive search and minisat to perform the detection of the specific intrusions. we also use the sliding network traffic window version of the adaptive search back-end detection mechanism to simulate live network traffic and evaluate the performance of the system in conditions near to real life networks.modelling distributed network attacks with constraints','Spoofing attacks'
'buffer overflow bof is a well-known, and one of the worst and oldest, vulnerabilities in programs. bof attacks overwrite data buffers and introduce wide ranges of attacks like execution of arbitrary injected code. many approaches are applied to mitigate buffer overflow vulnerabilities; however, mitigating bof vulnerabilities is a perennial task as these vulnerabilities elude the mitigation efforts and appear in the operational programs at run-time. monitoring is a popular approach for detecting bof attacks during program execution, and it can prevent or send warnings to take actions for avoiding the consequences of the exploitations. currently, there is no detailed classification of the proposed monitoring approaches to understand their common characteristics, objectives, and limitations. in this paper, the authors classify runtime bof attack monitoring and prevention approaches based on seven major characteristics. finally, these approaches are compared for attack detection coverage based on a set of bof attack types. the classification will enable researchers and practitioners to select an appropriate bof monitoring approach or provide guidelines to build a new one.monitoring buffer overflow attacks','Spoofing attacks'
'statistical saturation attacks have been introduced and applied to the block cipher present at ct-rsa 2009. in this paper, we consider their natural extensions. first, we investigate the existence of better trails than the one used in the former attack. for this purpose, we provide a theoretical evaluation of the trail distributions using probability transition matrices. since the exhaustive evaluation of all possible distributions turned out to be computationally hard, we additionally provide a heuristic branch-and-bound algorithm that allows us to generate a large number of good trails. these tools confirm that the trail of ct-rsa 2009 was among the best possible ones, but also suggest that numerous other trails have similar properties. as a consequence, we investigate the use of multiple trails and show that it allows significant improvements of the previous cryptanalysis attempts against present. estimated complexities indicate that present-80 is safe against key recovery, by a small security margin. we also discuss the impact of multiple trails for the security of the full present-128. we finally put forward a \"statistical hull\" effect that makes the precise theoretical analysis of our results difficult, when the number of block cipher rounds increases.multi-trail statistical saturation attacks','Spoofing attacks'
'in 1992, bartholdi, tovey, and trick [1992] opened the study of control attacks on elections--attempts to improve the election outcome by such actions as adding/deleting candidates or voters. that work has led to many results on how algorithms can be used to find attacks on elections and how complexity-theoretic hardness results can be used as shields against attacks. however, all the work in this line has assumed that the attacker employs just a single type of attack. in this paper, we model and study the case in which the attacker launches a multipronged (i.e., multimode) attack. we do so to more realistically capture the richness of reallife settings. for example, an attacker might simultaneously try to suppress some voters, attract new voters into the election, and introduce a spoiler candidate. our model provides a unified framework for such varied attacks, and by constructing polynomial-time multiprong attack algorithms we prove that for various election systems even such concerted, flexible attacks can be perfectly planned in deterministic polynomial time.multimode control attacks on elections','Spoofing attacks'
'back in 2007, hasegawa discovered a novel cross-site scripting (xss) vector based on the mistreatment of the backtick character in a single browser implementation. this initially looked like an implementation error that could easily be fixed. instead, as this paper shows, it was the first example of a new class of xss vectors, the class of mutation-based xss (mxss) vectors, which may occur in innerhtml and related properties. mxss affects all three major browser families: ie, firefox, and chrome. we were able to place stored mxss vectors in high-profile applications like yahoo! mail, rediff mail, openexchange, zimbra, roundcube, and several commercial products. mxss vectors bypassed widely deployed server-side xss protection techniques (like html purifier, kses, htmllawed, blueprint and google caja), client-side filters (xss auditor, ie xss filter), web application firewall (waf) systems, as well as intrusion detection and intrusion prevention systems (ids/ips). we describe a scenario in which seemingly immune entities are being rendered prone to an attack based on the behavior of an involved party, in our case the browser. moreover, it proves very difficult to mitigate these attacks: in browser implementations, mxss is closely related to performance enhancements applied to the html code before rendering; in server side filters, strict filter rules would break many web applications since the mxss vectors presented in this paper are harmless when sent to the browser. this paper introduces and discusses a set of seven different subclasses of mxss attacks, among which only one was previously known. the work evaluates the attack surface, showcases examples of vulnerable high-profile applications, and provides a set of practicable and low-overhead solutions to defend against these kinds of attacks.mxss attacks','Spoofing attacks'
'the common wisdom is that string comparison timing attacks against a hashed password are impossible. however, these attacks can still be effective if attackers give up on the ideal of stealing all the characters representing the user\'s password or the entire hash.network service authentication timing attacks','Spoofing attacks'
'this paper is concerned with a particular type of attack against cbc-macs, namely forgery attacks, i.e. attacks which enable an unauthorised party to obtain a mac on a data string. existing forgery attacks against cbc-macs are briefly reviewed, together with the effectiveness of various countermeasures. this motivates the main part of the paper, where a family of new forgery attacks are described, which raise serious questions about the effectiveness of certain countermeasures.new cbc-mac forgery attacks','Spoofing attacks'
'camellia is one of the most worldwide used block ciphers, which has been selected as a standard by iso/iec. in this paper, we propose several new 7-round impossible differentials of camellia with 2 fl/fl&#8722;1 layers, which turn out to be the first 7-round impossible differentials with 2 fl/fl&#8722;1 layers. combined with some basic techniques including the early abort approach and the key schedule consideration, we achieve the impossible differential attacks on 11-round camellia-128, 11-round camellia-192, 12-round camellia-192, and 14-round camellia-256, and the time complexity are 2123.8, 2121.7, 2171.4 and 2238.3 respectively. as far as we know, these are the best results against the reduced-round variants of camellia. especially, we give the first attack on 11-round camellia-128 reduced version with fl/fl&#8722;1 layers.new impossible differential attacks on camellia','Spoofing attacks'
'we consider the node replication attack, which is an application-independent attack unique to wireless sensor networks. the attack makes it possible for an adversary to prepare her own low-cost sensor nodes and induce the network to accept them as legitimate ones. to do so, the adversary only needs to physically capture one node, reveal its secret credentials, replicate the node in large quantity, and deploy these malicious nodes back into the network so as to subvert the network with little effort. recently, ko et al. proposed a neighbor-based detection scheme to cope with replication attacks. the scheme features distributed detection and takes node mobility into account. it harnesses the dynamic observations of the neighbors of a claimer node and avoids the protocol iterations typically found in distributed detections. unfortunately, we show that their proposal is subject to various replication attacks that can circumvent the detection. moreover, it is even possible for a sophisticated adversary to exploit the protocol to revoke legitimate nodes.node replication attacks in wireless sensor networks','Spoofing attacks'
'sensitivity analysis attacks constitute a powerful family of watermark \"removal\" attacks. they exploit vulnerability in some watermarking protocols: the attacker\'s unlimited access to the watermark detector. this paper proposes a mathematical framework for designing sensitivity analysis attacks and focuses on additive spread-spectrum embedding schemes. the detectors under attack range in complexity from basic correlation detectors to normalized correlation detectors and maximum-likelihood (ml) detectors. the new algorithms precisely estimate and then eliminate the watermark from the watermarked signal. this is accomplished by exploiting geometric properties of the detection boundary and the information leaked by the detector. several important extensions are presented, including the case of a partially unknown detection function, and the case of constrained detector inputs. in contrast with previous art, our algorithms are noniterative and require, at most, o(n) detection operations in order to precisely estimate the watermark, where n is the dimension of the signal. the cost of each detection operation is o(n); hence, the algorithms can be executed in quadratic time. the method is illustrated with an application to image watermarking using an ml detector based on a generalized gaussian model for imagesnoniterative algorithms for sensitivity analysis attacks','Spoofing attacks'
'in this work, jpeg2000 error resilience options and error concealment strategies are discussed and evaluated. error resilience options and error concealment strategies have been employed to mimic attacks against selective / partial jpeg2000 encryption schemes. thus the security evaluation of these selective / partial encryption schemes relies on the proper working of the jpeg2000 error concealment. recommendations for jpeg2000 encryption given in previous work have to be reassessed on the basis of our results. improvements to the error concealment code of the jpeg2000 reference software jj2000 are presented. on jpeg2000 error concealment attacks','Spoofing attacks'
'microprocessors are the heart of the devices we rely on every day. however, their non-volatile memory, which often contains sensitive information, can be manipulated by ultraviolet (uv) irradiation. this paper gives practical results demonstrating that the non-volatile memory can be erased with uv light by investigating the effects of uv-clight with a wavelength of 254nm on four different depackaged microcontrollers. we demonstrate that an adversary can use this effect to attack an aes software implementation by manipulating the 256-bit s-box table. we show that if only a single byte of the table is changed, 2 500 pairs of correct and faulty encrypted inputs are sufficient to recover the key with a probability of 90\%, in case the key schedule is not modified by the attack. furthermore, we emphasize this by presenting a practical attack on an aes implementation running on an 8-bit microcontroller. our attack involves only a standard decapsulation procedure and the use of alow-cost uv lamp.optical fault attacks on aes','Spoofing attacks'
'we describe a new class of attacks on secure microcontrollers and smartcards. illumination of a target transistor causes it to conduct, thereby inducing a transient fault. such attacks are practical; they do not even require expensive laser equipment. we have carried them out using a flashgun bought second-hand from a camera store for $30 and with an $8 laser pointer. as an illustration of the power of this attack, we developed techniques to set or reset any individual bit of sram in a microcontroller. unless suitable countermeasures are taken, optical probing may also be used to induce errors in cryptographic computations or protocols, and to disrupt the processor\'s control flow. it thus provides a powerful extension of existing glitching and fault analysis techniques. this vulnerability may pose a big problem for the industry, similar to those resulting from probing attacks in the mid-1990s and power analysis attacks in the late 1990s.we have therefore developed a technology to block these attacks. we use self-timed dual-rail circuit design techniques whereby a logical 1 or 0 is not encoded by a high or low voltage on a single line, but by (hl) or (lh) on a pair of lines. the combination (hh) signals an alarm, which will typically reset the processor. circuits can be designed so that single-transistor failures do not lead to security failure. this technology may also make power analysis attacks very much harder too.optical fault induction attacks','Spoofing attacks'
'this paper introduces some new types of optical fault attacks called fault masking attacks. these attacks are aimed at disrupting of the normal memory operation through preventing changes of the memory contents. the technique was demonstrated on an eeprom and flash memory inside pic microcontrollers. then it was improved with a backside approach and tested on a pic and msp430microcontrollers. these attacks can be used for the partial reverse engineering of semiconductor chips by spotting the areas of activity in reprogrammable non-volatile memory. this can assist in data analysis and other types of fault injection attacks later, thereby saving the time otherwise required for exhaustive search. practical limits for optical fault masking attacks in terms of sample preparation, operating conditions and chip technology are discussed, together with possible countermeasures.optical fault masking attacks','Spoofing attacks'
'in this paper, well-known attacks named oracle attacks are formulated within a realistic network communication model where they reveal to use suitable covert channels, we name oracle channels. by exploiting information-theoretic notions, we show how to modify detection/authentication watermarking algorithms in order to counteract oracle attacks. we present three proposals, one based on randomization, another one based on time delay and a third one based on both randomization and delay.oracle attacks and covert channels','Spoofing attacks'
'in this paper, we introduce a new class of side--channel attackscalled partitioning attacks. we have successfully launched a versionof the attack on several implementations of comp128, the popular gsmauthentication algorithm that has been deployed by different serviceproviders in several types of sim cards, to retrieve the 128 bit keyusing as few as 8 chosen plaintexts. we show how partitioning attackscan be used effectively to attack implementations that have beenequipped with ad hoc and inadequate countermeasures againstside--channel attacks. such ad hoc countermeasures are systemic inimplementations of cryptographic algorithms, such as comp128, whichrequire the use of large tables since there has been a mistaken beliefthat sound countermeasures require more resources than areavailable. to address this problem, we describe a newresource--efficient countermeasure for protecting table lookups incryptographic implementations and justify its correctness rigorously.partitioning attacks','Spoofing attacks'
'traffic analysis is the best known approach to uncover relationships amongst users of anonymous communication systems, such as mix networks. surprisingly, all previously published techniques require very specific user behavior to break the anonymity provided by mixes. at the same time, it is also well known that none of the considered user models reflects realistic behavior which casts some doubt on previous work with respect to real-life scenarios. we first present a user behavior model that, to the best of our knowledge, is the least restrictive scheme considered so far. second, we develop the perfect matching disclosure attack, an efficient attack based on graph theory that operates without any assumption on user behavior. the attack is highly effective when de-anonymizing mixing rounds because it considers all users in a round at once, rather than single users iteratively. furthermore, the extracted sender-receiver relationships can be used to enhance user profile estimations. we extensively study the effectiveness and efficiency of our attack and previous work when de-anonymizing users communicating through a threshold mix. empirical results show the advantage of our proposal. we also show how the attack can be refined and adapted to different scenarios including pool mixes, and how precision can be traded in for speed, which might be desirable in certain cases.perfect matching disclosure attacks','Spoofing attacks'
'phishing is a form of online identity theft employing both social engineering and technical subterfuge to steal user credentials such as usernames and passwords. targeted data sources include especially web pages, email spam, domain names. mounting a phishing attacks may take several ways but the popular one takes the form of a phishing message arrives in the user mailbox pretending to be from a bank, directing the user to a web page and asking him to enter his credentials, but the web page is not one actually associated with the bank. in this paper, we focus on the web site phishing, in which available solutions are based either on providing early warning of suspicious activity and rapid response or on the use of tls (transport layer security). we present the tls-srp (secure remote password) and tls-psk (pre shared key) protocols and we demonstrate how these two solutions can be useful to reduce the web site phishing threats.phishing attacks and solutions','Spoofing attacks'
'if an email that we receive appears actually to have been sent by our bank, we are less likely to question its authenticity, says dario forte, but we may still be the target of a phishing attack, whose objective is to trick us into revealing sensitive information. in the second part of a two-part article, he analyses the mechanics of a phishing attack as used in everyday cybercrime, and shows just how devious the proponents of this mature but still-effective method can be in their tactics. in the first of this two-part series, published last month, we explained the initial process of how phishers prepare the ground for their attacks. as stated in that article, phishing attacks can be subdivided into three phases: *creation of a bogus web site that mimics the web site of the bank that is the target of the attack. *uploading of the page onto one\'s own site or else the compromising of an existing site. *mass emailing to lure the unwary to the bogus site.  the combination of these three elements allows an attacker to carry out an attack. the success of the attack depends on many factors, such as the credibility of the site, the contents of the email message, and the final user\'s critical analysis capacity and it proficiency. this article will go into more depth on these issues.phishing attacks','Spoofing attacks'
'this paper presents a variety of plaintext-recovering attacks against ssh. we implemented a proof of concept of our attacks against openssh, where we can verifiably recover 14 bits of plaintext from an arbitrary block of ciphertext with probability $2^{-14}$ and 32 bits of plaintext from an arbitrary block of ciphertext with probability $2^{-18}$. these attacks assume the default configuration of a 128-bit block cipher operating in cbc mode. the paper explains why a combination of flaws in the basic design of ssh leads implementations such as openssh to be open to our attacks, why current provable security results for ssh do not cover our attacks, and how the attacks can be prevented in practice.plaintext recovery attacks against ssh','Spoofing attacks'
'formal verification of security protocols has become a key issue in computer security. yet, it has proven to be a hard task often error prone and discouraging for non-experts in formal methods.in this paper we show how security protocols can be specified and verified efficiently and effectively by embedding reasoning about actions into a logic programming language.in a nutshell, we view a protocol trace as a plan to achieve a goal, so that protocol attacks are plans achieving goals that correspond to security violations. building on results from logic programming and planning, we map the existence of an attack to a protocol into the existence of a model for the protocol specification that satisfies the specification of an attack. to streamline such way of modeling security protocols, we use a description language alsp which makes it possible to describe protocols with declarative ease and to search for attacks by relying on efficient model finders (e.g. the smodels systems by niemela and his group). this paper shows how to use alsp for modeling two significant case studies in protocol verification: the classical needham-schroeder public-key protocol, and aziz-diffie key agreement protocol for mobile communication.planning attacks to security protocols','Spoofing attacks'
'we present the observation of distributed denial-of-service attacks that use reflection of the flooding traffic off reflectors. this type of attack was used in massive attacks against internet infrastructure of czech republic in march, 2013. apart from common hosts in the network, honeypots were abused as the reflectors. it caused the false positive incident detection and helped attackers. honeypots, which are by default set to accept any incoming network connection, unintentionally amplified the effect of reflection. we present an analysis of the attack from the point of view of honeypots and show the risks of having honeypots respond to any incoming traffic. we also discuss the possibilities of attack detection and mitigation and present lessons learned from handling the attack. we point out a lack of communication and data sharing during the observed attack.poster: reflected attacks abusing honeypots','Spoofing attacks'
'because security is becoming a major concern for aircraft manufacturers and satellite makers, vulnerability discovery and countermeasures should be integrated into onboard computing systems early during their development. attacks against aerospace computer systems fall into two main classes. one aims to corrupt the computing system\'s core functions; the other targets fault-tolerance mechanisms (error detection and recovery).potential attacks on onboard aerospace systems','Spoofing attacks'
'in this paper we analyze recently introduced questions for masked logic styles in general and for one such logic style called mdpl in particular. the dpa resistance of mdpl suffers significantly from a problem called early propagation, which denotes a data-dependent time of evaluation of logic cells depending on input signal-delay differences. experiments on a prototype chip show that in case of specific mdpl modules like the analyzed aes coprocessor, early propagation does not unconditionally break the dpa resistance of mdpl. investigations indicate that this might be due to the regular structure of the particular mdpl circuit, which is assumed to cause only relatively \"small\" signal delay differences. furthermore, in this article it is shown that the recently proposed, so-called pdf-attack could not be turned into a successful practical attack in our environment. finally, the recently raised question whether mdpl has special requirements in terms of the generation of random mask bits or not is discussed theoretically. practical attacks on masked hardware','Spoofing attacks'
'near field communication (nfc) technology enables devices to communicate wirelessly within proximity distance. these nfc devices are often embedded into smart posters that offer the ability to exchange small files, photos and contact details. the nokia 6212 classic is currently the most popular nfc phone. it allows users to easily exchange digital objects using the nfc interface. to do so, two phones should be within the proximity coupling distance of 5 cm. this paper shows that the nfc feature that invokes a bluetooth connection without user consent can be abused to surreptitiously install malicious software on the phone. this results in a serious vulnerability when smart posters start installing malicious software or spreading viruses.practical attacks on nfc enabled cell phones','Spoofing attacks'
'at eurocrypt 2002, vaudenay introduced a powerful side-channel attack, which is called padding oracle attack, against cbc-mode encryption with pkcs#5 padding (see [6]). if there is an oracle which on receipt of a ciphertext, decrypts it and then replies to the sender whether the padding is correct or not, vaudenay shows how to use that oracle to efficiently decrypt data without knowing the encryption key. in this paper, we turn the padding oracle attack into a new set of practical web hacking techniques. we also introduce a new technique that allows attackers to use a padding oracle to encrypt messages of any length without knowing the secret key. finally, we show how to use that technique to mount advanced padding oracle exploits against popular web development frameworks.practical padding oracle attacks','Spoofing attacks'
'side-channel attacks pose a serious threat to implementations of cryptographic algorithms. in the pioneering article of chari, rao and rohatgi, the general idea behind template attacks was introduced. template attacks apply advanced statistical methods and can break implementations secure against other forms of side-channel attacks. however, in our research it turned out that several details, which are essential to practical implementations of template attacks, still need to be answered. in this article we provide answers to open issues, such as how to select points of interest in an efficient way, or how to preprocess noisy data. in addition, we show the benefits of trial classifications and we point out that in practice so-called amplified template attacks have to be considered as a potential threat.practical template attacks','Spoofing attacks'
'this paper presents ongoing work toward the first preimage attacks on hash function arirang, which is one of the first round candidates in the sha-3 competition. arirang has an unique design where the feed-forward operation is computed not only after the last step but also in a middle step. in fact, this design prevents previous preimage attacks. we apply a meet-in-the-middle preimage attacks to arirang. specifically, we propose a new initial-structure optimized for arirang and overcome the middle feed-forward.preimage attacks on full-arirang','Spoofing attacks'
'buffer overflow is known to be a common memory vulnerability affecting software. it is exploited to gain various kinds of privilege escalation. c and c++ are very commonly used to develop applications; due to the efficient &#8220;unmanaged&#8221; executions these languages are not safe. these attacks are highly successful as every executing copy of a shipped binary is the same. this work presents two approaches to randomizing the memory layout which does not require modifications at the developer end. both techniques are implemented at the user-end machines and have no requirement for source code. the feasibility of the two techniques is shown by randomizing complex applications and demonstrating that the run-time penalty for the randomization schemes is very less.preventing overflow attacks by memory randomization','Spoofing attacks'
'preventing syn flood dos attacks','Spoofing attacks'
'android is a modern and popular software platform for smartphones. among its predominant features is an advanced security model which is based on application-oriented mandatory access control and sandboxing. this allows developers and users to restrict the execution of an application to the privileges it has (mandatorily) assigned at installation time. the exploitation of vulnerabilities in program code is hence believed to be confined within the privilege boundaries of an application\'s sandbox. however, in this paper we show that a privilege escalation attack is possible. we show that a genuine application exploited at runtime or a malicious application can escalate granted permissions. our results immediately imply that android\'s security model cannot deal with a transitive permission usage attack and android\'s sandbox model fails as a last resort against malware and sophisticated runtime attacks.privilege escalation attacks on android','Spoofing attacks'
'this paper investigates a probabilistic algebraic attack on lfsr-based stream ciphers. we consider two scenarios (s3a and s3b) proposed by meier et al. at eurocrypt 2004. in order to derive the probability in this new algebraic attack, we quantify the distance between a boolean function and a function with annihilator of a certain degree. we show that in some cases the approximations can improve the algebraic attacks. moreover, this distance leads to other theoretical results such as the weights of the subfunctions and the distance to normal functions; it also provides information on the walsh spectrum of the function.probabilistic algebraic attacks','Spoofing attacks'
'the paper extends the intrusion detection methodology proposed by tarakanov et al. in [8] to k-dimensional shape spaces, for k greater or equal 2. k real vectors, representing antibodies, are used to recognize malicious (or, non-self) connection logs. we suggest a method for recognizing antigens (generating such antibodies) via singular value decomposition of a real-valued matrix obtained by preprocessing a database of connection logs [9]. new incoming connection requests are recognized by the antibodies as either self (normal request), or non-self (potential attack), by (a) mapping them into a k-dimensional shape space, and (b) evaluating the minimum hamming distance between their image and that of a known attack logs. it is easy to see that using a shape space of dimension greater than 2 significantly reduces false positives.profiling network attacks via ais','Spoofing attacks'
'fault attacks constitute a major threat toward cryptographic products supporting rsa-based technologies. most often, the public exponent is unknown, turning resistance to fault attacks into an intricate problem. over the past few years, several techniques for secure implementations have been published, but none of them is fully satisfactory. we propose a completely different approach by embedding the public exponent into [the description of] the private key. as a result, we obtain a very efficient countermeasure with a 100\% fault detection.protecting rsa against fault attacks','Spoofing attacks'
'with the help of satellites, the entire surface of the world can be covered, which provides the high speedy communications all over the world. consequently, security is becoming an important concern in the satellite multicast communications. however, due to the inherent dynamic broadcast nature of the communication medium, this multicast system is easily susceptible to interferences and interceptions. in addition, the satellite system generally has a large number of terminal members with the high frequent join-leave characteristic. therefore, the satellite systems face significant security challenges. the denial of service (dos) is one of the most harmful attacks to the satellite systems and also terrestrial fixed or mobile networks. it can maliciously prevent legitimate users from accessing the service. it is especially true for the disassociation dos attacks where an attacker sends bogus disassociation requests to disable the communication between the server and their legitimate clients. in this paper, the main focus of our work is to detect and defend against the disassociation dos attacks on the satellite system. we also provide preliminary modeling verifications and simulation results regarding the efficiency and practicability of this new approach. further analysis of the proposed method is also appended to demonstrate its feasibility.protecting satellite systems from disassociation dos attacks','Spoofing attacks'
'the domain name system is a critical piece of the internet and supports most internet applications. because it\'s organized in a hierarchy, its correct operation depends on the availability of just a few servers at the hierarchy\'s upper levels. these backbone servers are vulnerable to routing attacks in which adversaries controlling part of the routing system try to hijack the server address space. using routing attacks in this way, an adversary can compromise the internet\'s availability and integrity at a global scale. in this article, the authors evaluate the relative resilience to routing attacks of two alternative anycast dns implementations. the first operates at the network layer and the second at the application layer. the evaluation informs fundamental dns design decisions and an important debate on the routing architecture of the internet.protecting the dns from routing attacks','Spoofing attacks'
'protection against eeprom modification attacks','Spoofing attacks'
'approaches against phishing can be classified into modifications of the traditional pin/tan-authentication on the one hand and approaches that try to reduce the probability of a scammer being successful without changing the existing pin/tan-method on the other hand. we present a new approach, based on challenge-response-authentication. since our proposal does not require any new hardware on the client side, it can be implemented with little additional cost by financial institutions or other web retailers and therefore is a good compromise compared to the other approaches. a big drawback is that it doesn&#39;t protect against man-in-the-middle attacks but most of the other approaches don&#39;t either.protection mechanisms against phishing attacks','Spoofing attacks'
'tor is a real-world, circuit-based low-latency anonymous communication network, supporting tcp applications over the internet. in this paper, we present an extensive study of protocol-level attacks against tor. different from existing attacks, the attacks investigated in this paper can confirm anonymous communication relationships quickly and accurately by manipulating one single cell and pose a serious threat against tor. in these attacks, a malicious entry onion router may duplicate, modify, insert, or delete cells of a tcp stream from a sender, which can cause cell recognition errors at the exit onion router. if an accomplice of the attacker at the entry onion router also controls the exit onion router and recognizes such cell recognition errors, the communication relationship between the sender and receiver will be confirmed. these attacks can also be used for launching the denial-of-service (dos) attack to disrupt the operation of tor. we systematically analyze the impact of these attacks and our data indicate that these attacks may drastically degrade the anonymity service that tor provides, if the attacker is able to control a small number of tor routers. we have implemented these attacks on tor and our experiments validate their feasibility and effectiveness. we also present guidelines for defending against protocol-level attacks.protocol-level attacks against tor','Spoofing attacks'
'we formally introduce the concept of related-cipher attack. in this paper, we consider the related ciphers as block ciphers with the same round function but with different round numbers. if their key schedules do not depend on the total round number, then related-cipher attack could be applied if the same key is used. we applied this attack to block cipher square and show that square is vulnerable to this attack. we also show that a new aes key schedule proposed at acisp02 is weaker than the original one under this attack. we then classify the differential attacks into three categories: related-message attack (the original differential cryptanalysis), related-key attack and related-cipher attack. these attacks should be taken into consideration in cipher design.related-cipher attacks','Spoofing attacks'
'timing attacks are usually used to attack weak computing devices such as smartcards. we show that timing attacks apply to general software systems. specifically, we devise a timing attack against openssl. our experiments show that we can extract private keys from an openssl-based web server running on a machine in the local network. our results demonstrate that timing attacks against network servers are practical and therefore security systems should defend against them.remote timing attacks are practical','Spoofing attacks'
'replication does survive information warfare attacks','Spoofing attacks'
'in this paper we study the resistance of a block cipher against a class of general attacks which we call \"iterated attacks\". this class includes some elementary versions of differential and linear cryptanalysis. we prove that we can upper bound the complexity of the attack by using decorrelation techniques. our main theorem enables to prove the security against these attacks (in our model) of some recently proposed block ciphers coconut98 and peanut98, as well as the aes candidate dfc.we outline that decorrelation to the order 2d is required for proving security against iterated attacks of order d.resistance against general iterated attacks','Spoofing attacks'
'security devices are vulnerable to side-channel attacks that perform statistical analysis on data leaked from cryptographic computations. higher-order (ho) attacks are a powerful approach to break protected implementations. they inherently demand multivariate statistics because multiple aspects of signals have to be analyzed jointly. however, most works on ho attacks follow the approach to first apply a pre-processing function to map the multivariate problem to a univariate problem and then to apply established 1st order techniques. we propose a novel and different approach to ho attacks, multivariate mutual information analysis (mmia), that allows to directly evaluate joint statistics without pre-processing. while this approach can benefit from a good power model, it also works without an assumption. we present the first experimental results for 2nd and 3rd order mmia as well as state-of-the-art ho attacks based on real measurements. a thorough empirical evaluation confirms the advantage of the new approach: 3rd order mmia attacks require about 800 measurements to achieve 100\% success while state-of-the-art hodpa requires 1000 measurements to achieve about 40\% success. as a consequence, the security provided by the masking countermeasure needs to be reconsidered as 3rd and possibly higher order attacks become more practical.revisiting higher-order dpa attacks','Spoofing attacks'
'mobile ad hoc networks (manet) have been highly vulnerable to attacks due to the dynamic nature of its network infrastructure. among these attacks, routing attacks have received considerable attention since it could cause the most devastating damage to manet. even though there exist several intrusion response techniques to mitigate such critical attacks, existing solutions typically attempt to isolate malicious nodes based on binary or na&#239;ve fuzzy response decisions. however, binary responses may result in the unexpected network partition, causing additional damages to the network infrastructure, and na&#239;ve fuzzy responses could lead to uncertainty in countering routing attacks in manet. in this paper, we propose a risk-aware response mechanism to systematically cope with the identified routing attacks. our risk-aware approach is based on an extended dempster-shafer mathematical theory of evidence introducing a notion of importance factors. in addition, our experiments demonstrate the effectiveness of our approach with the consideration of several performance metrics.risk-aware mitigation for manet routing attacks','Spoofing attacks'
'copyright protection is a key issue for video sharing over public networks. to protect the video content from unauthorized redistribution, digital fingerprinting is commonly used. to develop an efficient collusion-resistant fingerprinting scheme, it is very important for the system designer to understand how the behavior dynamics of colluders affect the performance of collusion attack. in the literature, little effort has been made to explicitly study the relationship between risk, e.g., the probability of the colluders to be detected, and the distortion of the colluded signal. in this paper, we investigate the risk-distortion relationship for the linear video collusion attack with gaussian fingerprint. we formulate the optimal linear collusion attack as an optimization problem of finding the optimal collusion parameters to minimize the distortion subject to a risk constraint. by varying the risk constraint and solving the corresponding optimization problem, we can derive the optimal risk-distortion curve. moreover, based upon the observation that the detector/attacker can each improve the detection/ attack performance with the knowledge of his/her opponent\'s strategy, we formulate the attack and detection problem as a dynamic mouse and cat game and study the optimal strategies for both the attacker and detector. we show that if the detector uses a fixed detection strategy, the attacker can estimate the detector\'s strategy and choose the corresponding optimal strategy to attack the fingerprinted video with a small distortion. however, if the detector is powerful, i.e., the detector can always estimate the attacker\'s strategy, the best strategy for the attacker is the min-max strategy. finally, we conduct several experiments to verify the proposed risk-distortion model using real video data.risk-distortion analysis for video collusion attacks','Spoofing attacks'
'a rootkit is a type of malware designed to gain privileged access to a computer while hiding itself from the user and the operating system by, for example, compromising the communication channels within the os. a rootkit can hide files, data, processes, and network ports, and can typically survive a system restart. this stealthy design enables the rootkit to escape detection by most anti-malware tools, which may be effective in detecting/removing malware like viruses but are generally ineffective against rootkits. to answer this challenge, special anti-rootkit software applications have been developed. in this paper, we first review the status quo of windows-based rootkits and anti-rootkit software. we then discuss how rootkits may be incorporated into classes to teach computer and network security concepts.rootkit attacks and protection','Spoofing attacks'
'due to their high practical impact, cross-site scripting (xss) attacks have attracted a lot of attention from the security community members. in the same way, a plethora of more or less effective defense techniques have been proposed, addressing the causes and effects of xss vulnerabilities. noscript, and disabling scripting code in non-browser applications such as e-mail clients or instant messengers. as a result, an adversary often can no longer inject or even execute arbitrary scripting code in several real-life scenarios. in this paper, we examine the attack surface that remains after xss and similar scripting attacks are supposedly mitigated by preventing an attacker from executing javascript code. we address the question of whether an attacker really needs javascript or similar functionality to perform attacks aiming for information theft. the surprising result is that an attacker can also abuse cascading style sheets (css) in combination with other web techniques like plain html, inactive svg images or font files. through several case studies, we introduce the so called scriptless attacks and demonstrate that an adversary might not need to execute code to preserve his ability to extract sensitive information from well protected websites. more precisely, we show that an attacker can use seemingly benign features to build side channel attacks that measure and exfiltrate almost arbitrary data displayed on a given website. we conclude this paper with a discussion of potential mitigation techniques against this class of attacks. in addition, we have implemented a browser patch that enables a website to make a vital determination as to being loaded in a detached view or pop-up window. this approach proves useful for prevention of certain types of attacks we here discuss.scriptless attacks','Spoofing attacks'
'this paper analyzes the effect of replay attacks on a control system. we assume an attacker wishes to disrupt the operation of a control system in steady state. in order to inject an exogenous control input without being detected the attacker will hijack the sensors, observe and record their readings for a certain amount of time and repeat them afterwards while carrying out his attack. this is a very common and natural attack (we have seen numerous times intruders recording and replaying security videos while performing their attack undisturbed) for an attacker who does not know the dynamics of the system but is aware of the fact that the system itself is expected to be in steady state for the duration of the attack. we assume the control system to be a discrete time linear time invariant gaussian system applying an infinite horizon linear quadratic gaussian (lqg) controller. we also assume that the system is equipped with a &#967;2 failure detector. the main contributions of the paper, beyond the novelty of the problem formulation, consist in 1) providing conditions on the feasibility of the replay attack on the aforementioned system and 2) proposing a countermeasure that guarantees a desired probability of detection (with a fixed false alarm rate) by trading off either detection delay or lqg performance, either by decreasing control accuracy or increasing control effort.secure control against replay attacks','Spoofing attacks'
'the use of passwords is a major point of vulnerability in computer security, as passwords are often easy to guess by automated programs running dictionary attacks. passwords remain the most widely used authentication method despite their well-known security weaknesses. user authentication is clearly a practical problem. from the perspective of a service provider this problem needs to be solved within real-world constraints such as the available hardware and software infrastructures. from a user\'s perspective user-friendliness is a key requirement.in this paper we suggest a novel authentication scheme that preserves the advantages of conventional password authentication, while simultaneously raising the costs of online dictionary attacks by orders of magnitude. the proposed scheme is easy to implement and overcomes some of the difficulties of previously suggested methods of improving the security of user authentication schemes.our key idea is to efficiently combine traditional password authentication with a challenge that is very easy to answer by human users, but is (almost) infeasible for automated programs attempting to run dictionary attacks. this is done without affecting the usability of the system. the proposed scheme also provides better protection against denial of service attacks against user accounts.securing passwords against dictionary attacks','Spoofing attacks'
'structured overlay networks can greatly simplify data storage and management for a variety of distributed applications. despite their attractive features, these overlays remain vulnerable to the identity attack, where malicious nodes assume control of application components by intercepting and hijacking key-based routing requests. attackers can assume arbitrary application roles such as storage node for a given file, or return falsified contents of an online shopper\'s shopping cart. in this paper, we define a generalized form of the identity attack, and propose a lightweight detection and tracking system that protects applications by redirecting traffic away from attackers. we describe how this attack can be amplified by a sybil or eclipse attack, and analyze the costs of performing such an attack. finally, we present measurements of a deployed overlay that show our techniques to be significantly more lightweight than prior techniques, and highly effective at detecting and avoiding both single node and colluding attacks under a variety of conditions.securing structured overlays against identity attacks','Spoofing attacks'
'the automobile industry has grown to become an integral part of our everyday life. as vehicles evolve, the primarily mechanical solutions for vehicle control are gradually replaced by electronics and software solutions forming in-vehicle computer networks. an emerging trend is to introduce wireless technology in the vehicle domain by attaching a wireless gateway to the in-vehicle network. by allowing wireless communication, real-time information exchange between vehicles and between infrastructure and vehicles become reality. this communication allows for road condition reporting, decision making, and remote diagnostics and firmware updates over-the-air. however, allowing external parties wireless access to the in-vehicle network creates a potential entry-point for cyber attacks. in this paper, we investigate the security issues of allowing external wireless communication. we use a defense-in-depth perspective and discuss security challenges for each of the prevention, detection, deflection, countermeasures, and recovery layers.securing vehicles against cyber attacks','Spoofing attacks'
'in this paper, we propose a method of detecting and classifying web application attacks. in contrast to current signature-based security methods, our solution is an ontology based technique. it specifies web application attacks by using semantic rules, the context of consequence and the specifications of application protocols. the system is capable of detecting sophisticated attacks effectively and efficiently by analyzing the specified portion of a user request where attacks are possible. semantic rules help to capture the context of the application, possible attacks and the protocol that was used. these rules also allow inference to run over the ontological models in order to detect, the often complex polymorphic variations of web application attacks. the ontological model was developed using description logic that was based on the web ontology language (owl). the inference rules are horn logic statements and are implemented using the apache jena framework. the system is therefore platform and technology independent. prior to the evaluation of the system the knowledge model was validated by using ontoclean to remove inconsistency, incompleteness and redundancy in the specification of ontological concepts. the experimental results show that the detection capability and performance of our system is significantly better than existing state of the art solutions. the system successfully detects web application attacks whilst generating few false positives. the examples that are presented demonstrate that a semantic approach can be used to effectively detect zero day and more sophisticated attacks in a real-world environment.semantic security against web application attacks','Spoofing attacks'
'sensor networks will typically operate in hostile environments, where they are susceptible to physical attacks resulting in physical node destructions. in this paper, we study impacts of physical attacks on network configuration w.r.t. lifetime. while lifetime is constrained by limited energies and has been addressed before, they are not applicable under physical attacks. in this paper, we define a practical sensor network lifetime problem under physical attacks. we then analytically determine the minimum number and sensor deployment plan to meet lifetime requirement under physical attacks. we make several important observations, including the high sensitivity of lifetime to physical attacks.sensor network configuration under physical attacks','Spoofing attacks'
'content sniffing attacks occur if browsers render non-html files embedded with malicious html contents or javascript code as html files. the rendering of these embedded contents might cause unwanted effects such as the stealing of sensitive information through the execution of malicious javascript code. the primary source of these attacks can be stopped if the uploading of malicious files can be prevented from the server side. however, existing server side content sniffing attack detection approaches suffer from a number of limitations. first, file contents are checked only to a fixed amount of initial bytes whereas attack payloads might reside anywhere in the file. second, these approaches do not provide any mechanism to assess the malicious impact of the embedded contents on browsers. this paper addresses these issues by developing a server side content sniffing attack detection mechanism based on content analysis using html and javascript parsers and simulation of browser behavior via mock download testing. we have implemented our approach in a tool that can be integrated in web applications written in various languages. in addition, we have developed a benchmark suite for the evaluation purpose that contains both benign and malicious files. we have evaluated our approach on three real world php programs suffering from content sniffing vulnerabilities. the evaluation results indicate that our approach can secure programs against content sniffing attacks by successfully preventing the uploading of malicious files.server side detection of content sniffing attacks','Spoofing attacks'
'contemporary malware makes extensive use of different techniques such as packing, code obfuscation, polymorphism, and metamorphism, to evade signature-based detection. traditional signature-based detection technique is hard to catch up with latest malware or unknown malware. behavior-based detection models are being investigated as a new methodology to defeat malware. this kind of approaches typically relies on system call sequences/graphs to model a malicious specification/pattern. in this paper, we present a new class of attacks, namely \"shadow attacks\", to evade current behavior-based malware detectors by partitioning one piece of malware into multiple \"shadow processes\". none of the shadow processes contains a recognizable malicious behavior specification known to single-process-based malware detectors, yet those shadow processes as an ensemble can still fulfill the original malicious functionality. to demonstrate the feasibility of this attack, we have developed a compiler-level prototype tool, autoshadow, to automatically generate shadow-process version of malware given the source code of original malware. our preliminary result has demonstrated the effectiveness of shadow attacks in evading several behavior-based malware analysis/detection solutions in real world. with the increasing adoption of multi-core computers and multi-process programs, malware writers may exploit more such shadow attacks in the future. we hope our preliminary study can foster more discussion and research to improve current generation of behavior-based malware detectors to address this great potential threat before it becomes a security problem of the epidemic proportions.shadow attacks','Spoofing attacks'
'together with masking, shuffling is one of the most frequently considered solutions to improve the security of small embedded devices against side-channel attacks. in this paper, we provide a comprehensive study of this countermeasure, including improved implementations and a careful information theoretic and security analysis of its different variants. our analyses lead to important conclusions as they moderate the strong security improvements claimed in previous works. they suggest that simplified versions of shuffling (e.g. using random start indexes) can be significantly weaker than their counterpart using full permutations. we further show with an experimental case study that such simplified versions can be as easy to attack as unprotected implementations. we finally exhibit the existence of \"indirect leakages\" in shuffled implementations that can be exploited due to the different leakage models of the different resources used in cryptographic implementations. this suggests the design of fully shuffled (and efficient) implementations, were both the execution order of the instructions and the physical resources used are randomized, as an interesting scope for further research.shuffling against side-channel attacks','Spoofing attacks'
'when it comes to cryptographic software, side channels are an often-overlooked threat. a side channel is any observable side effect of computation that an attacker could measure and possibly influence. in the software world, side-channel attacks have sometimes been dismissed as impractical. however, new system architecture features, such as larger cache sizes and multicore processors, have increased the prevalence of side channels and quality of measurement available to an attacker. this article explains three recent side-channel attacks on cryptographic software, exploiting a comparison function, cpu cache timing, and branch prediction logic to recover a secret key. software developers must be aware of the potential for side-channel attacks and plan appropriately.side-channel attacks on cryptographic software','Spoofing attacks'
'contemporary vehicles contain a number of electronic control units (ecus), which are connected in a network and provide various vehicle functionalities. the firmware on the ecus need to be kept up-to-date to provide better and safer functionalities. an upcoming trend for automotive manufacturers is to create seamless interaction between the vehicle and fleet management to provide remote diagnostics and firmware updates over the air. to allow this, the previously isolated in-vehicle network must connect to an external network, and is thus exposed to a whole new range of threats, collectively known as cyber attacks. in this paper we have evaluated the ability of the current in-vehicle network to withstand cyber attacks by simulating a set of plausible attacks targeting the ecus on the can bus. the results show that the network lacks sufficient protection against these attacks, and we therefore extensively discuss the future security needs for preventing, detecting, countering and recovering from such attacks.simulated attacks on can buses','Spoofing attacks'
'low-rate denial-of-service attack is a novel category of attacks that are based on exploiting the adaptive behavior exhibited by several network and system protocols. this attack through periodically non-suspicious low-rate attack pulsing, to reduce the performance of the victims. based on the simulation of low-rate denial-of-service on ns2 platform, we analyze the defense performance of queue management mechanism itself to count the ldos attack. under the analysis of experiments result, we give some suggestions about counter technology.simulation and analysis of ldos attacks','Spoofing attacks'
'it is a general belief among the designers of block-ciphers that even a relatively weak cipher may become very strong if its number of rounds is made very large. in this paper we describe a new generic known- (or sometimes chosen-) plaintext attack on product ciphers, which we call the slide attack and which in many cases is independent of the number of rounds of a cipher. we illustrate the power of this new tool by giving practical attacks on several recently designed ciphers: treyfer, wake-rofb, and variants of des and blowfish.slide attacks','Spoofing attacks'
'in this paper, we define and illustrate a new form of attack in the context of software services: the software-based need-to-know (sn2k) attack. sn2k attacks can be carried out by dishonest provider of a software service so that it can maliciously gain access to sensitive information, even if the service does {\\em not need to know} such data in order to compute the functionalities offered by it. we prove that it is generally undecidable to detect whether a given implementation of a service is dishonest, i.e., it implements an sn2k attack. a certification scheme for honest services is also proposed; our scheme relies on program slicing and certain other aspects of static program analysis.sn2k attacks and honest services','Spoofing attacks'
'in this paper, using the lll reduction method and computing the integral points of two classes of conics, we develop attacks on dsa and ecdsa in case where the secret and the ephemeral key of a signed message or theirs modular inverses are sufficiently small and in case where the ephemeral keys or theirs modular inverses of two signed messages are sufficiently small.some lattice attacks on dsa and ecdsa','Spoofing attacks'
'we propose a decentralized privacy-preserving approach to spam filtering. our solution exploits robust digests to identify messages that are a slight variation of one another and a peer-to-peer architecture between mail servers to collaboratively share knowledge about spam.spam attacks','Spoofing attacks'
'research in biometric gait recognition has increased. earlier gait recognition works reported promising results, usually with a small sample size. recent studies with a larger sample size confirm gait potential as a biometric from which individuals can be identified. despite much research being carried out in gait recognition, the topic of vulnerability of gait to attacks has not received enough attention. in this paper, an analysis of minimal-effort impersonation attack and the closest person attack on gait biometrics are presented. unlike most previous gait recognition approaches, where gait is captured using a (video) camera from a distance, in our approach, gait is collected by an accelerometer sensor attached to the hip of subjects. hip acceleration in three orthogonal directions (up-down, forward-backward, and sideways) is utilized for recognition. we have collected 760 gait sequences from 100 subjects. the experiments consisted of two parts. in the first part, subjects walked in their normal walking style, and using the averaged cycle method, an eer of about 13\% was obtained. in the second part, subjects were trying to walk as someone else. analysis based on far errors indicates that a minimal-effort impersonation attack on gait biometric does not necessarily improve the chances of an impostor being accepted. however, attackers with knowledge of their closest person in the database can be a serious threat to the authentication system.spoof attacks on gait authentication system','Spoofing attacks'
'hash(0x310a61c)stealthy attacks in wireless ad hoc networks','Spoofing attacks'
'the goal of this paper is to present a set of design principles for avoiding replay attacks in cryptographic protocols. the principles are easily applied to real protocols and they do not consume excessive computing power or communications bandwidth. in particular, we describe how to type-tag messages with unique cryptographic functions, how to inexpensively implement the full information principle with hashes, and how to produce unique session keys without assuming mutual trust between the principals. the techniques do not guarantee security of protocols, but they are concrete ways for improving the robustness of the protocol design with relatively low cost.strategies against replay attacks','Spoofing attacks'
'fault attacks against cryptographic schemes as used in tamper- resistant devices have led to a vibrant research activity in the past. this area was recently augmented by the discovery of attacks even on the public key parts of asymmetric cryptographic schemes like rsa, dsa, and ecc. while being very powerful in principle, all existing attacks until now required very sophisticated hardware attacks to mount them practically - thus excluding them from being a critical break-once-run-everywhere attack. in contrast, this paper develops a purely software-based fault attack against the rsa verification process. this novel attack consists in completely replacing the modulus by attacking the structures managing the public key material. this approach contrasts strongly with known attacks which merely change some bits of the original modulus by introducing hardware faults. it is important to emphasize that the attack described in this paper poses a real threat: we demonstrate the practicality of our new public key attack against the rsa-based verification process of a highly protected and widely deployed conditional access device - a set-top box from microsoft used by many iptv providers. furthermore, we successfully applied our attack method against a 3g access point, leading to root access.structure-based rsa fault attacks','Spoofing attacks'
'when designing or analyzing applications or infrastructures with high reliability, safety, security, or survivability demands, the fundamental questions are: what is required of the application and can the infrastructure support these requirements. in the design and analysis of fault-tolerant systems, fault models have served us well to describe the theoretical limits. but with the inclusion of malicious acts, the direct application of fault models has exposed limited applicability. however, we can take advantage of the powerful fault models if we defer their direct application from the events that lead to faults, that is, the fault causes, and instead focus on the effects. this way one can avoid questions referring to the meaning of fault models in the context of previously unsuitable faults like trojan horses or denial of service (dos) attacks. instead, we can use fault models at the level of abstraction where the application maps on the infrastructure. in this paper fault models are discussed in the context of system survivability and malicious act. it is shown that these models can be used to balance the demands put on the application and the capabilities of the underlying infrastructure. active and imposed fault descriptions are defined that allow to match the mechanisms that provide survivability to the application with the infrastructure-imposed limitations. by defining a system as a collection of functionalities, individual functionalities and their associated fault descriptions can be analyzed in isolation.surviving attacks and intrusions','Spoofing attacks'
'in the event of a disaster, telecommunication infrastructures can be severely damaged or overloaded. hastily formed networks can provide communication services in an ad hoc manner. these networks are challenging due to the chaotic context where intermittent connection is the norm and the identity and number of participants cannot be assumed. in such environments malicious actors may try to disrupt the communications to create more chaos for their own benefit. this paper proposes a general security framework for monitoring and reacting to disruptive attacks. it includes a collection of functions to detect anomalies, diagnose them, and perform mitigation. the measures are deployed in each node in a fully distributed fashion, but their collective impact is a significant resilience to attacks, so that the actors can disseminate information under adverse conditions. the approach has been evaluated in the context of a simulated disaster area network with a manycast dissemination protocol, random walk gossip, with a store-and-forward mechanism. a challenging threat model where adversaries may attempt to reduce message dissemination or drain network resources without spending much of their own energy has been adopted.surviving attacks in challenged networks','Spoofing attacks'
'a series of distributed denial-of-service (ddos) attacks were launched against computer systems and services in the us and south korea beginning july 4th. a ddos attack is an attempt to make a computer service unavailable to its intended users. the mechanism for such attacks includes a network of a large number of compromised computers; the attack itself is carried out in several phases: discover vulnerable hosts, establish a botnet, launch an attack, and flood a target. there\'s no silver bullet for fending off a ddos attack. a practical defense includes prevention, detection, and mitigation strategies.surviving distributed denial-of-service attacks','Spoofing attacks'
'collaborative applications for co-located mobile users can be severely disrupted by a sybil attack to the point of being unusable. existing decentralized defences have largely been designed for peer-to-peer networks but not for mobile networks. that is why we propose a new decentralized defence for portable devices and call it mobid. the idea is that a device manages two small networks in which it stores information about the devices it meets: its network of friends contains honest devices, and its network of foes contains suspicious devices. by reasoning on these two networks, the device is then able to determine whether an unknown individual is carrying out a sybil attack or not. we evaluate the extent to which mobid reduces the number of interactions with sybil attackers and consequently enables collaborative applications.we do so using real mobility and social network data. we also assess computational and communication costs of mobid on mobile phones.sybil attacks against mobile users','Spoofing attacks'
'taming ip packet flooding attacks','Spoofing attacks'
'confining a program during its execution so that it can\'t leak information to other programs is an old concern. recently, several researchers succeeded in fingerprinting distant machines by measuring temperature side effects on clocks. but can temperature also leak secrets in a computer or a chip? we started by implementing a covert channel between two processes (a sender and a receiver) running on the same machine. producing heat is simple: all the sender must do is launch massive calculations. to sense temperature in the machine, we considered three options: fan-based solutions, built-in sensors; and faults as heat detectors.temperature attacks','Spoofing attacks'
'side-channel attacks are a serious threat to implementations of cryptographic algorithms. secret information is recovered based on power consumption, electromagnetic emanations or any other form of physical information leakage. template attacks are probabilistic side-channel attacks, which assume a gaussian noise model. using the maximum likelihood principle enables us to reveal (part of) the secret for each set of recordings (i.e., leakage trace). in practice, however, the major concerns are (i) how to select the points of interest of the traces, (ii) how to choose the minimal distance between these points, and (iii) how many points of interest are needed for attacking. so far, only heuristics were provided. in this work, we propose to perform template attacks in the principal subspace of the traces. this new type of attack addresses all practical issues in principled way and automatically. the approach is validated by attacking stream ciphers such as rc4. we also report analysis results of template style attacks against an fpga implementation of aes rijndael. roughly, the template attack we carried out requires five time less encrypted messages than the best reported correlation attack against similar block cipher implementations.template attacks in principal subspaces','Spoofing attacks'
'we present template attacks, the strongest form of side channel attack possible in an information theoretic sense. these attacks can break implementations and countermeasures whose security is dependent on the assumption that an adversary cannot obtain more than one or a limited number of side channel samples. they require that an adversary has access to an identical experimental device that he can program to his choosing. the success of these attacks in such constraining situations is due manner in which noise within each sample is handled. in contrast to previous approaches which viewed noise as a hindrance that had to be reduced or eliminated, our approach focuses on precisely modeling noise, and using this to fully extract information present in a single sample. we describe in detail how an implementation of rc4, not amenable to techniques such as spa and dpa, can easily be broken using template attacks with a single sample. other applications include attacks on certain des implementations which use dpa-resistant hardware and certain ssl accelerators which can be attacked by monitoring electromagnetic emanations from an rsa operation even from distances of fifteen feet.template attacks','Spoofing attacks'
'when speaking about attacks against networks and computers, people mainly think today about viruses, worms, trojans, keyloggers, denial of services, etc. in the last ten years a lot of new attacks were found against servers and smart cards. first are side-channels attacks: those are by using \"esoteric\" channels to obtain protected, secure and private informations. esoteric here means very often channels related to the communication channel (time of interaction), processors (power, electromagnetic radiations, caches, branching, etc.). second are the malicious faults related to secret key cryptography. the interaction of cryptographic algorithms with malicious faults must be carefully known and understood: one error sometimes means a totally broken system. we will survey the field with a focus on distributed systems and networks.the power of cryptographic attacks','Spoofing attacks'
'cross-site request forgery (csrf) vulnerability is extremely widespread and one of the top ten web application vulnerabilities of the open web application security project (owasp). in this paper, we explore the csrf vulnerabilities, illustrate the real-world csrf attack, and present novel csrf attack tree models. the threat models provide for exploring, understanding, and validating security protection features in realistic web application scenarios.threat modeling for csrf attacks','Spoofing attacks'
'distributed denial of service (ddos) attacks are attacks where a host of compromised systems are used to target a single system. this single system can be either an actual machine or a network resource. what makes these attacks so prevalent and hard to deal with is the fact that they are distributed. they come from a wide variety of machines, making them hard to trace and even harder to counter. this, in conjunction with the fact that many tools are becoming available on the market that make ddos attacks easier, makes preventing ddos attacks a very imperative issue. this paper proposes a mechanism, dddos, or triple dos, which will deal with ddos attacks on the internet layer. there are registration and authentication protocols used to connect clients and servers, so that an unregistered client cannot access the network and thus flood it with traffic. the triple dos service will only be activated when a ddos attack is detected (using clustering), and it will ensure that registered clients and servers can always communicate.towards defeating ddos attacks','Spoofing attacks'
'distributed denial-of-service (ddos) attacks are a critical threat to the internet. however, the memoryless feature of the internet routing mechanisms makes it extremely hard to trace back to the source of these attacks. as a result, there is no effective and efficient method to deal with this issue so far. in this paper, we propose a novel traceback method for ddos attacks that is based on entropy variations between normal and ddos attack traffic, which is fundamentally different from commonly used packet marking techniques. in comparison to the existing ddos traceback methods, the proposed strategy possesses a number of advantages&#8212;it is memory nonintensive, efficiently scalable, robust against packet pollution, and independent of attack traffic patterns. the results of extensive experimental and simulation studies are presented to demonstrate the effectiveness and efficiency of the proposed method. our experiments show that accurate traceback is possible within 20 seconds (approximately) in a large-scale attack network with thousands of zombies.traceback of ddos attacks using entropy variations','Spoofing attacks'
'the integrity of the internet is severely impaired by rampant denial of service and distributed dos attacks. it is by no means trivial to devise a countermeasure to address these attacks because of their anonymous and distributed nature. this article presents a brief survey of the most promising recently proposed schemes for tracing cyber attacks: ip traceback. since ip traceback technology is evolving rapidly, for the community to better comprehend and capture the properties of disparate traceback approaches, we first classify these schemes from multiple aspects. from the perspective of practicality and feasibility, we then analyze and explore the advantages and disadvantages of these schemes in depth so that shortcomings and possible enhancements of each scheme are highlighted. finally, open problems and future work are discussed, and concluding remarks are drawn.tracing cyber attacks from the practical perspective','Spoofing attacks'
'known for a long time, distributed denial-of-service (ddos) attacks are still prevalent today and cause harm on the internet on a daily basis. the main mechanism behind this kind of attacks is the use of so called botnets, i.e., networks of compromised machines under the control of an attacker. there are several different botnet families that focus on ddos attacks and are even used to sell such attacks as a service on underground markets. in this paper, we present an empirical study of modern ddos botnets and analyze one particular family of botnets in detail. we identified 35 command and control (c&#38;c) servers related to dirtjumper (also called ruskill), one of the popular ddos botnets in operation at this point in time. we monitored these c&#38;c servers for a period of several months, during which we observed almost two thousand different ddos attacks carried out by the botmasters behind the botnets. based on this empirical data, we performed an analysis of the characteristics of ddos attacks. to complement this c&#38;c-centric point of view, we briefly analyzed the information logged at two different victims of dirtjumper ddos attacks to study how such attacks are perceived at an endhost. our results provide insights into modern ddos attacks and help us to understand how such attacks are carried out nowadays.tracking ddos attacks','Spoofing attacks'
'skype is one of the most popular voice-over-ip (voip) service providers. one of the main reasons for the popularity of skype voip services is its unique set of features to protect privacy of voip calls such as strong encryption, proprietary protocols, unknown codecs, dynamic path selection, and the constant packet rate. in this paper, we propose a class of passive traffic analysis attacks to compromise privacy of skype voip calls. the proposed attacks are based on application-level features extracted from voip call traces. the proposed attacks are evaluated by extensive experiments over different types of networks including commercialized anonymity networks and our campus network. the experiment results show that the proposed traffic analysis attacks can greatly compromise the privacy of skype calls. possible countermeasure to mitigate the proposed traffic analysis attacks are analyzed in this paper.traffic analysis attacks on skype voip calls','Spoofing attacks'
'in 1993, neuman and stubblebin proposed a nonce-based mutual authentication protocol uing timestamps as nonces. in this paper, we show defects in their initial and subsequent authentication protocols. we give the reasons that cause these defects and modify their protocol to avoid these defects.two attacks on neuman-stubblebine authentication protocols','Spoofing attacks'
'in 1991 lai, massey and murphy introduced the ipes (improved proposed encryption standard), later renamed idea (international data encryption algorithm). in this paper we give two new attacks on a reduced number of rounds of idea. a truncated differential attack on idea reduced to 3.5 rounds and a differential-linear attack on idea reduced to 3 rounds. the truncated differential attack contains a novel method for determining the secret key.two attacks on reduced idea','Spoofing attacks'
'trivium is a stream cipher designed in 2005 by c. de canni&#232;re and b. preneel for the european project estream. it has an internal state of 288 bits and the key of length 80 bits. although the design has a simple and elegant structure, no attack on it has been found yet. in this paper a family of trivium-like designs is studied. we propose a set of techniques for methodological cryptanalysis of these structures in general, including state recovering and linear distinguishing attacks. in particular, we study the original trivium and present a state recovering attack with time complexity around c283.5, which is 230 faster than the best previous result. our attack clearly shows that trivium has a very thin safety margin and that in its current form it can not be used with longer 128-bit keys. finally, we identify interesting open problems and propose a new design trivium/128, which resists all of our attacks proposed in this paper. it also accepts a 128 bit secret key due to the improved security level.two trivial attacks on trivium','Spoofing attacks'
'in this paper, we study unconditionally secure stegosystems against active attacks over an insecure channel in which an adversary can read and write a message. more specifically, we propose an information-theoretic model for steganography in the presence of active adversaries by extending both simmons\' and cachin\'s works; and we show a generic construction of stegosystems secure against active attacks by using authenticated encryption in unconditional setting. although the idea behind this construction is already used in different models (i.e., computational models and/or information-theoretic models with passive adversaries) of steganography, our contribution lies in showing the construction methodology provides provable and unconditional security against active adversaries.unconditionally secure steganography against active attacks','Spoofing attacks'
'with dos/ddos attacks emerging as one of the primary security threats in today\'s internet, the search is on for an efficient ddos defense mechanism that would provide attack prevention, mitigation and traceback features, in as few packets as possible and with no collateral damage. although several techniques have been proposed to tackle this growing menace, there exists no effective solution to date, due to the growing sophistication of the attacks and also the increasingly complex internet architecture. in this paper, we propose an unified framework that integrates traceback and mitigation capabilities for an effective attack defense. some significant aspects of our approach include: (1) a novel data cube model to represent the traceback information, and its slicing along the lines of path signatures rather than router signatures, (2) characterizing traceback as a transmission scheduling problem on the data cube representation, and achieving scheduling optimality using a novel metric called utility, (3) and finally an information delivery architecture employing both packet marking and data logging in a distributed manner to achieve faster response times. the proposed scheme can thus provide both per-packet mitigation and multi-packet traceback capabilities due to effective data slicing of the cube, and can attain higher detection speeds due to novel utility rate analysis. we also contrast this unified scheme with other well-known schemes in literature to understand the performance tradeoffs, while providing an experimental evaluation of the proposed scheme on real data sets.unified defense against ddos attacks','Spoofing attacks'
'in this paper we present the unregister attack, a new kind of a denial of service attack on sip servers. in this attack, the attacker sends a spoofed \"unregister\" message to a sip server and cancels the registration of the victim at that server. this prevents the victim user from receiving any calls. we have tested common implementations of sip servers and show that the unregister attack is easily performed on sip servers which do not use authentication. even on sip servers with authentication, an attacker able to snif f the traffc between the client and server can still successfully attack common servers. we show that the root causes behind this vulnerability are either buggy implementations, or the sip specification rfc which does not require suffcient security from the implementations. we present a solution, the sip one-way hash function algorithm (soha), motivated by the one-time password mechanism [6]. soha prevents the unregister attack in all situations. the algorithm is easy to deploy since it requires only a minor modification, namely adding one header field into the sip messages. furthermore, the algorithm is fully backwards compatible and requires no additional configuration from the user or the server.unregister attacks in sip','Spoofing attacks'
'ad hoc low-power wireless networks are an exciting research direction in sensing and pervasive computing. prior security work in this area has focused primarily on denial of communication at the routing or medium access control levels. this paper explores resource depletion attacks at the routing protocol layer, which permanently disable networks by quickly draining nodes\' battery power. these \"vampire&#8221; attacks are not specific to any specific protocol, but rather rely on the properties of many popular classes of routing protocols. we find that all examined protocols are susceptible to vampire attacks, which are devastating, difficult to detect, and are easy to carry out using as few as one malicious insider sending only protocol-compliant messages. in the worst case, a single vampire can increase network-wide energy usage by a factor of o(n), where n in the number of network nodes. we discuss methods to mitigate these types of attacks, including a new proof-of-concept protocol that provably bounds the damage caused by vampires during the packet forwarding phase.vampire attacks','Spoofing attacks'
'watermark detection after quanization attacks','Spoofing attacks'
'last month, part one of this article discussed common types of vulnerability in web applications and the evolution of attacks against web servers. part two considers recent shifts in attacker focus, how compromised web servers are increasingly becoming an important stepping stone in attacks against web clients, and the threat of automated web attacks.web app attacks','Spoofing attacks'
'there are many factors that can dictate the success of a piece of malware. these include how and to whom it is delivered, how it is executed, how rapidly it propagates and how successfully it evades detection. the first two of these describe the process of threat delivery and execution, which are perhaps the most influential factors in the success of a threat. traditionally cybercriminals have used email as their preferred vector of attack, employing various social engineering tactics in order to entice the recipient into executing the malicious attachment. as companies have become more aggressive in blocking email content, criminals have shifted their attentions, and are now firmly focused on the web.web attacks','Spoofing attacks'
'first page of the articleweb hocking attacks and defense [book reviews]','Spoofing attacks'
'this article will be the first in a series that will review web application security issues and provide suggestions on how to avoid the classic pitfalls. this particular article will discuss code injection and specifically cross site scripting. injection into application elements other than the web server and the client (i.e. sql injection) will be discussed in later articles.web injection attacks','Spoofing attacks'
'wireless sensor networks are specific adhoc networks. they are characterized by their limited computing power and energy constraints. this paper proposes a study of security in this kind of network. we show what are the specificities and vulnerabilities of wireless sensor networks. we present a list of attacks, which can be found in these particular networks, and how they use their vulnerabilities. finally we discuss about different solutions made by the scientific community to secure wireless sensor networks.wireless sensor network attacks and security mechanisms','Spoofing attacks'
'as mobile ad hoc network applications are deployed, security emerges as a central requirement. in this paper, we introduce the wormhole attack, a severe attack in ad hoc networks that is particularly challenging to defend against. the wormhole attack is possible even if the attacker has not compromised any hosts, and even if all communication provides authenticity and confidentiality. in the wormhole attack, an attacker records packets (or bits) at one location in the network, tunnels them (possibly selectively) to another location, and retransmits them there into the network. the wormhole attack can form a serious threat in wireless networks, especially against many ad hoc network routing protocols and location-based wireless security systems. for example, most existing ad hoc network routing protocols, without some mechanism to defend against the wormhole attack, would be unable to find routes longer than one or two hops, severely disrupting communication. we present a general mechanism, called packet leashes, for detecting and, thus defending against wormhole attacks, and we present a specific protocol, called tik, that implements leashes. we also discuss topology-based wormhole detection, and show that it is impossible for these approaches to detect some wormhole topologies.wormhole attacks in wireless networks','Spoofing attacks'
'the prospect of infection by a network worm with no available patch is the stuff of bad dreams.zero-day attacks','Spoofing attacks'
'this paper proves that several interactive proof systems are zero-knowledge against general quantum attacks. this includes the well-known goldreich-micali-wigderson classical zero-knowledge protocols for graph isomorphism and graph 3-coloring (assuming the existence of quantum computationally concealing commitment schemes in the second case). also included is a quantum interactive proof system for a complete problem for the complexity class of problems having honest verifier quantum statistical zero-knowledge proofs, which therefore establishes that honest verifier and general quantum statistical zero-knowledge are equal: $\\mathrm{qszk}= \\mathrm{qszk}_{\\mathrm{hv}}$. previously no nontrivial interactive proof systems were known to be zero-knowledge against quantum attacks, except in restricted settings such as the honest verifier and common reference string models. this paper therefore establishes for the first time that true zero-knowledge is indeed possible in the presence of quantum information and computation.zero-knowledge against quantum attacks','Spoofing attacks'
'traditional anti-tampering system can not guarantee its own security. in order to reduce tampering and ensure the integrity of web pages and web server security and the stability of operation, the paper analyzes the current anti-tampering technology of several webs, and introduces the model three-threading technology based on the existing anti-tampering technology and the multi-ring structure of the node page tamper-resistant model. it can protect against tampering with the system. this paper describes the structures, working mechanism and the implementation process in details. the study illustrates that the security of the system on the website is guaranteed and the efficiency of the server is improved.a study and design of multi-node tamper-resistant web system based on ring structure','Tamper-proof and tamper-resistant designs'
'electronic health record (ehr) projects have been launched in most developed countries to increase the quality of healthcare while decreasing its cost. the benefits provided by centralizing the healthcare information in database systems are unquestionable in terms of information quality, availability, and protection against failure. yet, patients are reluctant to give to a distant server the control over highly sensitive data (e.g., data revealing a severe or shameful disease). this paper capitalizes on a new hardware portable device, associating the security of a smart card to the storage capacity of a usb key, to give back to the patient the control over his medical data. this paper shows how this device can complement a traditional ehr server to (1) protect and share highly sensitive data among trusted parties and (2) provide a seamless access to the data even in disconnected mode. the proposed architecture is experimented in the context of a medicosocial network providing medical care and social services at home for elderly people.a tamper-resistant and portable healthcare folder','Tamper-proof and tamper-resistant designs'
'an important and recurring security scenario involves the need to carry out trusted computations in the context of untrusted environments. it is shown how a tamper-resistant interpreter for a programming language&#8212;currently lisp 1.5&#8212;combined with the use of a secure coprocessor can address this problem. this solution executes the interpreter on the secure coprocessor while the code and data of the program reside in the larger memory of an associated untrusted host. this allows the coprocessor to utilize the host\'s memory without fear of tampering even by a hostile host. this approach has several advantages including ease of use, and the ability to provide tamper-resistance for any program that can be constructed using the language. the language approach enabled the development of two novel mechanisms for implementing tamper resistance. these mechanisms provide alternatives to pure merkle hash trees. simulated relative performance of the various mechanisms is provided and shows the relative merits of each mechanism.a tamper-resistant programming language system','Tamper-proof and tamper-resistant designs'
'tamper-resistant software has been studied as techniques to protect algorithm or secret data. there are many ways to realize tamper-resistant software including the method of making software hard to read. so far, no objective and quantitative method is known for evaluating tamper-resistant software. most of known evaluation methods require involvement of human being. that means their evaluation results deeply depend on the skill and subjectivity of human. therefore, it has been expected to devise an objective and quantitative evaluation method in place of subjective evaluation methods. in this paper we propose a new such method to measure how hard to read. the basic idea is to use the parse tree of a compiler for a programming language, and evaluate depth and weights of the tree for a code. we give some experimental results to examine its effectiveness.an approach to the objective and quantitative evaluation of tamper-resistant software','Tamper-proof and tamper-resistant designs'
'although there have been attempts to develop code transformations that yield tamper-resistant software, no reliable software-only methods are known. this paper studies the hardware implementation of a form of execute-only memory (xom) that allows instructions stored in memory to be executed but not otherwise manipulated. to support xom code we use a machine that supports internal compartments---a process in one compartment cannot read data from another compartment. all data that leaves the machine is encrypted, since we assume external memory is not secure. the design of this machine poses some interesting trade-offs between security, efficiency, and flexibility. we explore some of the potential security issues as one pushes the machine to become more efficient and flexible. although security carries a performance penalty, our analysis indicates that it is possible to create a normal multi-tasking machine where nearly all applications can be run in xom mode. while a virtual xom machine is possible, the underlying hardware needs to support a unique private key, private memory, and traps on cache misses. for efficient operation, hardware assist to provide fast symmetric ciphers is also required.architectural support for copy and tamper resistant software','Tamper-proof and tamper-resistant designs'
'trust management in a networked environment consists of authentication and integrity checking. in a mobile computing environment, both remote hosts and mobile code are suspect. we present a model that addresses trust negotiation between the remote host and the mobile code simultaneously. our model uses tamper resistant hardware, public key cryptography, and one-way hash functions.bidirectional mobile code trust management using tamper resistant hardware','Tamper-proof and tamper-resistant designs'
'tamper-resistant software (trs) consists of two functional components: tamper detection and tamper response. although both are equally critical to the effectiveness of a trs system, past research has focused primarily on the former, while giving little thought to the latter. not surprisingly, many successful breaks of commercial trs systems found their first breaches at the relatively na&#239;ve tamper-response modules. in this paper, we describe a novel tamper-response system that evades hacker detection by introducing delayed, probabilistic failures in a program. this is accomplished by corrupting the program\'s internal state at well-chosen locations. our tamper-response system smoothly blends in with the program and leaves no noticeable traces behind, making it very difficult for a hacker to detect its existence. the paper also presents empirical results to demonstrate the efficacy of our system.delayed and controlled failures in tamper-resistant software','Tamper-proof and tamper-resistant designs'
'this paper investigates secure ways to interact with tamper-resistant hardware leaking a strictly bounded amount of information. architectural support for the interaction mechanisms is studied and performance implications are evaluated. the interaction mechanisms are built on top of a recently-proposed secure processor ascend[ascend-stc12]. ascend is chosen because unlike other tamper-resistant hardware systems, ascend completely obfuscates pin traffic through the use of oblivious ram (oram) and periodic oram accesses. however, the original ascend proposal, with the exception of main memory, can only communicate with the outside world at the beginning or end of program execution; no intermediate information transfer is allowed. our system, stream-ascend, is an extension of ascend that enables intermediate interaction with the outside world. stream-ascend significantly improves the generality and efficiency of ascend in supporting many applications that fit into a streaming model, while maintaining the same security level.simulation results show that with smart scheduling algorithms, the performance overhead of stream-ascend relative to an insecure and idealized baseline processor is only 24.5\%, 0.7\%, and 3.9\% for a set of streaming benchmarks in a large dataset processing application. stream-ascend is able to achieve a very high security level with small overheads for a large class of applications.generalized external interaction with tamper-resistant hardware with bounded information leakage','Tamper-proof and tamper-resistant designs'
'content protection mechanisms are intended to enforce the usage rights on the content. these usage rights are carried by a license. sometimes, a license even carries the key that is used to unlock the protected content. unfortunately, license protection is difficult, yet it is important for digital rights management (drm). not many license protection schemes are available, and most if not all are proprietary. in this paper, we present a license protection scheme, which exploits tamper-resistant cryptographic hardware. the confidentiality and integrity of the license or parts thereof can be assured with our protection scheme. in addition, the keys to unlock the protected content are always protected and stored securely as part of the license. we verify secrecy and authentication aspects of one of our protocols. we implement the scheme in a prototype to assess the performance.license protection with a tamper-resistant token','Tamper-proof and tamper-resistant designs'
'low cost attacks on tamper resistant devices','Tamper-proof and tamper-resistant designs'
'with the rapid growth of broadband network, distribution of multimedia via internet is a must way to go. content protection has become one of the most significant and challenging problems of this field. in this paper, we propose a general scheme that combines public key cryptography and watermarking technology together, to achieve wonderful content protection. the scheme is reliable, flexible and efficient.multimedia content protection by cryptography and watermarking in tamper-resistant hardware','Tamper-proof and tamper-resistant designs'
'keys are indispensable in secure communication, and can only be adequately protected by tamper resistant hardware. conversely, once you have tamper resistant devices available, you may as well try to make the most of them. in this exposition, we present three novel ideas for the combination of pki and appropriate use of tamper resistant devices for other purposes than traditional key management. the first iselectronic negotiable instruments, such as cash, but there are other important examples, such as bills of lading and endorsable checks. the challenge is to prevent double spending without the use of a central registry to keep track of ownership. the second is a new light way digital signature scheme which seems to work well with tamper resistant hardware, but not in software, where it can be broken. we briefly discuss how to make use of this in a transparent pki solution to be employed by vehicles, which appears to be a hot research topic. finally we introduce the concept of a digital signature server with central storage of user keys and a central signing facility only, which is operated at the full control of the user using a secure channel for proper authentication. the first and last scenarios have both been deployed in live systems and have been patented, in contrast to the light weight digital signature.new pki protocols using tamper resistant hardware','Tamper-proof and tamper-resistant designs'
'we generalise results of jackson concerning cyclic hadamard designs admitting sl(2,2)^n as a pointtransitive automorphism group. the generalisation concerns the designs of gordon, mills and welch and wecharacterise these as designs admitting gm(m,q)^n acting in a certain way. we also generalise a constructiongiven by maschietti, using hyperovals, of cyclic hadamard designs, and characterise these amongst the designsof gordon, mills and welch.on gmw designs and cyclic hadamard designs','Tamper-proof and tamper-resistant designs'
'this paper describes a new type of attack on tamper-resistant cryptographic hardware. we show that by locally observing the value of a few ram or adress bus bits (possibly a single one) during the execution of a cryptographic algorithm, typically by the mean of a probe (needle), an attacker could easily recover information on the secret key being used; our attacks apply to public-key cryptosystems such as rsa or el gamal, as well as to secret-key encryption schemes including des and rc5.probing attacks on tamper-resistant devices','Tamper-proof and tamper-resistant designs'
'we specify a hardware architecture that supportstamper-resistant software by identifying an \"idealized\"model, which gives the abstracted actions available to asingle user program. this idealized model is compared toa concrete \"actual\" model that includes actions of an adversarialoperating system. the architecture is verified byusing a finite-state enumeration tool (a model checker) tocompare executions of the idealized and actual models. inthis approach, software tampering occurs if the system canenter a state where one model is inconsistent with the other.in performing the verification, we detected an replay attackscenario and were able to verify the security of our solutionto the problem. our methods were also able to verifythat all actions in the architecture are required, as well ascome up with a set of constraints on the operating system toguarantee liveness for users.specifying and verifying hardware for tamper-resistant software','Tamper-proof and tamper-resistant designs'
'there are many situations in which it is desirable to protect a piece of software from illegitimate tampering once it gets distributed to the users. protecting the software code means some level of assurance that the program will execute as expected even if it encounters the illegitimated modifications. we provide the method of protecting software from unauthorized modification. one important technique is an integrity-based encryption, by which a program, while running, checks itself to verify that it has not been modified and conceals some privacy sensitive parts of program.tamper resistant software by integrity-based encryption','Tamper-proof and tamper-resistant designs'
'tamper resistant software','Tamper-proof and tamper-resistant designs'
'due to limited available memory (of the order of kilobytes) on embedded devices (such as smart cards), we undertake an approach of partitioning the whole program when it does not fit in the memory. the program partitions are downloaded from the server on demand into the embedded device just before execution. we devise a method of partitioning the code and data of the program such that no information regarding the control flow behavior of the program is leaked out. this property is called tamper resistance and it is very important for secure embedded devices such as smart cards which could hold sensitive information and/or carry out critical computation such as financial transactions. a preliminary solution to this problem was proposed in our earlier work [1]. this work proposes a new and more comprehensive solution to the problem. first, we propose a new policy which is based on keeping nothing in terms of partitions on the smart card. this policy is unlike the one in previous work which mandated keeping partitions in memory to which control flow was guaranteed to return. based on this new policy, a new partitioning algorithm is proposed for minimal safe partitions which reduces their memory requirements over previous work. the drawback of this new policy is however lower execution speed due to frequent communication encountered. in order to not significantly degrade performance, we propose caching frequently executed functions on the smart card without violation of tamper resistance. a framework is designed to determine the set of functions to be cached in conjunction with specific minimal safe partitions. further reduction in memory requirements is achieved due to the data partitioning.the decrease in memory footprint over the previous method is 27\% for code memory and 32.4\% for data memory on average. the speed-up over the old method is quite significant when applied to whole programs in large benchmarks (500 times on average). the conclusion is that previous method [1] is not suitable as a whole program partitioning strategy whereas the new proposed method is a viable solution.tamper-resistant whole program partitioning','Tamper-proof and tamper-resistant designs'
'in recent years, many have suggested to apply encryption in the domain of software protection against malicious hosts. however, little information seems to be available on the implementation aspects or cost of the different schemes. this paper tries to fill the gap by presenting our experience with several encryption techniques: bulk encryption, an on-demand decryption scheme, and a combination of both techniques. our scheme offers maximal protection against both static and dynamic code analysis and tampering. we validate our techniques by applying them on several benchmark programs of the cpu2006 test suite. and finally, we propose a heuristic which trades off security versus performance, resulting in a decrease of the runtime overhead.towards tamper resistant code encryption','Tamper-proof and tamper-resistant designs'
'web application development frameworks, like the java server pages framework (jsp), provide web applications with essential functions such as maintaining state information across the application and access control. in the fast paced world of web applications, new frameworks are introduced and old ones are updated frequently. a framework is chosen during the initial phases of the project. hence, changing it to match the new requirements and demands is a cumbersome task. we propose an approach (based on water transformations) to migrate web applications between various web development frameworks. this migration process preserves the structure of the code and the location of comments to facilitate future manual maintenance of the migrated code. consequently, developers can move their applications to the framework that meets their current needs instead of being locked into their initial development framework. we give an example of using our approach to migrate a web application written using the active server pages (asp) framework to the netscape server pages (nsp) framework.a lightweight approach for migrating web frameworks','Trust frameworks'
'this paper presents an approach to modelling and reasoning about arguments that exploits and combines two of the most popular mechanisms used within computational modelling of argumentation: argumentation schemes and abstract argumentation frameworks. our proposal combines the desirable properties of each by representing the components of argumentation schemes as argumentation frameworks. this allows us to make use of the structure provided by the schemes to guide dialogues and provide contextual elements of evaluation, whilst retaining the desirable properties of abstract frameworks to enable evaluation with respect to the logical relations between arguments. our proposal takes account of dialogical aspects within a debate, such as burden of proof, and we illustrate our approach through a particular argumentation scheme, namely argument from expert opinion.abstract argumentation scheme frameworks','Trust frameworks'
'we present various new concepts and results related to abstract dialectical frameworks (adfs), a powerful generalization of dung\'s argumentation frameworks (afs). in particular, we show how the existing definitions of stable and preferred semantics which are restricted to the subcase of so-called bipolar adfs can be improved and generalized to arbitrary frameworks. furthermore, we introduce preference handling methods for adfs, allowing for both reasoning with and about preferences. finally, we present an implementation based on an encoding in answer set programming.abstract dialectical frameworks revisited','Trust frameworks'
'enforcing framework adaptability is one of the key points in the process of building an object-oriented application framework. when it comes to simulation, some adaptation mechanisms to configure components on-the-fly are usually required in order to produce quality software artifacts and alleviate development effort. the paper reports an experience using a simulation multi-agent framework, initially conceived to be used in fluid flow problems. the framework architecture demonstrated during its evolution a great potential regarding to flexibility and modularity, tackling a wide range of other problems ranging from a network protocol simulation to a soccer simulation.accomplishing adaptability in simulation frameworks','Trust frameworks'
'the paper applies some recent developments in computing to the problem of building very large software systems. we (1) emphasize the system\'s infrastructure as a way to keep large numbers of disparate components coordinated, and (2) show that our earlier wrapping research provides an active integration framework for heterogeneous system integration. the basic expressive notion is the \"posed problem\", and the basic computational component is the \"resource\". they are connected by the \"wrappings\", which consist of processes and associated knowledge bases that convert a posed problem into coordinated collections of resources that can address the problem.active integration frameworks','Trust frameworks'
'the global marketplace and the internet have served as catalysts for enterprise integration (ei), both within a company and between a company and its suppliers and partners. ei software architectures, or frameworks, are often built on top of standard middleware and typically lack the ability to function in dynamic environments where flexibility, adaptability, and knowledge management are crucial. furthermore, they do not scale well to wide-area enterprises that extend across organizational boundaries. these frameworks nevertheless offer a cost-effective basis for resource management, user coordination, knowledge exchange, and information extraction in a virtual enterprise. the paper considers how, by incorporating agents with their inherently distributed characteristics of autonomy, reasoning or intelligence, and goal-driven behavior, existing ei frameworks can be enhanced to support adaptive virtual enterprisesadding intelligent agents to existing ei frameworks','Trust frameworks'
'reusing similar requirements fragments is among the promising ways to reduce elaboration time and increase requirements quality. this paper investigates the application of analogical reasoning techniques to complete partial requirements specifications. a case base is assumed to be available; it contains requirements frameworks involving goals, constraints, objects, actions, and agents from systems already specified. we show how a rich requirements meta-model coupled with an expressive formal assertion language may increase the effectiveness of analogical reuse. an acquisition problem is first specified by the requirements engineer as a query formulated in the vocabulary of the specification fragments built so far. source cases and partial mappings are found by query generalization followed by search through the case base. once analogies have been confirmed, mappings are completed by use of relevance rules that distinguish in the formal assertions what is relevant to the analogy from what is irrelevant. best analogies are then selected and extended in such a way that logical properties of the answers to the query may be verified, thus increasing confidence in the analogy. the approach is illustrated by analogical acquisition of specifications of a meeting scheduler in the kaos goal-oriented specification language.analogical reuse of requirements frameworks','Trust frameworks'
'application frameworks have become a de-facto standard to implement business systems. in most organizations, when choosing either a development platform or a commercial solution, an application framework is part of the overall solution. this paper reviews my personal experience developing a proprietary application framework, its lifecycle, software engineering practices, successes and mistakes through its releases.application frameworks','Trust frameworks'
'in order to improve possibilities for language support for the development and use of object-oriented frameworks, we propose to elaborate on the conceptual understanding of frameworks with focus on architectural issues. in particular we propose the idea of framework components and connectorsarchitectural abstractions for frameworks','Trust frameworks'
'this paper describes experiences with several architectural frameworks. an \"architectural framework\" specifies what is included in the description of an architecture, independent of the specific system being described. the three frameworks are the u.s. dod c4isr architecture framework, the associated core architecture data model and the emerging ieee recommended practice on architecture description. from these experiences, we speculate on the further evolution of architecture frameworks and architectural descriptions.architectural frameworks','Trust frameworks'
'argumentation frameworks and schemes i','Trust frameworks'
'argumentation frameworks and schemes ii','Trust frameworks'
'in this paper, we introduce argumentation frameworks with necessities (afns), an extension of dung\'s argumentation frameworks (afs) taking into account a necessity relation as a kind of support relation between arguments (an argument is necessary for another). we redefine the acceptability semantics for these extended frameworks and we show how the necessity relation allows a direct and easy correspondence between a fragment of logic programs (lps) and afns. we introduce then a further generalization of afns that extends the necessity relation to deal with sets of arguments. we give a natural adaptation of the acceptability semantics to this new context and show that the generalized frameworks allow to encode arbitrary logic programs.argumentation frameworks with necessities','Trust frameworks'
'how can we deliver infrastructure capable of supporting the preservation of digital objects, as well as the services that can be applied to those digital objects, in ways that future unknown systems will understand? a critical problem in developing systems is the process of validating whether the delivered solution effectively reflects the validated requirements. this is a challenge also for the eu-funded shaman project, which aims to develop an integrated preservation framework using grid-technologies for distributed networks of digital preservation systems, for managing the storage, access, presentation, and manipulation of digital objects over time. recognising this, the project team ensured that alongside the user requirements an assessment framework was developed. this paper presents the assessment of the shaman demonstrators for the memory institution, industrial design and engineering and escience domains, from the point of view of user\'s needs and fitness for purpose. an innovative synergistic use of trac criteria, drambora risk registry and mitigation strategies, irods rules and information system models requirements has been designed, with the underlying goal to define associated policies, rules and state information, and make them wherever possible machine-encodable and enforceable. the described assessment framework can be valuable not only for the implementers of this project preservation framework, but for the wider digital preservation community, because it provides a holistic approach to assessing and validating the preservation of digital libraries, digital repositories and data centres.assessing digital preservation frameworks','Trust frameworks'
'at the forge: design frameworks','Trust frameworks'
'a direction&#x2013;length framework is a pair (g,p) where g=(v;d,l) is a &#x2018;mixed&#x2019; graph whose edges are labelled as &#x2018;direction&#x2019; or &#x2018;length&#x2019; edges and p is a map from v to &#x211d; d  for some d. the label of an edge uv represents a direction or length constraint between p(u) and p(v). let g + be obtained from g by adding, for each length edge e of&#x00a0;g, a direction edge with the same end vertices as e. we show that (g,p) is bounded if and only if (g +,p) is infinitesimally rigid. this gives a characterization of when (g,p) is bounded in terms of the rank of the rigidity matrix of (g +,p). we use this to characterize when a mixed graph is generically bounded in&#x00a0;&#x211d; d . as an application we deduce that if (g,p) is a globally rigid generic framework with at least two length edges and e is a length edge of&#x00a0;g then (g&#x2216;e,p) is bounded.bounded direction&#x2013;length frameworks','Trust frameworks'
'rhizome is an experimental, open source content management framework that can capture and represent informal, human-authored content in a semantically rich manner. rhizome aims to help bring about a new kind of commons--one of ideas. this commons wouldn\'t comprise just a web of interlinked pages of content, as is the current world wide web, but a web of relationships between the underlying ideas and distinctions the content implies: a permanent, universally accessible interlinking of content based on imputed semantics such as concepts, definitions, or structured argumentation.building a semantic wiki','Trust frameworks'
'carneades is a rather general framework for argumentation. unlike many other approaches, carneades captures a number of aspects, like proof burdens, proof standards etc., which are of central importance, in particular in legal argumentation. in this paper we show how carneades argument evaluation structures can be reconstructed as abstract dialectical frameworks (adfs), a recently proposed generalization of dung argumentation frameworks (afs). this not only provides at least an indirect link between carneades and afs, it also allows us to handle arbitrary argument cycles, thus lifting a restriction of carneades. at the same time it provides strong evidence for the usefulness of adfs as analytical/semantical tools in argumentation.carneades and abstract dialectical frameworks','Trust frameworks'
'in this paper we define a logical framework, called &;lambda;tt, that is well suited for semantic analysis. we introduce the notion of a fibration &lscr;1 : &fscr;1 &xrarr; &cscr;1 being internally definable in a fibration &lscr;2 : &fscr;2 &xrarr; &cscr;2. this notion amounts to distinguishing an internal category l in &lscr;2 and relating &lscr;1 to the externalization of l through a pullback. when both &lscr;1 and &lscr;2 are term models of typed calculi &lscr;1 and &lscr;2, respectively, we say that &lscr;1 is an internal typed calculus definable in the frame language &lscr;2. we will show by examples that if an object language is adequately represented in &#955;tt, then it is an internal typed calculus definable in the frame language &#955;tt. these examples also show a general phenomenon: if the term model of an object language has categorical structure s, then an adequate encoding of the language in &#955;tt imposes an explicit internal categorical structure s in the term model of &#955;tt and the two structures are related via internal definability. our categorical investigation of logical frameworks indicates a sensible model theory of encodings.categorical properties of logical frameworks','Trust frameworks'
'in this paper, we address the problem of change in an abstract argumentation system. we focus on a particular change: the addition of a new argument which interacts with previous arguments. we study the impact of such an addition on the outcome of the argumentation system, more particularly on the set of its extensions. several properties for this change operation are defined by comparing the new set of extensions to the initial one, these properties are called \"structural\" when the comparisons are based on set-cardinality or set-inclusion relations. several other properties are proposed where comparisons are based on the status of some particular arguments: the accepted arguments; these properties refer to the \"evolution of this status\" during the change, e.g., monotony and priority to recency. all these properties may be more or less desirable according to specific applications. they are studied under two particular semantics: the grounded and preferred semantics.change in abstract argumentation frameworks','Trust frameworks'
'object-oriented application frameworks present one of the most successful approaches to developing reusable assets in industry, but developing frameworks is both difficult and expensive. frameworks generally evolve through a number of iterations due to the incorporation of new requirements and better domain understanding. since changes to frameworks have a large impact on the applications build based on the asset, it is important to assess the stability of a framework. recently, an approach for assessing framework stability has been proposed [1]. we have extended and applied the assessment approach on one proprietary telecommunication framework and two commercial gui application frameworks. based on our findings we formulate a set of hypotheses, which characterize the stability of an object-oriented application framework. we believe these hypotheses to be the most promising ones for further studies of framework stability.characterizing stability in evolving frameworks','Trust frameworks'
'since argumentation is an inherently dynamic process, it is of great importance to understand the effect of incorporating new information into given argumentation frameworks. in this work, we address this issue by analyzing equivalence between argumentation frameworks under the assumption that the frameworks in question are incomplete, i.e. further information might be added later to both frameworks simultaneously. in other words, instead of the standard notion of equivalence (which holds between two frameworks, if they possess the same extensions), we require here that frameworks f and g are also equivalent when conjoined with any further framework h. due to the nonmonotonicity of argumentation semantics, this concept is different to (but obviously implies) the standard notion of equivalence. we thus call our new notion strong equivalence and study how strong equivalence can be decided with respect to the most important semantics for abstract argumentation frameworks. we also consider variants of strong equivalence in which we define equivalence with respect to the sets of arguments credulously (or skeptically) accepted, and restrict strong equivalence to augmentations h where no new arguments are raised.characterizing strong equivalence for argumentation frameworks','Trust frameworks'
'software frameworks are difficult for plugin developers to use, even when they are well designed and documented. some of these difficulties stem from the many constraints that frameworks impose on plugin code. these constraints might restrict operations from being called on certain objects, or they might restrict how long an object is available. additionally, the constraints are relative to the current context of the plugin, and they can involve multiple, interacting framework objects. this paper proposes a lightweight specification system and analysis to check plugins from a semantic perspective, rather than a purely structural view.checking semantic usage of frameworks','Trust frameworks'
'this presentation describes a concept proposal for a system that will create an archive of the activities of the studio at siggraph 2002. the initial parameters that constrain the design process are that the materials of such repository must be displayed and archived during the conference itself, and they must be formatted in such a manner that facilitates their retrieval and replay at a later date. additionally, in order to reflect the diversity of content present, the archive must be created in a collaborative manner, and with the help of other studio participants. the amplitude of the desired coverage is yet another factor to be considered. all of these factors must be considered in the light of the quantity of data that can be potentially generated by such endeavor.collaborative frameworks','Trust frameworks'
'comparing application frameworks','Trust frameworks'
'in recent years, web applications have become much more capable, driven primarily by the rise of javascript as a general-purpose programming language available in all major browsers. as browsers continue to compete with javascript performance, the web platform has become a viable alternative in many cases to applications installed natively.comparing javascript frameworks','Trust frameworks'
'a fundamental issue in the engineering of coordination models is to design coordination abstractions that are correct with respect to the specification of the coordination model they implement. the traditional semantic framework for coordination is focused on describing the admissible evolutions over time of a coordinated system, and is particularly suitable for specifying the laws of a coordination model. on the other hand, formally describing run-time aspects of an implementation requires a different framework, capturing as fundamental idea the interactive behavior of a coordination medium.in this paper, these two frameworks are compared by tackling a crucial issue of coordination models, that is, the conformance of an implementation with respect to a specification. in particular, a definition of conformance is introduced that is shown to be compatible with the standard notion of implementation by horizontal refinement promoted in the context of process algebras.comparing semantic frameworks for coordination','Trust frameworks'
'today, many cloud infrastructure as a service(iaas) frameworks exist. users, developers, and administrators have to make a decision about which environment is best suited for them. unfortunately, the comparison of such frameworks is difficult because either users do not have access to all of them or they are comparing the performance of such systems on different resources, which make it difficult to obtain objective comparisons. hence, the community benefits from the availability of a testbed on which comparisons between the iaas frameworks can be conducted. futuregrid aims to offer a number of iaas including nimbus, eucalyptus, openstack, and opennebula. one of the important features that futuregrid provides is not only the ability to compare between iaas frameworks, but also to compare them in regards to bare-metal and traditional high performance computing services. in this paper, we outline some of our initial findings by providing such a testbed. as one of our conclusions, we also present our work on making access to the various infrastructures on futuregrid easier.comparison of multiple cloud frameworks','Trust frameworks'
'this paper reports on an effort to use both the system theoretic devs (discrete event simulation) formalism and the javabeans component model as a basis for a component-based discrete event simulation framework. the result of the synergism of devs and javabeans is a powerful component-based simulation framework together with a set of flexible bean components for building simulation systems. component frameworks are dedicated and focused architectures with a set of policies for mechanisms at the component level. in this paper we describe the component frame-work we have developed for discrete event simulations. simulation components are based on this framework and can be composed for the creation of various simulation scenarios.component frameworks - a case study','Trust frameworks'
'component frameworks','Trust frameworks'
'this article describes a new model for a composable protocol framework that is suitable for the rapid development of any application-specific service and its deployment in the network. the class-hierarchy model described here enables users to compose their own custom, flexible frameworks from either predefined or custom protocol components tailored to an application\'s needs. we validate experimentally that application-specific frameworks implementing custom protocols can be developed to improve performance of applications over wireless networkscomposing protocol frameworks for active wireless networks','Trust frameworks'
'extended argumentation frameworks (eafs) are a recently proposed formalism that develop abstract argumentation frameworks (afs) by allowing attacks between arguments to be attacked themselves: hence eafs add a relationship d &#8838; x &#215; a to the arguments (x) and attacks (a &#8838; x &#215; x) in an af\'s basic directed graph structure &lt;x,a&gt;. this development provides a natural way to represent and reason about preferences between arguments. studies of eafs have thus far focussed on acceptability semantics, proof-theoretic processes, and applications. however, no detailed treatment of their practicality in computational settings has been undertaken. in this paper we address this lacuna, considering algorithmic and complexity properties specific to eafs. we show that (as for standard afs) the problem of determining if an argument is acceptable w.r.t. a subset of x is polynomial time decidable and, thus, determining the grounded extension and verifying admissibility are efficiently solvable. we, further, consider the status of a number of decision questions specific to the eaf formalism in the sense that these have no counterparts within afs.computation in extended argumentation frameworks','Trust frameworks'
'in recent years a large corpus of studies has arisen from dung\'s seminal abstract model of argumentation, including several extensions aimed at increasing its expressiveness. most of these works focus on the case of finite argumentation frameworks, leaving the potential practical applications of infinite frameworks largely unexplored. in the context of a recently proposed extension of dung\'s framework called afra (argumentation framework with recursive attacks), this paper makes a first step to fill this gap. it is shown that, under some reasonable restrictions, infinite frameworks admit a compact finite specification and that, on this basis, computational problems which are tractable for finite frameworks may preserve the same property in the infinite case. in particular we provide a polynomial-time algorithm to compute the finite representation of the (possibly infinite) grounded extension of an afra with infinite attacks. an example concerning the representation of a moral dilemma is introduced to illustrate and instantiate the proposal and gives a preliminary idea of its potential applicability.computing with infinite argumentation frameworks','Trust frameworks'
'rapid development in new information technology requires constant need for training end user. organisations spend heavily on training, but do not take an integrative approach in formulating training strategies. this is mainly due to lack of guidance from the literature. a few frameworks that have been proposed have not been evaluated. in this paper, we use one such framework and use it to evaluate the training strategy in an organisation. this summative evaluation is followed by the formative evaluation of the framework itself. based on our findings, we conclude that the framework is useful to evaluate organisational training efforts. however, we also point out deficiencies in the framework itself and suggest areas of improvement.conceptual frameworks in practice','Trust frameworks'
'in this paper, we combine separate works on (a) the transfer of infinitesimal rigidity results from an euclidean space to the next higher dimension by coning (whiteley in topol. struct. 8:53&#x2013;70, 1983), (b) the further transfer of these results to spherical space via associated rigidity matrices (saliola and whiteley in arxiv: 0709.3354, 2007), and (c) the prediction of finite motions from symmetric infinitesimal motions at regular points of the symmetry-derived orbit rigidity matrix (schulze and whiteley in discrete comput. geom. 46:561&#x2013;598, 2011). each of these techniques is reworked and simplified to apply across several metrics, including the minkowskian metric $\\mathbb{m}^{d}$ and the hyperbolic metric &#x210d; d . this leads to a set of new results transferring infinitesimal and finite motions associated with corresponding symmetric frameworks among $\\mathbb{e}^{d}$, cones in $\\mathbb{e}^{d+1}$, $\\mathbb{s}^{d}$, $\\mathbb{m}^{d}$, and &#x210d; d . we also consider the further extensions associated with the other cayley&#x2013;klein geometries overlaid on the shared underlying projective geometry.coning, symmetry and spherical frameworks','Trust frameworks'
'context information helps an application decide on what to do in order to adapt to its user&#39;s needs. to easily develop ubiquitous applications, there has been increased research in the design and development of frameworks called pervasive computing frameworks. although these frameworks help application developers create ubiquitous applications easily, interoperability has been a problem because of the different representation of context information and protocols used. this research attempts to solve this problem by creating a context information mediator (cim) which will serve as a translation gateway between different applications created using different frameworks. to test our system, we developed two versions of an inventory system application that keeps track of items inside a building. the idea here is to let these applications communicate with each other and share information through the cim.connecting pervasive frameworks through mediation','Trust frameworks'
'constructing new interface frameworks','Trust frameworks'
'this research examines the framework used by computer science students at the conclusion of their first semester of study of database concepts. to discover which concepts are considered as essential components in students\' mental frameworks for database systems, a questionnaire listing 31 database concepts was given to computer science students upon completion of their first database course. this survey was given to two student groups: one during the last week of their first database course, and the other during the first meeting of a second database course. to identify which topics were crucial, students were asked to rate each concept on a ten-point scale. from their responses, we calculated the average perceived importance for each concept. this paper analyses the results of this survey for the two student groups. we then compare the student ratings with word frequencies exhibited by authors of database textbooks. in this way, we are able to show how the database framework of students relates to frameworks presented in textbooks.database frameworks','Trust frameworks'
'given the availability of high-speed ethernet and hw based protocol offload, clustered systems using a commodity network fabric (e.g., tcp/ip over ethernet) are expected to become more attractive for a range of e-business and data center applications. in this paper, we describe a comprehensive simulation to study the performance of clustered database systems using such a fabric. the simulation model currently supports both tcp and sctp as the transport protocol and models an oracle 9i like clustered dbms running a tpc-c like workload. the model can be used to study a wide variety of issues regarding the performance of clustered dbms systems including the impact of enhancements to network layers (transport, ip, mac), qos mechanisms or latency improvements, and cluster-wide power control issues.dclue','Trust frameworks'
'since its introduction, mapreduce implementations have been primarily focused towards static compute cluster sizes. in this paper, we introduce the concept of dynamic elasticity to mapreduce. we present the design decisions and implementation tradeoffs for delma, (dynamically elastic mapreduce), a framework that follows the mapreduce paradigm, just like hadoop mapreduce, but that is capable of growing and shrinking its cluster size, as jobs are underway. in our study, we test delma in diverse performance scenarios, ranging from diverse node additions to node additions at various points in the application run-time with various dataset sizes. the applicability of the mapreduce paradigm extends far beyond its use with large-scale data intensive applications, and can also be brought to bear in processing long running distributed applications executing on small-sized clusters. in this work, we focus both on the performance of processing hierarchical data in distributed scientific applications, as well as the processing of smaller but demanding input sizes primarily used in small clusters. we run experiments for datasets that require cpu intensive processing, ranging in size from millions of input data elements to process, up to over half a billion elements, and observe the positive scalability patterns exhibited by the system. we show that for such sizes, performance increases accordingly with data and cluster size increases. we conclude on the benefits of providing mapreduce with the capability of dynamically growing and shrinking its cluster configuration by adding and removing nodes during jobs, and explain the possibilities presented by this model.delma','Trust frameworks'
'with the advent of rich application frameworks like flash and silverlight as well as the increased exposure to interaction models they make possible (does anyone want a mobile device that doesn\'t have an iphone-like interface?) it isn\'t difficult to imagine that usability and design professionals may be feeling a little vulnerable. after all, until recently, usability and design professionals were the last, best hope in the face of early web design, business systems left over from the 80s and clunky mobile phone menus. we helped create an environment in which users expected more (at least on the web). today, however, developers have at their disposal an arsenal of tools designed to provide users with experiences that take advantage of asynchronous server calls, highdefinition multimedia and slick, natural-feeling interactions. have we been relegated to the role of usability testing? surely, our profession has more to offer. the good news is that our role is the same as it has ever been. like any platform or technology, rich application frameworks are the medium through which design is expressed. as such, they are no different from any previous platform that was ready to revolutionize the manner in which people interact with information, the world or each other. they are the tools through which researchers, designers and technologists enable users to complete tasks and make decisions. they are the paint and canvas, the clay and plastic molds, with which we bring our designs to life. a well-designed system is the result of a well-defined design process. that process includes the expertise of an interdisciplinary team with individual backgrounds in graphic design, fine art, architecture, cognitive psychology, anthropology, human-computer interaction, and other fields. this kind of design team has the training and experience to bridge the gap between business, technology and human requirements. they (we!) practice a design process that is mindful of the features, functions and legacy systems that must be somehow united, implemented and maintained. they are equally mindful of who will be using these systems (from motorcycle enthusiasts to financial analysts, from students to ceos), their experiences and mental models, where the systems will be used (from hospital emergency rooms to living rooms and executive boardrooms) and what they need from technology to improve rather than impede outcomes. if, at any point, the user must wrestle with the interface, then research and design have failed. our job, therefore, remains one of understanding the ways in which users need to have information presented to them, the ways in which they need to interact with it and the decisions they must make. the capabilities made available via rich internet applications provide a larger toolset from which to choose in order to meet these requirements.design and rich application frameworks','Trust frameworks'
'object-oriented frameworks and design patterns are useful abstractions that are relatively new to the object-oriented paradigm. the implementation of these abstractions, however, suffers from a number of problems due to the fact that insuffcient language support is provided by traditional object-oriented languages. in this paper, we analyse these problems, study the different approaches for providing extended language support that can be identified and specify the requirements that have to be fulfilled by such approaches.design patterns & frameworks','Trust frameworks'
'generative programming methods provide somesignificant advantages for the repeated deployment ofproduct line architectures. this paper considers xml asa tool for building and describing applications that usegenerative programming methods. it describestechniques for the creation of a generative framework,presents a case study and discusses the results ofpractical application of these methods in a real world,enterprise scale, product line architecture. the paperpresents the advantages of using an xml descriptor thatcan be easily transformed to generate both static anddynamically configurable software components for directdeployment in an application framework. twoimplementation approaches are considered: an indirectapproach using xsl for the transformations; and a directapproach where the xml descriptor is parsed and dealtwith programmatically. the relative advantages of thesetwo approaches are discussed. the paper providespractical examples and presents lessons learned from theapplication of the techniques.developing generative frameworks using xml','Trust frameworks'
'difficulties with object-oriented frameworks','Trust frameworks'
'grid computing presents a new trend to distributed and internet computing to coordinate large scale resources sharing and problem solving in dynamic, multi-institutional virtual organizations. due to the diverse failures and error conditions in the grid environments, developing, deploying, and executing applications over the grid is a challenge, thus dependability is a key factor for grid computing. this paper presents a dependable grid computing framework, called dric, to provide an adaptive failure detection service and a policy-based failure handling mechanism. the failure detection service in dric is adaptive to users\' qos requirements and system conditions, and the failure-handling mechanism can be set optimized based on decision-making method by a policy engine. the performance evaluation results show that this framework is scalable, high efficiency and low overhead.dric','Trust frameworks'
'my thesis work aims to study change operations for argumentation systems, especially for abstract argumentation systems &#224; la dung. this paper presents a first study of the agm revision adapted to the case of argumentation. we also sketch future research works planned to complete the one already achieved.dynamic of argumentation frameworks','Trust frameworks'
'the success of innovative e-participation solutions depends heavily on the organizational planning and the incorporation of such initiatives into the different stages of the policy life-cycle. e-participation often demands to introduce new participation facilities into the traditional processes of policy formulation and decision making. accommodating the various requirements from distinct perspectives calls for a holistic engineering approach for e-participation systems analysis and design. enterprise architectures (ea) have evolved in information systems research as an approach to give guidance in developing complex socio-technical systems. this paper analyzes the application of ea frameworks in the context of e-participation. eparticipation domain and implementation models are investigated to identify crucial tasks and aspects in e-participation project development and implementation. related to the tasks identified, two ea frameworks are analyzed: the zachman framework and togaf. we explain how ea frameworks can support the development and implementation of e-participation projects. finally, the needs for a reference framework for e-participation are argued and a reference framework is presented.e-participation and enterprise architecture frameworks','Trust frameworks'
'the paper presents the experiences made during the development of an application framework supporting the design, implementation and deployment of telecommunication services. it introduces the concepts of a tina-c-compliant (telecommunications information networking architecture consortium) environment as an example for a framework in an enterprise specific, vertical application domain. the paper shows the advantages of the definition of a supporting engineering framework including a set of related design patterns that allow to design application frameworks in a technology independent way. it presents the major concepts of a related middleware that has been designed as an odp-compliant (open distributed processing) distributed processing environment and that has been implemented in a heterogeneous corba 2 (common object request broker architecture) environment. the presented work is a part of the research and development program of gmd fokus and builds the core of the y middleware.engineering frameworks','Trust frameworks'
'this paper addresses the problem of revising a dung-style argumentation framework by adding finitely many new arguments which may interact with old ones. we study the behavior of the extensions of the augmented argumentation frameworks, taking also into account possible changes of the underlying semantics (which may be interpreted as corresponding changes of proof standards). we show both possibility and impossibility results related to the problem of enforcing a desired set of arguments. furthermore, we prove some monotonicity results for a special class of expansions with respect to the cardinality of the set of extensions and the justification state.expanding argumentation frameworks','Trust frameworks'
'no abstract availableextensible integration frameworks for measurement','Trust frameworks'
'facts, fantasies and frameworks','Trust frameworks'
'most data mining algorithms and tools stop at the mining and delivery of patterns satisfying expected technical interestingness. there are often many patterns mined but business people either are not interested in them or do not know what follow-up actions to take to support their business decisions. this issue has seriously affected the widespread employment of advanced data mining techniques in greatly promoting enterprise operational quality and productivity. in this paper, we present a formal view of actionable knowledge discovery (akd) from the system and decision-making perspectives. akd is a closed optimization problem-solving process from problem definition, framework/model design to actionable pattern discovery, and is designed to deliver operable business rules that can be seamlessly associated or integrated with business processes and systems. to support such processes, we correspondingly propose, formalize, and illustrate four types of generic akd frameworks: postanalysis-based akd, unified-interestingness-based akd, combined-mining-based akd, and multisource combined-mining-based akd (mscm-akd). a real-life case study of mscm-based akd is demonstrated to extract debt prevention patterns from social security data. substantial experiments show that the proposed frameworks are sufficiently general, flexible, and practical to tackle many complex problems and applications by extracting actionable deliverables for instant decision making.flexible frameworks for actionable knowledge discovery','Trust frameworks'
'frameworks are increasingly being recognised as very useful components in the emerging paradigm of component-based software development (cbd). they are widely accepted as better units of reuse than objects. the cbd methodology catalysis, for instance, uses frameworks. however, at present, catalysis frameworks are described only informally, which means we cannot reason formally about frameworks, in particular their composition, and thereby their reuse. for this, we would need to specify frameworks (and their composition) formally. in this paper, we describe our approach for doing so.formal specification of catalysis frameworks','Trust frameworks'
'formal specifications as reusable frameworks','Trust frameworks'
'today\'s complex leading-edge design processes require the use of multiple cad tools that operate in multiple frameworks making management of the complete design process difficult. this paper introduces the concept of framework encapsulations: software wrappers around complete cad frameworks that allow the design data and flow management services of a framework to be utilized by a common design process management tool. this concept has been applied to the minerva ii design process manager, enabling minerva ii to manage the design process across multiple cad frameworks, and potentially multiple design disciplines.framework encapsulations','Trust frameworks'
'the development of a system engineering environment called abe, a better environment, as part of the strategic computing program of the us dept. of defense\'s defense advanced research projects agency (darpa) is described. the goal was to create technologies and methodologies for building cooperative, intelligent systems with modular, heterogeneous components. the motivating problems are discussed. the specific needs of intelligent-system developers that the work addressed and the key technical approaches that were adopted are examined. the abe software system is described. as an example of the use of abe, pmr, a system for plan monitoring and replanning, is presented. the lessons learned in developing abe and the issues remaining to be addressed are discussed.frameworks for developing intelligent systems','Trust frameworks'
'entity matching is a crucial and difficult task for data integration. entity matching frameworks provide several methods and their combination to effectively solve different match tasks. in this paper, we comparatively analyze 11 proposed frameworks for entity matching. our study considers both frameworks which do or do not utilize training data to semi-automatically find an entity matching strategy to solve a given match task. moreover, we consider support for blocking and the combination of different match algorithms. we further study how the different frameworks have been evaluated. the study aims at exploring the current state of the art in research prototypes of entity matching frameworks and their evaluations. the proposed criteria should be helpful to identify promising framework approaches and enable categorizing and comparatively assessing additional entity matching frameworks and their evaluations.frameworks for entity matching','Trust frameworks'
'we have developed a new type of television named ftv (free viewpoint tv). ftv is an innovative media that enables us to view a 3d scene by freely changing our viewpoints. ftv will be widely used since it is an ultimate 3dtv, a natural interface between human and environment, and an innovative tool to create new types of content and art. ftv has been contributing to the standardization of mvc (multi-view video coding) and 3dv (3d video) at mpeg. mvc is the first phase of ftv and 3dv is the second phase of ftv. an ftv system can be constructed in various ways, but typically includes multiple viewpoint video capture, image correction, depth estimation, coding/decoding and view interpolation. coding is related to view generation in ftv. in this paper, 4 frameworks for ftv coding are discussed and an ftv data format as a combination of ftv data units is proposed.frameworks for ftv coding','Trust frameworks'
'in the last decade, there has been a dramatic growth in research and development of massively parallel many-core architectures like graphics hardware, both in academia and industry. this changed also the way programs are written in order to leverage the processing power of a multitude of cores on the same hardware. in the beginning, programmers had to use special graphics programming interfaces to express general purpose computations on graphics hardware. today, several frameworks exist to relieve the programmer from such tasks. in this paper, we present five frameworks for parallelization on gpu accelerators, namely rapidmind, pgi accelerator, hmpp workbench, opencl, and cuda. to evaluate these frameworks, a real world application from medical imaging is investigated, the 2d/3d image registration.frameworks for gpu accelerators','Trust frameworks'
'the development of standard processors changed in the last years moving from bigger, more complex, and faster cores to putting several more simple cores onto one chip. this changed also the way programs are written in order to leverage the processing power of multiple cores of the same processor. in the beginning, programmers had to divide and distribute the work by hand to the available cores and to manage threads in order to use more than one core. today, several frameworks exist to relieve the programmer from such tasks. in this paper, we present five such frameworks for parallelization on shared memory multi-core architectures, namely openmp, cilk++, threading building blocks, rapidmind, and opencl. to evaluate these frameworks, a real world application from medical imaging is investigated, the 2d/3d image registration. in an empirical study, a fine-grained data parallel and a coarse-grained task parallel parallelization approach are used to evaluate and estimate different aspects like usability, performance, and overhead of each framework.frameworks for multi-core architectures','Trust frameworks'
'we present an approach to software framework development that includes the generation of domain-specific languages (dsls) and pattern languages as goals for the process. our model is made of three workflows&#8212;framework, metamodel, and patterns&#8212;and three phases&#8212;inception, construction, and formalization. the main conclusion is that when developing a framework, we can produce with minimal overhead&#8212;almost as a side effect&#8212;a metamodel with an associated dsl and a pattern language. both outputs will not only help the framework evolve in the right direction, but will also be valuable in themselves. in order to illustrate these ideas, we present a case study in the multimedia domain. for several years, we have been developing a multimedia framework. the process has produced a full-fledged domain-specific metamodel for the multimedia domain, with an associated dsl and a pattern language.frameworks generate domain-specific languages','Trust frameworks'
'in oo design, it is widely recognised that the distribution of tasks between objects and the contracts between them are key to effective design. in composing designs from reusable parts, the parts are therefore frameworks, namely descriptions of the interactive relationships between objects which participate in the interactions. designs are then built by composing these frameworks, and any object in the final design will play (various) roles from several frameworks. practitioners of oo design use pictorial notations for design. however, in order to reason formally about design, we need a sound (formal) semantics for the diagrams. in this paper, we show that frameworks can be formalised as many-sorted theories, and then present a pictorial representation of such theories, developed in the catalysis project.frameworks in catalysis','Trust frameworks'
'frameworks session','Trust frameworks'
'\"frameworks\" is an interactive installation trying to enhance the experience of an architectural space. this installation is about generating warmth to medium like walls of an architectural space that would otherwise be considered a cold and insignificant space. through this installation, we witness an opportunity to interact with the walls to add a new meaning to that space. this installation showcases how interactivity can be explored for opening up new possibilities to create more immersive experiences. the artists have tried to achieve this immersive experience by developing a game, where a virtual character plays with walls and their depth, in an actual physical space. a game is considered as one of the most engaging mediums. the artist tries to take a simple game and transform it into a more engaging and immersive experience primarily because of the scale and the seamless integration of the architectural spaces and structures into the game play. the installation will be designed in response to specific spaces or public sites selected by the artist.frameworks','Trust frameworks'
'tangible interaction is a growing area of human&#8211;computer interaction research that has become popular in recent years. yet designers and researchers are still trying to comprehend and clarify its nature, characteristics, and implications. one approach has been to create frameworks that help us look back at and categorize past tangible interaction systems, and look forward at the possibilities and opportunities for developing new systems. to date, a number of different frameworks have been proposed that each provide different perspectives on the tangible interaction design space, and which can guide designers of new systems in different ways. in this paper, we map the space of tangible interaction frameworks. we order existing frameworks by their general type, and by the facets of tangible interaction design they address. one of our main conclusions is that most frameworks focus predominantly on the conceptual design of tangible systems, whereas fewer frameworks abstract the knowledge gained from previous systems, and hardly any framework provides concrete steps or tools for building new tangible systems. in addition, the facets most represented in existing frameworks are those that address the interactions with or the physicality of the designed systems. other facets, such as domain-specific technology and experience, are rare. this focus on design, interaction, and physicality is interesting, as the origins of the field are rooted in engineering methods and have only recently started to incorporate more design-inspired approaches. as such, we expected more frameworks to focus on technologies and to provide concrete building suggestions for new tangible interaction systems.framing tangible interaction frameworks','Trust frameworks'
'this paper introduces the use of fuzzy labels in argumentation. the first approach we propose is built as a natural extension of the in, out, undec labeling to real valued labels, coupled with an unsupervised learning algorithm that assigns consistent labels starting from a random initial assignment. the second approach regards argument (fuzzy) labels as degrees of certitude in the argument\'s acceptability. this translates into a system of equations that provides among its solutions the labelings that describe complete extensions.fuzzy labeling for argumentation frameworks','Trust frameworks'
'to develop real-time vision applications for use in highly dynamic environments, such as transportation, researchers must gather large amounts of data coming from different sensors and systems at different rates. the artificial vision and intelligent system laboratory (vislab) has been developing the gold (general obstacle and lane detection) framework for more than 10 years to meet these requirements. it provides software that can perform data acquisition, synchronization, logging, processing, and visualization in real time. it\'s an efficient software layer that offers an engine for many automotive applications but could serve in many other application domains as well.gold','Trust frameworks'
'quadrotor helicopters are micro air vehicles with vertical take-off and landing capabilities controlled by varying the rotation speed of four fixed pitch propellers. due to their rather simple mechanical design they have grown to popularity as platform for various research projects. despite most of them being individually highly successful, they are typically tailored to a specific purpose making it hard to utilise them for further research and education. in this paper, we present the novel design of the i4copter quadrotor. it has been developed to provide a stable demonstration quadrotor platform for various kinds of research and education projects targeting cross-field challenges in real-time and embedded systems, distributed systems, robotics and cybernetics. the modular and open architecture of our platform allows an application-specific, fine-grained extension, adaption and replacement of software and hardware components. the safe extensibility is supported by strict temporal and spatial isolation between the software modules. we validated our approach by two distinct cross-field use cases: an evaluation platform for modularised control algorithms enabling trajectory tracking and an implementation that is resilient to transient hardware errors.i4copter','Trust frameworks'
'the nature of the relationship between immersion and gameplay experience is investigated, focusing primarily on the literature related to flow. in particular, this paper proposes that immersion and gameplay experience are conceptually different, but empirically positively related through mechanisms related to flow. furthermore, this study examines gamers\' characteristics to determine the influence between immersion and gameplay experiences. the study involves 48 observations in one game setting. regression analyses including tests for moderation and simple slope analysis are used to reveal gamers\' age, experience, and understanding of the game, which moderate the relationship between immersion and gameplay experience. the results suggest that immersion is more positive for gameplay experience when the gamer lacks experience and understanding of the game as well as when the gamer is relatively older. implications and recommendations for future research are discussed at length in the paper.immersion and gameplay experience','Trust frameworks'
'the standardisation process of ieee 802.11s requires some effort to be devoted to assess the effectiveness of the proposed solutions. specifically, the path selection and forwarding facilities, embodied by the hybrid wireless mesh protocol (hwmp), are the major and most delicate components. a working prototype can be profitably used to experimentally evaluate such features, thus returning immediate and significant feedback. in this scenario, the paper offers a threefold contribution. first, we review the current 802.11s implementation efforts, such as the open 80211s and one laptop per child projects, highlighting the pros and cons of each solution. then, we describe a prototype ieee 802.11s mesh access point (map) we developed starting from common off-the-shelf hardware and software. the main innovation of this prototype is the support of both modes of operation of hwmp. conversely to the other projects, which implement only the reactive mode, our device also supports the proactive mode, and runs it in the same way as suggested by the draft standard, thus becoming a more complete evaluation platform. as a final contribution, we provide some simulation results to assess the behaviour of the mixed proactive-reactive mode. from this evaluation, we can confirm that employing the proactive algorithm can bring some benefits to the operation of the 802.11s network, even though a careful approach must be taken to avoid some drawbacks.implementation frameworks for ieee 802.11s systems','Trust frameworks'
'interacting frameworks in catalysis','Trust frameworks'
'this article is a collection of papers collected from the nonprofits that run three critical frameworks in use by it organizations around the globe&amp;#x2013;the project management institute, capability maturity model integration, and the innovation value institute. these papers outline the current state and future direction of these organizations and their frameworks.it frameworks','Trust frameworks'
'java 2 enterprise edition has excelled at standardizing many important middleware concepts. for example, j2ee provides a standard interface for distributed transaction management, directory services, and messaging. in addition, java 2 standard edition (j2se), which underpins j2ee, provides a largely successful standard for java interaction with relational databases. however, the platform has failed to deliver a satisfactory application programming model. many in the open source community, especially smaller vendors, have chosen the alternative of developing frameworks designed to simplify the experience of building j2ee applications. popular frameworks such as struts, hibernate, and the spring framework play an important role in many of today\'s j2ee development projects.j2ee development frameworks','Trust frameworks'
'while theories abound concerning knowledge transfer in organisations, little empirical work has been undertaken to assess any possible relationship between repositories of knowledge and those responsible for the use of knowledge. this paper develops a knowledge transfer framework based on an empirical analysis of part of the uk operation of a fortune 100 corporation, which extends existing knowledge transfer theory. the proposed framework integrates knowledge storage and knowledge administration within a model of effective knowledge transfer. this integrated framework encompasses five components: the actors engaged in the transfer of knowledge, the typology of organisational knowledge that is transferred between the actors, the mechanisms by which the knowledge transfer is carried out, the repositories where explicit knowledge is retained and the knowledge administrator equivalent whose function is to manage and maintain knowledge. the paper concludes that a &#x2018;hybridisation&#x2019; of knowledge transfer approach, revealed by the framework, offers some promise in organisational applications. &#169; 2012 wiley periodicals, inc.knowledge transfer frameworks','Trust frameworks'
'logic frameworks for logic programs','Trust frameworks'
'making frameworks count','Trust frameworks'
'various issues make framework development harder than regular development. building product lines and frameworks requires increased coordination and communication between stakeholders and across the organization. the difficulty of building the right abstractions ranges from understanding the domain models, selecting and evaluating the framework architecture, to designing the right interfaces, and adds to the complexity of a framework project.making frameworks work','Trust frameworks'
'measuring frameworks','Trust frameworks'
'many logical systems today describe intelligent interacting agents over time. frameworks include interpreted systems (is, fagin et al. [5]), epistemic-temporal logic (etl, parikh & ramanujam [13]), stit (belnap et al. [4]), process algebra and game semantics (abramsky [1]). this variety is an asset, as different modeling tools can be fine-tuned to specific applications. but it may also be an obstacle, when barriers between paradigms and schools go up.merging frameworks for interaction','Trust frameworks'
'proving theorems is a creative act demanding new combinations of ideas and on occasion new methods of argument. for this reason, theorem proving systems need to be extensible. the provers should also remain correct under extension, so there must be a secure mechanism for doing this. the tactic-style provers pioneered by edinburgh lcf provide a very effective way to achieve secure extensions, but in such systems, all new methods must be reduced to tactics. this is a drawback because there are other useful proof generating tools such as decision procedures; these include, for example, algorithms which reduce a deduction problem, such as arithmetic provability, to a computation on graphs.the nuprl system pioneered the combination of fixed decision procedures with tactics, but the issue of securely adding new ones was not solved. in this paper we show how to safely include user-defined decision procedures in theorem provers. the idea is to prove properties of the procedure inside the prover&#8217;s logic and then invoke a reflection rule to connect the procedure to the system. we also show that using a rich underlying logic permits an abstract account of the approach so that the results carry over to different implementations and other logics.metalogical frameworks ii','Trust frameworks'
'minimalist documentation of frameworks','Trust frameworks'
'more than five years ago, the omg proposed the model driven architecture (mda&#8482;) approach to deal with the separation of platform dependent and independent aspects in information systems. since then, the initial idea of mda evolved and model driven engineering (mde) is being increasingly promoted to handle separation and combination of various kinds of concerns in software or data engineering. mde is more general than the set of standards and practices recommended by the omg\'s mda proposal. in mde the concept of model designates not only omg models but a lot of other artifacts like xml documents, java programs, rdbms data, etc. today we observe another evolutionary step. a convergence between mde and dsl (domain specific language) engineering is rapidly appearing. in the same way as mde is a generalization of mda, the dsl engineering may be viewed as a generalization of mde. one of the goals of this paper is to explore the potential of this important evolution of engineering practices. in order to anchor the discussion on practical grounds, we present a set of typical problems that could be solved by classical (object-oriented and others), mde, or dsl-based techniques. solutions to these problems will be based on current platforms (emf, amma, gme, etc.). this paper illustrates how powerful model-based frameworks, allowing to use and build a variety of dsls, may help to solve complex problems in a more efficient way.model-based dsl frameworks','Trust frameworks'
'abstract argument frameworks have been used for various applications within multi-agent systems, including reasoning and negotiation. different argument frameworks make use of different inter-argument relations and semantics to identify some subset of arguments as coherent, yet there is no easy way to map between these frameworks; most commonly, this is done manually according to human intuition. in response, in this paper, we show how a set of arguments described using dung\'s or nielsen\'s argument frameworks can be mapped from and to an argument framework that includes both attack and support relations. this mapping preserves the framework\'s semantics in the sense that an argument deemed coherent in one framework is coherent in the other under a related semantics. interestingly, this translation is not unique, with one set of arguments in the support based framework mapping to multiple argument sets within the attack only framework. additionally, we show how eaf can be mapped into a subset of the argument interchange format (aif). by using this mapping, any other argument framework using this subset of aif can be translated into a daf while preserving its semantics.moving between argumentation frameworks','Trust frameworks'
'network forensics is the science that deals with capture, recording, and analysis of network traffic for detecting intrusions and investigating them. this paper makes an exhaustive survey of various network forensic frameworks proposed till date. a generic process model for network forensics is proposed which is built on various existing models of digital forensics. definition, categorization and motivation for network forensics are clearly stated. the functionality of various network forensic analysis tools (nfats) and network security monitoring tools, available for forensics examiners is discussed. the specific research gaps existing in implementation frameworks, process models and analysis tools are identified and major challenges are highlighted. the significance of this work is that it presents an overview on network forensics covering tools, process models and framework implementations, which will be very much useful for security practitioners and researchers in exploring this upcoming and young discipline.network forensic frameworks','Trust frameworks'
'object-oriented frameworks','Trust frameworks'
'management performance evaluation means assessment of scalability, complexity, accuracy, throughput, delays and resources consumptions. in this paper, we focus on the evaluation of management frameworks delays through a set of specific metrics. we investigate the statistical properties of these metrics when the number of management nodes increases. we show that management delays measured at the application level are statistically modeled by distributions with heavy tails, especially the weibull distribution. given that delays can substantially degrade the capacity of management algorithms to react and resolve problems it is useful to get a finer model to describe them. we suggest the weibull distribution as a model of delays for the analysis and simulations of such algorithms.on delays in management frameworks','Trust frameworks'
'on the generic rigidity of bar-frameworks','Trust frameworks'
'one-story buildings as tensegrity frameworks iii','Trust frameworks'
'we have developed a semantic data framework that supports interdisciplinary virtual observatory projects across the fields of solar physics, space physics and solar-terrestrial physics. this work required a formal, machine understandable representation for concepts, relations and attributes of physical quantities in the domains of interest as well as their underlying data representations. to fulfill this need, we developed a set of solar-terrestrial ontologies as formal encodings of the knowledge in the ontology web language-description logic (owl-dl) format. we present our knowledge representation and reasoning needs motivated by the context of virtual observatories, from fields spanning upper atmospheric terrestrial physics to solar physics, whose intent is to provide access to observational datasets. the resulting data framework is built upon semantic web methodologies and technologies and provides virtual access to distributed and heterogeneous sets of data as if all resources appear to be organized, stored and retrieved from a local environment. our conclusion is that the combination of use case-driven, small and modular ontology development, coupled with free and open-source software tools and languages provides sufficient expressiveness and capabilities for an initial production implementation and sets the stage for a more complete semantic-enablement of future frameworks.ontology-supported scientific data frameworks','Trust frameworks'
'virtualization technologies radically changed the way in which distributed architectures are exploited. with the contribution of vm capabilities and with the emergence of iaas platforms, more and more frameworks tend to manage vms across distributed architectures like operating systems handle processes on a single node. taking into account that most of these frameworks follow a centralized model -- where roughly one node is in charge of the management of vms -- and considering the growing size of infrastructures in terms of nodes and vms, new proposals relying on more autonomic and decentralized approaches should be submitted. designing and implementing such models is a tedious and complex task. however, as well as research studies on oses and hyper visors are complementary at the node level, we advocate that virtualization frameworks can benefit from lessons learnt from distributed operating system proposals. in this article, we motivate such a position by analyzing similarities between oses and virtualization frameworks. more precisely, we focus on the management of processes and vms, first at the node level and then on a cluster scale. from our point of view, such investigations can guide the community to design and implement new proposals in a more autonomic and distributed way.operating systems and virtualization frameworks','Trust frameworks'
'good design and implementation are necessary but not sufficient pre-requisites for the successful reuse of object-oriented frameworks. although not always recognized, good documentation is crucial for effective framework reuse but comes with many issues. writing good quality documentation for a framework is often hard, costly, and tiresome, especially when not aware of its key problems and the best ways to address them. this document presents two of a set of related patterns that describe proven solutions to help non-experts on solving recurrent problems of documenting object-oriented frameworks. the patterns here presented address the problems of describing the customization points of the framework and how such customization is supported, respectively the patterns \"customization points\" and \"design internals\".patterns for documenting frameworks','Trust frameworks'
'learning and understanding a framework is usually a major obstacle to its effective reuse. before being able to use a framework successfully, users often go through a steep learning curve by spending a lot of effort understanding its underlying architecture and design principles. this is mainly due to users having to understand not only single isolated classes, but also complex designs of several classes whose instances collaborate for many different purposes, and using many different mechanisms. in addition, frameworks are also full of delocalized plans, and use inheritance and delegation intensively, which makes their design more difficult to grasp. how to obtain the necessary information from the framework itself and its accompanying documentation is the main problem with framework understanding. considering its importance, this paper presents an initial attempt to capture, in the pattern form, a set of proven solutions to recurrent problems of understanding frameworks. the fundamental objective of this work is to help non-experts on being more effective when trying to learn and understand object-oriented frameworks.patterns for understanding frameworks','Trust frameworks'
'flexibility studies of macromolecules modeled as mechanical frameworks rely on computationally expensive, yet numerically imprecise simulations. much faster approaches for degree-of-freedom counting and rigid component calculations are known for finite structures characterized by theorems of maxwell-laman type, but such results are exceedingly rare and difficult to obtain. the situation is even more complex for infinite, periodic structures such as those appearing in the study of crystalline materials. here, an adequate rigidity theoretical formulation has been proposed only recently, opening the way to a combinatorial treatment. abstractions of crystalline materials known as periodic body-and-bar frameworks are made of rigid bodies connected by fixed-length bars and subject to the action of a group of translations. in this paper, we give a maxwell-laman characterization for generic minimally rigid periodic body-and-bar frameworks in terms of their quotient graphs. as a consequence we obtain efficient polynomial time algorithms for their recognition based on matroid partition and pebble games.periodic body-and-bar frameworks','Trust frameworks'
'the preferred semantics for argumentation frameworks seems to capture well the intuition behind the stable semantics while avoiding several of its drawbacks. although the stable semantics has been thoroughly studied, and several algorithms have been proposed for solving problems related to it, it seems that the algorithmic side of the preferred semantics has received less attention. in this paper, we propose algorithms, based on the enumeration of some subsets of a given set of arguments, for the following tasks: 1) deciding if a given argument is in a preferred extension of a given argumentation framework; 2) deciding if the argument is in all the preferred extensions of the framework; 3) generating the preferred extensions of the framework.preferred extensions of argumentation frameworks','Trust frameworks'
'in this paper, we extend dung\'s seminal argument framework to form a probabilistic argument framework by associating probabilities with arguments and defeats. we then compute the likelihood of some set of arguments appearing within an arbitrary argument framework induced from this probabilistic framework. we show that the complexity of computing this likelihood precisely is exponential in the number of arguments and defeats, and thus describe an approximate approach to computing these likelihoods based on monte-carlo simulation. evaluating the latter approach against the exact approach shows significant computational savings. our probabilistic argument framework is applicable to a number of real world problems; we show its utility by applying it to the problem of coalition formation.probabilistic argumentation frameworks','Trust frameworks'
'a process is a time-dependent sequence of events governed by a process framework. a group process has five components: the entities performing the process, the steps or elements of a process, the relationship between any pair of elements, the links to other processes, and the resources and their characteristics-in-use involved with the elements. a process framework is denoted byy =f( c) wherey is the set of outcomes or consequences of a process,c is the set of considerations or elements in the process, andf is the network linking the considerations to each other and to the outcomes. the properties of the set of considerations, the linkages between pairs of consequences, the set of outcomes or consequences, the network,f, and the use of process frameworks are discussed in detail with examples. process models are compared to variable models.processes and their frameworks','Trust frameworks'
'the manufacturing trend toward mass customization has awakened great interest in automatic product configuration techniques. the authors identify specific aspects of configuration as a reasoning task and describe the most representative frameworks that have been developed for representing and solving configuration problems.product configuration frameworks-a survey','Trust frameworks'
'in this paper, we introduce public announcement logic in different geometric frameworks. first, we consider topological models, and then extend our discussion to a more expressive model, namely, subset space models. furthermore, we prove the completeness of public announcement logic in those frameworks. after that, we apply our results to different issues: announcement stabilization, backward induction and persistence.public announcement logic in geometric frameworks','Trust frameworks'
'extract-transform-load (etl) processes are used for extracting data, transforming it and loading it into data warehouses (dws). many tools for creating etl processes exist. the dominating tools all use graphical user interfaces (guis) where the developer visually defines the data flow and operations. in this paper, we challenge this approach and propose to do etl programming by writing code. to make the programming easy, we present the (python-based) framework pygrametl which offers commonly used functionality for etl development. by using the framework, the developer can efficiently create effective etl solutions from which the full power of programming can be exploited. our experiments show that when pygrametl is used, both the development time and running time are short when compared to an existing gui-based tool.pygrametl','Trust frameworks'
'real-world groupware frameworks (tutorial)','Trust frameworks'
'the abstract nature of dung\'s seminal theory of argumentation accounts for its widespread application as a general framework for various species of non-monotonic reasoning, and, more generally, reasoning in the presence of conflict. a dung argumentation framework is instantiated by arguments and a binary conflict based attack relation, defined by some underlying logical theory. the justified arguments under different extensional semantics are then evaluated, and the claims of these arguments define the inferences of the underlying theory. to determine a unique set of justified arguments often requires a preference relation on arguments to determine the success of attacks between arguments. however, preference information is often itself defeasible, conflicting and so subject to argumentation. hence, in this paper we extend dung\'s theory to accommodate arguments that claim preferences between other arguments, thus incorporating meta-level argumentation based reasoning about preferences in the object level. we then define and study application of the full range of dung\'s extensional semantics to the extended framework, and study special classes of the extended framework. the extended theory preserves the abstract nature of dung\'s approach, thus aiming at a general framework for non-monotonic formalisms that accommodate defeasible reasoning about as well as with preference information. we illustrate by formalising argument based logic programming with defeasible priorities in the extended theory.reasoning about preferences in argumentation frameworks','Trust frameworks'
'argumentation is a reasoning model based on constructing arguments, determining potential conflicts between arguments and determining acceptable arguments. dung\'s argumentation theory is an abstract framework based on a binary defeat relation between arguments. due to this abstract representation, it has been instantiated in different ways. in particular, preference-based argumentation frameworks take into account a preference relation over arguments together with a (non necessarily symmetric) attack relation. we show that preference-based argumentation frameworks faithfully instantiate dung\'s framework only when the attack relation is symmetric. moreover the latter condition prevents undesirable results. we also promote a higher impact of preferences in preference-based argumentation frameworks and propose different ways to rank-order sets of acceptable arguments.refined preference-based argumentation frameworks','Trust frameworks'
'relationships between logical frameworks','Trust frameworks'
'argumentation is a reasoning model based on the construction and evaluation of arguments. dung has proposed an abstract argumentation framework in which arguments are assumed to have the same strength. this assumption is unfortunately not realistic. consequently, three main extensions of the framework have been proposed in the literature. the basic idea is that if an argument is stronger than its attacker, the attack fails. the aim of the paper is twofold: first, it shows that the three extensions of dung framework may lead to unintended results. second, it proposes a new approach that takes into account the strengths of arguments, and that ensures sound results. we start by presenting two minimal requirements that any preference-based argumentation framework should satisfy, namely the conflict-freeness of arguments extensions and the generalization of dung\'s framework. inspired from works on handling inconsistency in knowledge bases, the proposed approach defines a binary relation on the powerset of arguments. the maximal elements of this relation represent the extensions of the new framework.repairing preference-based argumentation frameworks','Trust frameworks'
'scala developers have many options for web development frameworks tailored to their language, in additional to other java virtual machine-based web frameworks written in other languages. scala\'s best-known web framework is lift, which was explored in two previous columns. this column surveys the other scala alternatives and dives into three representative examples &#x2014; play, scalatra, and finagle.scala web frameworks','Trust frameworks'
'in 1996 zhou and hansen proposed a first-order interval logic called neighbourhood logic (nl) for specifying liveness and fairness of computing systems and defining notions of real analysis in terms of expanding modalities. after that, roy and zhou developed a sound and relatively complete duration calculus as an extension of nl. we present an embedding of nl into an idempotent semiring of intervals. this embedding allows us to extend nl from single intervals to sets of intervals as well as to extend the approach to arbitrary idempotent semirings. we show that most of the required properties follow directly from galois connections, hence we get many properties for free. as one important result we obtain that some of the axioms which were postulated for nl can be dropped since they are theorems in our generalisation. furthermore, we discuss other interval operations like allen\'s 13 relations between intervals and their relationship to semiring neighbours. then we present some possible interpretations for neighbours beyond interval settings. here we discuss for example reachability in graphs and applications to hybrid systems. at the end of the paper we add finite and infinite iteration to nl and extend idempotent semirings to kleene algebras and @w algebras. these extensions are useful for formulating properties of repetitive procedures like loops.semiring neighbours','Trust frameworks'
'dsal composition frameworks are tools used in the process of composing multiple dsal mechanisms into a single multi-dsal weaver. the dsal composition process starts with specifying the desired interactions between the dsal mechanisms being composed, and concludes with producing a multi-dsal weaver which satisfies the composition specification. however, the lack of tool support for defining the composition specification, and the coding effort required in composition frameworks to implement the specification, make this process complex and error prone. this work presents a specification-based approach to dsal composition. the approach is based on having a specifiction manifest file for the composition and for each of the individual mechanisms involved. a novel tool, named spectackle, analyzes the manifests and helps the composition designer define the desired specification. based on the composition specification produced, the composition framework can generate a significant part of the implementation code for the mechanisms and for the multi-dsal weaver. the specification-based dsal composition process is illustrated in the context of the awesome composition framework.spectackle','Trust frameworks'
'in a recent paper baumann [1] has shown that splitting results, similar to those known for logic programs under answer set semantics and default logic, can also be obtained for dung argumentation frameworks (afs). under certain conditions a given af a can be split into subparts a1 and a2 such that extensions of a can be computed by (1) computing an extension e1 of a1, (2) modifying a2 based on e1, and (3) combining e1 and an extension e2 of the modified variant of a2. in this paper we perform a systematic empirical evaluation of the effects of splitting on the computation of extensions. our study shows that the performance of algorithms may drastically improve when splitting is applied.splitting argumentation frameworks','Trust frameworks'
'prior research has characterized the adoption of technical and process standards as a multi-stage effort ranging from the initial implementation through institutionalization. however, the relationships between these adoption stages have not been examined significantly. in this study i analyze performance data from a large service provider that has implemented a process standardization framework for services off shoring. i evaluate the extent to which process standardization influences service delivery performance, and how the effect of standardization differs based on the implementation duration of each new process and the complexity of the task for which the process is implemented. the results indicate that longer implementation durations are associated with greater performance improvement. performance on complex tasks also increases to a greater extent after standardization than performance on simple tasks. contrary to expectations, performance on complex tasks improves to a greater extent following shorter implementation durations than after longer durations.standardization frameworks in services offshoring','Trust frameworks'
'frameworks [4] are usually large and complex, and typically reusers need to understand them well enough to effectively use them. this research concentrates on verifying applications built on top of oo frameworks. the idea is to get framework builders to specify a set of constraints for the correct usage of the framework and check them using static analysis techniques.supporting the deployment of object oriented frameworks','Trust frameworks'
'this paper is centered on the family of dung&#39;s finite argumentation frameworks when the attacks relation is symmetric (and nonempty and irreflexive). we show that while this family does not contain any well-founded framework, every element of it is both coherent and relatively grounded. then we focus on the acceptability problems for the various semantics introduced by dung, yet generalized to sets of arguments. we show that only two distinct forms of acceptability are possible when the considered frameworks are symmetric. those forms of acceptability are quite simple, but tractable; this contrasts with the general case for which all the forms of acceptability are intractable (except for the ones based on grounded or naive extensions).symmetric argumentation frameworks','Trust frameworks'
'the edge set of a graph g is partitioned into two subsets e\"c@?e\"s. a tensegrity framework with underlying graph g and with cables for e\"c and struts for e\"s is proved to be rigidly embeddable into a one-dimensional line if and only if g is 2-edge-connected and every 2-vertex-connected component of g intersects both e\"c and e\"s. polynomial algorithms are given for finding an embedding of such graphs and for checking the rigidity of a given one-dimensional embedding.tensegrity frameworks in one-dimensional space','Trust frameworks'
'the practice of logical frameworks','Trust frameworks'
'three features for component frameworks','Trust frameworks'
'three frameworks are recommended to analyze multilevel governance across a spectrum of complex human systems, including nations, states, cities, universities, hospitals, hotels, and homes, with which service researchers are concerned. this spectrum of complex human systems can be seen as instances of nested, networked holistic service systems that provision whole service to the people inside them and depend heavily on shared systems of rules to change over time. increasingly, service researchers benefit from improved analysis/design frameworks for complex human systems that (1) improve multilevel governance, making it more likely that local optimizations contribute to global resilience and sustainability, and (2) can integrate across diverse disciplines, systems, and cultures. three frameworks are considered: service science, management, engineering, and design (ssme+d); viable systems approach (vsa); and institutional analysis and development (iad). each framework has a focal building block (rule-rich entity architectures)---namely, a service system (ssme+d), a viable system (vsa), and a polycentric system (iad). our goals are to (1) provide a conceptual foundation and recommended methodology to illustrate the type of future empirical work that might someday provide more efficient and effective ways to compare, contrast, and search for improved service research frameworks and entity architectures, and (2) encourage service researchers to move beyond dyads, be they provider-to-customer, business-to-business, or even government-to-citizen, and toward a view of multilevel rule-rich nested, networked systems in the wild.three frameworks for service research','Trust frameworks'
'the prevalence of chip multiprocessors opens opportunities of running data-parallel applications originally in clusters on a single machine with many cores. mapreduce, a simple and elegant programming model to program large-scale clusters, has recently been shown a promising alternative to harness the multicore platform. the differences such as memory hierarchy and communication patterns between clusters and multicore platforms raise new challenges to design and implement an efficient mapreduce system on multicore. this article argues that it is more efficient for mapreduce to iteratively process small chunks of data in turn than processing a large chunk of data at a time on shared memory multicore platforms. based on the argument, we extend the general mapreduce programming model with a &#8220;tiling strategy&#8221;, called tiled-mapreduce (tmr). tmr partitions a large mapreduce job into a number of small subjobs and iteratively processes one subjob at a time with efficient use of resources; tmr finally merges the results of all subjobs for output. based on tiled-mapreduce, we design and implement several optimizing techniques targeting multicore, including the reuse of the input buffer among subjobs, a nuca/numa-aware scheduler, and pipelining a subjob&#8217;s reduce phase with the successive subjob&#8217;s map phase, to optimize the memory, cache, and cpu resources accordingly. further, we demonstrate that tiled-mapreduce supports fine-grained fault tolerance and enables several usage scenarios such as online and incremental computing on multicore machines. performance evaluation with our prototype system called ostrich on a 48-core machine shows that ostrich saves up to 87.6&percnt; memory, causes less cache misses, and makes more efficient use of cpu cores, resulting in a speedup ranging from 1.86x to 3.07x over phoenix. ostrich also efficiently supports fine-grained fault tolerance, online, and incremental computing with small performance penalty.tiled-mapreduce','Trust frameworks'
'in this paper, we propose a decentralized self-adaptive management framework that infers the current context of the hosting environment and dynamically decides what management strategy should be used in order to minimize the cost and maximize the performance of management. we investigate the use of this framework in the case of the aggregated information monitoring, where only one scheme (i.e. situated or global) is generally used for collecting aggregated information whatever the nature of the management information and its environment are. we propose a self-adaptive aggregation mechanism based on fuzzy logic, that estimates the cost and the performance of given aggregation schemes.towards self-adaptive management frameworks','Trust frameworks'
'object-oriented frameworks are often hard to learn and use (j. bosch et al., in: acm computing survey\'s symposia on object oriented application frameworks, 1998&semi; m. fayad and d.c. schmidt, communication of the acm, special issue on object-oriented application frameworks 1997&semi;40(10)). as a result, software cost rises and quality suffers. thus the capability to automatically detect errors occurring at the boundary between frameworks and applications is considered crucial to mitigate the problem. this paper introduces the notion of framework, constraints and a specification language, fcl (framework constraints language), to formally specify them. framework constraints are rules that frameworks impose on the code of framework-based applications. the semantics of fcl is primarily based on first order predicate logic and set theory though the evolving syntax is designed to resemble that of programming languages as much as possible. we take examples from the mfc (microsoft foundation classes) framework (g. shepherd and s. wingo, mfc internals: inside the microsoft foundation classes architecture. reading, ma: addison wesley, 1996) demonstrating both the nature of framework constraints and the semantics of fcl. essentially, framework constraints can be regarded as framework-specific typing rules conveyed by the specification language fcl, and thus can be enforced by techniques analogous to those of conventional type checking.towards specifying constraints for object-oriented frameworks','Trust frameworks'
'for a reciprocal relation q on a set of alternatives a, two transitivity frameworks which generalize both t-transitivity and stochastic transitivity are compared: the framework of cycle-transitivity, introduced by the present authors (soc. choice welf., to appear) and which is based upon the ordering of the numbers q(a,b), q(b,c) and q(c,a) for all (a,b,c)@?a^3, and the framework of fg-transitivity, introduced by switalski (fuzzy sets and systems 137 (2003) 85) as an immediate generalization of stochastic transitivity. the rules that enable to express fg-transitivity in the form of cycle-transitivity and cycle-transitivity in the form of fg-transitivity, illustrate that for reciprocal relations the concept of cycle-transitivity provides a framework that can cover more types of transitivity than does the concept of fg-transitivity.transitivity frameworks for reciprocal relations','Trust frameworks'
'translations for comparing soft frameworks','Trust frameworks'
'frameworks are large building blocks of systems, encapsulating the commonalities of a family of applications. for reuse of these common features, frameworks are instantiated by smaller-sized components, plugins, to specific products. however, the framework instantiation process is often difficult, because not all aspects of the interplay of the framework and its plugins can be captured by standard type systems. application developers instantiating a framework often fail to develop correct applications. thus, this paper surveys several typical framework instantiation problems. a simple facet-based classification of the problems is given. it is shown how the different problem classes are related to phases of the software process and how they can be tackled appropriately. finally, the paper derives several research challenges, in particular, the challenge to define appropriate framework instantiation languages.trustworthy instantiation of frameworks','Trust frameworks'
'smartphones are conquering the mobile phone market; they are not just phones; they also act as media players, gaming consoles, personal calendars, storage, etc. they are portable computers with fewer computing capabilities than personal computers. however, unlike personal computers, users can carry their smartphone with them at all times. the ubiquity of mobile phones and their computing capabilities provide an opportunity of using them as a life-logging device. life-logs (personal e-memories) are used to record users\' daily life events and assist them in memory augmentation. in a more technical sense, life-logs sense and store users\' contextual information from their environment through sensors, which are core components of life-logs. spatio-temporal aggregation of sensor information can be mapped to users\' life events. we propose ubiqlog, a lightweight, configurable, and extendable life-log framework, which uses mobile phone as a device for life logging. the proposed framework extends previous research in this field, which investigated mobile phones as life-log tool through continuous sensing. its openness in terms of sensor configuration allows developers to create flexible, multipurpose life-log tools. in addition to that, this framework contains a data model and an architecture, which can be used as reference model for further life-log development, including its extension to other devices, such as ebook readers, t.v.s, etc.ubiqlog','Trust frameworks'
'in this paper we explore frameworks for performing unit testing in java. the vehicle for this exploration is a student-written, skeleton program developed for the computer graphics course. our analysis of this one experiment leads us to speculate what benefits in program development and design might accrue by requiring students to unit test their own programs.unit testing frameworks','Trust frameworks'
'in this paper we introduce web design frameworks as a conceptual approach to maximize reuse in web applications. we first discuss the need for building abstract and reusable navigational design structures, exemplifying with different kinds of web information systems. then, we briefly review the state of the art of object-oriented application frameworks and present the rationale for a slightly different approach focusing on design reuse instead of code reuse. next, we present oohdm-frame, a syntax for defining the hot-spots of generic web application designs. we illustrate the use of oohdm-frame with a case study in the field of electronic commerce. we finally discuss how to implement web design frameworks in different kind of web platforms.web design frameworks','Trust frameworks'
'between 1962 and 1986, the information processing techniques office (ipto) of the defense advanced research projects agency (darpa) provided significant support for computer science r&d. the design and implementation of the support programs of this office was the responsibility of a small group of computer scientists who emerged from the growing computer science community. program directors focused on radical technologies, organized programs to develop them, and promoted their use in various settings, with substantial success. a better understanding of the evolution of the department of defense\'s policy for computing r&d can be gained from an analysis of the backgrounds, research experience, interests and methods of the people engaged to design and implement this policy in iptochanging computing','Trusted computing'
'in this article, the author describes the history of the development, modern state, and future considerations of cloud (diffused) computing as one of the modern innovative technologies. the models of cloud computing and its advantages and disadvantages are analyzed. a number of cloud operating systems, cloud computing vendors, and the capabilities of their platforms are considered.cloud computing','Trusted computing'
'cluster computing','Trusted computing'
'how likely is it that neuroscientists could map your brain and place your consciousness into a virtual world?computing hell','Trusted computing'
'computing perspectives','Trusted computing'
'the convex hull is one of computational geometry\'s fundamental structures, offering a simple way to approximate a point set\'s shape. quickhull is a simple algorithm for computing convex hulls that takes a divide-and-conquer approach and proves efficient in practice.computing prescriptions','Trusted computing'
'computing prespectives','Trusted computing'
'a procedure is given for finding the independent sets in an undirected graph by xeroxing onto transparent plastic sheets. let an undirected graph having n vertices and m edges be given. a list of all the independent subsets of the set of vertices of the graph is constructed by using a xerox machine in a manner that requires the formation of only n + m + 1 successive transparencies. an accompanying list of the counts of the elements in each independent set is then constructed using only o(n             2) additional transparencies. the list with counts provides a list of all maximum independent sets. this gives an o(n             2) step solution for the classical problem of finding the cardinality of a maximal independent set in a graph. the applicability of these procedures is limited, of course, by the increase in the information density on the transparencies when n is large. our ultimate purpose here is to give hand tested `ultra parallel\' algorithmic procedures that may prove suitable for realization using future optical technologies. computing transparently','Trusted computing'
'in recent months, there has been considerable discussion about the value of computing as an academic discipline and the value of computing studies in furthering one\'s career. kevin carey (2010) recently wrote an article showing how a student who took only a few computing courses and then changed majors was able to exploit the knowledge and discipline he learned to solve some serious problems in other domains. carey\'s paper focused on the knowledge gained from learning how to program, but what about the other knowledge that is fundamental to our field? should people outside of computing learn about computing fundamentals because they are as important as, say, basic science or math? this prompted me to examine the role of computing in my own career, and to make some observations on computing jobs in industry and how we might better appreciate and promulgate what we know.computing','Trusted computing'
'evolutionary computing','Trusted computing'
'library computing','Trusted computing'
'personal computing','Trusted computing'
'as nasa spacecraft explore deeper into the cosmos, speed-of-light-limited signal delays make it increasingly impractical to command missions from earth. future spacecraft will need greater onboard computing capacity to mimic human-level intelligence and autonomy. unfortunately, computer manufacturers will have difficulty providing the vastly increased computing power the space-exploration community will need.the solution might well come from quantum computers, which offer properties of size, power, and robustness that are ideally suited to the space environment.the potential of quantum technologies goes far beyond enhanced computing capacity. future space missions will involve direct participation of non-nasa scientists. this will necessitate allowing more open access to spacecraft systems via free-space communication links. quantum cryptography would allow such channels to be made absolutely secure and invulnerable to attack by malevolent hackers. to explore these possibilities, this article describes the progress to date in understanding how quantum computers and related quantum information-processing devices might advance space exploration.quantum computing','Trusted computing'
'search computing focuses on building answers to complex search queries (for example, \"where can i attend an interesting conference in my field near a sunny beach?\") by interacting with a constellation of cooperating search services, and using result ranking and joining as the dominant factors for service composition. the service computing paradigm has so far been neutral to the specific features of search applications and services. to address this weakness, search computing advocates a new approach in which search, join, and ranking are the central aspects for service composition.search computing','Trusted computing'
'today, users interact with computers in an explicit manner and the system\'s response is independent from their situations. hence, it is difficult to integrate computers with working life as embedded tools, which can facilitate users to accomplish real world objectives easily. situated computing is a new paradigm for mobile computer users based on their physical context and activities carried out as a part of their working business. it provides the mechanism to have a mobile computer as a utility to satisfy the user\'s real world requirements as well as an infrastructure for the situated interaction using applications. in this paper, we are presenting a metaphor called situation metaphor to model interaction between the user and mobile computers in order to achieve expectations of situated computing. a three-layered schema is followed in developing situation metaphor. we discuss extensively the theoretical foundation and framework for the situation metaphor, and followed by applications developed based on the framework.situated computing','Trusted computing'
'infrastructure-as-a-service (iaas) cloud computing is revolutionizing how we approach computing. compute resource consumers can eliminate the expense inherent in acquiring, managing, and operating it infrastructure and instead lease resources on a pay-as-you-go basis. it infrastructure providers can exploit economies of scale to mitigate the cost of buying and operating resources and avoid the complexity required to manage multiple customer-specific environments and applications. the authors describe the context in which cloud computing arose, discuss its current strengths and shortcomings, and point to an emerging computing pattern it enables that they call sky computing.sky computing','Trusted computing'
'how has social computing diversified the ai community? what influence will ai have over social computing\'s development? and how will social intelligence emerge?social computing','Trusted computing'
'as device sizes shrink, manufacturing challenges at the device level are resulting in increased variability in physical circuit characteristics. exponentially increasing circuit density has not only brought about concerns in the reliable manufacturing of circuits but also has exaggerated variations in dynamic circuit behavior. the resulting uncertainty in performance, power, and reliability imposed by compounding static and dynamic nondeterminism threatens the continuation of moore\'s law, which has been arguably the primary driving force behind technology and innovation for decades. this situation is exacerbated by emerging computing applications, which exert considerable power and performance pressure on processors. paradoxically, the problem is not nondeterminism, per se, but rather the approaches that designers have used to deal with it. the traditional response to variability has been to enforce determinism on an increasingly nondeterministic substrate through guardbands. as variability in circuit behavior increases, achieving deterministic behavior becomes increasingly expensive, as performance and energy penalties must be paid to ensure that all devices work correctly under all possible conditions. as such, the benefits of technology scaling are vanishing, due to the overheads of dealing with hardware variations through traditional means. clearly, status quo cannot continue. despite the above trends, the contract between hardware and software has, for the most part, remained unchanged. software expects flawless results from hardware under all possible operating conditions. this rigid contract leaves potential performance gains and energy savings on the table, sacrificing efficiency in the common case in exchange for guaranteed correctness in all cases. however, as the marginal benefits of technology scaling continue to languish, a new vision for computing has begun to emerge. rather than hiding variations under expensive guardbands, designers have begun to relax traditional correctness constraints and deliberately expose hardware variability to higher levels of the compute stack, thus tapping into potentially significant performance and energy benefits and also opening the potential for errors. rather than paying the increasing price of hiding the true, stochastic nature of hardware, emerging stochastic computing techniques account for the inevitable variability and exploit it to increase efficiency. stochastic computing techniques have been proposed at nearly all levels of the computing stack, including stochastic design optimizations, architecture frameworks, compiler optimizations, application transformations, programming language support, and testing techniques. in this monograph, we review work in the area of stochastic computing and discuss the promise and challenges of the field.stochastic computing','Trusted computing'
'ubiquitous computing is considered as a promising technological path of innovation. intensive r&d activities and political strategies are addressing the objective to foster marketable technologies and applications. this article explores the state-of-the-art on the way towards the \'\'internet of things\'\'. which application fields have already proved their potential for realising the vision and promises related to the new technology? what are the technical, legal and social challenges that have to be addressed - and how can policy-makers contribute? we deal with these questions in the light of recent developments in research and business, illustrating the findings by examples in retail, logistics and health care. the article concludes that further efforts by all stakeholders from businesses, society and politics are necessary to make ubiquitous computing applications economically sustainable and socially compatible in order to tap its full potential.ubiquitous computing','Trusted computing'
'logical methods can be used to design, implement, and analyze mechanisms for enforcing security and privacy policies. this article summarizes some significant results in this area, discusses the potential for wider adoption of these methods, and highlights remaining challenges that are being addressed by ongoing research.logical methods in security and privacy','Usability in security and privacy'
'at the \"computers, freedom, and privacy (cfp) conference held in berkeley, california, the spotlight was on the twin weights of national security and personal liberty - with technology the fulcrum on which all turns. it highlights included sessions devoted to the new international cybercrime treaty, a global crusade to spread technology to underdeveloped nations, laws meant to block illegal sites at the ip-address level, and wiretapping voice-over-ip (voip) communications.preserving security and privacy','Usability in security and privacy'
'the third in a series of articles providing basic information on legal issues facing people and businesses that operate in computing-related markets focuses on the responsibility to ensure privacy and data security. the featured web extra is an audio podcast by brian m. gaff and thomas j. smedinghoff, two of the article\'s coauthors.privacy and data security','Usability in security and privacy'
'there is a growing understanding that privacy is an essential component of security. in order to decrease the probability of having data breaches, the design of information systems, processes and architectures should incorporate considerations related to both privacy and security. this incorporation may benefit from the offering of appropriate training. in this way, this paper intends to discuss how to better offer training while considering new developments that involve both multimedia production and the \"gamification\" of training. the paper suggests the use in conjunction of two frameworks: the edupmo framework, useful for the management of large scale projects that may involve a consortium of organizations developing multimedia for the offering of training, and the game development framework, useful for the identification of the main components of the serious game for training on privacy by design to be developed as part of the training offering.privacy and security in cyberspace','Usability in security and privacy'
'the nearly seven years of concern with data privacy and security in computerized information systems have produced a variety of hardware and software techniques for protecting sensitive information against unauthorized access or modification. however, systematic procedures for cost-effective implementation of these safeguards are still lacking.privacy and security in databank systems','Usability in security and privacy'
'privacy is a very important element in every one\'s everyday life. most users would not like to have their data exposed to other people on the internet. the initial approach used for attacking a user\'s privacy and security is done by scanning the nodes on a network. this gives an attacker the ability to obtain the ip addresses in use by this node so that this information can then be used to initiate further attacks against this node, such as tracking them via their ip address across the networks, and then, later correlating the user\'s activities with his ip address. the first attempt by the internet engineering task force (ietf) to protect a user\'s privacy was defined in the privacy extension rfc [13]. unfortunately this rfc has some deficiencies which makes its use vulnerable to privacy related attacks. to address this problem, and solve the deficiencies that exist with the use of this rfc, we introduce our new algorithm, which not only maintains a node\'s lifetime, but also provides a user with a method for randomized interface id (iid) generations.privacy and security in ipv6 networks','Usability in security and privacy'
'a law now in effect in the united states requires protection of individual privacy in computerized personal information record-keeping systems maintained by the federal government. similar laws apply in certain state and local governments. legislation has also been introduced to extend the requirements for privacy protection to the private sphere. central in privacy protection are the rights of an individual to know what data are maintained on him, challenge their veracity and relevance, limit their nonroutine use or dissemination, and be assured that their quality, integrity, and confidentiality are maintained. in all computer systems that maintain and process valuable information, or provide services to multiple users concurrently, it is necessary to provide security safeguards against unauthorized access, use, or modifications of any data file. this difficult problem has not yet been solved in the general case. computer systems must also be protected against unauthorized use, disruption of operations, and physical damage. the growing number of computer applications involving valuable information or assets plus the growing number of criminal actions directed against computer applications and systems or perpetrated by using computers underscore the need for finding effective solutions to the computer security problem. in the future, concerns for privacy and security must become integral in the planning and design of computer systems and their applications.privacy and security issues in information systems','Usability in security and privacy'
'i will argue that one class of issues in computer ethics oftenassociated with privacy and a putative right to privacy isbest-analyzed in terms that make no substantive reference toprivacy at all. these issues concern the way that networkedinformation technology creates new ways in which conventionalrights to personal security can be threatened. however onechooses to analyze rights, rights to secure person and propertywill be among the most basic, the least controversial, and themost universally recognized. a risk-based approach to theseissues provides a clearer statement of what is ethicallyimportant, as well as what is ethically problematic. once theissues of security have been articulated clearly, it becomespossible to make out genuine issues of privacy in contrast tothem.privacy, secrecy and security','Usability in security and privacy'
'the telecommunication industry has been successful in turning the internet into a mobile service and stimulating the creation of a new set of networked, remote services. in this paper we argue that embracing cloud computing solutions is fundamental for the telecommunication industry to remain competitive. however, there are legal, regulatory, business, market related and technical challenges that must be considered. in this paper we list such challenges and define a set of privacy, security and trust requirements that must be taken into account before cloud computing solutions can be fully integrated and deployed by telecommunication providers.privacy, security and trust in cloud computing','Usability in security and privacy'
'in this paper, we outline an approach to the identification of entities for access control that is based on the membership of groups, rather than individuals. by using group membership as a level of indirection between the individual and the system, we can increase privacy and provide incentives for better behaviour. privacy comes from the use of pseudonyms generated within the group and which can be authenticated as belonging to the group. the incentives for better behaviour come from the continuous nature of groups - members may come and go, but the group lives on, and groups are organised so as to ensure group-longevity, and prevent actions which may harm the group\'s reputation. we present a novel pseudonym generation mechanism suitable for use in groups without a centralised administration. finally, we argue that the use of group membership as the basis for formulating policies on interaction is more efficient for disconnected operation, facilitating proxies and the efficient storage of revoked membership and distrusted organisations within bloom filters for small memory footprints.reconciling privacy and security in pervasive computing','Usability in security and privacy'
'death is an uncomfortable subject for many people, and digital systems are rarely designed to deal with this event. in particular, the wide array of existing digital authentication infrastructure rarely deals with gracefully retiring credentials in a uniform fashion. this research paper highlights an emerging paradigm: gracefully dealing with expired digital identities in a secure, privacy-preserving fashion. it examines the confluence of modern browser technology, cloud services, and human factors involved in managing a person\'s digital footprint while they live and retiring it when they die. we contemplate a potential approach to dealing with credentials after death by using cloud computing. we consider the reasons that such an approach may actually provide an opportunity for enhancing authentication security by frustrating identity stealing attacks. we note that this paper is not aimed at trivializing the real grief and loss that people feel, but rather an attempt to understand how security and privacy concerns are shaped by the end of life, with the ultimate goal of easing this transition for friends and family.security and privacy considerations in digital death','Usability in security and privacy'
'security and privacy in a ubiquitous society','Usability in security and privacy'
'because it is increasingly difficult if not impossible to define the perimeter that separates the trusted inside from the untrusted outside, many security and privacy mechanisms no longer work in an online world.security and privacy in an online world','Usability in security and privacy'
'cloud computing is becoming a well-known buzzword nowadays. many companies, such as amazon, google, microsoft and so on, accelerate their paces in developing cloud computing systems and enhancing their services to provide for a larger amount of users. however, security and privacy issues present a strong barrier for users to adapt into cloud computing systems. in this paper, we investigate several cloud computing system providers about their concerns on security and privacy issues. we find those concerns are not adequate and more should be added in terms of five aspects (i.e., availability, confidentiality, data integrity, control, audit) for security. moreover, released acts on privacy are out of date to protect users\' private information in the new environment (i.e., cloud computing system environment) since they are no longer applicable to the new relationship between users and providers, which contains three parties (i.e., cloud service user, cloud service provider/cloud user, cloud provider). multi located data storage and services (i.e., applications) in the cloud make privacy issues even worse. hence, adapting released acts for new scenarios in the cloud, it will result in more users to step into cloud. we claim that the prosperity in cloud computing literature is to be coming after those security and privacy issues having be resolved.security and privacy in cloud computing','Usability in security and privacy'
'security and privacy in collaborative distributed systems','Usability in security and privacy'
'privacy and security are relevant topics in both--research and practice. although they are often used together, implicitly assuming that they represent the same concept, they actually represent different concepts that are closely related. first, this paper presents a way to differentiate between these two topics from a conceptual point of view. furthermore, it depicts some commonly accepted privacy regulations that exist in the oecd, eu and us. second, we show how privacy and security are defined and implemented in practice, based on three interviews, conducted in different austrian companies. the interviews picture the specific situation in the companies. similarities and differences between the three interviews as well as between the interviews as a whole and the conceptual considerations were found and are described. to explain the maturity of these companies in terms of their understanding of privacy and security, we analyzed and visualized the interviews.security and privacy in companies','Usability in security and privacy'
'with the advent of computer systems which share the resources of the configuration among several users or several problems, there is the risk that information from one user (or computer program) will be coupled to another user (or program). in many cases, the information in question will bear a military classification or be sensitive for some reason, and safeguards must be provided to guard against the leakage of information. this session is concerned with accidents or deliberate attempts which divulge computer-resident information to unauthorized parties.security and privacy in computer systems','Usability in security and privacy'
'in recent years, many countries have adopted electronic passport systems, which merge the technologies of radio frequency identification &#40;rfid&#41; or contactless smart card and biometric identification. this paper describes various security issues applied to electronic passports and analyses the technology being implemented, with case studies of electronic passport systems in various countries. the case studies include the security measures that each country has taken to secure the electronic passports, as well as how the breach of security takes place. this has implications on how security can be improved as more countries are adopting electronic passports.security and privacy in electronic passports','Usability in security and privacy'
'wireless communication is continuing to make inroads into many facets of society and is gradually becoming more and more ubiquitous. while in the past wireless communication (as well as mobility) was largely limited to the first and last transmission hops, today\'s wireless networks are starting to offer purely wireless, often mobile, and even opportunistically connected operation. the purpose of this article is to examine security and privacy issues in some new and emerging types of wireless networks, and attempt to identify directions for future research.security and privacy in emerging wireless networks','Usability in security and privacy'
'social networking becomes increasingly important due to the recent surge in online interaction. social network analysis can be used to study the functioning of computer networks, information flow patterns in communities, and emergent behavior of physical and biological systems. in this paper, the mathematical formulation and computational models for security and privacy of social network data are discussed. several possible ways for an attacker to attack are presented so that the mathematical formulation can take them into account. the metrics for measuring the amount of security and privacy in an online social network (osn) are discussed so that we have an idea of how good a model is. based on these current techniques and attack strategies, future directions of research are discussed.security and privacy in online social networks','Usability in security and privacy'
'sensor networks offer economically viable solutions for a variety of applications. for example, current implementations monitor factory instrumentation, pollution levels, free-way traffic, and the structural integrity of buildings. other applications include climate sensing and control in office buildings and home environmental sensing systems for temperature, light, moisture, and motion.security and privacy in sensor networks','Usability in security and privacy'
'adding digital intelligence and two-way functionalities to the power grid is one of the most flourishing topics in both academic and public institution communities. efficiency, improved reliability and safety are the benefits promised by the new smart grid at the price of privacy and security challenges which are only in part similar to the security issues of it networks. we survey the current grid architecture and the relation among the smart grid operators to analyze the security and privacy threats which needs to be addressed to secure the smart grid digital infrastructure.security and privacy in smart grid infrastructures','Usability in security and privacy'
'over the past several years, social networking sites have arisen to facilitate social interactions on the internet while revolutionizing how online users interact with others. most social networking sites offer the basic features of online interaction, communication, and interest sharing, letting individuals create online profiles that other users can view. unfortunately, current trends in social networks indirectly require users to become system and policy administrators to protect their online contents. social networks\' security and privacy requirements still aren\'t well understood or fully defined. nevertheless, it\'s clear that they\'ll be quite different from classic security and privacy requirements because social networks involve user-centric concerns and allow multiple users to specify security policies on shared data. so, we must bring a depth of security experience from multiple security domains and technologies to this field, as well as a breadth of knowledge about social networks.security and privacy in social networks','Usability in security and privacy'
'in recent years, sensors and sensor networks have been extremely popular in the research community. one of the most exciting aspects of sensor networks research is the confluence of diverse areas, such as databases, networking, distributed systems and security. in particular, security issues in wsns have received a lot of attention. due to low cost of individual sensors and commensurately meager resources, security in sensor networks poses some unique and formidable challenges. a large body of research has been accumulated in recent years, dealing with various aspects of sensor security, such as: key management, data authentication, privacy, secure aggregation, secure routing as well as attack detection and mitigation. one common assumption in prior wsn security research has been that data collection is performed in (or near) real time: a trusted entity --- usually called a sink --- is assumed to be always (or mostly) present. data sensing can be event-driven (triggered by some changes in the sensing environment), on-demand (initiated by a query from the sink) or scheduled (prompted by a timer). no matter how sensing is activated, the presence of an on-line sink allows nodes to submit measurements soon after sensing. in this model, an adversary capable of compromising nodes and corrupting data has relatively little time to pursue its goals. while many wsns operate in this general setting, there are emerging wsn scenarios and applications that fall outside the real-time data collection model. we refer to such networks as \"unattended wsns\" or uwsns. for example, wsns deployed in military or law enforcement environments might not have the luxury of an ever-present sink: sensed data can be off-loaded only when the sink visits the network. another example might be a wsn monitoring compliance with a nuclear non-proliferation treaty operating in a rogue country. we further narrow our scope to uwsns operating in hostile environments. unattended sensors deployed in such environment represent an attractive and easy target for an adversary. the sensors\' inability to off-load data in real time exposes them and their data to increased risk. without external connectivity, sensors can be compromised with impunity and collected data can be read, altered or simply erased. sensor compromise is a realistic threat, since a typical sensor is a mass-produced commodity device with no specialized secure hardware or tamper-resistant components. prior security research typically assumed that some number of sensors can be compromised during the entire operation of the network and the main challenge is to detect such compromise. this is a reasonable assumption, since --- with a constantly present sink --- attacks can be detected and isolated. the sink can then immediately take appropriate actions to prevent compromise of any more sensors. in contrast, in the uwsn setting, the adversary can compromise up to a certain number of sensors within a particular time interval. this interval can be much shorter than the time between successive sink visits. given enough intervals, the adversary can subvert the entire network as it moves between sets of compromised nodes, gradually undermining security. the adversary\'s goals might include: reading, erasing or modifying data collected by unattended sensors. in this talk, we discuss in detail a number of security challenges in unattended wsns. in doing so, our main goal is to bring the problem to light and engender interest from the research community to investigate it further.security and privacy in unattended sensor networks','Usability in security and privacy'
'within the next year, travelers from dozens of nations may be carrying a new form of passport in response to a mandate by the united states government. the e-passport, as it is sometimes called, represents a bold initiative in the deployment of two new technologies: radio-frequency identification (rfid) and biometrics. important in their own right, e-passports are also the harbinger of a wave of next-generation id cards: several national governments plan to deploy identity cards integrating rfid and biometrics for domestic use. we explore the privacy and security implications of this impending worldwide experiment in next-generation authentication technology. we describe privacy and security issues that apply to e-passports, then analyze these issues in the context of the international civil aviation organization (icao) standard for e-passports.security and privacy issues in e-passports','Usability in security and privacy'
'the emergence of innovative uses of mobile and wireless devices in mobile learning &#40;m&#45;learning&#41; go beyond the convenience of combining electronic learning applications with the mobility of portable devices. technologies, protocols and policies that provide security and privacy in networked applications have to be augmented and adjusted to take into account the new uses and requirements of m&#45;learning. this article reviews recent developments in m&#45;learning with a focus on privacy and security issues. developers of m&#45;learning applications and organisations that deploy them have to understand the vulnerabilities and plan to address them early on in order to avoid potential exploitation efforts.security and privacy issues in mobile learning','Usability in security and privacy'
'this research proposes an e-government security trust model and develops a typology of antecedents in the context of citizen tax software use and e-filing. we propose that tax software use and electronic filing (e-filing) offer a novel and interesting research setting that is relevant to e-government and security because of (1) the use of software to complete tax returns by a large portion of the citizenry, (2) the necessity of security for transmittal of information during e-filing, (3) the privacy of the subject matter, (4) the current promotion of e-filing by the american tax collection agency (irs), and (5) individual taxpayer ambivalence or negative attitude toward taxes and the government in general. we suggest that when the information system serves as surrogate for a tax domain expert several antecedents to security and privacy trust are potential determinants of use.security and privacy trust in e-government','Usability in security and privacy'
'security and privacy','Usability in security and privacy'
'security, privacy and dependability are crucial issues if one wants to build a real smart home. first, in addition to established home security requirements, smart home adoption requires to solve brand-new security vulnerabilities deriving from the automated facets of smart homes. thereafter, pervasive computing and ambient intelligence allow to collect a lot of information, to analyze it to derive new facts, and make them explicit. finally, systems that are usually safe and dependable can fail when their behavior is becoming controlled as the result of complex interactions between many intertwined information systems. unfortunately, application developers in smart home environments are usually neither security experts, nor familiar with ethical and legal requirements related to privacy. security patterns can help to anticipate, overcome, and document systematically these difficult issues in building pervasive information systems in smart homes for cognitively impaired people. in this paper, we illustrate how security patterns can be extended and applied to smart home to foster autonomy of elderly or cognitively impaired people, then, we sketch the structure of the catalog which will be populated with a few patterns.security, privacy, and dependability in smart homes','Usability in security and privacy'
'an important aspect of e-business is the area of e-commerce. one of the most severe restraining factors for the proliferation of e-commerce, is the lack of trust between customers and sellers, consumer privacy concerns and the lack of security measures required to assure both businesses and customers that their business relationship and transactions will be carried out in privacy, correctly, and timely. this paper considers trust privacy and security issues in e-commerce applications and discusses methods and technologies that can be used to fulfil the pertinent requirements.trust, privacy and security in e-business','Usability in security and privacy'
'based on the available crime and intelligence knowledge, federal, state, and local authorities can make timely and accurate decisions to select effective strategies and tactics as well as allocate the appropriate amount of resources to detect, prevent, and respond to future attacks. facing the critical mission of international security and various data and technical challenges, there is a pressing need to develop the science of security informatics. the main objective is the development of advanced information technologies, systems, algorithms, and databases for security-related applications using an integrated technological, organizational, and policy-based approach. intelligent systems have much to contribute for this emerging field.ai and security informatics','Virtualization and security'
'this is an excellent time to make security fundamental to any real next-generation network, and for the technical community to get involved in continuing efforts to preserve liberty as well as safety.balancing security and liberty','Virtualization and security'
'one of the unanticipated consequences of the internet age is a pervasive loss of context. information is often filtered, sampled, repackaged, condensed, or altered to suit any number of purposes. over time, the entropy of these processes causes information to lose its essential validity. this column argues the needs, applications, and challenges of providing greater access to data provenance in information systems.data provenance and security','Virtualization and security'
'while researching his new book, security expert bruce schneier examined the role morals play in providing security. because security professionals spend most of their time dealing with attackers for whom morals aren\'t sufficient to keep them from doing what they shouldn\'t be doing, schneier suggests that we may need to start looking at ways to enhance the natural security systems our species has evolved over the millennia.empathy and security','Virtualization and security'
'intelligence and security informatics (isi) is an emerging field of study aimed at developing advanced information technologies, systems, algorithms, and databases for national- and homeland-security-related applications, through an integrated technological, organizational, and policy-based approach. this paper summarizes the broad application and policy context for this emerging field. three detailed case studies are presented to illustrate several key isi research areas, including cross-jurisdiction information sharing; terrorism information collection, analysis, and visualization; and \"smart-border\" and bioterrorism applications. a specific emphasis of this paper is to note various homeland-security-related applications that have direct relevance to transportation researchers and to advocate security informatics studies that tightly integrate transportation research and information technologies.intelligence and security informatics for homeland security','Virtualization and security'
'this article briefly describes the introduction and evolution of information security management systems (isms), their application and the introduction of national and regulatory requirements to protect information and how these regulations may be mapped into an isms.isms, security standards and security regulations','Virtualization and security'
'the practicality of mobile agents hinges on realistic security techniques. mobile agent systems are combination client/servers that transport, and provide an interface with host computers for, mobile agents. transport of mobile agents takes place between mobile agent systems, which are located on heterogeneous platforms, making up an infrastructure that has the potential to scale to the size of any underlying network. mobile agents can be rapidly deployed, and can respond to each other and their environment. these abilities expose flaws in current security technology. this article surveys the risks connected with the use of mobile agents, and security techniques available to protect mobile agents and their hosts. the inadequacies of the security techniques developed from the information fortress model are identified. they are the result of using a good model in an inappropriate context (i.e. a closed system model in a globally distributed networking computing base). problems with commercially available techniques include: (1) conflicts between security techniques protecting hosts and mobile agents, (2) inability to handle multiple collaborative mobile agents, and (3) emphasis on the credentials of software instead of on the integrity of software to determine the level of trust.mobile agents and security','Virtualization and security'
'mobile code and security','Virtualization and security'
'it has been hypothesized that storage security and network security are essentiallythe same, at least insofar as mapping solutions from one domain in a straightforwardmanner to the other.we discuss similarities and differences that shed some doubt onthe property of equating the two.while there are many ways to apply methods fromone domain to another, there are fundamental differences between data at rest and data in motion.storage is often an endpoint as well as a link, and it requires differenttreatment under such circumstances.network security and storage security','Virtualization and security'
'at the \"computers, freedom, and privacy (cfp) conference held in berkeley, california, the spotlight was on the twin weights of national security and personal liberty - with technology the fulcrum on which all turns. it highlights included sessions devoted to the new international cybercrime treaty, a global crusade to spread technology to underdeveloped nations, laws meant to block illegal sites at the ip-address level, and wiretapping voice-over-ip (voip) communications.preserving security and privacy','Virtualization and security'
'the third in a series of articles providing basic information on legal issues facing people and businesses that operate in computing-related markets focuses on the responsibility to ensure privacy and data security. the featured web extra is an audio podcast by brian m. gaff and thomas j. smedinghoff, two of the article\'s coauthors.privacy and data security','Virtualization and security'
'i will argue that one class of issues in computer ethics oftenassociated with privacy and a putative right to privacy isbest-analyzed in terms that make no substantive reference toprivacy at all. these issues concern the way that networkedinformation technology creates new ways in which conventionalrights to personal security can be threatened. however onechooses to analyze rights, rights to secure person and propertywill be among the most basic, the least controversial, and themost universally recognized. a risk-based approach to theseissues provides a clearer statement of what is ethicallyimportant, as well as what is ethically problematic. once theissues of security have been articulated clearly, it becomespossible to make out genuine issues of privacy in contrast tothem.privacy, secrecy and security','Virtualization and security'
'one approach to secure systems is through the analysis of audit trails. an audit trail is a record of all events that take place in a system and across a network, i.e., it provides a trace of user/system actions so that security events can be related to the actions of a specific individual or system component. audit trails can be inspected for the presence or absence of certain patterns. this paper advocates the use of process mining techniques to analyze audit trails for security violations. it is shown how a specific algorithm, called the @a-algorithm, can be used to support security efforts at various levels ranging from low-level intrusion detection to high-level fraud prevention.process mining and security','Virtualization and security'
'modern critical infrastructures have command and control systems. these command and control systems are commonly called supervisory control and data acquisition (scada). in the past, scada system has a closed operational environment, so these systems were designed without security functionality. nowadays, as a demand for connecting the scada system to the open network growths, the study of scada system security is an issue. a key-management scheme is critical for securing scada communications. numerous key-management structures for scada also have been suggested. 11770-2 mechanism 9 key establishment protocol has been used in scada communication however a security proof for the 11770-2 mechanism 9 protocol is needed. the purpose of this paper is to provide a general overview about scada system, and its related security issues. furthermore, we try to investigate the importance of key management protocol and the need of formal security poof.scada system security, complexity, and security proof','Virtualization and security'
'security and privacy','Virtualization and security'
'planning information security investment is somewhere between art and science. this paper reviews and compares existing scientific approaches and discusses the relation between security investment models and security metrics. to structure the exposition, the high-level security production function is decomposed into two steps: cost of security is mapped to a security level, which is then mapped to benefits. this allows to structure data sources and metrics, to rethink the notion of security productivity, and to distinguish sources of indeterminacy as measurement error and attacker behavior. it is further argued that recently proposed investment models, which try to capture more features specific to information security, should be used for all strategic security investment decisions beneath defining the overall security budget.security metrics and security investment models','Virtualization and security'
'security modeling centers on identifying system behavior, including any security defenses; the system adversary\'s power; and the properties that constitute system security. once a security model is clearly defined, security analysis evaluates whether the adversary, interacting with the system, can defeat the desired security properties. although the authors illustrate security analysis using model checking, analysts can use various methods and tools to evaluate system security, including manual and automated theorem-proving tools that provide assurance about the absence of attacks in a specified threat model. this article describes a uniform approach for evaluating system security and illustrates the approach by summarizing three case studies. security modeling and analysis also provides a basis for comparative evaluation and some forms of security metrics.security modeling and analysis','Virtualization and security'
'scalable trusted computing seeks to apply and extend the fundamental technologies of trusted computing to large-scale systems. to provide the functionality demanded by users, bootstrapping a trusted platform is but the first of many steps in a complex, evolving mesh of components. the bigger picture involves building up many additional layers to allow computing and communication across large-scale systems, while delivering a system retaining some hint of the original trust goal. not to be lost in the shuffle is the most important element: the system\'s human users. unlike 40 years ago, they cannot all be assumed to be computer experts, under the employ of government agencies which provide rigorous and regular training, always on tightly controlled hardware and software platforms. it seems obvious that the design of scalable trusted computing systems necessarily must involve, as an immutable design constraint, realistic expectations of the actions and capabilities of normal human users. experience shows otherwise. the security community does not have a strong track record of learning from user studies, nor of acknowledging that it is generally impossible to predict the actions of ordinary users other than by observing (e.g., through user experience studies) the actions such users actually take in the precise target conditions. we assert that because the design of scalable trusted computing systems spans the full spectrum from hardware to software to human users, experts in all these areas are essential to the end-goal of scalable trusted computing.system security, platform security and usability','Virtualization and security'
'desktop virtualisation is attracting a lot of interest from companies of all sizes, as they face the challenges of managing user environments and keeping large numbers of desktops up to date. however, this technology is still very new, and the security of these virtual desktop environments is often not considered when implementing pilot schemes or production projects. so what are the areas to be aware of when looking at desktop virtualisation, and what steps should you take to keep these environments secure?vdi and security','Virtualization and security'
'many industrial applications use object range data. therefore, various range scanners, based on different working principles are developed. among these, scanners using laser stripe-based triangulation are the most promising ones. unfortunately, obtaining the range data of shiny objects using these scanners is problematic under ambient light. for specular objects, the laser stripe causes multiple reflections on the object. even if this problem is solved, the ambient light also affects the scanning problem by introducing highlights on the (specular or shiny) object surface. to solve this problem, we develop two stripe-based scanners in this study. our first scanner is based on line laser and mechanical rotation parts. our second scanner is based on a projection machine (providing multicolor line stripes). for both systems, our main contribution is eliminating the affect of highlights (originating from ambient light) in detecting the line stripe (from line laser or projection machine) in range data extraction. to do so, we introduce a novel color invariant. we test and report the performance of our novel stripe detection method under various controlled experiments. we also test and report the overall range data extraction performance of our scanners on several shiny and matte objects. these objects have different color, texture and shape characteristics. we compare our range scanners with a commercial one as a benchmark. extensive testings indicate the success of our range scanners, especially for scanning shiny surfaces under ambient light.a color invariant for line stripe-based range scanners','Vulnerability scanners'
'network attacks often employ scanning to locate vulnerable hosts and services. fast and accurate detection of local scanners is key to containing an epidemic in its early stage. existing scan detection schemes use statically determined detection criteria, and as a result do not respond well to traffic perturbations. we present two adaptive scan detection schemes, success based (sb) and failure based (fb), which change detection criteria based on traffic statistics. we evaluate the proposed schemes analytically and empirically using network traces. against fast scanners, the adaptive schemes render detection precision similar to the traditional static schemes. for slow scanners, the adaptive schemes are much more effective, both in terms of detection precision and speed. sb and fb have non-linear properties not present in other schemes. these properties permit a lower sustained scanning threshold and a robustness against perturbations in the background traffic.adaptive detection of local scanners','Vulnerability scanners'
'flow level information is important for many applications in network measurement and analysis. in this work, we tackle the \'\'top spreaders\'\' and \'\'top scanners\'\' problems, where hosts that are spreading the largest numbers of flows, especially small flows, must be efficiently and accurately identified. the identification of these top users can be very helpful in network management, traffic engineering, application behavior analysis, and anomaly detection. we propose novel streaming algorithms and a \'\'filter-tracker-digester\'\' framework to catch the top spreaders and scanners online. our framework combines sampling and streaming algorithms, as well as deterministic and randomized algorithms, in such a way that they can effectively help each other to improve accuracy while reducing memory usage and processing time. to our knowledge, we are the first to tackle the \'\'top scanners\'\' problem in a streaming way. we address several challenges, namely: traffic scale, skewness, speed, memory usage, and result accuracy. the performance bounds of our algorithms are derived analytically, and are also evaluated by both real and synthetic traces, where we show our algorithm can achieve accuracy and speed of at least an order of magnitude higher than existing approaches.an online framework for catching top spreaders and scanners','Vulnerability scanners'
'given independent multiple access logs, we develop a mathematical model to identify the number of malicious hosts in the current internet. in our model, the number of malicious hosts is formalized as a function taking two inputs, namely the duration of observation and the number of sensors. under the assumption that malicious hosts with statically assigned global addresses perform random port scans to independent sensors uniformly distributed over the address space, our model gives the asymptotic number of malicious source addresses in two ways. firstly, it gives the cumulative number of unique source addresses in terms of the duration of observation. secondly, it estimates the cumulative number of unique source addresses in terms of the number of sensors. to evaluate the proposed method, we apply the mathematical model to actual data packets observed by isdas distributed sensors over a one-year duration from september 2004, and check the accuracy of identification of the number of malicious hosts.estimation of behavior of scanners based on isdas distributed sensors','Vulnerability scanners'
'in the design and evaluation of color scanners and cameras, it is useful to have a single figure of merit that closely agrees with perceived color accuracy. in the past, several measures of goodness for color scanning filters have been proposed to fulfil such a requirement. most of the proposed measures have had shortcomings in that they are either based on error metrics in color spaces that are not perceptually uniform, or in that they do not take into account the effects of measurement noise. an extension of the most promising measure, based on linearized cielab space, is proposed to obtain a new figure of merit that has a high degree of perceptual relevance and also accounts for the varying noise performance of different filters. the paper also provides a common framework for the different figures of merit and a comprehensive comparison of their computational complexity and reliabilityfigures of merit for color scanners','Vulnerability scanners'
'we develop a novel technique for authenticating physical documentsby using random, naturally occurring imperfections in paper texture.to this end, we devised a new method for measuring thethree-dimensional surface of a paper without modifying the documentin any way, using only a commodity scanner.from this physicalfeature, we generate a concise fingerprint that uniquely identifiesthe document.our method is secure against counterfeiting, robustto harsh handling, and applicable even before any content is printedon a page.it has a wide range of applications, including detectingforged currency and tickets, authenticating passports, and haltingcounterfeit goods.on a more sinister note, document identificationcould be used to de-anonymize printed surveys and to compromise thesecrecy of paper ballots.fingerprinting blank paper using commodity scanners','Vulnerability scanners'
'the development of compact, low-height bar code readers for european supermarkets, where clerks sit at checkstands arranged much like office desks, is described. the technology used to meet all the requirements for this application is described. the resulting reader features advanced hologram lenses, for less-expensive optics and greater efficiencyhalogram lenses lead to compact scanners','Vulnerability scanners'
'handy scanners','Vulnerability scanners'
'given independent multiple access-logs, we try to identify how many malicious hosts in the internet. our model of number of malicious hosts is a formalized as a function taking two inputs, a duration of sensing and a number of sensors. under some assumptions for simplifying our model, by fitting the function into the experimental data observed for three sensors, in 13 weeks, we identify the size of the set of malicious hosts and the average number of scans they perform routinely. main results of our study are as follows; the total number of malicious hosts that periodically performs port-scans is from 4,900 to 96,000, the malicious hosts density is about 1 out of 15,000 hosts, and an average malicious host performs 78 port-scans per second.how many malicious scanners are in the internet?','Vulnerability scanners'
'the detection and tracking of moving objects is an essential task in robotics. the cmu-ri navlab group has developed such a system that uses a laser scanner as its primary sensor. we will describe our algorithm and its use in several applications. our system worked successfully on indoor and outdoor platforms and with several different kinds and configurations of two-dimensional and three-dimensional laser scanners. the applications vary from collision warning systems, people classification, observing human tracks, and input to a dynamic planner. several of these systems were evaluated in live field tests and shown to be robust and reliable. &#x00a9; 2012 wiley periodicals, inc. &#169; 2013 wiley periodicals, inc.moving object detection with laser scanners','Vulnerability scanners'
'mri scanners','Vulnerability scanners'
'in this work we present a multi-touch wall display system that equips easily and considers occlusions problem using multiple range scanners. our system is implemented on an existing large display embedded in a wall with two laser range scanners. each scanner detects touches events including positions and/or areas. some touch positions causes relative occlusions due to a range scanner. we reduce the problem by using multiple scanners. if the touch events from different scanners are the same, these are combined into one touch event. detected multi-touch events are sent to the network as tuio events. this system is simple and adaptable to various existing displays including front projection screens, and to various tuio applications.multi-touch wall display system using multiple laser range scanners','Vulnerability scanners'
'this paper addresses the problem of evaluating the ldquooperational qualityrdquo of fingerprint scanners, that is, the ability of acquiring images that maximize the accuracy of automated fingerprint recognition. the quality parameters commonly used to quantify the fidelity of a scanner in sensing the input pattern have been analyzed and a large experimentation has been carried out to understand their effects on fingerprint recognition accuracy. the experimental results show that some parameters have a strong impact, while others appear to be less relevant.on the operational quality of fingerprint scanners','Vulnerability scanners'
'paranoid penguin: checking your work with scanners, part ii','Vulnerability scanners'
'we describe the physical-optics modelling of a millimetre-wave imaging system intended to enable automated detection of threats hidden under clothes. this paper outlines the theoretical basis of the formation of millimetre-wave images and provides the model of the simulated imaging system. results of simulated images are presented and the validation with real ones is carried out. finally, we present a brief study of the potential materials to be classified in this system.physical optics modelling of millimetre-wave personnel scanners','Vulnerability scanners'
'due to its pervasiveness and communication capabilities, bluetooth can be used as an infrastructure for several situated interaction and massive sensing scenarios. this paper shows how bluetooth scanning can be used in gate counting scenarios, where the main goal is to provide an accurate count for the number of unique devices sighted. to this end, we present an analysis of several stochastic counting techniques that not only provide an accurate count for the number of unique devices, but offer privacy guarantees as well.privacy preserving gate counting with collaborative bluetooth scanners','Vulnerability scanners'
'the paper describes a pipeline for 3d scanning acquisition and processing that allow to exploit the utmost precision and quality out of tof scanners. the proposed approach capitalize on the knowledge of the distribution of the noise to apply sophisticated fairing techniques for cleaning up the data. leveraging on the very dense sampling of this kind of scanners we show that is possible to attain high accuracy. we present a practical application of the proposed approach for the scanning of a large (5mt) statue with millimetric precision.pushing time-of-flight scanners to the limit','Vulnerability scanners'
'this article will introduce the security threats most prevalent today, explain the reasoning for automated scanning and why the software solutions are not always as perfect as you would desire.scanners','Vulnerability scanners'
'software is key to commercial magnetic resonance imaging (mri) scanners, the medical devices that make images of the living human body for clinical purposes.software in mri scanners','Vulnerability scanners'
'speed-dating portable scanners','Vulnerability scanners'
'the impact of scanners on employment in supermarkets','Vulnerability scanners'
'software development needs continuous quality control for a timely detection and removal of quality problems. this includes frequent quality assessments, which need to be automated as far as possible to be feasible. one way of automation in assessing the security of software are application scanners that test an executing software for vulnerabilities. at present, common quality assessments do not integrate such scanners for giving an overall quality statement. this paper presents an integration of application scanners into a general quality assessment method based on explicit quality models and bayesian nets. its applicability and the detection capabilities of common scanners are investigated in a case study with two open-source web shops.the use of application scanners in software product quality assessment','Vulnerability scanners'
'using freeware vulnerablility scanners','Vulnerability scanners'
'a process for digitizing cultural heritage&#8212;in this case, a historic building in england&#8212;involves capturing data with a mid-range laser scanner, processing the data, and then creating a model. such a process is becoming more accessible to nonspecialists. however, technical barriers to processing as well as mindsets inherited from 2d mapping and imaging hinder understanding. given that data capture is easier than processing, sustainable data management strategies are necessary, such as empirical provenance, archiving, and the long-term preservation of high-definition, dense 3d data. open source and active communities of user-creators will likely be the future for the generation and preservation of such data.using mid-range laser scanners to digitize cultural-heritage sites','Vulnerability scanners'
'high definition three-dimensional (3d) surface scanners, based on structured light or laser light section techniques, have found a wide range of applications, especially for technical and industrial applications (mostly for measuring and inspection tasks). since about 10 years, systems adapted for the requirements of arts and cultural heritage (ch) support 3d digitization of art objects. although the use of digital 3d models in ch is rapidly growing, many of the users are not yet completely familiar with terminology and all details of technical specifications. as most of the users are practitioners there is sometimes only little experience with terms as data quality, accuracy, resolution, measurement uncertainty, especially because these terms are used in very different ways, in manuals and brochures of scanner manufacturers as well as by authors of scientific papers. moreover, the objective of many applications is digitization instead of measurement; therefore, many users are not even aware, that they nevertheless have to care about metrology issues such as verification and acceptance tests of the used equipment to get a reliable scanning result. in its first part, the paper will give an overview the fundamentals of data acquisition and data processing, presenting also advantages and benefits, limitations and drawbacks as well as correlations between different performance parameters of high definition 3d surface scanners. our goal is also to rectify a number of typical misunderstandings and to clarify related terms and definitions. in its second part, the paper will concentrate on verification and acceptance tests of high definition 3d scanners, reviewing the german guidelines vdi/vde 2634/2 and proposing some preliminary extensions required to cope better with the ch domain.verification and acceptance tests for high definition 3d surface scanners','Vulnerability scanners'
'workgroup scanners','Vulnerability scanners'
'being able to detect and recognize human activities is essential for 3d collaborative applications for efficient quality of service provisioning and device management. a broad range of research has been devoted to analyze media data to identify human activity, which requires the knowledge of data format, application-specific coding technique and computationally expensive image analysis. in this paper, we propose a human activity detection technique based on application generated metadata and related system metadata. our approach does not depend on specific data format or coding technique. we evaluate our algorithm with different cyber-physical setups, and show that we can achieve very high accuracy (above 97\%) by using a good learning model.3d teleimmersive activity classification based on application-system metadata','Web application security'
'the importance of teaching application security at an undergraduate level is well-understood. however, comprehensive coverage of application security must cover a vast range of topics from system administration to secure software development. in our experience, providing students with hands-on experience poses a challenge: either the entire project is limited to a specific area, such as system administration, or the project consists of disconnected assignments each covering one area. neither option is satisfactory as both fail to address an important learning outcome of any security course: securing computing infrastructure requires a comprehensive approach. in this paper, we describe a semester-long project for an undergraduate application security course that (a) provides students with a comprehensive view of security and (b) reinforces the theoretical skills with intensive hands-on experience. the project consists of several independent assignments that enable students to accomplish smaller tasks as they implement a fully integrated solution. the project requires limited laboratory facilities and utilizes software tools and and technologies that are freely available to academic institutions.a comprehensive undergraduate application security project','Web application security'
'the applicability of the security protocols, such as wssecurity, ws-trust, ws-secureconversation, wsfederation, ws-authorization, and ws-securitypolicy, is limited as they only protect soa (service oriented architecture) communication between two trusted parties with an established security association. the pervasiveness of web services and soap api that can be invoked by anonymous consumers introduces security vulnerabilities are not addressed by the existing standards. in this paper, an integrated application and protocol-based framework is proposed to tackle the existing ws security problems. the proposed iapf techniques are envisioned to be a part of the design and implementation structure of a web service endpoint within the application and transaction handling logic of the soap/web service producer. these techniques will empower application level web services developers to design and implement soa producers to the iapf standard to firstly prevent dos and ddos based attacks and secondly mitigate the effects of these attacks.a framework for enhancing web services security','Web application security'
'this paper presents a framework that employs security ontologies and security patterns to provide application developers with a way to utilize security expertise. through the development of a security ontology, developers locate the major security-related concepts relevant to their application context. security patterns are then integrated with these concepts to provide tested solutions for accommodating security requirements.a framework for exploiting security expertise in application development','Web application security'
'access control services integrated in current middleware technologies fall short whenever application-specific access control policies must be enforced. as a consequence, developers embedaccess control logic in the code, resulting in an unmaintainable access control enforcement. theauthors use aspect-oriented software development techniques to better separate application logic andaccess control by describing the design and implementation of a modular access control service. theyhave implemented a prototype in caesarj, a research aspect-oriented programming language.a modular access control service for supporting application-specific policies','Web application security'
'web services security (wss) has been approved as a standard by oasis and widely adopted in the industry as a solution for enhancing the security of web services. however, the performance of wss remains a concern due to the additional security contents added to soap message and the extra service time for processing these security contents. this paper aims at clarifying this concern by conducting a performance evaluation of wss. a simple web service is designed and used for performance testing with a variety of wss polices and message sizes. the test results are categorized, compared and analyzed to work out the overheads for individual security setting. this work is expected to provide an overview and guidance for wss performance overhead.a performance evaluation of web services security','Web application security'
'this presentation follows up the talk last year to winforms (the washington institute for operations research and the management sciences) in which an approach developed for the analysis of military command and control during crises was shown to be relevant to surveillance for an infectious disease outbreak. promedmail is an internet-based system dedicated to rapid global dissemination of information on outbreaks of infectious diseases and acute exposures to toxins that affect human health. the presentation will demonstrate how the collection, formatting, and analysis of raw data, using promed-mail, can point to emerging biological incidents and allow the real-time dissemination of results to local, regional, and central health facilities.a public health application of data analysis for homeland security','Web application security'
'an exciting type of data-integrated application based on web is sprouting up all across the internet, which is called mashup application. at the same time, some new technologies and social challenges reveal themselves sequentially. this paper researches on mashup application from a perspective of security, discusses how to assess the risks of mashup application, and puts forward a risk evaluation model and some possible metrics for the actual assessment.a security risk evaluation model for mashup application','Web application security'
'a security roundtable','Web application security'
'web service has its characteristic of statelessness. but, many applications on grid need the stateful services. the advanced search on hvem datagrid is one example. the search service provides a multi-depth search and an associated search. the service should trace the search states and provide the access control as well as the service security in the search process. in addition, we should remember that the web services or the resources can stretch multiple virtual organization(vo)s and certificate authority(ca)s. in this paper, we propose a secure stateful web service to satisfy the requirements stated. we describe the stateful web service with the scalable security (s3ws) as its service environment and its protocol in detail. through the security proof, the scalability analysis and the estimation of performance and overhead, we demonstrate the stateful web service proposed to be secure and light weight enough to be realistic.a stateful web service with scalable security on hvem datagrid','Web application security'
'the fluidity of application markets complicate smartphone security. although recent efforts have shed light on particular security issues, there remains little insight into broader security characteristics of smartphone applications. this paper seeks to better understand smartphone application security by studying 1,100 popular free android applications. we introduce the ded decompiler, which recovers android application source code directly from its installation image. we design and execute a horizontal study of smartphone applications based on static analysis of 21 million lines of recovered code. our analysis uncovered pervasive use/misuse of personal/ phone identifiers, and deep penetration of advertising and analytics networks. however, we did not find evidence of malware or exploitable vulnerabilities in the studied applications. we conclude by considering the implications of these preliminary findings and offer directions for future analysis.a study of android application security','Web application security'
'with the rapid development of economy, information security is becoming more and more important for any modern organization in recent years. therefore, how to teach information security is a challenge to universities. this paper aims to apply schema theory to information security curriculum. the teaching result shows that there is a significant difference in final exam and practice exam between the proposed model and a regular teaching model.a teaching model application in the course of information security','Web application security'
'the rapid development phases and extremely short turnaround time of web applications make it difficult to eliminate their vulnerabilities. here we study how software testing techniques such as fault injection and runtime monitoring can be applied to web applications. we implemented our proposed mechanisms in the web application vulnerability and error scanner (waves)-a black-box testing framework for automated web application security assessment. real-world situations are used to test waves and to compare it with other tools. our results show that waves is a feasible platform for assessing web application security.a testing framework for web application security assessment','Web application security'
'this paper introduces an innovative wireless sensor network architecture, which has intrinsic reliability and can therefore be used for some components of homeland security applications such as intrusion detection. the proposed architecture includes a set of communication protocols in the different layers of the model. dtsn is a transport protocol used for reliable data transfer and dsdv is a routing protocol. the distributed and reactive data storage is realized using the tinydsm middleware. the architecture provides also security features such as data secrecy and integrity. the cipher means used for this are ecc and skipjack which have been developed for wireless sensor networks. our homeland security architecture has been implemented as a demonstrator using 15 micaz motes equipped with acoustic and passive infra red sensors. this paper provides insight into the architecture and presents the main lessons learnt from a prototype demonstrator. a wireless sensor network architecture for homeland security application','Web application security'
'application-level web security refers to vulnerabilities inherent in the code of a web-application itself (irrespective of the technologies in which it is implemented or the security of the web-server/back-end database on which it is built). in the last few months application-level vulnerabilities have been exploited with serious consequences: hackers have tricked e-commerce sites into shipping goods for no charge, user-names and passwords have been harvested and condential information (such as addresses and credit-card numbers) has been leaked.in this paper we investigate new tools and techniques which address the problem of application-level web security. we (i) describe a scalable structuring mechanism facilitating the abstraction of security policies from large web-applications developed in heterogenous multi-platform environments; (ii) present a tool which assists programmers develop secure applications which are resilient to a wide range of common attacks; and (iii) report results and experience arising from our implementation of these techniques.abstracting application-level web security','Web application security'
'software tend to be omnipresent in all modern systems. it often manipulates critical resources which interests pirates and need to be secured. given the fact that most common software attacks can\'t be stopped or detected using conventional security mechanisms, malicious intruders try hack into systems by exploiting a software vulnerability. vulnerabilities result from the use of traditional development processes - not focusing on security concerns - and the lack of necessary knowledge and guidance on how to produce secure software. they include implementation bugs such as buffer overflows and design flaws such as inconsistent error handling. several efforts are undertaken, to improve secure software engineering, however, developers still miss or misuse acquired knowledge due to domain immaturity, newness of the field, process complexity and absence of environments supporting such development. this paper presents our approach addressing software application security issues through its development process using a strategy oriented process model. the main feature of the proposed process model is that it provides a two level guidance: 1) a strategic guidance helping the developer to choose one among a compilations of the existing methods, standards and best practices and 2) a tactic guidance helping the developer to achieve his selection. this process model is easily extensible and allows building customized processes adapted to the context, the developer\'s finalities and the product state.addressing software application security issues','Web application security'
'based on the pawlak rough set theory, this paper investigates separations in covering approximation spaces, give some characterizations of these separations and some relations among these separations. as an application of these results, investigations on network security are converted into investigations on separations in covering approximation spaces by taking covering approximation spaces as mathematical models of networks. results of this paper give further applications of the pawlak rough set theory in pattern recognition and artificial intelligence, which makes it possible to research network security by logical methods and mathematical methods in computer science. this contributes to giving risk assessments of securities and to raise grades of securities for networks.an application of covering approximation spaces on network security','Web application security'
'after the information security audit, the auditor commonly points out the importance of information assets, the vulnerability of the audited information system, and the need of countermeasures. on such an occasion, the audited often ask the auditor for the quantitative assessment of the risk so that they can take specific measures. nevertheless, in reality, the auditor can hardly meet this requirement because they do not have any appropriate methods to assess the risk quantitatively and systematically. therefore, this paper proposes the approach that makes it possible to identify the scenarios of information security accidents systematically, to assess the risk of the occurrence of the scenario quantitatively, and to point out the importance of taking countermeasures by incorporating probabilistic risk assessment in information security audit. for the concrete description and explanation of this approach, this paper takes the case of the audit of password management as an example. by enumerating the possible scenarios that indicate how initiating events, the vulnerability of mitigation systems, and the failures of operations can allow illegal accesses to the information assets, this paper shows that it is possible to assess the security risks by the pair of defenseless time span and its occurrence frequency of each scenario. finally, since the parameters necessary for risk quantification such as the occurrence frequency of password theft, the probability of theft detection, and the probability of taking countermeasure after the theft have uncertainty, the uncertainty of the occurrence of the scenario itself is assessed by propagating the incompleteness of the knowledge of these parameters with random digits.an application of probabilistic risk assessment to information security audit','Web application security'
'smart phones equipped with near field communication (nfc) provide a simple way to initiate contactless transactions and data exchange without having the need to carry additional items such as credit cards, personal ids, and access keys. to prevent unauthorized nfc transactions in the case of lost or stolen devices, the user needs to be authenticated before each transaction, which adds extra burden on users. in this paper we propose an nfc security framework that simplifies the initiation of secure nfc transactions. the framework calculates a current measure of device security based on user activities and behavior. nfc transactions are authorized if the current device security measure meets the minimum requirement of the application. the framework uses a combination of authentication methods such as password, pin, pattern, finger print, voice and face recognition. in addition, we propose adjusting the device security level dynamically based on user activities, behavior, and background face and voice authentication. as a case study, the framework has been implemented on the google android platform. the nfc security framework minimizes the need to intrusively authenticate the user for every nfc transaction thus maintaining the simplicity of using nfc while enhancing its security.an application security framework for near field communication','Web application security'
'esa is developing, deploying, and operating a wide variety of mission data systems. these are mainly used for the command & control of spacecraft and the exploitation and dissemination of space-based services to end users. a new esa activity, the european space situational awareness (ssa) initiative, requires a novel generation of mission data systems to be developed. these systems are based on a service-oriented architecture (soa) and capable of supporting a large system-of-systems environment. at the same time, information security is an area of growing concern in the space business and among space agencies. especially in the area of soa-based environments, where interconnectivity of components is a core principle, an efficient and robust security concept needs to be put in place to ensure secure mission operations. in this paper, we describe an application security framework for soa-based mission data systems. this framework increases significantly the robustness and security of web services and web applications through use of a secure software development lifecycle (ssdlc) and provision of tools & templates for ssa mission data system developers. we are confident that the application security framework will drastically improve the security and robustness of soa-based mission data systems that will be used in the european ssa initiative and other esa projects, while at the same time keeping the related additional effort minimal.an application security framework for soa-based mission data systems','Web application security'
'denial of service attacks, viruses and worms are common tools for malicious adversarial behavior in networks. with the increasing ubiquity of personal computing handheld devices, such as mobile phones and pdas, together with deployment of sensor networks, experience shows that over the last few years attacker who are specializing in disrupting and hijacking these wireless peripheral devices are gaining widespread access to potentially lucrative corporate and government information. several of these tools have probably been used increasingly as part of hostile behavior either independently, or in conjunction with other forms of attack in conventional or asymmetric warfare, as well as in other forms of malicious behavior. in this paper we concentrate on distributed denial of service attacks (ddos) detections by applying sensor nodes where one or more attackers generate flooding traffics and direct it from multiple sources towards a selected node. the dynamic and wide range of connectivity in between states (busy, idle, suspend and off) provides means of determining thresholds of normal resource usability and activities in each state within sensor nodes. we then present a technique that can be used for ddos detection based on resource usability of sensor nodes. we measure resource dissipation in different nodes, make comparison and evaluate their resource usability. we use hawk sensor nodes to do experiment on our test-bed to show the positive outcomes that ddos attack on sensor nodes can be detected from the resource usability.an application-driven perspective on wireless devises security','Web application security'
'we investigate trust relationships between and within a security policy and a security mechanism to assess system trust of software application. it has been recognized that trust assessment of security systems in dynamic environments with multiple entities, each with its own changing needs from the security mechanisms, is a complex task. in this paper, we propose a novel architectural approach to assess system trust of service oriented environments. the primary goal of this architecture is to show a way for constructing an automated system for trust assessment of web services. particularly, we consider beliefs of an entity about a specific security mechanism of a service and the behavior of the service. in addition, we present new trust metrics for assessing system trust of a web service. furthermore, trust and trust related issues in literature are reviewed to make clear the pros of our approach for trust assessment.an architectural approach for assessing system trust based on security policy specifications and security mechanisms','Web application security'
'with the rapid development of network technologies and deteriorating of network environment, traditional single-net security system cannot satisfy the security requirement. the excellent security performance of biological systems impels the bio-inspired network security theory to be a hot research area currently. based on bio-inspired multidimensional network security model we have put forward, we have advanced bio-inspired multi-net security system by implementing the functional distribution of different subnets of different transient states in multi-net paralleling structure. firstly, parameter estimation and modified algorithms of hidden markov model are introduced to construct the mathematical mode of b-mns; secondly, the integrated performance of our modified b-mns has been tested and its simulation has been carried out. so the feasibility, validity and high efficiency of our model have been demonstrated theoretically, and practically.analysis and application of bio-inspired multi-net security model','Web application security'
'this paper sets up an analysis model of urban rail transit arrangement based on the application of delphi and ahp. furthermore, this paper proposes a design of a more comprehensive and more scientific evaluation index system of urban rail transit performance assessment. the proposed system calculates every index weight using the weighting method--gl, which is based on differential principle, it appraises comprehensive performance using fuzzy analytic hierarchy process evaluation method. verification of the proposed system was done by conducting an experiment using the guangzhou rail transit systems to measure its performance status., results provide the theoretical foundation for development and correction of guangzhou urban rail transit system.application and analysis of zigbee security services specification','Web application security'
'application level security using an object-oriented graphical user interface','Web application security'
'the security requirements for mobile devices are inherently different from stationary machines. mobility exposes them to different threat environments and excludes them from relying on external physical security. productive application from enterprise, government, and military will invariably deal with sensitive data. a risk management and security framework is needed to protect applications and data on mobile devices when they are lost. we propose an application lockbox concept that compartmentalizes mobile devices at the application level. it combines policy enforcement mechanisms and support for sophisticated access polices to mitigate the exposure when the device is lost. it is a practical approach that improves the security of mobile devices without requiring significant changes in the current mobile technology.application lockbox for mobile device security','Web application security'
'network video makes communication much easier. but data security becomes a troublesome issue. fully using the characteristic of chaotic systems, the embedded system gets designed to enhance the security of network video. the fpga and uclinux are adopted as the platform. the qi hyper chaotic, logistic mapping, baker mapping and cat mapping algorithm is applied to video encryption. combination of on-line and off-line encryption, random encryption and double encryption methods make the security of data stronger. the system has been tested to run a long time and proved to be stable with a high security. as a platform, the system implements the complex algorithms at a very low cost: a 87mhz processor and 10m memory. but it still has the potential to develop more complex algorithms and applications. so the system has a broad prospect both in the area of application and research.application of chaos in network video security','Web application security'
'security has become a main concern in corporate networks. in order to keep a network protected it is necessary to periodically perform security tests to control devices and services, and also identify possible vulnerabilities. never two networks behave the same way; thus, results obtained from security tests may substantially differ from one to another. in this case, trying to manually find a behavior pattern for all networks becomes a difficult task. unsupervised techniques can help security analysts finding certain devices patterns, and also help revealing hidden problems in network security. this paper proposes a solution based on unsupervised techniques to help security analysts handling all the information obtained from security tests in order to detect abnormal groups of devices or atypical system behaviors.application of clustering techniques in a network security testing system','Web application security'
'cobit is a collection of good practices and processes for it governance. it provides the effective measures, indicators and activities for enterprise. cobit has also been applied to the other governance, e. g., software process, security governance, it service management. however, since cobit is too general-purpose, it requires deep expert knowledge for the implementation of each application. although the guideline of security management is also published, its contents are abstract. therefore, we examined the contents of cobit and defined a framework which specializes in security engineering from the guideline. this paper presents the framework and its application to information systems development. the framework effectively utilizes the cobit-based security management and solves various subjects of security in the development.application of cobit to security management in information systems development','Web application security'
'local area network (lan) security evaluation has not only become more important but also affected the lan or internet advancement and development. above all, the lan security characteristics of are analyzed. on the basis, the quality evaluation system framework of lan security indicator is established. fuzzy evaluation index system is established to consider various influential factors. as to indexes system, comprehensive fuzzy evaluation (cfe) model and methods are setup and used to check for practical example. using cfe model and case study, it is concluded that, based on lan security evaluation indexes system, cfe method is used to evaluate and solve the unclear boundary of some lan security indicators and inaccurate fuzziness, which is proved to be practicable.application of comprehensive fuzzy evaluation in lan security','Web application security'
'we present a computationally sound technique of static analysis for confidentiality in cryptographic protocols. the technique is a combination of the dependency flow graphs presented by beck and pingali and our earlier works - we start with the protocol representation as a dependency graph indicating possible flows of data in all possible runs of the protocol and replace the cryptographic operations with constructions which are \"obviously secure\". transformations are made in such a way that the semantics of the resulting graph remains computationally indistinguishable from the semantics of the original graph. the transformed graphs are analysed again; the transformations are applied until no more transformations are possible. a protocol is deemed secure if its transformed version is secure; the transformed versions are amenable to a very simple security analysis. the framework is well-suited for producing fully automated (with zero user input) proofs for protocol security.application of dependency graphs to security protocol analysis','Web application security'
'today, e-commerce has achieved wide applications with its carrier network and information technology\'s rapid developments. but various security problems set up a lot of barriers for its further progress and the security becomes a bottleneck. unless these problems are entirely solved, e-commerce cannot go along smoothly.according to these facts, the article tries to find a systematic security solution for the banks engaged in e-commerce. that is: security management strategy. the security strategy includes four parts: security risk analysis, determination of risk control targets, design and application of concrete strategy, and real-time monitor and audit. the risk analysis is introduced to the bank\'s security management and serves as the start point. by integrating security technology solutions and management solutions tightly, this strategy presents a macroscopical plan, and at the same time, demonstrates the detailed practical methods. finally, security management strategy returns to a new security risk analysis. basing on analyzing the new risks, another strategy starts.as the strategy starts from risk analysis and the course of determinate the security ascends continuously, thereby the bank\'s security solution turns much more close to actual demands, and can function flexibly according to the fast changing environment.so we say, the security management strategy is a good solution for the banks dealing with e-commerce.application of e-commerce security management strategy in banking','Web application security'
'integrated circuit metrics (icmetrics) technology is concerned with the extraction of measurable features of an embedded system, capable of uniquely identifying the system\'s behaviour. any changes in these identifiers (profiles) during consequent devices\' operation would signal about a possible safety or security breach within the electronic system. this paper explores the combination of program counter (pc) and cycles per instructions (cpi) of a processor core as a potential icmetrics source for embedded system security. the use of this combination exhibits that while isolated pc values may not always generate a stable identifier (profile) for a device that would distinguish the device from the rest in a considered set, the pc and cpi sequences and frequencies in the execution flow may serve as suitable icmetrics features.application of icmetrics for embedded system security','Web application security'
'payment security in e-commerce systems is a very important problem that catches many researchers\' and businessmen\'s eyes. in this paper we put forward a solution of applying intelligent-agent into the payment security. we present the intelligent-agent based distributed system architecture and elaborate three kinds of task intelligent agents.application of intelligent agent in network payment security','Web application security'
'my paper mainly introduces some applications of the technologies of the internet of things (iot) which offer capabilities to identify and connect worldwide physical objects into a unified system. with the rapid development of the construction industries, people are eager to get more intelligent living conditions. the intelligent community security system (icss) is becoming one of the biggest applications of the iot. the purposes of the icss are preventing invasion, automatic property management, real-time alarm and quickly treatments of accidents, etc. the system can improve the community security automatically, the management efficiency and economize human resources. this paper describes the basic characteristics and architectures of the iot and introduces an intelligent model to realize a practical and intelligent community management system.application of internet of things in the community security management','Web application security'
'an active xml (axml in short) has been developed to provide efficient data management and integration by allowing web services calls to be embedded in xml document. axml documents have new security issues due to the possibility of malicious documents and attackers. to solve this security problem, document-level security with embedded service calls has been proposed to overcome the limitation of traditional security protocols. the aim of this paper is to show how existing model checking technique, with csp and fdr, used for traditional message-based security protocols, can be adapted to specify and verify axml document-based security. to illustrate our approach, we present the framework for modelling and analyzing axml document&#39;s security. then, we demonstrate how this technique can be applied to analyze electronic patient record taken from [13]. finally, we show the possible vulnerabilities due to delegated query and malicious service call.application of model checking to axml system\'s security','Web application security'
'the increment of intrusions and bad uses in computer systems and internal networks of a great number of companies has caused an increase in concern for computer security. for some time one comes applying measures based on fire walls and in systems of intrusion detection (ids). in this document we present an alternative to the problem of the ids based on rules, using two different neural networks, a multi-layer perceptron, and a self organizative map. a series of experiments are carried out and the results are shown to be better than others found in the literature.application of neural networks in network control and information security','Web application security'
'with the widespread application of large and complicated network, network safety has become an important issue. in this paper, a security operation center (soc) concept based on multi-sensor data fusion technology is presented from the viewpoint of the network security. a structure of a soc system based on radial basis function neural (rbfn) network is proposed, and the detailed method of data fusion in soc is discussed. a prototype of soc system is developed according to this structure of the soc, experimental results indicate that the soc system based on rbfn network can increase greatly the correctness of detection intrusion and decrease the rate of false positive.application of radial function neural network in network security','Web application security'
'with rapid development of computer networks, users need a new solution for network security management, aiming at integration. this paper focuses on context-aware alert analysis, which is one of its key functionalities. a practical and efficient approach to guarantee unified representation of context information, background knowledge and attack knowledge for security alerts is still lacking these days. this paper applies security ontology by means of owl+swrl+owl-s based on cim schema to describe context information and security knowledge in a unified manner. we argue that, our proposed approach improves existing alert analysis techniques by providing formal representations with the use of security ontology, which may possibly be an important stage for implementation of unified network security management.application of security ontology to context-aware alert analysis','Web application security'
'this paper presents the application of support vector machine classifier for security surveillance system. recently, research in image processing has raised much interest in the security surveillance systems community. weapon detection is one of the greatest challenges facing by the community recently. in order to overcome this issue, application of the popularly used support vector machine classifier is performed to focus on the need of detecting dangerous weapons. in this paper, we take advantage of the classifier to categorize images object with the hope to detect dangerous weapons effectively. in order to validate the effectiveness of support vector machine classifier, several classifiers are used to compare the overall accuracy of the system. these classifiers include neural network, decision trees, na&#239;ve bayes and k-nearest neighbor methods. the final outcome of this research clearly indicates that support vector machine has the ability in improving the classification accuracy using the extracted features.application of support vector machine classifier for security surveillance system','Web application security'
'the objective of the poster is to describe the application of topic maps technology in context of e-learning environment used at the university. topic maps documents are presented as tool that can simplify navigation in plenty of various information and knowledge resources: except common digital learning objects, also printed materials or contacts to human experts can be linked to a single topic map document.application of topic maps in e-learning environment','Web application security'
'web services security (ws-security) is a specification that protects soap messages to ensure end-to-end security for web services. ws-security was approved as the oasis standard in april 2004 and the first stage of standardization has been completed. although the interoperability of ws-security itself has been examined, business applications of ws-security have not yet been fully investigated. applying ws-security to actual businesses is the next step. we conducted a large-scale demonstration experiment with web services using a travel industry model. we applied ws-security to travel booking transactions and succeeded in ensuring end-to-end security by signing and encrypting credit card numbers. we give an overview of the experiment, point out the problems experienced and provide a possible solution. the experiment revealed that problems still remain with respect to communication via an intermediary.application of web services security using travel industry model','Web application security'
'the security services within applications have received recent attention. it has been suggested that this may be the only way to increase overall information system assurance in an era where ict governance and compliance have taken on new force and the use of commodity level ict products for critical information systems continues. while it has been argued that an application can be no more secure than its underlying computer subsystems, security at the application layer was always envisaged as playing a major role, e.g. in the \"open systems interconnection (osi)\" security model. at a time when \"end-user\" programming is being advocated, the needs and parameters of security education and training are rapidly changing, and increased threats from global internet connection are rapidly rising, there is a need to reconsider security schemes at the application level. this paper examines current trends in application design, development, deployment and management and evaluates these against known system vulnerabilities and threats.application security - myth or reality?','Web application security'
'the last few years have witnessed a rapid growth in cyber attacks, with daily new vulnerabilities being discovered in computer applications. various security&#45;related technologies, e.g., anti&#45;virus programs, intrusion detection systems &#40;idss&#41;&#38;&#35;47;intrusion prevention systems &#40;ipss&#41;, firewalls, etc., are deployed to minimise the number of attacks and incurred losses. however, such technologies are not enough to completely eliminate the attacks to some extent; they can only minimise them. therefore, software assurance is becoming a priority and an important characteristic of the software development life cycle. application code analysis is gaining importance, as it can help in writing safe code during the development phase by detecting bugs that may lead to vulnerabilities. as a result, tremendous research on code analysis has been carried out by industry and academia and there exist many commercial and open source tools and approaches for this purpose. these have their own pros and cons. therefore, the main objective of this article is to explore the state&#45;of&#45;the&#45;art in code analysis and a few major tools which benefit not only security professionals, but also novice information technology &#40;it&#41; professionals. we study the tools and techniques under the basic four types of analysis &#40;static source code &#40;ssc&#41;, static binary code &#40;sbc&#41;, dynamic source code &#40;dsc&#41; and dynamic binary code &#40;dbc&#41; analysis&#41; and briefly discuss them.application security code analysis&#58; a step towards software assurance','Web application security'
'mobile agents are a distributed computing paradigm based on mobile autonomous programs. mobile applications must balance security requirements with available security mechanisms in order to meet application level security goals. we introduce a trust framework to reason about application security requirements, trust expression, and agent protection mechanisms. we develop application security models that capture initial trust relationships and consider their use for mobile agent security.application security models for mobile agent systems','Web application security'
'application security is typically coded in the application. in kernelsec, we are investigating mechanisms to implement application security in an operating system kernel. the mechanisms are oriented towards providing authorization properties, and this goal drives the design of permissions and protection mechanisms.the resulting system is dynamic, allowing the set of permissions for a program to evolve during program execution. this reduces the need for users and applications to be aware of protection mechanism, since the protection mechanism provides the user with more freedom in how they do things. we explore these properties through a number of examples.kernelsec also supports a group (role) mechanism which can define constrained groups enabling groups which only grow, only shrink, are constant, are mutually exclusive with other groups, and which allow inheritance. moreover groups are used to regulate group membership and allow group administration by non-privileged users.application security support in the operating system kernel','Web application security'
'this paper describes a new approach to collecting real-time transaction information from a server application and forwarding the data to an intrusion detection system. while the few existing application-based intrusion detection systems tend to read log files, the proposed application-integrated approach uses a module coupled with the application to extract the desired information. the paper describes the advantages of this approach in general, and how it complements traditional network-based and host-based data collection methods. the most compelling benefit is the ability to monitor transactions that are encrypted when transported to the application and therefore not visible to network traffic monitors. further benefits include full insight into how the application interprets the transaction, and data collection that is independent of network line speed. to evaluate the proposed approach, we designed and implemented a data-collection module for the apache web server. our experiments showed that the required implementation effort was moderate, that existing communication and analysis components could be used without incurring adaptation costs, and that the performance impact on the web server is tolerable.application-integrated data collection for security monitoring','Web application security'
'we introduce and describe a novel network simulation tool called nessi (network security simulator). nessi incorporates a variety of features relevant to network security distinguishing it from general-purpose network simulators. its capabilities such as profilebased automated attack generation, traffic analysis and interface support for the plug-in of detection algorithms allow it to be used for security research and evaluation purposes. nessi has been utilized for testing intrusion detection algorithms, conducting network security analysis, and developing distributed security frameworks at the application level. nessi is built upon the agent component-ware framework jiac [5], resulting in a distributed and easy-to-extend architecture. in this paper, we provide an overview of the nessi architecture and briefly demonstrate its usage in three example security research projects. these projects comprise of evaluation of stand-alone detection unit performance, detection device deployment at central nodes in the network and comparison of different detection algorithms.application-level simulation for network security','Web application security'
'along with the development of the internet and strengthening of network protocol, it becomes more and more difficult to monitor network. and some sensitive information has been leaked outside the application environment. a major problem with current security monitoring is the large number of newly emerging applications using more complicated communication structures and patterns than traditional applications. the ability to accurately identify the internet traffic associated with different application-level protocols is essential to a security monitoring system. traditional traffic identification method based on well-known port numbers is becoming more inaccurate and not appropriate for the identification of p2p and other new types of traffics. this paper proposes a new method to identify application level traffic.first, we categorized most application level protocols according to their characters. with this classification, we use signatures matching to determine the name of the traffic. finally, a test has been carried out to evaluate the accuracy and efficiency of this methodapplication-level traffic identification of network security monitoring','Web application security'
'we define the notion of the application-oriented security policy and suggest that it differs from that of a system-level, global security policy. we view a policy as a conjunction of security properties and argue that these properties are not always independent and, hence, cannot be analyzed (e.g., composed) individually. we also argue that some necessary policy properties fall outside of the alpern-schneider safety/liveness domain and, hence, are not subject to the abadi-lamport composition principle. we suggest several areas of research in policy definition, composition, and administration.application-oriented security policies and their composition (position paper)','Web application security'
'application-oriented security policies and their composition (transcript of discussion)','Web application security'
'mobile phones are becoming a convergent platform for sensing, computing, and communications. this talk will describe our attempts to jointly harness these capabilities, thereby enabling new classes of personal and collaborative applications. our long term vision is to develop a search engine that will run on a platform of scattered mobile devices, and allow internet users to browse and search content in the physical world. as an example, an internet user may be able to view the eiffel tower in real time by stitching the camera views from different phones around that location; another user may be able to query mobile phones inside a specific coffee shop to learn if the shop offers free wifi; yet another user may be able to search for quiet study places in the campus by aggregating ambient sound measurements from phones. this talk will revolve around envisioning a variety of such applications and carving out research questions that underlie them.applications and application-driven research','Web application security'
'as users download applications to their mobile phones, security is a critical issue. in this paper we present a process for the security assurance of applications. it uses existing vulnerability databases and application development guidelines to identify potential security issues. the identified issues are then validated using a variety of techniques including black-box testing, unit testing code inspection and static analysis. this process is illustrated using an application for the android platform.applying security assurance techniques to a mobile phone application','Web application security'
'probable security is an important criteria for analyzing the security of cryptographic protocols. however, writing and verifying proofs by hand are prone to errors. this paper introduces the game-based approach of writing security proofs and its automatic technique. it advocates the automatic security proof approach based on process calculus, and presents the initial game and observational equivalences of oaep+.automatic approach of provable security and its application for oaep+','Web application security'
'pervasive computing is providing its usability and scope in almost every aspect nowadays. in order to make better use of pervasive services in nomadic devices, pervasive client download might be needed, which would result in serious security problems due to executing untrusted applications. recently security-by-contract has been proposed to address this problem, where an application is required to come with the contract containing a description of the relevant security features while mobile platform will match the contract with its own policy. in this paper a compositional approach to specifying security contract is introduced in the form of extended context free grammar. then a framework for automatic generation and enforcement of security contract has been presented for java platform. the main contributions of this paper include: (1) formal definition of security contract is given in the terms of security related operations and the relationship among arguments of these operation; (2) static analysis is utilized to automatically generate security contract for java source program. the security contract of a java program can be composed from those of all the methods it invokes; (3) runtime enforcement has been applied to security contracts and achieved by implementing execution monitor in jvm.automatic generation and enforcement of security contract for pervasive application','Web application security'
'computed tomography (ct) is used widely to image patients for medical diagnosis and to scan baggage for threatening materials. automated reading of these images can be used to reduce the costs of a human operator, extract quantitative information from the images or support the judgements of a human operator. object quantification requires an image segmentation to make measurements about object size, material composition and morphology. medical applications mostly require the segmentation of prespecified objects, such as specific organs or lesions, which allows the use of customized algorithms that take advantage of training data to provide orientation and anatomical context of the segmentation targets. in contrast, baggage screening requires the segmentation algorithm to provide segmentation of an unspecified number of objects with enormous variability in size, shape, appearance and spatial context. furthermore, security systems demand 3d segmentation algorithms that can quickly and reliably detect threats. to address this problem, we present a segmentation algorithm for 3d ct images that makes no assumptions on the number of objects in the image or on the composition of these objects. the algorithm features a new automatic quality measure (aqua) model that measures the segmentation confidence for any single object (from any segmentation method) and uses this confidence measure to both control splitting and to optimize the segmentation parameters at runtime for each dataset. the algorithm is tested on 27 bags that were packed with a large variety of different objects.automatic segmentation of unknown objects, with application to baggage security','Web application security'
'we present an automatic test approach to improve the security of web application, which detects vulnerable spots based on black box test through three phases of craw, test, and report. the test process considers a blind point for security through the development life cycle, the faults of web application and server setup in a various point of attackers, etc. the test approach is applied to the web applications in industry, analyzed, and compared with the existing test tool.automatic test approach of web application for security (autoinspect)','Web application security'
'nowadays, with the rapid development of printing and publishing industry, security printing technology becomes more and more important. meanwhile, security pattern design is one of the most important techniques of security printing. a well designed security pattern should be difficult to counterfeit in any way. therefore it can be used in printed security documents, such as banknotes, passports and certificates, etc. in this paper, based on the generalization of coons patch construction, we propose a novel scheme of security pattern design. the practical pattern examples are presented, and some other applications of the novel method are also discussed.bilinear coons patch and its application in security pattern design','Web application security'
'as security professionals we have a good handle on securing our perimeters, yet security compromises continue to rise. hackers have found a new attack vector and are successfully exploiting it. application exploits are to blame for this rise in security compromises and security professionals need to identify and secure the application. while risk cannot be completely eliminated, a strong application security program can identify and mitigate these risks to a more manageable level. organizational support, framework selection, and adherence to compliance and regulatory requirements are vital to the success of the program and the security of your applications. if you lack any of these elements the program will fail. there are many frameworks to choose from, so careful consideration must be taken to ensure the right framework is chosen for your organization. a successful application security program will be fully integrated within the sdlc. it will enable your organization to identify and remediate risks with applications. if implanted and executed effectively it will also meet the requirements for fisma compliance.building an application security program','Web application security'
'business-driven development and management of secure applications and solutions is emerging as a key requirement in the realization of an on demand enterprise. in a given enterprise, individuals acting in various roles contribute to the modeling, development, deployment, and management of the security aspects of a business application. we look at the business-application life cycle and propose a policy-driven approach overlaid on a model-driven paradigm for addressing security requirements. our approach suggests that security policies are to be modeled using policies and rule templates associated with business processes and models, designed and implemented through infrastructure-managed or application-managed environments based on modeled artifacts, deployed into an infrastructure and potentially customized to meet the security requirements of the consumer, and monitored and managed to reflect a consistent set of policies across the enterprise and all layers of its application infrastructure. we use a pragmatic approach to identify intersection points between the platform-independent modeling of security policies and their concrete articulation and enforcement. this approach offers a way to manage and monitor systems behavior for adherence and compliance to policies. monitoring may be enabled through both information technology (it) and business dashboards. systematic approaches to connect business artifacts to implementation artifacts help implement business policies in system implementations. best practices and security usage patterns influence the design of reusable and customizable templates. because interoperability and portability are important in service-oriented architecture (soa) environments, we list enhancements to standards (e.g., business process execution language [bpel], unified modeling languagetm [uml&#174;]) that must be addressed to achieve an effective life cycle.business-driven application security','Web application security'
'can the cloud truly be secured? can enterprises, universities, small businesses and governments securely utilize the cloud for their critical infrastructure? it will take rethinking our current security policies and what we consider secure. this session will cover what is necessary to utilize the cloud securely today and how the cloud should adapt for the future.cloud security','Web application security'
'automatic model checking can be employed to verify that security properties are fulfilled by a system model. however, since security requirements constrain most, if not all, functional modules of a system, such a proof needs to consider nearly all of the system\'s control and data flows. for complex real-life applications, that leads to a large state space to be explored effectively restricting the applicability of a model checker. to deal with this problem, we advocate a compositional approach utilizing the features of our model-based engineering technique space. both functional behavior and security-related aspects are specified using uml 2 activities. further, we supplement each activity with an interface behavior description which will be extended by a security contract modeling certain security properties to be fulfilled by the activity. this enables us to verify application-level security properties by using contracts instead of their respective activities in model checker runs so that the number of states to be checked is significantly reduced. the approach is exemplified by an android application example in which one\'s location must only be shared with certain recipients.compositional verification of application-level security properties','Web application security'
'computer network security','Web application security'
'to assure network security, the paper constructs a kind of reasonable security defense model based on analyzing security character for physical layer, network layer, system layer, application layer, and management layer of network. supporting by router, ethernet switch, firewall, network management, and operational platform, the model is a perfect defense system which composed by aspects of security authentication, access control, filter examination, scan, detection, vpn, audit analysis, policy service. last, combining e-government application, a practical example is presented to improve security performance of network.construction and application of network security defense model','Web application security'
'this paper focuses on the emerging importance of database and application security, textbooks and other supplementary materials to teach these topics and where to place these topics in a curriculum. the paper emphasizes 1) the growing concerns of database technologies and database applications, 2) existing books and supporting materials, and 3) and zayed university\'s approach to teaching these topics. at zayed, we incorporate database and database application security in two different courses.database and database application security','Web application security'
'there is a large amount of work dedicated to the formal verification of security protocols. in this article, we revisit and extend the np-complete decision procedure for a bounded number of sessions. we use a, now standard, deducibility constraint formalism for modeling security protocols. our first contribution is to give a simple set of constraint simplification rules, that allows to reduce any deducibility constraint to a set of solved forms, representing all solutions (within the bound on sessions). as a consequence, we prove that deciding the existence of key cycles is np-complete for a bounded number of sessions. the problem of key-cycles has been put forward by recent works relating computational and symbolic models. the so-called soundness of the symbolic model requires indeed that no key cycle (e.g., enc(k, k)) ever occurs in the execution of the protocol. otherwise, stronger security assumptions (such as kdm-security) are required. we show that our decision procedure can also be applied to prove again the decidability of authentication-like properties and the decidability of a significant fragment of protocols with timestamps.deciding security properties for cryptographic protocols. application to key cycles','Web application security'
'with the incoming of information era, web-based service has been developed rapidly and offered more and more business. these &#8220;open&#8221;, and widely &#8220;web enabled&#8221; applications are subject to greater and greater levels and types of attacks as hackers exploit vulnerabilities within the software like sql injection and cross site scripts (xss) attack. in this paper, we proposed a type of novel embedded markov model (emm) to detect different web application attacks, monitor the on-line user behavior and defend the malevolent user promptly. comparing to previous web application attacks detecting approaches, our emm approach can not only detect user&#8217;s invalidated input errors but also find out the unreasonable page transition behavior. by detecting unreasonable page transition, we can immediately defend the malevolent or silly user behavior to avoid the further web system failures and sensitive information disclosure. furthermore, we implement an on-line user behavior surveillance system and use the real web traffic to evaluate the performance of our system. the experiment results show that our proposed emm method can discover the abnormal behavior of malevolent user and detect the invalidated input attacks like sql injection, xss and string buffer overflow attacks.defending on-line web application security with user-behavior surveillance','Web application security'
'design of a security platform for corba based application','Web application security'
'in this paper, we provide a novel analytical hierarchy process decision-making framework for web service security profiles. this framework aids in solving the dilemma of which web service security profile is most fitting in a particular situation. this is because the developed framework allows architects and developers to take informed decisions following a systematic and manageable approach. in developing the framework, we followed the design-science research paradigm within which we incorporated a number of laboratory experiments.developing a decision-making framework for web service security profiles','Web application security'
'development of an intranet security infrastructure and its application','Web application security'
'recent activities in applications security research and industry have inspired companies and researchers to create a standard way of defining and describing vulnerabilities, expressing different application needs with respect to various aspects of vulnerabilities, making vulnerability descriptions accessible to disparate computer systems. this article describes the state of the art today and possible directions in the future. it summarizes industry efforts and focuses on application security related xml schemas being developed within oasis.dissecting application security xml schemas','Web application security'
'dna computing is a new computational paradigm by harnessing the potential massive parallelism, high density information of bio-molecules and low power consumption, which brings potential challenges and opportunities to traditional cryptography. in this paper, on the basis of reviewing the principle of dna computing and the development situation of dna computing briefly, we analyze some schemes with secret key searching and introduce the application of dna computing in encryption, steganography and authentication.dna computing and its application to information security field','Web application security'
'this is a case study on using a dsp board to construct an encryption/decryption module embedded in a e-commerce web server. the idea of using dsp is to push beyond the key length limits of encryption/decryption algorithms and computational power in a software environment while avoiding the heavy investment in a dedicated hardware encryptor/encryptor. the low cost, high computational power, high flexibility of dsp and the ubiquitous availability of a pc peripheral component interconnect slot for the dsp can provide any web browser or web server an excellent cost-effective option to improve the security level of internet applications. the paper provides a step-by-step procedure and reveals every detail of a successful implementation of a dsp rsa encryptor/decryptor for an e-commerce web server by using the latest tms320c6000/sup tm/ evaluation module (evm) dsp hardware. a strong prime concept and garner algorithm are introduced to generate more secure keys and compute encryption/decryption more efficiently than that of recent publications. experiments show that the performance of using dsp hardware encryption can be 300 times faster than that in software environment.dsp application in e-commerce security','Web application security'
'although there is a large body of research on detection and prevention of such memory corruption attacks as buffer overflow, integer overflow, and format string attacks, the web application security problem receives relatively less attention from the research community by comparison. the majority of web application security problems originate from the fact that web applications fail to perform sanity checks on inputs from the network that are eventually used as operands of security-sensitive operations. therefore, a promising approach to this problem is to apply proper checks on tainted portions of the operands used in security-sensitive operations, where a byte is tainted if it is data/control dependent on some network packet(s). this paper presents the design, implementation and evaluation of a dynamic checking compiler called wasc, which automatically adds checks into web applications used in three-tier internet services to protect them from the most common two types of web application attacks: sql- and script-injection attack. in addition to including a taint analysis infrastructure for multi-process and multi-language applications, wasc features the use of sql and html parsers to defeat evasion techniques that exploit interpretation differences between attack detection engines and target applications. experiments with a fully operational wasc prototype show that it can indeed stop all sql/script injection attacks that we have tested. moreover, the end-to-end latency penalty associated with the checks inserted by wasc is less than 30\% for the test web applications used in our performance study.dynamic multi-process information flow tracking for web application security','Web application security'
'recent paradigms like &#8220;database as a service&#8221; require an additional infrastructure to guarantee data security. data protection laws such as hipaa (health insurance portability and accountability act), gramm-leach-bliley act of 1999, data protection act, sarbanes oxleys act are demanding for the data security to an extent that the critical information should be seen only by the authorized users which means the integrity and confidentiality of the database must be properly accommodated. hence we aim at building up a wrapper/interface in between encrypted database server and applications that ensures the data privacy and integrity. specifically, we worked on querying over encrypted databases and our approach produces query results on encrypted data with no false hits and hence reduces the network consumption between applications and encrypted server.eisa &#8211; an enterprise application security solution for databases','Web application security'
'in this article we describe an extension of java archives that allows to keep data encrypted for multiple recipients. encrypted data is accessible only by selected access groups. java archives may be used as containers of mobile agents, which allows agents to keep confidential data unaccessible while residing on untrusted hosts. however, additional protective measures are required in order to prevent cut & paste attacks on mobile agents by malicious hosts. one such mechanism is described. the usefulness of the concepts is illustrated by an example application for user profile management in an electronic commerce setting.encrypting java archives and its application to mobile agent security','Web application security'
'web applications are important, ubiquitous distributed systems whose current security relies primarily on server-side mechanisms. this paper makes the end-to-end argument that the client and server must collaborate to achieve security goals, to eliminate common security exploits, and to secure the emerging class of rich, cross-domain web applications referred to as web 2.0. in order to support end-to-end security, web clients must be enhanced. we introduce mutation-event transforms: an easy-to-use client-side mechanism that can enforce even fine-grained, application-specific security policies, and whose implementation requires only straightforward changes to existing web browsers. we give numerous examples of attractive, new security policies that demonstrate the advantages of end-to-end web application security and of our proposed mechanism.end-to-end web application security','Web application security'
'existing policy enforcement points (peps) typically call a local policy decision point (pdp) running at the local site, either embedded in the application, or running as a local stand alone service. in distributed applications, the pdps at each site do not usually coordinate decision making amongst themselves, and do not pass policies between themselves. thus it becomes very difficult to enforce \"sticky\" policies such as privacy policies and obligations at all the sites in a distributed application. this paper looks at different ways in which the peps and pdps of a distributed application may share policies between themselves so as to enforce \"sticky\" policies throughout a distributed application. three alternative models are described, the application protocol enhancement model, the encapsulating security layer model and the back channel model. the strengths and weaknesses of the three models are evaluated, and we compare them to prior research in the field.enforcing \"sticky\" security policies throughout a distributed application','Web application security'
'ws-security and ws-security policy are the common standards for ensuring integrity and confidentiality for web service messages. on the one hand they allow very flexible definition of security requirements. on the other hand they lead to complex security administration and low performance message processing. in this paper, we present our solution for a security gateway, which uses complete event-based xml and ws-security processing to create policy conforming soap messages. the evaluation of our implementation shows that the event-based approach leads to a much better performance than tree-based ws-security implementations. further, we discuss some problematical issues of ws-security policy processing, such as determination of digital identities.event-based application of ws-security policy on soap messages','Web application security'
'how should software engineers choose which tools to use to develop secure web applications? different developers have different opinions regarding which language, framework, or vulnerability-finding tool tends to yield more secure software than another; some believe that there is no difference at all between such tools. this paper adds quantitative data to the discussion and debate. we use manual source code review and an automated black-box penetration testing tool to find security vulnerabilities in 9 implementations of the same web application in 3 different programming languages. we explore the relationship between programming languages and number of vulnerabilities, and between framework support for security concerns and the number of vulnerabilities. we also compare the vulnerabilities found by manual source code review and automated black-box penetration testing. our findings are: (1) we do not find a relationship between choice of programming language and application security, (2) automatic framework protection mechanisms, such as for csrf and session management, appear to be effective at precluding vulnerabilities, while manual protection mechanisms provide little value, and (3) manual source code review is more effective than automated black-box testing, but testing is complementary.exploring the relationship betweenweb application development tools and security','Web application security'
'the attack graph is an abstraction that reveals the ways an attacker can leverage vulnerabilities in a network to violate a security policy. when used with attack graph-based security metrics, the attack graph may be used to quantitatively assess security-relevant aspects of a network. the shortest path metric, the number of paths metric, and the mean of path lengths metric are three attack graph-based security metrics that can extract security-relevant information. however, one\'s usage of these metrics can lead to misleading results. the shortest path metric and the mean of path lengths metric fail to adequately account for the number of ways an attacker may violate a security policy. the number of paths metric fails to adequately account for the attack effort associated with the attack paths. to overcome these shortcomings, we propose a complimentary suite of attack graph-based security metrics and specify an algorithm for combining the usage of these metrics. we present simulated results that suggest that our approach reaches a conclusion about which of two attack graphs correspond to a network that is most secure in many instances.extending attack graph-based security metrics and aggregating their application','Web application security'
'a number of effective error detection tools have been built in recent years to check if a program conforms to certain design rules. an important class of design rules deals with sequences of events asso-ciated with a set of related objects. this paper presents a language called pql (program query language) that allows programmers to express such questions easily in an application-specific context. a query looks like a code excerpt corresponding to the shortest amount of code that would violate a design rule. details of the tar-get application\'s precise implementation are abstracted away. the programmer may also specify actions to perform when a match is found, such as recording relevant information or even correcting an erroneous execution on the fly.we have developed both static and dynamic techniques to find solutions to pql queries. our static analyzer finds all potential matches conservatively using a context-sensitive, flow-insensitive, inclusion-based pointer alias analysis. static results are also use-ful in reducing the number of instrumentation points for dynamic analysis. our dynamic analyzer instruments the source program to catch all violations precisely as the program runs and to optionally perform user-specified actions.we have implemented the techniques described in this paper and found 206 errors in 6 large real-world open-source java applica-tions containing a total of nearly 60,000 classes. these errors are important security flaws, resource leaks, and violations of consis-tency invariants. the combination of static and dynamic analysis proves effective at addressing a wide range of debugging and pro-gram comprehension queries. we have found that dynamic analysis is especially suitable for preventing errors such as security vulner-abilities at runtime.finding application errors and security flaws using pql','Web application security'
'forward secrecy and its application to future mobile communications security','Web application security'
'we describe an approach to learning patterns in relational data represented as a graph. the approach, implemented in the subdue system, searches for patterns that maximally compress the input graph. subdue can be used for supervised learning, as well as unsupervised pattern discovery and clustering. we apply subdue in domains related to homeland security and social network analysis.graph-based relational learning with application to security','Web application security'
'this paper analyzes current threats in computer security for web-based applications with a sql database. we conduct a penetration test in a real-case scenario of multiple attacks against the network, the web application and the sql database. the test infrastructure includes two servers, a firewall and one machine that acts as an attacker\'s computer. based on our empirical analysis we diagnose specific vulnerabilities and we formulate best practices to improve security against common attack. the article contributes to the discussion of state-of-the art security techniques and illustrates the value of penetration testing for diagnosing attacks against specific technologies.guidelines for discovering and improving application security','Web application security'
'computers are notoriously insecure, in part because application security policies do not map well onto traditional protection mechanisms such as unix user accounts or hardware page tables. recent work has shown that application policies can be expressed in terms of information flow restrictions and enforced in an os kernel, providing a strong assurance of security. this paper shows that enforcement of these policies can be pushed largely into the processor itself, by using tagged memory support, which can provide stronger security guarantees by enforcing application security even if the os kernel is compromised. we present the loki tagged memory architecture, along with a novel operating system structure that takes advantage of tagged memory to enforce application security policies in hardware. we built a full-system prototype of loki by modifying a synthesizable sparc core, mapping it to an fpga board, and porting histar, a unix-like operating system, to run on it. one result is that loki allows histar, an os already designed to have a small trusted kernel, to further reduce the amount of trusted code by a factor of two, and to enforce security despite kernel compromises. using various workloads, we also demonstrate that histar running on loki incurs a low performance overhead.hardware enforcement of application security policies using tagged memory','Web application security'
'hiding data and code security for application hosting infrastructure','Web application security'
'in a service oriented architecture, certain requirements can be tested by observing the interface of the service whereas other requirements such as data privacy, confidentiality and integrity cannot be tested in this way. after deployment, a requirements monitor is used to analyze the conformance of a web service to such requirements. the integrity of the reported conformance results is as good as of the integrity of the monitor especially when the requirements monitor is executing in an untrustworthy environment. in this paper, we propose a hardware-based dynamic attestation mechanism to validate the integrity of the requirements monitor. to evaluate our approach, we have conducted a case study using a commercial requirements monitor and a collection of web service implementations available with apache axis. our case study demonstrates the feasibility of verifying the conformance of a web service executing in an untrustworthy environment.how to trust a web service monitor deployed in an untrusted environment?','Web application security'
'human-centric security service and its application in smart space','Web application security'
'the purpose of this paper is to review the capabilities of the six sigma and lean enterprise methodologies, and provide an illustration how the capabilities can be delivered under a uniform structure such as lean six sigma. stope will then be reviewed along with the mapping to iso/iec 17799. finally, the planned release of iso/iec 27004 and the applicability of lean and six sigma methodologies will be discussed.identification of process improvement methodologies with application in information security','Web application security'
'include application security in your powerbuilder applications','Web application security'
'inside risks: risks in email security','Web application security'
'it\'s difficult to transition application security initiatives from identifying vulnerabilities after software has been produced to proactively mitigating vulnerabilities during the entire software development process. learn about a simple approach for introducing application security into ongoing software development projects.integrating application security into software development','Web application security'
'authentication plays a very critical role in security related applications like ecommerce. there are a number of methods and techniques for accomplishing this key process. biometrics is gaining increasing attention in these days. security systems, having realized the value of biometrics, use biometrics for two basic purposes: to verify or identify users. the use of fingerprints, facial characteristics and other biometrics for identification is becoming more common. the paper overviews best of biometric application for security management. the acquisition of biometric data introduces human research and privacy concerns that must be addressed by the organizations. this paper focus iris is the best biometric feature for identity management.iris biometrics recognition application in security management','Web application security'
'advances in network technology and computing systems are leading to a new model of distributed system. such a model stems from integration of application and provides users with services that are accessible via a web-based infrastructure regardless of client\'s physical location and/or devices. we recognize that even though middleware technologies make easier the diversity and heterogeneity problem, they do not completely solve it. the paper provides general information about key points in building web-based services such as session management and web service security.key issues in building web-based services','Web application security'
'mine! mine! mine!','Web application security'
'along with traditional applications that record, process, and report financial transactions, virtually all organizations use end user computing applications (eucas) such as microsoft (ms) excel spreadsheets, ms access databases, business intelligence (bi) reports, and ms word documents for data storage, calculations, and reporting. these eucas are important components of the financial reporting process and are &#8220;data feeders&#8221; to erp applications such as sap&#174;, oracle financials&#174;, and peoplesoft&#174;. eucas are also key elements in supporting processes like quote to collect, procure to pay, planning/budgeting, and financial reporting. in this paper, we investigate the components of a typical euca, define the risks associated with reliance on such systems, and provide solutions that can be deployed to partially address the standard security concerns (e.g., confidentiality, integrity, availability). the need to mitigate the risks of sensitive information assets is in direct alignment the objectives of mission assurance and operational resilience for private sector organizations.mitigating security risks for end user computing application (euca) data','Web application security'
'measurement units and knowledge of security properties are hardly known. this process causes the complex systems decomposition into simpler and smaller systems thus allowing the estimative of properties that will help the understanding and measurement of software systems security properties. this process provides the security model and the score of security attributes priority is calculated by ahp methodology. a security model example to illustrate this approach is presented.models for measuring access security of web application','Web application security'
'in service oriented architecture (soa) environment, the communication and infrastructure security is crucial. the most important specification addressing web services security is ws-security, which collaborates with the soap message specifications, providing integrity, confidentiality and authentication for web services. however, ws-security focuses soap message security between trusted partners. in soa applications, there are other vulnerabilities which can be exploited to attack by anonymous customer or even trusted partners, and these vulnerabilities do not gain enough attention as ws-security. among them, denial-of-service (dos) is one attack cluster, which exhausts computer and network resources and reduces the availability of web services. another one is sensitive data leakage in a specific application domain. in this paper, the security of soa applications is viewed as the security domain and a three-tier domain was divided based on security domain analysis. for each security sub-domain, security requirement scenario and requirements are presented. the security domain models were given which can be used to build up security services for sub-domain. based on security model and security service assets, which can evolve along with understanding on security domain, the developers can establish the security implementation for soa application integration.multi-tier security feature modeling for service-oriented application integration','Web application security'
'this paper propose new measurement method also know as s-vector based on two security standards iso 17799:2005 and sse-cmm v3.0, which can be an assessment tool for web application security. s-vector consists of three components, there are procedural, structural and technical aspects. result suggests that security controls outlined in iso 17799:2005 can be incorporated into s-vector as procedural and structural components. iso 17799 controls may be mapped to specific data, specific web applications, or across multiple systems. eleven of sse-cmm\'s security-related process areas can be implemented into an s-vector implementation by providing a framework in which to administer procedural components. the capability levels of sse-cmm measure a process\' maturity and can be integrated into s-vector if scoring objectives are to measure process maturity and not the quality of process output.new measurement method for web application security','Web application security'
'in this paper, we present a newsletter engine application, with which newsletters can be created and sent to different subscribers. the application permits to users to create or delete newsletter articles, to select a visual template of the newsletter\'s layout and to send the newsletters to its subscribers.newsletter engine application','Web application security'
'the world wide web has become a sophisticated platform capable of delivering a broad range of applications. however, its rapid growth has resulted in numerous security problems that current technologies cannot address. researchers from both academic and private sector are devoting a considerable amount of resources to the development of web application security scanners (i.e., automated software testing platforms for web application security auditing) with some success. however, little is known about their potential side effects. it is possible for an auditing process to induce permanent changes in an application\'s state. due to this potential, we have so far avoided large-scale empirical evaluations of our web application vulnerability and error scanner (waves). in this paper we introduce a testing methodology that allows for harmless auditing, define three testing modes-heavy, relaxed, and safe modes, and report our results from two experiments. in the first, we compared the coverage and side effects of the three scanning modes using 5 real-world web applications chosen from the 38 found vulnerable in a previous static verification effort. in the second, we used the relaxed mode to conduct a 48-hour test involving 1120 random websites, of which 55 were found to be vulnerable.non-detrimental web application security scanning','Web application security'
'reputation plays a critical role in managing trust in decentralized systems. quite a few reputation-based trust functions have been proposed in the literature for many different application domains. however, one cannot always obtain all information required by the trust evaluation process. for example, access control restrictions or high collect costs might limit the ability gather all required records. thus, one key question is how to analytically quantify the quality of scores computed using incomplete information. in this paper, we start a first effort to answer the above question by studying the following problem: given the existence of certain missing information, what are the worst and best trust scores (i.e., the bounds of trust) a target entity can be assigned? we formulate this problem based on a general model of reputation systems, and examine the monotonicity property of representative trust functions in the literature. we show that most existing trust functions are monotonic in terms of direct missing information about the target of a trust evaluation.poster','Web application security'
'information risk security management is an area that is constantly moving to respond to new threats, standards and technologies. security is now a part of information risk management, which in turn has a place in the overall business risk management strategy. the security model can help with explaining why security is important, and can support justifications for that \'rather expensive\' piece of technology, depending on the point of view, security policy and business appetite for risk.practical application of information security models','Web application security'
'as cellular telephones and high capacity memory sticks emerge as users\' primary repository for data and applications, users will often run applications and display data on remote hosts. the biggest challenge in supporting this mobile data, mobile applications, stationary platform model is ensuring the security and privacy of user applications and data during execution on the remote platforms. private computing on public platforms (pcpp) is a new application security approach which isolates applications to allow for secure and private execution on third party systems. this paper introduces pcpp and details its five basic building blocks which together ensure that the pcpp protected application\'s executable code, context, and data remain unaltered, unmonitored, and unrecorded before, during, and after exposure to the remote platform. copyright &#169; 2009 john wiley & sons, ltd. private computing on public platforms (pcpp) is a new application security approach which isolates applications to allow for secure and private execution on remote third party systems. this paper introduces pcpp and details its five basic building blocks which together ensure that the pcpp protected application\'s executable code, context, and data remain unaltered, unmonitored, and unrecorded before, during, and after exposure to the remote platform.private computing on public platforms: portable application security','Web application security'
'researchers at telcordia technologies and the state university of new york, stony brook, are working on an approach that gives computer users new capabilities for defending against exploitation of application security vulnerabilities by allowing rapid development and deployment of real-time defenses. their solution involves monitoring and changing an application\'s behavior by intercepting the system calls it requests.remediation of application-specific security vulnerabilities at runtime','Web application security'
'the article studies the process that the double signature in electronic payment system process cannot resist replay attack and gives an improved scheme, and then analyzes the security of the scheme. it is proved to be able to resist the replay attack after the analysis of the scheme.research and application of the stronger security dual signature','Web application security'
'with the wide application of web technology, web database system is playing an increasingly important role in network application. the storage amount of web database information is growing, at the same time, the security of web database has been facing extremely daunting challenges. how to strengthen the security of web database system has become an important issue of network researches. this paper has introduced implementation techniques of database security, described the sub-key encryption algorithm of database in details, and applied it to the management system of information student.research and application of web database security technology','Web application security'
'a detection method based on behaviors of running mobile applications is proposed in this paper. it can detect all mobile application software and judge whether they are tampered or harmful. further more, applications are signed and then the authoritative white list library can be established. therefore, the events of mutual malicious detection between companies can be avoided.research on behavior-based detection method for mobile application security','Web application security'
'we propose an rfid-based ale application framework (aaf) providing context-aware security services, which could dynamically adapt security policies. from the proposed framework, we can construct rfid application fast and efficiently through general, reusable, and extensible api due to the software reusability. the proposed model consists of an adaptive security level algorithm based on maut and simple heuristics. the security level algorithm could adopt diverse security services according to the contextual information in the network environment. therefore, the proposed model is expected to provide more flexible security management in the heterogeneous network environments.rfid-based ale application framework using context-based security service','Web application security'
'this position paper outlines a staged approach to search-based application security testing. in the first stage one searches for candidate tests in the input space that have a chance of leading to good security tests. in the second stage one selects individual candidates and uses them to select and parametrize specialized search techniques. this approach has its roots in exploratory security testing. in the first stage, the fitness of tests depends on their ability to provoke vulnerability symptoms at all, and on their relation to other tests in a test suite. in the second stage, the fittest tests are those that come closest to an exploit of a specific type of vulnerability. to evaluate the performance of such a staged approach one might use web application vulnerability scanners as a baseline.search-based application security testing','Web application security'
'mobile ad hoc networks offer quick and easy deployment of network in situations where it is not possible otherwise. manet offer unique benefits and versatility if the environment and application is appropriate and m-governance is one such application. manet can be the best option for m-governance services where there is no predefined infrastructure. due to this reason manets can be easily adapted to m-governance. because of their ad hoc nature manets are vulnerable to security attacks. most of the research in manets has focused on routing issues and security has been given a low priority. the layered architecture draws huge support with its success in case of internet. the cross layer design architecture is becoming more popular with its performance improvements. in this paper, security architecture is proposed for cross layer design architecture of manet. this paper analyzes the security mechanism in m-governance applications in scope of proposed security architecture.security architecture for manet and its application in m-governance','Web application security'
'vulnerabilities in network protocol software have been problematic since internet infrastructure was deployed. these vulnerabilities damage the reliability of network software and create security holes in computing environment. many critical security vulnerabilities exist in application network services of which specification or description has not been published. in this paper, we propose a security assessment methodology based on fault injection techniques to improve reliability of the application network services with no specifications published. we also implement a tool for security testing based on the proposed methodology. windows rpc network services are chosen as an application network service considering its unknown protocol specification and are validated by the methodology. it turns out that the tool detects unknown vulnerabilities in windows network module.security assessment for application network services using fault injection','Web application security'
'web services (ws) have become a significant part of the web because of such attractive features as simple to use, platform independence, and xml/soap support. however, these features make ws vulnerable to many new and inherited old security threats. semantic ws, which are capable of publishing semantic data about their functional and nonfunctional properties, add even more security issues. now, it becomes easier to attack ws because their semantic data is publicly available. to register and prevent these attacks, especially distributed attacks, new distributed firewalls and intrusion detection systems (f/ids) have to be applied. however, these f/ids can be developed by different vendors and they do not have the way to cooperate with each other. this problem can be solved if various f/ids share a common vocabulary, which can be based on ontologies, to allow them to interact with each other. in this paper, we describe ws security threats and state that they have to be analysed and classified systematically in order to allow the development of better distributed defensive mechanisms for ws using f/ids. we choose ontologies and owl/owl-s over taxonomies because ontologies allow different parties to evolve and share a common understanding of information which can be reasoned and analysed automatically. we develop the security attack ontology for ws and illustrate the benefits of using it with an example.security attack ontology for web services','Web application security'
'automated composition of semantic web services has become one of the recent critical issues in today\'s web environment. despite the importance of ai-planning techniques for web service composition, previous works in that area do not address security issues, which is the focus of this paper. based on our prior work, i.e. aimo, we present an approach called scaimo to achieve security conscious composition of semantic web services. moreover, we propose a secure task matchmaker which is responsible of matching security conscious tasks with operators and methods based on description logics and web service modeling ontology (wsmo). we test our approach on a case study and the result shows scaimo can provide an applicable solution.security conscious composition of semantic web services based on ai planning and description logic','Web application security'
'security considerations for active messages','Web application security'
'shows a management-oriented description of the background, design principles and security measures of the austrian nationwide electronic purse scheme (\"quick\"), as well as an experience report about the ongoing process of stating a national security policy and evaluating the level of security for the electronic purse. the considerations address decision-makers and experts interested in the practical issues of maintaining payment security. the author first describes the development and features of the austrian \"paychip\" application (a chip-card based payment infrastructure by europay austria), focusing on the quick electronic purse. the objectives of the security policy are then discussed, and the author finally explains the background and practical experience of the security evaluation process.security measures for the austrian \"paychip\" electronic purse application','Web application security'
'with the widespread use of electronic health record (ehr), building a secure ehr sharing environment has attracted a lot of attention in both healthcare industry and academic community. cloud computing paradigm is one of the popular healthit infrastructure for facilitating ehr sharing and ehr integration. in this paper we discuss important concepts related to ehr sharing and integration in healthcare clouds and analyze the arising security and privacy issues in access and management of ehrs. we describe an ehr security reference model for managing security issues in healthcare clouds, which highlights three important core components in securing an ehr cloud. we illustrate the development of the ehr security reference model through a use-case scenario and describe the corresponding security countermeasures and state of art security techniques that can be applied as basic security guards.security models and requirements for healthcare application clouds','Web application security'
'password-based key exchange schemes are designed to provide entities communicating over a public network, and sharing a (short) password only, with a session key (e.g, the key is used for data integrity and/or confidentiality). the focus of the present paper is on the analysis of very efficient schemes that have been proposed to the ieee p1363 standard working group on password-based authenticated key-exchange methods, but which actual security was an open problem. we analyze the autha key exchange scheme and give a complete proof of its security. our analysis shows that the autha protocol and its multiple modes of operations are provably secure under the computational diffie-hellman intractability assumption, in both the random-oracle and the ideal-ciphers models.security proofs for an efficient password-based key exchange','Web application security'
'anticipating and mitigating security threats is critical during software development. this work investigates security vulnerabilities and mitigation strategies to help software developers build secure applications. the work examines common vulnerabilities, and relevant mitigation strategies, from several perspectives, including the input environment used to supply the software with needed data, the internal data and structures used to store and retrieve the data, the algorithms and computations performed on the data, the outputs, and the extensibility and mobile software. examining software security from these vantage points is the key to understanding the difficulty of producing secure software applications.security vulnerabilities and mitigation strategies for application development','Web application security'
'non-functional descriptions of web services and busi- ness rules play an important role in specification and analy- sis of the security constraints of web services. as existing approaches do not provide logic and semantic model for the web services security constraints, sharing and reason- ing over them are infeasible. the proposal builds upon the project akt\'s1 work in defining a semantic web constraint interchange format (cif), which itself builds on the pro- posed semantic web rule language (swrl). the main contributions of this paper are a new ontology for representing security constraints as policy and a seman- tic policy framework for the management of the policies; we also show the possibility to integrate the business rules and non-functional descriptions into policy specification by means of converting them into constraint satisfaction prob- lem (csp) using cif.semantic descriptions ofweb services security constraints','Web application security'
'in this paper, we propose a method of detecting and classifying web application attacks. in contrast to current signature-based security methods, our solution is an ontology based technique. it specifies web application attacks by using semantic rules, the context of consequence and the specifications of application protocols. the system is capable of detecting sophisticated attacks effectively and efficiently by analyzing the specified portion of a user request where attacks are possible. semantic rules help to capture the context of the application, possible attacks and the protocol that was used. these rules also allow inference to run over the ontological models in order to detect, the often complex polymorphic variations of web application attacks. the ontological model was developed using description logic that was based on the web ontology language (owl). the inference rules are horn logic statements and are implemented using the apache jena framework. the system is therefore platform and technology independent. prior to the evaluation of the system the knowledge model was validated by using ontoclean to remove inconsistency, incompleteness and redundancy in the specification of ontological concepts. the experimental results show that the detection capability and performance of our system is significantly better than existing state of the art solutions. the system successfully detects web application attacks whilst generating few false positives. the examples that are presented demonstrate that a semantic approach can be used to effectively detect zero day and more sophisticated attacks in a real-world environment.semantic security against web application attacks','Web application security'
'smartphones are now ubiquitous. however, the security requirements of these relatively new systems and the applications they support are still being understood. as a result, the security infrastructure available in current smartphone operating systems is largely underdeveloped. in this paper, we consider the security requirements of smartphone applications and augment the existing android operating system with a framework to meet them. we present secure application interaction (saint), a modified infrastructure that governs install-time permission assignment and their run-time use as dictated by application provider policy. an in-depth description of the semantics of application policy is presented. the architecture and technical detail of saint is given, and areas for extension, optimization, and improvement explored. as we show through concrete example, saint provides necessary utility for applications to assert and control the security decisions on the platform.semantically rich application-centric security in android','Web application security'
'this paper describes an approach to model complex applications by modeling application requirements separately from security requirements in use case models. by careful separation of concerns, the security requirements are captured in security use cases separately from the application requirements, which are captured in application use cases. the approach reduces system complexity caused by mixing security requirements with business application requirements with the goal of making complex systems more maintainable. furthermore, the security use cases can be reused by other software applications. this paper describes how the application and security concerns are modeled separately, and how they can be woven together into an application.separating application and security concerns in use case models','Web application security'
'sneak preview','Web application security'
'biometric technology enables automatic identification or verification of an individual based on the person\'s physiological or behavioural characteristics. soft biometrics is a research topic that has attracted a lot of attention recently. the application of soft biometrics in security technology is very promising because it has strong privacy measures and it improves service quality because service is provided that is appropriate to each subject of the soft biometrics application, i.e., the person picked up on camera. soft biometrics is defined as characteristics that provide some information about an individual but that lack high distinctiveness and performance to sufficiently differentiate any two individuals. we introduce here a summary of soft biometrics and its applications. we also introduce our work in this field.soft biometrics and its application to security and business','Web application security'
'some security principles and their application to computer security','Web application security'
'application-level web security refers to vulnerabilities inherent in the code of a web-application itself (irrespective of the technologies in which it is implemented or the security of the web-server/back-end database on which it is built). in the last few months, application-level vulnerabilities have been exploited with serious consequences: hackers have tricked e-commerce sites into shipping goods for no charge, usernames and passwords have been harvested, and confidential information (such as addresses and credit-card numbers) has been leaked. in this paper, we investigate new tools and techniques which address the problem of application-level web security. we 1) describe a scalable structuring mechanism facilitating the abstraction of security policies from large web-applications developed in heterogeneous multiplatform environments; 2) present a set of tools which assist programmers in developing secure applications which are resilient to a wide range of common attacks; and 3) report results and experience arising from our implementation of these techniques.specifying and enforcing application-level web security policies','Web application security'
'a workflow process involves the execution of a set of related activities over time to perform a specific task. security requires that such activities may only be performed by authorised subjects. in order to enforce such requirements, access to the underlying data objects has to be controlled. we refer to such access control as level 1 access control. in addition, when an individual is authorised to perform an activity, access should be limited to the time that the activity is being performed: access to activity information before an activity commences or after it has terminated may be undesirable. this we will refer to as level 2 security. finally, applications often specify application-oriented (level 3) security requirements. this paper considers security restrictions in the latter category and proposes a rigorous approach that may be used to specify such policies. enforcement (implementation) of such policies is also considered. the paper assumes that level 1 and level 2 mechanisms are in place and builds level 3 security mechanisms on these underlying levels.specifying application-level security in workflow systems','Web application security'
'despite, in recent year, zigbee has wide applications in our lives because of many outstanding advantages, defects of its technology lead to poor application of its security. so it\'s very important for us to have a study on application of zigbee security. first of all, this paper analyzes the stack structure of zigbee protocol. and then it has a discussion on the zigbee date security features, encryption technology, key management and typical attack models of network. finally, it introduces a security mode based on advanced encryption standard (aes) algorithm to realize the security application in the zigbee application support sub-layer (aps).study and application of security based on zigbee standard','Web application security'
'service discovery protocols are used in distributed systems to locate services for clients. the services that are located as well as the clients requesting service are commonly assumed to be trustworthy and reliable. this assumption can lead to security problems, particularly when clients and services are transient, as is the case with mobile networks. authentication is commonly used in an attempt to circumvent this problem, but this only provides proof of identity, and does not vouchsafe behaviour. in this paper we present a new protocol, superstringrep, which combines service discovery with service scores to create a system-wide score for services which reflects the quality of service they offer. this integration of service discovery and reputation provides the service scores right when the client needs them: while selecting a service to interact with.superstringrep','Web application security'
'the application of cryptographic transformations for the purpose of enhancing the security in data base systems is discussed. these transformations have been recognized in the past as a valuable protection mechanism but their relation to data base security has not been identified. the major reason is the lack of a suitable data base model for investigating the questions of security and cryptography. a multi-level model of a data base is presented in this paper. this model helps to understand the connection between the data base structure and the cryptographic transformations applied to the data base. it is shown that cryptographic transformations can be applied between the different levels of the data base. several types of these transformations are identified and the possible ways of using and controlling them are also discussed. the multi-level model can provide a useful framework for further research in the area of cryptography and data base security.the application of cryptography for data base security','Web application security'
'nowadays the network information security is an important research direction on data communications. in order to solve a number of shortcomings in information security password system, in this paper, a new criterion based on image segmentation on face recognition is applied to network security. firstly, in proposed approach, the original images are divided into modular images, which are also called sub-images. then, the well-known fisher linear discriminant analysis is directly employed to the sub-images obtained from the previous step. finally the recognition results are obtained by the general minimum distance classifier. the orl face image database is made use of to simulate, and when the training sample is only one, the recognition rate of 90.83 percent is achieved.the application of face recognition in network security','Web application security'
'web applications need for extensive testing before deployment and use, for early detecting security vulnerabilities to improve the quality of the safety of the software, the purpose of this paper is to research the fuzzing applications in security vulnerabilities. this article first introduces the common web software security vulnerabilities, and then provide a comprehensive overview of the fuzzing technology, and using fuzzing tools web fuzz to execute a software vulnerability testing, test whether there is a software security hole. test results prove that fuzzing is suitable for software security vulnerabilities testing, but this methodology applies only to security research field, and in the aspect of software security vulnerabilities detection is still insufficient.the application of fuzzing in web software security vulnerabilities test','Web application security'
'because of the defect of only the single data encryption and the use of famous encryption algorithm, which was not improved in traditional methods of the registration process, a combined encryption algorithm is proposed in this thesis. that is, the algorithm security is greatly improved, through researching several famous data encryption algorithms, and improving some data encryption algorithms, and arranging encryption algorithms in some order. finally, the combined encryption algorithm is successfully made by using the initial encryption algorithm, micro genard encryption algorithm and the famous base64 encryption algorithm. that is, in accordance with the order of the initial encryption algorithm, the improved micro genard encryption algorithm and the famous base64 encryption algorithm, the user\'s information is gradually encrypted, and the algorithm security is greatly enhanced. besides, to video surveillance software system for instance, which is widely used in the field of the traffic security management, the combined encryption algorithm is completely validated, and its security is very high.the application of hybrid encryption algorithm in software security','Web application security'
'in this paper, the approaches to introducing security policy into role-based access control (rbac) and the common data security architecture (cdsa) are proposed. we apply security policy to a role\'s privileges in rbac. an extended rbac using pki and role-assignment policy is described. the improved cdsa supports user-definable trust policy enforcement using trust policy description files. a policy-based cdsa is also presented. furthermore, a role definition language is given, and a policy representation language is discussed.the application of security policy to role-based access control and the common data security architecture','Web application security'
'it is very important to protect critical resources such as private data and code in computer systems. it is promising to protect private data and to improve the system security by leveraging the isolation attribute of virtual machine(vm). the isolation attribute of vm is provided by virtual machine monitor (vmm) that runs in higher priority than guest oses. if the critical components are isolated in vms,access control can be enforced or attestation can be made when the subject is accessing via vms. in this way, vms can improve the security level of critical components such as os, kernel, data, and applications. for computer system security, vms can be used to detect malware intrusions and to protect critical components, which can be implemented by integrating detection or protection mechanism in either vmm or vms. authentication is required to create trustful vms. this paper surveys technologies related of using virtual machines to enhance system security.the application of virtual machines on system security','Web application security'
'nowadays digital multimedia copyright protection is up against austere challenge. the watermarking and encryption used together is more and more important in digital multimedia security (dms) application. so in this paper, a novel approximation zero-tree-wise wavelet watermarking with difference chaos shift key coding encryption method of dms information security is present. firstly the dms information is got by proprietor or authority as initial watermarks. then encrypt key is generated by dm&#8217;s data. to encrypt initial watermarks\' information is to apply chaos sequence as final watermarks\' information. then the final watermark is embedded into the approximation zero-tree-wise wavelet coefficients of the image by digital chaos shift key (dcsk) coding method that makes watermarks&#8217; information more secure and secret. the chaos key is generated automatically by image decomposed with lut mapped.as a result, approximation zero-tree-wise wavelet with dcsk coding watermarking is more secure and secret characteristic in protecting dms information. furthermore blind-detection watermarking can be achieved, encrypt key can be regain by dm data. that method possesses research value.the application of wavelet & dcsk watermarking in multimedia security','Web application security'
'grid was developed and utilized on the base of the internet. as the problems of information security taken by the openness of the internet are getting more and more serious, the demands for security data exchange in the grid grow day by day. in the application environment of grid service, the intermediary need to forward the message on transmission path, which exceed the point-to-point security range of the transport layer. thus, the paper proposes a model based on ws-security for safe grid data exchange to maintain data interaction security between the requester and provider of service, and a method to implement the security model is presented based on its research. finally, a test case is proposed to verify the security model. the results show that the model can effectively ensure the safety of grid data exchange.the application of web security technology in grid service','Web application security'
'by analyzing the superiority and security applicationof xml technology, and considering the defect of traditionactuality, this paper gives a system frame model based xml. itintroduces the xml security application about e-commercesystem, and describes the method that is used in implementingthe data dynamic release on web browser based on mvc modeland xml technology, taking contract information sub-system as an example.the application of xml security technology in e-commerce system','Web application security'
'against the nature of hyperchaos dynamic system, a modified hyper-chaotic sequence encryption algorithm was given. this method used the dynamic system of tnc hyperchaos. and proved to have the effective ability of exhaustive attack and anti-nonlinear reorganization; used sub-nyquist sampling interval to increase the key space, this method has the ability of anti-nonlinear reorganization attack; realized the algorithm\'s encryption and decryption by using matlab simulation platform and got some satisfactory results which make out that the arithmetic is faster and easier implementation by software, had a large key space and so on.the application research of hyperchaos encryption in security communications','Web application security'
'the important role information hiding technology plays in the network information security system has been illustrated in paper on the basis of the information hiding principle, the characteristics and its key technologies. then, the feature and effect of hiding information in network environment were discussed, as well as several hiding technique and detecting technique.the application research of information hiding technology in network security','Web application security'
'mifare classic widely used as a public transport card based on the weak cipher crypto-1 broken three years ago with a number of serious attacks published by researchers from the dutch university of nijmegen and still another was developed at university college of london. the report entitled cloning reactivation published in the polish computerworld magazine presented the security of warsaw city card at that time. it also announced that starting from 2010 the security of the warsaw system would undergo an upgrade with the usage of 3des algorithm. while in london all new oyster cards emitted since 2010 are more securedesfire cards, the security of the warsaw card stays nearly the same.the security of the multi-application public transport card','Web application security'
'recent years have seen the emergence of a new programming paradigm for web applications that emphasizes the reuse of external content, the mashup. although the mashup paradigm enables the creation of innovative web applications with emergent features, its openness introduces trust problems. these trust issues are particularly prominent in javascript code mashup -- a type of mashup that integrated external javascript libraries to achieve function and software reuse. with javascript code mashup, external libraries are usually given full privileges to manipulate data of the mashup application and executing arbitrary code. this imposes considerable risk on the mashup developers and the end users. one major causes for these trust problems is that the mashup developers tend to focus on the functional aspects of the application and implicitly trust the external code libraries to satisfy security, privacy and other non-functional requirements. in this paper, we present tomato, a development tool that combines a novel trust policy language and a static code analysis engine to examine whether the external libraries satisfy the non-functional requirements. tomato gives the mashup developers three essential capabilities for building trustworthy javascript code mashup: (1) to specify trust policy, (2) to assess policy adherence, and (3) to handle policy violation. the contributions of the paper are: (1) a description of javascript code mashup and its trust issues, and (2) a development tool (tomato) for building trustworthy javascript code mashup.tomato','Web application security'
'complexity in commodity operating systems makes compromises inevitable. consequently, a great deal of work has examined how to protect security-critical portions of applications from the os through mechanisms such as microkernels, virtual machine monitors, and new processor architectures. unfortunately, most work has focused on cpu and memory isolation and neglected os semantics. thus, while much is known about how to prevent os and application processes from modifying each other, far less is understood about how different os components can undermine application security if they turn malicious. we consider this problem in the context of our work on overshadow, a virtual-machine-based system for retrofitting protection in commodity operating systems. we explore how malicious behavior in each major os sub-system can undermine application security, and present potential mitigations. while our discussion is presented in terms of overshadow and linux, many of the problems and solutions are applicable to other systems where trusted applications rely on untrusted, potentially malicious os components.towards application security on untrusted operating systems','Web application security'
'as android became the most popular mobile operating system, malicious activities targeting android and its applications are rising rapidly. while technical approaches may mitigate the attacks with varying effectiveness, understanding the economic incentives of the criminals may shed light on the most effective defense. in this talk, i will focus on application plagiarism on android markets. first, i will describe the unique characteristics of android applications, the fundamental differences between plagiarism of android applications and that of non-mobile software, and the relationship between plagiarism and mobile advertising. next, i will illustrate the challenges in measuring the scale, severity, and impact of android application plagiarism. to achieve this, we need not only detect plagiarism among the large number of applications on different android markets but also measure their usage and impact on advertising on a large, live network. i will describe how we correlated plagiarized applications detected through static analysis to their advertising traffic captured on a live network. i will characterize properties of the cloned applications, including their distribution across different markets, application categories, and advertising libraries. to examine how plagiarized applications affect the original developers, i will estimate on the advertising revenue and user base that plagiarized applications have siphoned from the original developers. finally, i will discuss defenses against application plagiarism.underground economy of android application plagiarism','Web application security'
'recently, twitter has emerged as a popular platform for discovering real-time information on the web, such as news stories and people\'s reaction to them. like the web, twitter has become a target for link farming, where users, especially spammers, try to acquire large numbers of follower links in the social network. acquiring followers not only increases the size of a user\'s direct audience, but also contributes to the perceived influence of the user, which in turn impacts the ranking of the user\'s tweets by search engines. in this paper, we first investigate link farming in the twitter network and then explore mechanisms to discourage the activity. to this end, we conducted a detailed analysis of links acquired by over 40,000 spammer accounts suspended by twitter. we find that link farming is wide spread and that a majority of spammers\' links are farmed from a small fraction of twitter users, the social capitalists, who are themselves seeking to amass social capital and links by following back anyone who follows them. our findings shed light on the social dynamics that are at the root of the link farming problem in twitter network and they have important implications for future designs of link spam defenses. in particular, we show that a simple user ranking scheme that penalizes users for connecting to spammers can effectively address the problem by disincentivizing users from linking with other users simply to gain influence.understanding and combating link farming in the twitter social network','Web application security'
'unified messaging - the &#8220;quiet&#8221; application','Web application security'
'today, an e-commerce transaction is typically protected using ssl/tls@. however, there remain some risks in such use of ssl/tls@. these include that of information being stored in clear at the end point of the communication link and lack of user authentication. although ssl/tls does offer the latter, the security service is optional and usually omitted. this is because of the fact that users typically do not have the necessary asymmetric key pair. since ssl/tls protects data only while it is being transmitted, the merchant has access to sensitive information such as the debit/credit card number. the storage of unencrypted debit/credit card information at the merchant server therefore represents a risk that is not currently addressed by the use of ssl/tls to secure electronic payment transactions.in this paper, we propose a payment protocol in which the risk of having debit/credit card details stored at a merchant server is eliminated. user authentication is also provided. this is achieved by utilising the gsm data confidentiality service to encrypt sensitive information. the gsm security service is also used to provide user identity authentication. the additional security is realised in such a way that no management overhead is imposed on the user.using gsm to enhance e-commerce security','Web application security'
'software developers are not necessarily security specialists, security patterns provide developers with the knowledge of security specialists. although security patterns are reusable and include security knowledge, it is possible to inappropriately apply a security pattern or that a properly applied pattern does not mitigate threats and vulnerabilities. herein we propose a method to validate security pattern applications. our method provides extended security patterns, which include requirement- and design-level patterns as well as a new model testing process using these patterns. developers specify the threats and vulnerabilities in the target system during an early stage of development, and then our method validates whether the security patterns are properly applied and assesses whether these vulnerabilities are resolved.validating security design patterns application using model testing','Web application security'
'we explain how the formal language lotos can be used to specify security protocols and cryptographic operations. we describe how security properties can be modelled as safety properties and how a model-based verification method can be used to verify the robustness of a protocol against attacks of an intruder. we illustrate our technique on a concrete registration protocol. we find an attack, correct the protocol, propose a simpler yet secure protocol, and finally a more sophisticated protocol that allows a better discrimination between intruder\'s attacks and classical protocol errors.verification of security protocols using lotos-method and application','Web application security'
'the world wide web (www) is one of the fastest growing technologies today. the content and design of websites has generally evolved over the years to create a cyberspace with greater information diversity as well as a wider spectrum of functionalities. the contribution of this paper lies primarily in the area of application of the www, specifically by south african health institutions. in this paper the researcher investigated how these institutions are utilizing the www and sought to find out whether as a provider of essential services to society, health organizations are taking advantage of the web by using its interactivity features. by developing a framework to enable the study of both content and interactivity features of these websites, the study was able to find out to what extent web technologies were being utilized and for what purposes. findings are presented and recommendations made on gaps in possible utilizations of the web by health organizations, based on results from the study.web application by south african health institutions','Web application security'
'security testing a web application or web site requires careful thought and planning due to both tool and industry immaturity. finding the right tools involves several steps, including analyzing the development environment and process, business needs, and the web application\'s complexity.web application security assessment tools','Web application security'
'this paper introduces the security of trusted network connection into web applications. to solve the security of web applications and the application limitations of trusted network connection, which is only widely used in lan and vpn, a new method used for web application security is presented in the paper based on the thought of trusted network connection. through the model design and the system realization, it can prove that the thought of trusted network connection can be applied to web applications and improve the security of web applications. at the mean time, the thought of trusted network connection can reduce the attack of viruses and trojans and broaden the fields of trusted network connection application.web application security based on trusted network connection','Web application security'
'integrating security throughout the life cycle can improve overall web application security. with adetailed review of the steps involved in applying security-specific activities throughout the softwaredevelopment life cycle, the author walks practitioners through effective, efficient application design,development, and testing.web application security engineering','Web application security'
'we present the design and implementation of the webdaemon security gateway (wdsg) with the techniques of event-driving, non-blocking io multiplexing, secure cookies, ssl and caches based on pki framework and role-based access control (rbac) policy. it not only supports massive concurrency and avoids the pitfalls of traditional block i/o based design, but also is able to secure all the resources of an enterprise and reduce the cost and complexity of administration.web application security gateway with java non-blocking io','Web application security'
'the grid problem is how to enable coordinated resources sharing and problem solving in dynamic, multi-institutional cross organizations called virtual organizations (vos) that collect a large number of nodes grouped into grid sites. an overlay of policies governs access within a collaboration that is granted to users by a vo and by a site to vos, nodes and users through site admission-control policies. most resources consist of web applications and web services shared by a site that is part of a vo. the paper proposes an analysis of web application security by referring to xml-based applications accessed through a grid system. the different organizational domains involved transfer security from grid-wide to the vo level, and from the site and machine to the application level in order to allow local control to be retained.web application security in a crossing boundaries grid system','Web application security'
'in the paper we present a novel approach based on applying a modern metaheuristic gene expression programming (gep) to detecting web application attacks. this class of attacks relates to malicious activity of an intruder against applications, which use a database for storing data. the application uses sql to retrieve data from the database and web server mechanisms to put them in a web browser. a poor implementation allows an attacker to modify sql statements originally developed by a programmer, which leads to stealing or modifying data to which the attacker has not privileges. intrusion detection problem is transformed into classification problem, which the objective is to classify sql queries between either normal or malicious queries. gep is used to find a function used for classification of sql queries. experimental results are presented on the basis of sql queries of different length. the findings show that the efficiency of detecting sql statements representing attacks depends on the length of sql statements. web application security through gene expression programming','Web application security'
'web application development is a large and growing area of employment for computer science graduates. while our graduates have learned how to design and implement web applications that work correctly with expected inputs, few have learned how to design and implement software that is secure against common web application vulnerabilities. the most common security vulnerabilities in software are cross-site scripting, sql injection, and php include bugs. all three problems are web application vulnerabilities. these vulnerabilities can allow attackers to access applications without permission, obtain sensitive information like credit card or social security numbers, and steal merchandise or transfer funds from commercial web sites. the number of vulnerabilities discovered each year has increased at an exponential rate since 2000. in this tutorial, we will describe how attackers exploit common web application vulnerabilities and show live demonstrations of such attacks. we will show participants how to teach their students to design and write secure code that is immune to these attacks. the tutorial will present resources that participants can use to incorporate web application security into programming, database, web development, and information security courses.web application security tutorial','Web application security'
'the importance of the web service technology for business, government, among other sectors, is growing. its use in these sectors demands security concern. the web services security standard is a step towards satisfying this demand. however, in the current security approach, the mechanism used for describing security properties of web services restricts security policy specification and intersection. in environments that include loosely-coupled components, a rich description of components is needed to determine whether they can interact in a secure manner. the goal of this paper is to propose a security approach for web services, which combines web services policy framework policies and a web ontology language ontology to overcome the limitation of the current syntactic approach. the main contribution of this paper is an extended approach based on semantics-enriched security policies.web service security management using semantic web techniques','Web application security'
'here\'s a sobering thought for all managers responsible for web applications: without proactive consideration for an application\'s security, attackers can bypass nearly all lower-layer security controls simply by using the application in a way its developers didn\'t envision. learn how to address vulnerabilities proactively and early on to avoid the devastating consequences of a successful attack.web-application security','Web application security'
'working group report on application security','Web application security'
'the apache server combining with mysql and php has becoming a new platform, the lamp for web based applications. the platform level security had been dealt with by the security improvement of the os, firewall and http server. yet the application level security problems seemed to be overlooked. in this paper an application level security mechanism using once-only url is proposed. also an apache plug-in module to support this mechanism is developed and implemented.zero-configuration security module for lamp application system','Web application security'
'this paper introduces an asynchronous optimistic certified email protocol, with stateless recipients, that relies on key chains to considerably reduce the storage requirements of the trusted third party. the proposed protocol thereby outperforms the existing schemes that achieve strong fairness. the paper also discusses the revocation of compromised keys as well as practical considerations regarding the implementation of the protocol.a certified email protocol using key chains','Web protocol security'
'the quest for the formal certification of properties of systems is one of the most challenging research issues in the field of formal methods. it requires the development of formal models together with effective verification techniques. in this paper, we describe a formal methodology for verifying security protocols based on ideas borrowed from the analysis of open systems, where applications interact with one another by dynamically sharing common resources and services in a not fully trusted environment. the methodology is supported by aspasya, a tool based on symbolic model checking techniques.a coordination-based methodology for security protocol verification','Web protocol security'
'the importance of wireless technology in modern medicine has increased in the last years. it is anticipated that a large number of wireless communication devices for e-health will operate in unlicensed frequency bands in indoor environments. this represents a coexistence problem, which will be particularly challenging in confined areas of hospitals. electromagnetic interference (emi) from wireless devices can disrupt the performance of non-communication electronic medical equipment. cognitive radio is a technology that can ease the coexistence by protecting non-communication electronic medical equipment. in this work we improved a cognitive radio emi-aware protocol for e-health applications. the original protocol protects medical equipment from harmful interference by preventing wireless transmissions when interference immunity levels are exceeded. however, this leads to high outage probability in areas where protected medical apparatuses are located. in order to maintain a low outage probability under this scheme, we propose the use of an additional channel in a different frequency band for control/data transmission from potential interference sources. we considered the recently allocated 2360--2400 mhz for medical body area networks and the 902--928 mhz band for allocation of the additional control/data channel. simulation results demonstrated that the use of the proposed dual-band emi-aware protocol using the 902--928 mhz band significantly reduces the outage probability.a dual-band mac protocol for indoor cognitive radio networks','Web protocol security'
'on-line surveillance for safety and security is a major requirement of public transport and other public places to address the modern demands of mobility in major urban areas and to effect improvements in quality of life and environment protection. the surveillance task is a complex one involving technology, management procedures and people. visual surveillance based on closed circuit television system is an important part of such systems, but visual processing is not sufficient and the geographical distribution of devices and management has to be taken into account. in this paper we present a surveillance architecture that reflects the distributed nature of the monitoring task and allows for distributed detection processes, not only dealing with visual processing but also with devices such as acoustic signature detection and mobile smart cards, actuators and a range of other possible sensors. the design uses ideas from control engineering and distributed communications networks resulting in a communications architecture based on corba and xml messaging. we have shown how to define a generic device/sensor model appropriate for the surveillance task and sufficiently flexible so as to allow for scalability, expansion and customisation of a practical surveillance task. the paper gives sufficient details on the protocols to show how intelligent detection modules can be integrated as part of this kind of system. the system components have been implemented and integrated in two major successful trials in metropolitan railway stations in london and in paris, as part of a major eu-funded project (prismatica).a flexible communications protocol for a distributed surveillance system','Web protocol security'
'epcglobal class-1 generation-2 specification (gen2 in brief) has been approved as iso18000-6c for global use, but the identity of tag (tid) is transmitted in plaintext which makes the tag traceable and clonable. several solutions have been proposed based on traditional encryption methods, such as symmetric or asymmetric ciphers, but they are not suitable for low-cost rfid tags. recently, some lightweight authentication protocols conforming to gen2 have been proposed. however, the message flow of these protocols is different from gen2. existing readers may fail to read new tags. in this paper, we propose a novel authentication protocol based on gen2, called gen2^{+}, for low-cost rfid tags. our protocol follows every message flow in gen2 to provide backward compatibility. gen2^{+} is a multiple round protocol using shared pseudonyms and cyclic redundancy check (crc) to achieve reader-to-tag authentication. conversely, gen2^{+} uses the memory read command defined in gen2 to achieve tag-to-reader authentication. we show that gen2^{+} is more secure under tracing and cloning attacks.a gen2-based rfid authentication protocol for security and privacy','Web protocol security'
'sensor nodes used to transmit sensitive data, especially in military applications, require securing the data transmitted through the wsns to maintain the confidentiality of the data and authenticate the participating sensor nodes. since sensor nodes suffer from limited resources, in memory storage, computing power, energy capabilities and transmission rates, available network security protocols are inadequate. symmetric algorithms cannot provide the same degree of security as public key algorithms, leading us to devise a new algorithm shesp that uses public keys within the limitations of sensor nodes. this paper presents a way to utilise existing public key algorithms such as rsa, diffie&#45;hellmann and elliptic curve in the field of wsn security by dividing the network into clusters. our algorithm supplies data confidentiality, node authentication and data integrity while remaining within acceptable memory, time and energy constraints. we provide theoretical and experimental evidence to validate our algorithms. results reveal significant improvement in data availability, data confidentiality and authenticity while reducing the communication and computation overhead.a hybrid security protocol for sensor networks','Web protocol security'
'a form of advertisement which is becoming very popular on the web is based on electronic coupon (e-coupon) distribution. e-coupons are the digital analogue of paper coupons which are used to provide customers with discounts or gift in order to incentive the purchase of some products. nowadays, the potential of digital coupons has not been fully exploited on the web. this is mostly due to the lack of \"efficient\" techniques to handle the generation and distribution of e-coupons. in this paper we discuss models and protocols for e-coupons satisfying a number of security requirements. our protocol is lightweight and preserves the privacy of the users, since it does not require any registration phase.a lightweight protocol for the generation and distribution of secure e-coupons','Web protocol security'
'peer-to-peer (p2p) applications have recently seen an enormous success and have reached millions of users. the main reason of this success is the anonymity the users enjoy. however, as recent experiences with p2p networks show, this anonymity offers an opportunity to exploit the network for abuses (e.g., the spread of malware). in this paper we extend our previous work on p2prep, a reputation management protocol for pure p2p networks, in the case of super-peer networks. we present the design and implementation of reputation-aware servents.a protocol for reputation management in super-peer networks','Web protocol security'
'documents allow end-users to encapsulate information related to a collaborative business process into a package that can be saved, emailed, digitally signed, and used as the basis of interaction in an activity or an ad hoc workflow. while documents are used incidentally today in web applications, for example in html presentations of content stored otherwise in back-end systems, they are not yet the central artifact for developers of dynamic, data intensive web applications. this paper unifies the storage and management of the various artifacts of web applications into an interactive web document (iwd). data content, presentation, behavior, attachments, and digital signatures collected throughout the business process are unified into a single composite web resource. we describe a rest-based protocol for interacting with iwds and a standards-based approach to packaging their multiple constituent artifacts into iwd archives based on the open document format standard.a rest protocol and composite format for interactive web documents','Web protocol security'
'a security architecture for the internet protocol','Web protocol security'
'ad-hoc networks are characterized by open medium dynamic topology distributed cooperation and constrained capability. these characteristics set more challenges for security. if the routing protocol is attacked, the whole network would have been paralyzed. as a result, routing security is the most important factor in the security of the entire network. however, few of current routing protocols have the consideration about the security problems. the potential insecure factors in the aodv protocol are analyzed. furthermore, a security routing protocol based on the credence model is proposed, which can react quickly when detecting some malicious behaviors in the network and effectively protect the network from kinds of attacks.a security enhanced aodv routing protocol','Web protocol security'
'in this paper, it first analyzes the present security protocol for mobile communications. furthermore, a security framework for the mobile e-service is proposed. in the mean time, using the inherent security and expansibility of j2me, we design a security message flow model to support communication protocol for the mobile e-service oriented architecture (msoa), which emphasizes the authentication and the security of information transmission to the cooperative mobile users. finally, under the background application of county-level mobile e-service platform, we validate the security and feasibility of the communication protocol, thus providing firm and satisfactory technical support for the county-level mobile e-government system.a security protocol for mobile e-service oriented architecture (msoa)','Web protocol security'
'vehicular ad-hoc networks (vanets) have the potential to optimize traffic in modern urban areas, reduce congestions and pollution, and increase passenger safety and comfort. applications designed for such networks pose new security constraints. the mobile devices have limited resources to spare, and the network connectivity is reduced because of the mobility of cars. we present a security protocol designed for vanet environments. it guarantees the content of messages against possible attackers. because privacy of the passengers must be preserved in vanet, the security protocol is designed not to rely on the drivers&#8217; identity. the protocol also proves the time and location when a message was sent. we present evaluation results demonstrating that the protocol is able to correctly handle different security threats.a security protocol for vehicular distributed systems','Web protocol security'
'mobile ad-hoc networks are composed of heterogeneous mobile systems. securing their communications may be difficult due to differences in the supported algorithms and protocols. in this paper we propose a protocol to negotiate security settings for the communications. this protocol aims at minimizing the power consumption and at providing the highest possible security level associated with the communications.a security service protocol for manets','Web protocol security'
'web service has its characteristic of statelessness. but, many applications on grid need the stateful services. the advanced search on hvem datagrid is one example. the search service provides a multi-depth search and an associated search. the service should trace the search states and provide the access control as well as the service security in the search process. in addition, we should remember that the web services or the resources can stretch multiple virtual organization(vo)s and certificate authority(ca)s. in this paper, we propose a secure stateful web service to satisfy the requirements stated. we describe the stateful web service with the scalable security (s3ws) as its service environment and its protocol in detail. through the security proof, the scalability analysis and the estimation of performance and overhead, we demonstrate the stateful web service proposed to be secure and light weight enough to be realistic.a stateful web service with scalable security on hvem datagrid','Web protocol security'
'the mobile computing paradigm has introduced new problems for application developers. challenges include heterogeneity of hardware, software, and communication protocols, variability of resource limitations and varying wireless channel quality. in this scenario, security becomes a major concern for mobile users and applications. security requirements for each application are different, as well as the hardware capabilities of each device. to make things worse, wireless medium conditions may change dramatically with time, incurring great impact on performance and qos guarantees for the application. currently, most of the security solutions for mobile devices use a static set of algorithms and protocols for services such as cryptography and hashes. in this work we propose a security service, which works as a middleware, with the ability to dynamically change the security protocols used between two peers. these changes can occur based on variations on wireless medium parameters and system resource usage, available hardware resources, application-defined qos metrics, and desired data \'\'security levels\'\'. we compare our solution to some widespread static security protocols, demonstrate how our middleware is able to adapt itself over different conditions of medium and system, and how it can provide a performance gain in the execution of cryptographic primitives, through the use of data semantics.adaptive security protocol selection for mobile computing','Web protocol security'
'denial of service (dos) attacks represent, in today\'s internet, one of the most complex issues to address. a session is under a dos attack if it cannot achieve its intended throughput due to the misbehavior of other sessions. many research studies dealt with dos, proposing modelsand/or architectures mostly based on an attack prevention approach. prevention techniques lead to different models, each suitable for a single type of misbehavior, but do not guarantee the protection of a system from a more general dos attack. in this work we analyze the fundamental requirements to be satisfied in order to protect hosts and routers from any form of distributed dos (ddos). then we propose a network signaling protocol, named active security protocol (asp), which satisfies most of the defined requirements. asp provides an active protection from a ddos attack, being able to adapt its defense strategies to the current type of violation. protocol specification and design are performed using an object oriented methodology: we used unified modeling language (uml) as a software description language.an active security protocol against dos attacks','Web protocol security'
'we investigate trust relationships between and within a security policy and a security mechanism to assess system trust of software application. it has been recognized that trust assessment of security systems in dynamic environments with multiple entities, each with its own changing needs from the security mechanisms, is a complex task. in this paper, we propose a novel architectural approach to assess system trust of service oriented environments. the primary goal of this architecture is to show a way for constructing an automated system for trust assessment of web services. particularly, we consider beliefs of an entity about a specific security mechanism of a service and the behavior of the service. in addition, we present new trust metrics for assessing system trust of a web service. furthermore, trust and trust related issues in literature are reviewed to make clear the pros of our approach for trust assessment.an architectural approach for assessing system trust based on security policy specifications and security mechanisms','Web protocol security'
'customers are usually passive in an electronic commerce transaction. based on a new two times concurrent signature, this article presents a fair electronic payment (e-payment) protocol. it can protect both of the participants\' profits. what&#8217;s more, in this protocol customers have more initiative, and they can terminate the transaction before possible cheats (while they also should be responsible for the quit), so their security is enhanced. as there is no traditional trusted third party (ttp) in our scheme, network congestion and conspiracy problem can be avoided. moreover, the protocol satisfies fairness and non-repudiation, and it can also be applied in the digital signature or product exchange process.an enhanced-security fair e-payment protocol','Web protocol security'
'jfkr is a security protocol that establishes a shared encryption key between two participants. this paper briefly describes the different components of jfkr and the security property each component is intended to provide. it then describes an executable model, interleaving pieces of code to help the reader understand how the model represents the protocol specification. finally, it presents some theorems about the model. the contributions of this work include (1) an executable model for a key establishment protocol about which we can reason, (2) a model for an attacker that permits the injection, modification, and removal of messages between the participants, and (3) formalizations of a subset of desired security properties.an executable model for security protocol jfkr','Web protocol security'
'people interested in cultural heritage constitute a community of users characterized by a high degree of mobility. users visiting cultural sites hosted by local institutions expect to access information pertaining to cultural/artistic heritage from anywhere at anytime, as they can through the desk at their own homes or offices. the ecumene web information system, developed in the groundwork of the ecumene project, offers a framework for accessing the artistic and cultural information through a web application. in this paper, we expose the underlying communication network architecture, based on the ipv6 protocol, and investigate the benefits of its adoption, in terms of mobility and security support. indeed, the need of granting different access rights to different user classes enforces the security requirements of the system. lastly, the open issues related to the practical integration of an experimental testbed with mobile ipv6 and ipsec support are detailed, and possible solutions are presented.an experience in ipv6 networking supporting ecumene web information system for cultural heritage','Web protocol security'
'current e-mail security systems base their security on the secrecy of the long-term private key. if this private key is ever compromised, an attacker can decrypt any messages-past, present or future-encrypted with the corresponding public key. the system described in this paper uses short-term private-key/public-key key pairs to reduce the magnitude of this vulnerability.an improved e-mail security protocol','Web protocol security'
'this paper proposes an improved free-roaming mobile agent security protocol. the scheme uses \"one hop backwards and two hops forwards\" chain relation as the protocol core to implement the generally accepted mobile agent security properties. this scheme defends most known attacks, especially colluded truncation attacks and several special cases.an improved free-roaming mobile agent security protocol against colluded truncation attacks','Web protocol security'
'we study the security of password protocols against off-line dictionary attacks. in addition to the standard adversary abilities, we also consider further cryptographic advantages given to the adversary when considering the password protocol being instantiated with particular encryption schemes. we work with the applied pi calculus of abadi and fournet, in which we present novel equational theories to model the (new) adversary abilities.these new abilities are crucial in the analysis of our case studies, the encrypted password transmission (ept) protocol of halevi and krawczyk, and the well-known encrypted key exchange (eke) of bellovin and merritt. in the latter, we find an attack that arises when considering the ability of distinguishing ciphertexts from random noise. we propose a modification to eke that prevents this attack.analysing password protocol security against off-line dictionary attacks','Web protocol security'
'the @mcrl language and its corresponding tool set form a powerful methodology for the verification of distributed systems. we demonstrate its use by applying it to the well-known bilateral key exchange security protocol.analysing the bke-security protocol with &#956;crl','Web protocol security'
'cryptographic protocols are used to achieve secure communication over insecure networks. weaknesses in such protocols are hard to identify, as they can be the result of subtle design flaws. formal verification techniques provide rigid and thorough means to evaluate security protocols. this paper demonstrates the process of formal verification by applying a logic to a security protocol intended for use in mobile communications. as a result of the verification, 8 failed protocol goals are identified. further, a new attack on the protocol is outlined. the presence of weaknesses in published protocols highlights the importance of formal verification to prevent insecure protocols reaching the public domain.analysis of a mobile communication security protocol','Web protocol security'
'the avispa tool is a push-button tool for the automated validation of internet security protocols and applications. it provides a modular and expressive formal language for specifying protocols and their security properties, and integrates different back-ends that implement a variety of automatic protocol analysis techniques. experimental results, carried out on a large library of internet security protocols, indicate that the avispa tool is a state-of-the-art tool for internet security protocol analysis as, to our knowledge, no other tool exhibits the same level of scope and robustness while enjoying the same performance and scalability.automated security protocol analysis with the avispa tool','Web protocol security'
'an important missing link in the construction of secure systems is finding a practical way to establish a correspondence between a software specification and its implementation. we address this problem for the case of crypto-based java implementations (such as crypto protocols) with an approach using automated theorem provers for first-order logic, by linking the implementation to a specification model. in this paper, we present details on an application of this approach to the open-source java implementation jessie of the ssl protocol. we also shortly comment on how these results can be transferred to the standard java secure sockets extension (jsse) library that was recently open-sourced by sun.automated security verification for crypto protocol implementations','Web protocol security'
'can the cloud truly be secured? can enterprises, universities, small businesses and governments securely utilize the cloud for their critical infrastructure? it will take rethinking our current security policies and what we consider secure. this session will cover what is necessary to utilize the cloud securely today and how the cloud should adapt for the future.cloud security','Web protocol security'
'computer network security','Web protocol security'
'authentication header protocol was introduced to neighbor discovery protocol to ensure its security. a security neighbor discovery protocol model based authentication header was proposed. in this model, multicast internet key exchange and management protocol was designed, star structure key management algorithm was presented, and ip addresses and link-layer address was bound in the ah authentication data. compared with the current security neighbor discovery algorithm, experiments results show that the proposed security model can effectively ensure security of neighbors, and defense to forge ip address, and improve the security of next generation internet.design of security neighbor discovery protocol','Web protocol security'
'the most of users of personal computer use web browsers such as ms(microsoft) internet explorer, mozilla firefox, and so on for accessing to internet. especially ms internet explorer which is used for internet access is a module to execute local files and to install softwares that are activated by install shield. also explorer is the same as shell. analyzing &#8220;index.dat&#8221; log which is the history of executing files and accessing web sites makes security audit effective. in this paper, by analyzing windows &#8220;index.dat&#8221; and mozilla firefox cache files with time analysis, we suggest a method to perform auditing of cyber crimes such as information leakage and hard disk vulnerability.designing security auditing protocol with web browsers','Web protocol security'
'the distributed temporal logic dtl is an expressive logic, well suited for formalizing properties of concurrent, communicating agents. we show how dtl can be used as a metalogic to reason about and relate different security protocol models. this includes reasoning about model simplifications, where models are transformed to have fewer agents or behaviors, and verifying model reductions, where to establish the validity of a property it suffices to consider its satisfaction on only a subset of models. we illustrate how dtl can be used to formalize security models, protocols, and properties, and then present three concrete examples of metareasoning. first, we prove a general theorem about sufficient conditions for data to remain secret during communication. second, we prove the equivalence of two models for guaranteeing message-origin authentication. finally, we relate channel-based and intruder-centric models, showing that it is sufficient to consider models in which the intruder completely controls the network. while some of these results belong to the folklore or have been shown, mutatis mutandis, using other formalisms, dtl provides a uniform means to prove them within the same formalism. it also allows us to clarify subtle aspects of these model transformations that are often neglected or cannot be specified in the first place.distributed temporal logic for the analysis of security protocol models','Web protocol security'
'we analyze the security of the bgp routing protocol and identify a number of vulnerabilities in its design and the corresponding threats. we then present modifications to the protocol that minimize or eliminate the most significant threats. the innovation we introduce is the protection of the second-to-last hop information contained in the as_path attributes by digital signatures, and the use of this predecessor information to verify the path of the selected route. with these techniques, we are able to secure complete path information in near constant space, avoiding the recursive protection mechanisms proposed for bgp in the past.efficient security mechanisms for the border gateway routing protocol','Web protocol security'
'encapsulation security protocol design for local area networks','Web protocol security'
'we argue that formal analysis tools for security protocols are not achieving their full potential, and give only limited aid to designers of more complex modern protocols, protocols in constrained environments, and security apis. we believe that typical assumptions such as perfect encryption can and must be relaxed, while other threats, including the partial leakage of information, must be considered if formal tools are to continue to be useful and gain widespread, real world utilisation. using simple example protocols, we illustrate a number of attacks that are vital to avoid in security api design, but that have yet to be modelled using a formal analysis tool. we seek to extract the basic ideas behind these attacks and package them into a wish list of functionality for future research and tool development.extending security protocol analysis','Web protocol security'
'this paper proposes flexible security protocol for synchronous and asynchronous collaborative editing environment. the flexible security protocol comprises of preventive and corrective measures. the preventive measures ensure user authentication, access control and data confidentiality. the corrective measure includes fault tolerant measures to ensure data integrity.flexible security protocol in collaborative editing environment','Web protocol security'
'hazard analysis for security protocol requirements','Web protocol security'
'nowadays, short message service (sms) is faced with various security threats. thus, the fields of high confidentiality(e.g., mobile e-commerce) require a higher level of security protection on sms. secure communication in incredible mobile network has very important significance. this paper presents a high security communication protocol for sms. through authentication, decryption and integrity protection, it establishes an end-to-end secure channel between server-side and mobile terminal. through analyzed it by svo logic, this protocol is proved to ensure confidentiality, integrity and non-repudiation of sms messages.high security communication protocol for sms','Web protocol security'
'wireless body area networks (wbans) are receiving a significant interest as an emerging technology in next generation wireless networks. security challenges as well as this interest are also beginning to rise to the surface. we derived specific security requirements from wbans, and proposed a new hybrid security protocol for wbans which satisfies these requirements. this proposed protocol dealt with the overall security including up to the in- on-, and out-body, and utilized two heterogeneous cryptosystems (the symmetric and asymmetric) for diverse environments of wbans. our protocol shows that it satisfies more security requirements than existing security protocols including the overall coverage of wbans. copyright &#x00a9; 2010 john wiley & sons, ltd. (this proposed protocol dealt with the overall security including up to the in-, on-, and out-body, and utilized two heterogeneous cryptosystems (the symmetric and asymmetric) for diverse environments of wbans. our protocol shows that it satisfies more security requirements than existing security protocols including the overall coverage of wbans.)hybrid security protocol for wireless body area networks','Web protocol security'
'a security layer for the asymptotically secure ping-pong protocol is proposed and analyzed in the paper. the operation of the improvement exploits inevitable errors introduced by the eavesdropping in the control and message modes. its role is similar to the privacy amplification algorithms known from the quantum key distribution schemes. messages are processed in blocks which guarantees that an eavesdropper is faced with a computationally infeasible problem as long as the system parameters are within reasonable limits. the introduced additional information preprocessing does not require quantum memory registers and confidential communication is possible without prior key agreement or some shared secret.improving security of the ping-pong protocol','Web protocol security'
'with the development and wide application of wsn (wireless sensor network), the information security becomes a serious problem, especially when the wsn applied in military field. whereas the general wsn routing protocols focus on the transmission efficiency, reliability and energy-efficiency, not on the security. this paper integrates the international popular aes encryption standard, rsa public-key encryption mechanism and digital signature technology to improve classic aodv routing protocol, and shows an saodv routing protocol which has both energy efficiency and information security. the theoretical analysis and simulation show: when adding security mechanism to aodv in the routing layer, the system owns the defensive ability for the hidden security problem, such as the eavesdropping, coaxing, imitation, replay, denial of service and hello diffusion etc and the normal performance of the network almost conform to the aodv protocol.information security routing protocol in the wsn','Web protocol security'
'in this paper we offer a novel methodology for verifying correctness of (timed) security protocols. the idea consists in computing the time of a correct execution of a session and finding out whether the intruder can change it to shorter or longer by an active attack. moreover, we generalize the correspondence property so that attacks can be also discovered when some time constraints are not satisfied. as case studies we verify generalized authentication of kerberos, tmn, neumann stubblebine protocol, andrew secure protocol, wmf, and nspk.is your security protocol on time?','Web protocol security'
'this paper discusses progress in the verification of security protocols. focusing on a small, classic example, it stresses the use of program-like representations of protocols, and their automatic analysis in symbolic and computational models. models and proofs of protocol security','Web protocol security'
'the dolev-yao model of security protocol analysis may beformalized using a notation based on multi-set rewriting with existential quantification. this presentation describes the multiset rewriting approach to security protocol analysis, algorithmic upper and lower bounds on specific forms of protocol analysis, and some of the ways this model is useful for formalizing sublte properties of specific protocols.multiset rewriting and security protocol analysis','Web protocol security'
'security issues become more and more significant in rfid development. recently, chien proposed an ultralightweight rfid authentication protocol in order to achieve privacy and authenticity with limited computation and transmission resources. however, we find two desynchronization attacks to break the protocol. in order to repair the protocol, two patches that slightly modify the protocol are presented in the paper.on the security of chien\'s ultralightweight rfid authentication protocol','Web protocol security'
'the peer-to-peer (p2p) network model differs from the well established client-server model in that all members of the network are assigned an equal role. p2p networks are recently gaining increasing popularity. providing security in distributed content sharing in p2p networks is an important challenge. this paper identifies security vulnerabilities in the protocols for sharing servents&#253; reputations in the gnutella p2p system, proposed recently. it demonstrates attacks on the protocols that allow an attacker to alter the results of the voting procedure. the paper then presents a protocol that is resilient to the attacks described. in the proposed protocol, enhanced security against various attacks is achieved using smart design and a combination of various techniques such as the use of digital signatures for message integrity and random numbers for message freshness.on the security of polling protocols in peer-to-peer systems','Web protocol security'
'recently qian et al. (2012)  [26] have proposed a new attack for rfid systems, called counting attack, where the attacker just aims to estimate the number of tagged objects instead of steal the tags\' private information. they have stated that most of the existing rfid mutual authentication protocols are vulnerable to this attack. to defend against counting attack, they proposed a novel anti-counting security protocol called acsp. the designers of acsp have claimed that their protocol is resistant against counting attack and also the other known rfid security threats. however in this paper we present the following efficient attacks against this protocol:*two tag impersonation attack: the success probability of each attack is \'\'1\'\' while the complexity is at most three runs of the protocol. *two single tag de-synchronization attacks, the success probability of both attacks is \'\'1\'\' while the complexity is at most two runs of the protocol. *group of tags de-synchronization attack: this attack, which can de-synchronize all tags in the range at once, has a success probability of \'\'1\'\' while its complexity is one run of the protocol. *traceability attack: the adversary\'s advantage in this attack is almost the maximum of possible advantages for an adversary in the same model, i.e., 12. the complexity of this attack is three runs of the protocol.  to counteract such flaws, we improve the acsp protocol by applying some modifications so that it provides the desired security.on the security of rfid anti-counting security protocol (acsp)','Web protocol security'
'security protocols used in today\'s communication are complex and it is very difficult to analyze and optimize them. literature reports some results which optimize security protocols. in the case of devices with limited resources (mobile phones, pda, sensors) the speed and efficiency of the process is crucial for their stable work. security methods used during transporting the data between parties are crucial as for as efficiency is concerned. however, optimization cannot significantly reduce the security of the process. we must remember that in many fields (e.g. e-banking, e-court etc.) security level will always be the main factor. in this paper, we show how to optimize security protocols in terms of the security level. we present the visualization tool for the adaptable security model, which defines the protection level of the transmitted data. these elements help us analyze and optimize a cryptographic protocol. the presented optimization results are based on the tls protocol. we describe this protocol by the adaptable model and we create different versions of the protocol. finally, we discuss differences between them and their impact on the protection level.optimization of tls security protocol using the adaptable security model','Web protocol security'
'pervasive environments are characterized by the presence of wireless devices ranging from sensors to mobile pda-phones to laptops. peer-to-peer (p2p) information sharing in such environments presents a tremendous opportunity for people and devices to exchange information such as music, pictures, documents and stock tickers with peers. information that has a monetary value introduces a whole new set of problems for p2p interaction in pervasive environments. in this work, we present the design of a middleware that attempts to solve these problems using concepts of the contract net protocol, semantic service discovery and secure transaction protocols.p2p m-commerce in pervasive environments','Web protocol security'
'in this paper we explore partial order reduction that make the task of verifying cryptographic protocols more efficient. these reduction techniques have been implemented in our tool brutus. a lthough we have implemented several reduction techniques in our tool brutus, due to space restrictions in this paper we only focus on partial order reductions. partial order reductions have proved very useful in the domain of model checking reactive systems. these reductions are not directly applicable in our context because of additional complications caused by tracking knowledge of various agents. we present partial order reductions in the context of verifying security protocols and prove their correctness. experimental results showing the benefits of this reduction technique are also presented.partial order reductions for security protocol verification','Web protocol security'
'the portal security transaction protocol (pstp) is a new signature technology that adds signature semantics to one-time password technology. pstp was developed to secure transactions in the financial services industry; however, pstp may be applicable to signatures in other spaces. pstp technology provides high signature strength of mechanism without requiring asymmetric key pairs deployed to client machines. pstp provides cryptographic after-the-fact evidence of a transaction event in a secured log.portable security transaction protocol','Web protocol security'
'we consider the problem of deciding the security of cryptographic protocols for a bounded number of sessions, taking into account some algebraic properties of the security primitives, for instance abelian group properties. we propose a general method for deriving decision algorithms, splitting the task into 4 properties of the rewriting system describing the intruder capabilities: locality, conservativity, finite variant property and decidability of one-step deducibility constraints. we illustrate this method on a non trivial example, combining several abelian group properties, exponentiation and a homomorphism, showing a decidability result for this combination. protocol security and algebraic properties','Web protocol security'
'all-or-nothing disclosure of secrets, or andos for brevity, is an interesting cryptographic task, which involves two parties, say alice and bob. alice has a few secrets, which she regards as equally valuable. she would like to sell any of them to bob, with the guarantee that no information about the other secrets will be obtained if he pays for only one secret. moreover, bob can freely choose his secret and wants to ensure that alice can obtain no information about which of her secrets he has picked. in this paper, we present a new quantum andos scheme which achieves the same functionality, but which is of unconditional security and is able to contain arbitrary number of secrets.quantum andos protocol with unconditional security','Web protocol security'
'many results have been published in the literature based on performance measurements obtained from simulations of vehicular networks (vanets). these simulations use as input traces of vehicle movements that have been generated by traffic simulators which are based on traffic theory models. to our knowledge, no one has published any work based on actual large-scale recordings of vehicle movements. we use recordings of actual vehicle movements on various roadways. in order to enable analysis on this scale, we have developed a new vanet simulator, which can handle many more vehicles than ns-2 [1]. to enable us to use our own simulator, we present results of a cross-validation between ns-2 and our simulator, showing that both simulators produce results that are statistically the same. we use our simulator to analyze the proposed authentication mechanism, which relies on ecdsa signatures [2], comparing it to broadcast authentication using tesla [3]. we perform our evaluations using real vehicle mobility, which we believe to be the first simulations using real vehicle mobility. our comparison shows strengths and weaknesses for each of these authentication schemes in terms of the resulting reception rates and latency of broadcast packets.real-world vanet security protocol performance','Web protocol security'
'after the analysis of the working principle of wapi, its security weaknesses are pointed out: it does not securely realize the goal of identity authentication and key agreement. according to the security weaknesses, the key agreement part is improved without changing the framework of wapi. the improved protocol is not only sk-secure but also uc-secure, thus realizing identity authentication and key agreement organically.security analysis and improvement of wapi protocol','Web protocol security'
'modadugu, boneh and kim proposed two rsa key generation protocols (mbk protocols) to generate the rsa keys efficiently on a low-power handheld device with the help of the untrusted servers, and the servers do not get any useful information about the keys they helped generation. the security of mbk protocols is based on the assumption that the two servers are unable to share information with each other. to resists a &#8221;collusion attack&#8221; ,namely the attack which the two servers collude to share information in mbk protocols, chen et al. proposed two improved protocols and claimed that their protocols are secure against such collusion attack. this paper shows that chen et al.&#39;s standard rsa key generation protocol cannot resist collusion attack and then cannot be used in practice.security analysis of a server-aided rsa key generation protocol','Web protocol security'
'in a secure group communication, a group key agreement is to provide a secret key exchange among a group of users. when a new user joins the group, a new group key will be established. in this paper, we analyse horng\'s joint protocol and show that this protocol does not provide backward secrecy. this means that a new joining user is able to discover the previous group key used by the previous group member.security analysis of joint group key agreement protocol','Web protocol security'
'the ultralightweight rfid protocols only involve simple bit-wise operations (like xor, and, or, etc.) on tags. in this paper, we show that the ultralightweight strong authentication and strong integrity (sasi) protocol has two security vulnerabilities, namely denial-of-service (dos) and anonymity tracing based on a compromised tag. the former permanently disables the authentication capability of a rfid tag by destroying synchronization between the tag and the rfid reader. the latter links a compromised tag with past actions performed on this tag.security analysis of the sasi protocol','Web protocol security'
'in this paper, we present security analysis on temporally-ordered routing algorithm (tora) routing protocol. we first identify three attack goals, namely route disruption, route invasion and resource consumption. then, we study on how to achieve these attack goals through misuses of routing messages. the analysis shows that three misuse actions on the routing messages, including drop, modify and forward, and active forge, enable the malicious attacker to conduct real time attacks on tora protocol. we demonstrate the attacks using ns-2 software and then analyze the simulation results. the simulation results verify our analysis and we observe that through certain misuses, an inside attacker can degrade the network performance, disrupting the route creation process and consume scarce network resource.security analysis of tora routing protocol','Web protocol security'
'the authentication process of wireless local area network authentication and privacy infrastructure, namely, wai protocol, is researched and analyzed, and the security analysis is made to its the key-agreement process by using ck model. the analysis shows that: wai can realize security attributes such as mutual key-control, key confirmation and so on, and it can also statisfy the secure goal of wireless local area network. thus, it can be used to enhance the security of wireless local area network instead of wep.security analysis of wapi key negotiation protocol','Web protocol security'
'many applications, such as e-passport, e-health, credit cards, and personal devices that utilize radio frequency identification (rfid) devices for authentication require strict security and privacy. however, rfid tags suffer from some inherent weaknesses due to restricted hardware capabilities and are vulnerable to eavesdropping, interception, or modification. the synchronization and untraceability characteristics are the major determinants of rfid authentication protocols. they are strongly related to privacy of tags and availability, respectively. in this paper, we analyze a new lightweight rfid authentication protocol, song and mitchell, in terms of privacy and security. we prove that not only is the scheme vulnerable to desynchronization attack, but it suffers from traceability and backward traceability as well. finally, our improved scheme is proposed which can prevent aforementioned attacks.security and privacy analysis of song---mitchell rfid authentication protocol','Web protocol security'
'wirelesshart is a secure and reliable communication standard for industrial process automation. the wirelesshart specifications are well organized in all aspects except security: there are no separate specifications of security requirements or features. rather, security mechanisms are described throughout the documentation. this hinders implementation of the standard and development of applications since it requires profound knowledge of all the core specifications on the part of the developer. in this paper we provide a comprehensive overview of wirelesshart security: we analyze the provided security mechanisms against well known threats in the wireless medium, and propose recommendations to mitigate shortcomings. furthermore, we elucidate the specifications of the security manager, its placement in the network, and interaction with the network manager.security considerations for the wireless hart protocol','Web protocol security'
'security for the dod transmission control protocol','Web protocol security'
'cryptographic protocol designers work incrementally. having achieved some goals for confidentiality and authentication in a protocol &#928;1 , they transform it to a richer &#928;2 to achieve new goals. but do the original goals still hold? more precisely, if a goal formula &#915; holds whenever &#928;1 runs against an adversary, does a translation of &#915; hold whenever &#928;2 runs against it? we prove that a transformation preserves goal formulas if a labeled transition system for analyzing &#928;1 simulates a portion of an lts for analyzing &#928;2 , while preserving progress in that portion. thus, we examine the process of analyzing a protocol &#928;. we use ltss that describe &lt;em&gt;our&lt;/em&gt; activity when &lt;em&gt;analyzing&lt;/em&gt; &#928;, not that of the principals &lt;em&gt;executing&lt;/em&gt; &#928;. each analysis step considers--for an observed message reception--what earlier transmissions would explain it. the lts then contains a transition from a fragmentary execution containing the reception to a richer one containing an explaining transmission. the strand space protocol analysis tool cpsa generates some of the ltss used.security goals and protocol transformations','Web protocol security'
'security issues play an important role in modern communication worlds. via distrusted networks, exchanged messages need to be encrypted by a session key for security requirements. session keys are preferred to be generated by communication parties, and how to authenticate the other communication party is an important problem needed to be solved. in three-party encrypted key exchange (3peke) protocols, a special type of indirect-authenticated protocols, a trusted third party is involved to have two communication parties be able to authenticate each other and negotiate one session key. in 2008, yoon and yoo proposed a three-party encrypted key exchange protocol and claimed their protocol provides a more secure way for exchanging messages. unfortunately, we find that their protocol suffers from undetectable one-line password guessing attacks. in this manuscript, we first review the important related works and show how to mount attacks on yoon and yoo&#8217;s 3peke protocolsecurity of indirect-authenticated key exchange protocol','Web protocol security'
'with the booming of smartphone and wireless networks, contactless payment systems have been gradually accepted by a range of sectors and industries. near field communication (nfc), as a de facto standard for radio communication, dominates this market and has accumulated many applications, such as use as a digital wallet and ticketing system, due to its convenience and versatility. the prevalent use and large cash flow of nfc have attracted the attention of attackers and criminals. in this paper, we present an overview of the security issues surrounding the nfc protocol and its applications. we investigate the existing and potential vulnerabilities of nfc, discuss effective countermeasures for mitigating these vulnerabilities, and further present design considerations for new nfc applications.security of the near field communication protocol','Web protocol security'
'secure protocol is the foundation of the manet. it is very critical to ensure the correctness of the security properties in the designed routing protocols. however, nowadays, new designed protocols are predominantly validated by an interpretation of simulation methods, lacking rigid formal analysis to ensure security properties. firstly, this paper presents a formal analysis procedure of srp routing protocols in the context of attack model which is suitable to the manet and gives the denotational semantics and abstract interpretation model. secondly, we propose an attack model which fits for the open traits of manet. thirdly, we give a detailed analysis procedure of srp protocol in our attach model. finally, we test and demonstrate our idea by the sprite tool based on spi calculus.security properties analysis of routing protocol for manet','Web protocol security'
'ta4sp is a state-of-art tool of avispa that can automatically verify security protocol with unbounded number of parallel sessions. but it still has some limitations and can&#8217;t verify hierarchy of authentication automatically. in this paper, we use an approximation-based model to define security protocol and design an algorithm close to the real implementation to calculate the fix-point tree automata based on the tool ta4sp. a method is proposed for analyzing hierarchy of authentication properties as extension of ta4sp. we illustrate the effectiveness of this model with the example of nspk protocol.security protocol analysis based on rewriting approximation','Web protocol security'
'this paper proposes the improved authentication tests in order to find the potential attacks on security protocols. it is based on the authentication tests theory and enhances the original methods by introducing the notion of message type. formalized definition of replay attacks have been integrated into the original theoretical models for further verification of security protocols. the thoroughly proof of initial and subsequent authentications in neuman-stubblebine protocol shows that the improved authentication tests can find flaws of the protocol more efficiently than the original ones.security protocol analysis with improved authentication tests','Web protocol security'
'security protocol deployment risk (transcript of discussion)','Web protocol security'
'security protocol participants are software and/or hardware agents that are -- as with any system -- potentially vulnerable to failure. protocol analysis should extend not just to an analysis of the protocol specification, but also to its implementation and configuration in its target environment. however, an in-depth formal analysis that considers the behaviour and interaction of all components in their environment is not feasible in practice. this paper considers the analysis of protocol deployment rather than implementation. instead of concentrating on detailed semantics and formal verification of the protocol and implementation, we are concerned more with with the ability to trace, at a practical level of abstraction, how the protocol deployment, that is, the configuration of the protocol components, relate to each other and the overall protocol goals. we believe that a complete security verification of a system is not currently achievable in practice and seek some degree of useful feedback from an analysis that a particular protocol deployment is reasonable.security protocol deployment risk','Web protocol security'
'we describe a protocol design process, and illustrate its use by creating atspec, anauthentication test-based secure protocol for electronic commerce transactions. the design process is organized around the authentication tests, a method for protocol verification based on the strand space theory. the authentication tests dictate how randomly generated values such as nonces may be combined with encryption to achieve authentication and freshness.atspec offers functionality and security guarantees akin to the purchase request, payment authorization, and payment capture phases of set, the secure electronic transaction standard created by the major credit card firms.security protocol design via authentication tests','Web protocol security'
'in this paper we present an attack injectionapproach for security protocol testing aiming atvulnerability detection. we use attack tree model todescribe known attacks and derive injection testscenarios to test the security properties of the protocolunder evaluation. the test scenarios are converted to aspecific fault injector script after performing sometransformations. the attacker is emulated using a faultinjector. this model based approach facilitates thereusability and maintainability of the generatedinjection attacks as well as the generation of faultinjectors scripts. the approach is applied to anexisting mobile security protocol. we performedexperiments with truncation and dos attacks; resultsshow good precision and efficiency in the injectionmethod.security protocol testing using attack trees','Web protocol security'
'security protocol verification has been a very active research area since the 1990s. this paper surveys various approaches in this area, considering the verification in the symbolic model, as well as the more recent approaches that rely on the computational model or that verify protocol implementations rather than specifications. additionally, we briefly describe our symbolic security protocol verifier proverif and situate it among these approaches.security protocol verification','Web protocol security'
'we present sadsr (security-aware adaptive dsr), a securerouting protocol for mobile ad hoc networks. sadsrauthenticates the routing protocol messages using digitalsignatures based on asymmetric cryptography. the basicidea behind sadsr is to have multiple routes to each destinationand store a local trust value for each node in thenetwork. a trust value is assigned to each path based ontrust values of the nodes which occur on that path. thepaths with higher trust values are preferred for routing. weimplemented our approach in ns2 simulator and comparedthe performance of sadsr and dsr. our results show thatin the presence of malicious nodes sadsr outperforms dsrin packet delivery ratio with an acceptable network load.security-aware adaptive dynamic source routing protocol','Web protocol security'
'query-response based protocols between a client and a server such as ssl, tls, ssh are asymmetric in the sense that the querying client and the responding server play different roles, and for which there is a need for two-way linkability between queries and responses within the protocol. we are motivated by the observation that though results exist in other related contexts, no provably secure scheme has been applied to the setting of client-server protocols, which differ from conventional communications on the above points. we show how to secure the communication of queries and responses in these client-server protocols in a provably secure setting. in doing so, we propose a new primitive: a query-response encapsulation scheme; we give an instantiation, and we demonstrate how this primitive can be used for our purpose. in our proof of secure encapsulation, we show how to preserve the notion of \"local-security\".security-preserving asymmetric protocol encapsulation','Web protocol security'
'in recent years, many mobile payment (mp) schemes have been proposed and used in practise. however, a prerequisite for extended acceptance and adoption of mp technologies is to deploy an effective mp system. so far, there is no such a standardised and scalable mp platform. most current mp schemes are circumscribed by its mobile network infrastructures. fortunately, the fast advancement of 3g technology equips next generation mobile phone network more benefits. following this direction, we propose simpa --- a sip-based mobile payment architecture for next generation mobile network, which not only supports p2p payment communications between customers and merchants using session initiation protocol (sip), but also supports several traditional internet security protocols, to enhance privacy, confidentiality and integrity during the transaction. this paper depicts detailed protocol and system architecture of simpa. some application examples from customers&#8217; view are shown to demonstrate its function and feature.simpa','Web protocol security'
'we model any network configuration arising from the execution of a security protocol as a soft constraint satisfaction problem (scsp). we formalise the protocol goal of confidentiality as a property of the solution for an scsp, hence confidentiality always holds with a certain security level. the policy scsp models the network configuration where all admissible protocol sessions have terminated successfully, and an imputable scsp models a given network configuration. comparing the solutions of these two problems elicits whether the given configuration hides a confidentiality attack. we can also compare attacks and decide which is the most significant. the approach is demonstrated on the asymmetric needham-schroeder protocol.soft constraints for security protocol analysis','Web protocol security'
'we propose a class of protocol transformations, which can be used to (1) develop (families of) security protocols by refinement and (2) abstract existing protocols to increase the efficiency of verification tools. we prove the soundness of these transformations with respect to an expressive security property specification language covering secrecy and authentication properties. our work clarifies and significantly extends the scope of earlier work in this area. we illustrate the usefulness of our approach on a family of key establishment protocols.sound security protocol transformations','Web protocol security'
'we introduce aviss, a tool for security protocol analysis that supports the integration of back-ends implementing different search techniques, allowing for their systematic and quantitative comparison and paving the way to their effective interaction. as a significant example, we have implemented three back-ends, and used the aviss tool to analyze and find flaws in 36 protocols, including 31 problems in the clark-jacob\'s protocol library and a previously unreported flaw in the denning-sacco protocol.the aviss security protocol analysis tool','Web protocol security'
'the ansi t10 object-based storage devices (osd) standard is a new standard. it evolves the storage interface from fixed size blocks to variable size objects and includes an integrated security protocol that protects storage. this paper presents the requirements, the design tradeoffs, and the final security protocol as defined in the standard. the resulting protocol is based on a secure capability-based model, enabling fine-grained access control that protects both the entire storage device and individual objects from unauthorized access. the protocol defines three methods of security based on the applications\' requirements. furthermore, the protocol\'s key management algorithm allows keys to be changed quickly, without disrupting normal operations. finally, the protocol is currently being enhanced for version 2.0 of the ansi t10 osd standard; future extensions will include data-encryption and access-control on sections of storage objects.the osd security protocol','Web protocol security'
'research e-commerce security protocols, describes the development of e-business status, analysis of e-commerce security threats and security requirements, given the status of e-commerce security protocol to study the security of e-commerce in the secure sockets layer protocol used mainly ssl protocols and secure electronic transaction protocol.the study of e-commerce security protocol','Web protocol security'
'recent years have seen the emergence of a new programming paradigm for web applications that emphasizes the reuse of external content, the mashup. although the mashup paradigm enables the creation of innovative web applications with emergent features, its openness introduces trust problems. these trust issues are particularly prominent in javascript code mashup -- a type of mashup that integrated external javascript libraries to achieve function and software reuse. with javascript code mashup, external libraries are usually given full privileges to manipulate data of the mashup application and executing arbitrary code. this imposes considerable risk on the mashup developers and the end users. one major causes for these trust problems is that the mashup developers tend to focus on the functional aspects of the application and implicitly trust the external code libraries to satisfy security, privacy and other non-functional requirements. in this paper, we present tomato, a development tool that combines a novel trust policy language and a static code analysis engine to examine whether the external libraries satisfy the non-functional requirements. tomato gives the mashup developers three essential capabilities for building trustworthy javascript code mashup: (1) to specify trust policy, (2) to assess policy adherence, and (3) to handle policy violation. the contributions of the paper are: (1) a description of javascript code mashup and its trust issues, and (2) a development tool (tomato) for building trustworthy javascript code mashup.tomato','Web protocol security'
'in 2004, the mis and misauth protocols &#40;shortly misp&#41; were proposed for secure and fast connection in wireless lan &#40;wlan&#41;. since then, several studies were conducted to evaluate misp. however, because of the lack of formal security analysis, the security measures provided by them cannot be trusted to be precise and reasonable. motivated by this, we formally analyse misp based on both ban&#45;logic and the automated validation of internet security protocols and applications &#40;avispas&#41; tool. this analysis clearly shows that misp suffers from weak session key &#40;sk&#41; while being vulnerable to denial of service &#40;dos&#41; attack. we discuss these weaknesses and related attacks in detail, and also provide considerable comments for implementation of misp.towards formal analysis of wireless lan security with mis protocol','Web protocol security'
'we describe a formal approach to the analysis of security aspects of an identity federation protocol for web services in convergent networks. this network protocol was proposed by telecom italia as a solution to allow end users to access services on the web through different access networks without explicitly providing any credentials, while the service providers can trust the user\'s identity information provided by the access networks and access some user data. as a first step towards a full-blown formal security analysis of the protocol, we specify three user scenarios in the process algebra crypto-ccs and verify the vulnerability of one of these specifications w.r.t. a man-in-the-middle attack with the model checker pamochsa.towards security analyses of an identity federation protocol for web services in convergent networks','Web protocol security'
'classic message exchange based on asymmetric cryptography can protect message content from tampering and authenticate terminal by signing with the private key. keys are communication endpoint s&#8217; improperly managed keys can result in loss of security during content transmission. additionally, improperly configured endpoints may also threaten content security of storage. certainly the security can be enhanced by providing both security key management and terminal&#8217;s security authentication mechanisms. those mechanisms can be obtained from trusted pc architecture specified by trusted computing group. so based on the new development of those mechanisms, such as property-based sealing, we adopted some cryptography schemes and initiated a security provably trusted transmission protocol, which not only enhances the content security during transmission, but also ensures content security of storage.trusted transmission protocol for content security','Web protocol security'
'border gateway protocol (bgp) acts as a vital part of the global infrastructure. attacks against bgp are increasing in number and severity. unfortunately, most security mechanisms based on public key cryptography suffer from performance, trust model and other issues. this paper proposes a solution that takes advantages of the power-law and rich-club features of the as-level topology, and proposes the notion of as alliance and a new trust model &#8212; translator trust model (ttm). ttm avoids the global distribution of certificates by trust translating between different trust domains. it achieves that with much less memory overhead than traditional solutions, and a shorter validation chain. we develop a novel se-bgp (security enhanced bgp) mechanism based on ttm. it introduces new path attributes to carry origin certificates and path signatures, and the algorithms to process origin authentication and path authentication. our analyses indicate that se-bgp is a viable solution.ttm based security enhancement for inter-domain routing protocol','Web protocol security'
'we propose a novel method to construct user-space internet protocol stacks whose security properties can be formally explored and verified. the proposed method allows construction of protocol stacks using a c++ subset. we define a formal state-transformer representation of protocol stacks in which the protocol stack is specified in terms of three primary operations, which are constructed from sub-operations, in a compositional manner. we also define a kripke model that captures the sequencing and attributes of stack operations. we propose a novel approach, called split verification, which combines theorem-proving and model-checking to establish properties for a protocol stack specification. in split verification, properties to be established for the stack are expressed as a combination of properties for primitive operations to be established via theorem-proving as well as temporal properties on operation sequencing, called promotion conditions, to be established via model-checking on the stack operations model. we use abstract z specifications to represent operation properties and computational tree logic (ctl) formulae to represent promotion conditions. operation properties are established by checking whether the operation(s) under consideration are correct refinements of the abstract z specification(s). our conclusion is that split verification: (a) avoids scalability issues caused by state-space explosion in model-checking and long unwieldy proofs in theorem-proving, and, (b) lowers cost of proof maintenance for localized changes in the stack.verifying security properties of internet protocol stacks','Web protocol security'
'pairing devices over insecure wireless channels is a difficult task, especially when there is no trusted third party or when either the delivering network or the third party itself can&#8217;t be trusted. in this work we present a light-weight yet sophisticated protocol for establishing secure communications channels betweendevices. the protocol uses any asymmetric cryptosystem&#8217;s keys and a novel exchange mechanism to establish a symmetric key that encrypts any data exchanged over the channel. the protocol requires user intervention for its operation; challenge signature/tokens are sent at first that are verified at the end by the user; if the exchanged tokens are verified then the channel is secured by the symmetric key exchanged inline. the only requirement posed on the devices is that they must be equipped with a display. the protocol is ideal for secure pairing among any display-equipped user device with an adequate processor to play mp3 music.vidpsec visual device pairing security protocol','Web protocol security'
'this paper argues that the existing model-driven architecture paradigm does not adequately cover the visual modeling of security protocols: sequences of interactions between principals. a security protocol modeling formalism should be not only well-defined but also support event-based, compositional, comprehensive, laconic, lucid, sound, and complete modeling. candidate visual approaches from both the omg\'s mda and other more well-defined formalisms fail to satisfy one or more of these criteria. by means of two example security protocol models, we present the gspml visual formalism as a solution.visual security protocol modeling','Web protocol security'
'security is a major issue in voip communications over the internet, especially in mobile environments. voice interactive personalized security (vipsec) constitutes a method for leveraging the security for internet communications, with biometric based authentication, exploiting the nature of the application. during the establishment of the communication session, the end-peers exchange a challenge/signature token, the integrity of which is confirmed vocally when the voice communication initiates. in the present work we define the protocol message set in detail and we present an extensive security analysis over the protocol operation. the method is appropriate for ensuring the security of a voip or collaboration application, guaranteeing the integrity of the session key exchanged in the beginning of the conversation. it requires minimal resources from the user handsets and no additional support from the network, so it is inherently scalable and readily deployable as it only needs an appropriately enhanced secure handset.voice interactive personalized security protocol','Web protocol security'
